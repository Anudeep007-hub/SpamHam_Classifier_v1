{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweets</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank u!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Going for dinner.msg you after.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi i won't b ard 4 christmas. But do enjoy n m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1. Tension face 2. Smiling face 3. Waste face ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1 I don't have her number and 2 its gonna be a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                             tweets Unnamed: 2  \\\n",
       "0      0                                           Thank u!        NaN   \n",
       "1      0                    Going for dinner.msg you after.        NaN   \n",
       "2      0  Hi i won't b ard 4 christmas. But do enjoy n m...        NaN   \n",
       "3      0  1. Tension face 2. Smiling face 3. Waste face ...        NaN   \n",
       "4      0  1 I don't have her number and 2 its gonna be a...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = os.path.join(\"data\")\n",
    "raw_data_file_path = os.path.join(folder_path, \"raw_data.csv\")\n",
    "df = pd.read_csv(raw_data_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, :2] \n",
    "clened_data_file_path = os.path.join(folder_path, \"cleaned_data.csv\")\n",
    "df.to_csv(clened_data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   class   5572 non-null   int64 \n",
      " 1   tweets  5572 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank u!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Going for dinner.msg you after.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi i won't b ard 4 christmas. But do enjoy n m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1. Tension face 2. Smiling face 3. Waste face ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1 I don't have her number and 2 its gonna be a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                             tweets\n",
       "0      0                                           Thank u!\n",
       "1      0                    Going for dinner.msg you after.\n",
       "2      0  Hi i won't b ard 4 christmas. But do enjoy n m...\n",
       "3      0  1. Tension face 2. Smiling face 3. Waste face ...\n",
       "4      0  1 I don't have her number and 2 its gonna be a..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CleanText import SentTokenize,Token_to_Sent \n",
    "from  RemoveStopWords import stopWords_removal\n",
    "from PorterStemmer import porter_stemmer \n",
    "\n",
    "def pipeline(input_phrase):\n",
    "    obj = SentTokenize(phrase=input_phrase) \n",
    "    obj = obj.get_tokenized_sentence() \n",
    "    print(f\"Tokenized sentence: {obj}\")\n",
    "\n",
    "    obj = stopWords_removal(tokenized_sentence=obj) \n",
    "    obj = obj.get_refined_tokeinzed_sentence()\n",
    "    print(f\"After stop words removal: {obj}\")\n",
    "\n",
    "    obj = porter_stemmer(tokenised_phrase=obj)\n",
    "    obj = obj.get_stemmed_tokens() \n",
    "    \n",
    "    print(f\"After stemming with porters algorithm: {obj}\") \n",
    "    return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['thank', 'u']\n",
      "After stop words removal: ['thank', 'u']\n",
      "After stemming with porters algorithm: ['thank']\n",
      "Tokenized sentence: ['going', 'for', 'dinner', 'msg', 'you', 'after']\n",
      "After stop words removal: ['going', 'dinner', 'msg']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'dinner', 'msg']\n",
      "Tokenized sentence: ['hi', 'i', 'won', 't', 'b', 'ard', 'christmas', 'but', 'do', 'enjoy', 'n', 'merry', 'x', 'mas']\n",
      "After stop words removal: ['hi', 'b', 'ard', 'christmas', 'enjoy', 'n', 'merry', 'x', 'mas']\n",
      "After stemming with porters algorithm: ['ard', 'christma', 'enjoi', 'merri', 'ma']\n",
      "Tokenized sentence: ['tension', 'face', 'smiling', 'face', 'waste', 'face', 'innocent', 'face', 'terror', 'face', 'cruel', 'face', 'romantic', 'face', 'lovable', 'face', 'decent', 'face', 'lt', 'gt', 'joker', 'face']\n",
      "After stop words removal: ['tension', 'face', 'smiling', 'face', 'waste', 'face', 'innocent', 'face', 'terror', 'face', 'cruel', 'face', 'romantic', 'face', 'lovable', 'face', 'decent', 'face', 'lt', 'gt', 'joker', 'face']\n",
      "smil\n",
      "After stemming with porters algorithm: ['tension', 'face', 'smile', 'face', 'wast', 'face', 'innoc', 'face', 'terror', 'face', 'cruel', 'face', 'romant', 'face', 'lovab', 'face', 'decent', 'face', 'joker', 'face']\n",
      "Tokenized sentence: ['i', 'don', 't', 'have', 'her', 'number', 'and', 'its', 'gonna', 'be', 'a', 'massive', 'pain', 'in', 'the', 'ass', 'and', 'i', 'd', 'rather', 'not', 'get', 'involved', 'if', 'that', 's', 'possible']\n",
      "After stop words removal: ['number', 'gonna', 'massive', 'pain', 'ass', 'rather', 'get', 'involved', 'possible']\n",
      "After stemming with porters algorithm: ['number', 'gonna', 'massiv', 'pain', 'ass', 'rather', 'get', 'invol', 'possib']\n",
      "Tokenized sentence: ['yes', 'i', 'will', 'be', 'there', 'glad', 'you', 'made', 'it']\n",
      "After stop words removal: ['yes', 'glad', 'made']\n",
      "After stemming with porters algorithm: ['ye', 'glad', 'made']\n",
      "Tokenized sentence: ['desires', 'u', 'going', 'to', 'doctor', 'liver', 'and', 'get', 'a', 'bit', 'stylish', 'get', 'ur', 'hair', 'managed', 'thats', 'it']\n",
      "After stop words removal: ['desires', 'u', 'going', 'doctor', 'liver', 'get', 'bit', 'stylish', 'get', 'ur', 'hair', 'managed', 'thats']\n",
      "go\n",
      "After stemming with porters algorithm: ['desir', 'go', 'doctor', 'liver', 'get', 'bit', 'stylish', 'get', 'hair', 'manag', 'that']\n",
      "Tokenized sentence: ['oops', 'my', 'phone', 'died', 'and', 'i', 'didn', 't', 'even', 'know', 'yeah', 'i', 'like', 'it', 'better']\n",
      "After stop words removal: ['oops', 'phone', 'died', 'even', 'know', 'yeah', 'like', 'better']\n",
      "After stemming with porters algorithm: ['oop', 'phone', 'di', 'even', 'know', 'yeah', 'like', 'better']\n",
      "Tokenized sentence: ['i', 'think', 'that', 'tantrum', 's', 'finished', 'so', 'yeah', 'i', 'll', 'be', 'by', 'at', 'some', 'point']\n",
      "After stop words removal: ['think', 'tantrum', 'finished', 'yeah', 'point']\n",
      "After stemming with porters algorithm: ['think', 'tantrum', 'finis', 'yeah', 'point']\n",
      "Tokenized sentence: ['ultimately', 'tor', 'motive', 'tui', 'achieve', 'korli']\n",
      "After stop words removal: ['ultimately', 'tor', 'motive', 'tui', 'achieve', 'korli']\n",
      "After stemming with porters algorithm: ['ultim', 'tor', 'motiv', 'tui', 'achiev', 'korli']\n",
      "Tokenized sentence: ['if', 'i', 'was', 'i', 'wasn', 't', 'paying', 'attention']\n",
      "After stop words removal: ['paying', 'attention']\n",
      "pay\n",
      "After stemming with porters algorithm: ['pai', 'attent']\n",
      "Tokenized sentence: ['i', 'm', 'now', 'but', 'have', 'to', 'wait', 'till', 'for', 'the', 'bus', 'to', 'pick', 'me']\n",
      "After stop words removal: ['wait', 'till', 'bus', 'pick']\n",
      "After stemming with porters algorithm: ['wait', 'till', 'bu', 'pick']\n",
      "Tokenized sentence: ['oh', 'yes', 'why', 'is', 'it', 'like', 'torture', 'watching', 'england']\n",
      "After stop words removal: ['oh', 'yes', 'like', 'torture', 'watching', 'england']\n",
      "watch\n",
      "After stemming with porters algorithm: ['ye', 'like', 'tortur', 'watc', 'england']\n",
      "Tokenized sentence: ['a', 'gram', 'usually', 'runs', 'like', 'lt', 'gt', 'a', 'half', 'eighth', 'is', 'smarter', 'though', 'and', 'gets', 'you', 'almost', 'a', 'whole', 'second', 'gram', 'for', 'lt', 'gt']\n",
      "After stop words removal: ['gram', 'usually', 'runs', 'like', 'lt', 'gt', 'half', 'eighth', 'smarter', 'though', 'gets', 'almost', 'whole', 'second', 'gram', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['gram', 'usual', 'run', 'like', 'half', 'eighth', 'smarter', 'though', 'get', 'almost', 'whole', 'second', 'gram']\n",
      "Tokenized sentence: ['i', 'hope', 'you', 'arnt', 'pissed', 'off', 'but', 'id', 'would', 'really', 'like', 'to', 'see', 'you', 'tomorrow', 'love', 'me', 'xxxxxxxxxxxxxx']\n",
      "After stop words removal: ['hope', 'arnt', 'pissed', 'id', 'would', 'really', 'like', 'see', 'tomorrow', 'love', 'xxxxxxxxxxxxxx']\n",
      "After stemming with porters algorithm: ['hope', 'arnt', 'piss', 'would', 'realli', 'like', 'see', 'tomorrow', 'love', 'xxxxxxxxxxxxxx']\n",
      "Tokenized sentence: ['ho', 'ho', 'big', 'belly', 'laugh', 'see', 'ya', 'tomo']\n",
      "After stop words removal: ['ho', 'ho', 'big', 'belly', 'laugh', 'see', 'ya', 'tomo']\n",
      "After stemming with porters algorithm: ['big', 'belli', 'laugh', 'see', 'tomo']\n",
      "Tokenized sentence: ['i', 'm', 'there', 'and', 'i', 'can', 'see', 'you', 'but', 'you', 'can', 't', 'see', 'me', 'maybe', 'you', 'should', 'reboot', 'ym', 'i', 'seen', 'the', 'buzz']\n",
      "After stop words removal: ['see', 'see', 'maybe', 'reboot', 'ym', 'seen', 'buzz']\n",
      "After stemming with porters algorithm: ['see', 'see', 'mayb', 'reboot', 'seen', 'buzz']\n",
      "Tokenized sentence: ['how', 'is', 'my', 'boy', 'no', 'sweet', 'words', 'left', 'for', 'me', 'this', 'morning', 'sighs', 'how', 'goes', 'you', 'day', 'my', 'love', 'did', 'you', 'start', 'your', 'studying']\n",
      "After stop words removal: ['boy', 'sweet', 'words', 'left', 'morning', 'sighs', 'goes', 'day', 'love', 'start', 'studying']\n",
      "morn\n",
      "study\n",
      "After stemming with porters algorithm: ['boi', 'sweet', 'word', 'left', 'mor', 'sigh', 'goe', 'dai', 'love', 'start', 'stud']\n",
      "Tokenized sentence: ['smile', 'in', 'pleasure', 'smile', 'in', 'pain', 'smile', 'when', 'trouble', 'pours', 'like', 'rain', 'smile', 'when', 'sum', 'hurts', 'u', 'smile', 'becoz', 'someone', 'still', 'loves', 'to', 'see', 'u', 'smiling']\n",
      "After stop words removal: ['smile', 'pleasure', 'smile', 'pain', 'smile', 'trouble', 'pours', 'like', 'rain', 'smile', 'sum', 'hurts', 'u', 'smile', 'becoz', 'someone', 'still', 'loves', 'see', 'u', 'smiling']\n",
      "smil\n",
      "After stemming with porters algorithm: ['smile', 'pleasur', 'smile', 'pain', 'smile', 'troubl', 'pour', 'like', 'rain', 'smile', 'sum', 'hurt', 'smile', 'becoz', 'someon', 'still', 'love', 'see', 'smile']\n",
      "Tokenized sentence: ['fine', 'am', 'simply', 'sitting']\n",
      "After stop words removal: ['fine', 'simply', 'sitting']\n",
      "sitt\n",
      "After stemming with porters algorithm: ['fine', 'simpli', 'sit']\n",
      "Tokenized sentence: ['nationwide', 'auto', 'centre', 'or', 'something', 'like', 'that', 'on', 'newport', 'road', 'i', 'liked', 'them', 'there']\n",
      "After stop words removal: ['nationwide', 'auto', 'centre', 'something', 'like', 'newport', 'road', 'liked']\n",
      "someth\n",
      "After stemming with porters algorithm: ['nationwid', 'auto', 'centr', 'somet', 'like', 'newport', 'road', 'like']\n",
      "Tokenized sentence: ['super', 'msg', 'da', 'nalla', 'timing']\n",
      "After stop words removal: ['super', 'msg', 'da', 'nalla', 'timing']\n",
      "tim\n",
      "After stemming with porters algorithm: ['super', 'msg', 'nalla', 'time']\n",
      "Tokenized sentence: ['if', 'you', 'text', 'on', 'your', 'way', 'to', 'cup', 'stop', 'that', 'should', 'work', 'and', 'that', 'should', 'be', 'bus']\n",
      "After stop words removal: ['text', 'way', 'cup', 'stop', 'work', 'bus']\n",
      "After stemming with porters algorithm: ['text', 'wai', 'cup', 'stop', 'work', 'bu']\n",
      "Tokenized sentence: ['hi', 'technical', 'support', 'providing', 'assistance', 'to', 'us', 'customer', 'through', 'call', 'and', 'email']\n",
      "After stop words removal: ['hi', 'technical', 'support', 'providing', 'assistance', 'us', 'customer', 'call', 'email']\n",
      "provid\n",
      "After stemming with porters algorithm: ['technic', 'support', 'provid', 'assist', 'custom', 'call', 'email']\n",
      "Tokenized sentence: ['when', 'you', 'guys', 'planning', 'on', 'coming', 'over']\n",
      "After stop words removal: ['guys', 'planning', 'coming']\n",
      "plann\n",
      "com\n",
      "After stemming with porters algorithm: ['gui', 'plan', 'come']\n",
      "Tokenized sentence: ['may', 'b', 'approve', 'panalam', 'but', 'it', 'should', 'have', 'more', 'posts']\n",
      "After stop words removal: ['may', 'b', 'approve', 'panalam', 'posts']\n",
      "After stemming with porters algorithm: ['mai', 'approv', 'panalam', 'post']\n",
      "Tokenized sentence: ['just', 'now', 'saw', 'your', 'message', 'it', 'k', 'da']\n",
      "After stop words removal: ['saw', 'message', 'k', 'da']\n",
      "After stemming with porters algorithm: ['saw', 'messag']\n",
      "Tokenized sentence: ['i', 'm', 'not', 'she', 'lip', 'synced', 'with', 'shangela']\n",
      "After stop words removal: ['lip', 'synced', 'shangela']\n",
      "After stemming with porters algorithm: ['lip', 'synced', 'shangela']\n",
      "Tokenized sentence: ['you', 'can', 'never', 'do', 'nothing']\n",
      "After stop words removal: ['never', 'nothing']\n",
      "noth\n",
      "After stemming with porters algorithm: ['never', 'not']\n",
      "Tokenized sentence: ['k', 'k', 'congratulation']\n",
      "After stop words removal: ['k', 'k', 'congratulation']\n",
      "After stemming with porters algorithm: ['congratul']\n",
      "Tokenized sentence: ['are', 'you', 'planning', 'to', 'come', 'chennai']\n",
      "After stop words removal: ['planning', 'come', 'chennai']\n",
      "plann\n",
      "After stemming with porters algorithm: ['plan', 'come', 'chennai']\n",
      "Tokenized sentence: ['yeah', 'we', 'wouldn', 't', 'leave', 'for', 'an', 'hour', 'at', 'least', 'how', 's', 'sound']\n",
      "After stop words removal: ['yeah', 'leave', 'hour', 'least', 'sound']\n",
      "After stemming with porters algorithm: ['yeah', 'leav', 'hour', 'least', 'sound']\n",
      "Tokenized sentence: ['can', 'not', 'use', 'foreign', 'stamps', 'in', 'this', 'country']\n",
      "After stop words removal: ['use', 'foreign', 'stamps', 'country']\n",
      "After stemming with porters algorithm: ['us', 'foreign', 'stamp', 'countri']\n",
      "Tokenized sentence: ['both', 'i', 'shoot', 'big', 'loads', 'so', 'get', 'ready']\n",
      "After stop words removal: ['shoot', 'big', 'loads', 'get', 'ready']\n",
      "After stemming with porters algorithm: ['shoot', 'big', 'load', 'get', 'readi']\n",
      "Tokenized sentence: ['from', 'tomorrow', 'onwards', 'eve', 'to', 'work']\n",
      "After stop words removal: ['tomorrow', 'onwards', 'eve', 'work']\n",
      "After stemming with porters algorithm: ['tomorrow', 'onward', 'ev', 'work']\n",
      "Tokenized sentence: ['every', 'day', 'i', 'use', 'to', 'sleep', 'after', 'lt', 'gt', 'so', 'only']\n",
      "After stop words removal: ['every', 'day', 'use', 'sleep', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['everi', 'dai', 'us', 'sleep']\n",
      "Tokenized sentence: ['ill', 'be', 'at', 'yours', 'in', 'about', 'mins', 'but', 'look', 'out', 'for', 'me']\n",
      "After stop words removal: ['ill', 'mins', 'look']\n",
      "After stemming with porters algorithm: ['ill', 'min', 'look']\n",
      "Tokenized sentence: ['the', 'oz', 'guy', 'is', 'being', 'kinda', 'flaky', 'but', 'one', 'friend', 'is', 'interested', 'in', 'picking', 'up', 'lt', 'gt', 'worth', 'tonight', 'if', 'possible']\n",
      "After stop words removal: ['oz', 'guy', 'kinda', 'flaky', 'one', 'friend', 'interested', 'picking', 'lt', 'gt', 'worth', 'tonight', 'possible']\n",
      "pick\n",
      "After stemming with porters algorithm: ['gui', 'kinda', 'flaki', 'on', 'friend', 'interes', 'pic', 'worth', 'tonight', 'possib']\n",
      "Tokenized sentence: ['that', 's', 'y', 'i', 'said', 'it', 's', 'bad', 'dat', 'all', 'e', 'gals', 'know', 'u', 'wat', 'u', 'doing', 'now']\n",
      "After stop words removal: ['said', 'bad', 'dat', 'e', 'gals', 'know', 'u', 'wat', 'u']\n",
      "After stemming with porters algorithm: ['said', 'bad', 'dat', 'gal', 'know', 'wat']\n",
      "Tokenized sentence: ['hmm', 'bits', 'and', 'pieces', 'lol', 'sighs']\n",
      "After stop words removal: ['hmm', 'bits', 'pieces', 'lol', 'sighs']\n",
      "After stemming with porters algorithm: ['hmm', 'bit', 'piec', 'lol', 'sigh']\n",
      "Tokenized sentence: ['oh', 'i', 'asked', 'for', 'fun', 'haha', 'take', 'care']\n",
      "After stop words removal: ['oh', 'asked', 'fun', 'haha', 'take', 'care']\n",
      "After stemming with porters algorithm: ['as', 'fun', 'haha', 'take', 'care']\n",
      "Tokenized sentence: ['u', 'can', 'win', 'of', 'music', 'gift', 'vouchers', 'every', 'week', 'starting', 'now', 'txt', 'the', 'word', 'draw', 'to', 'tscs', 'www', 'ldew', 'com', 'skillgame', 'winaweek', 'age', 'ppermesssubscription']\n",
      "After stop words removal: ['u', 'win', 'music', 'gift', 'vouchers', 'every', 'week', 'starting', 'txt', 'word', 'draw', 'tscs', 'www', 'ldew', 'com', 'skillgame', 'winaweek', 'age', 'ppermesssubscription']\n",
      "start\n",
      "After stemming with porters algorithm: ['win', 'music', 'gift', 'voucher', 'everi', 'week', 'star', 'txt', 'word', 'draw', 'tsc', 'www', 'ldew', 'com', 'skillgam', 'winaweek', 'ag', 'ppermesssubscript']\n",
      "Tokenized sentence: ['you', 'getting', 'back', 'any', 'time', 'soon']\n",
      "After stop words removal: ['getting', 'back', 'time', 'soon']\n",
      "gett\n",
      "After stemming with porters algorithm: ['get', 'back', 'time', 'soon']\n",
      "Tokenized sentence: ['as', 'per', 'your', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', 'to', 'copy', 'your', 'friends', 'callertune']\n",
      "After stop words removal: ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'callers', 'press', 'copy', 'friends', 'callertune']\n",
      "After stemming with porters algorithm: ['per', 'request', 'mell', 'mell', 'oru', 'minnaminungint', 'nurungu', 'vettam', 'set', 'callertun', 'caller', 'press', 'copi', 'friend', 'callertun']\n",
      "Tokenized sentence: ['total', 'video', 'converter', 'free', 'download', 'type', 'this', 'in', 'google', 'search']\n",
      "After stop words removal: ['total', 'video', 'converter', 'free', 'download', 'type', 'google', 'search']\n",
      "After stemming with porters algorithm: ['total', 'video', 'convert', 'free', 'download', 'type', 'googl', 'search']\n",
      "Tokenized sentence: ['what', 'hello', 'wats', 'talks', 'email', 'address']\n",
      "After stop words removal: ['hello', 'wats', 'talks', 'email', 'address']\n",
      "After stemming with porters algorithm: ['hello', 'wat', 'talk', 'email', 'address']\n",
      "Tokenized sentence: ['do', 'you', 'know', 'why', 'god', 'created', 'gap', 'between', 'your', 'fingers', 'so', 'that', 'one', 'who', 'is', 'made', 'for', 'you', 'comes', 'amp', 'fills', 'those', 'gaps', 'by', 'holding', 'your', 'hand', 'with', 'love']\n",
      "After stop words removal: ['know', 'god', 'created', 'gap', 'fingers', 'one', 'made', 'comes', 'amp', 'fills', 'gaps', 'holding', 'hand', 'love']\n",
      "create\n",
      "hold\n",
      "After stemming with porters algorithm: ['know', 'god', 'creat', 'gap', 'finger', 'on', 'made', 'come', 'amp', 'fill', 'gap', 'hol', 'hand', 'love']\n",
      "Tokenized sentence: ['hi', 'good', 'mornin', 'thanku', 'wish', 'u', 'd', 'same']\n",
      "After stop words removal: ['hi', 'good', 'mornin', 'thanku', 'wish', 'u']\n",
      "After stemming with porters algorithm: ['good', 'mornin', 'thanku', 'wish']\n",
      "Tokenized sentence: ['call', 'to', 'use', 'ur', 'mins', 'calls', 'cast', 'p', 'min', 'mob', 'vary', 'service', 'provided', 'by', 'aom', 'just', 'gbp', 'month', 'aom', 'box', 'm', 'er', 'until', 'u', 'stop', 'ages', 'only']\n",
      "After stop words removal: ['call', 'use', 'ur', 'mins', 'calls', 'cast', 'p', 'min', 'mob', 'vary', 'service', 'provided', 'aom', 'gbp', 'month', 'aom', 'box', 'er', 'u', 'stop', 'ages']\n",
      "After stemming with porters algorithm: ['call', 'us', 'min', 'call', 'cast', 'min', 'mob', 'vari', 'servic', 'provid', 'aom', 'gbp', 'month', 'aom', 'box', 'stop', 'ag']\n",
      "Tokenized sentence: ['it', 's', 'fine', 'imma', 'get', 'a', 'drink', 'or', 'somethin', 'want', 'me', 'to', 'come', 'find', 'you']\n",
      "After stop words removal: ['fine', 'imma', 'get', 'drink', 'somethin', 'want', 'come', 'find']\n",
      "After stemming with porters algorithm: ['fine', 'imma', 'get', 'drink', 'somethin', 'want', 'come', 'find']\n",
      "Tokenized sentence: ['okie', 'wan', 'meet', 'at', 'bishan', 'cos', 'me', 'at', 'bishan', 'now', 'i', 'm', 'not', 'driving', 'today']\n",
      "After stop words removal: ['okie', 'wan', 'meet', 'bishan', 'cos', 'bishan', 'driving', 'today']\n",
      "driv\n",
      "After stemming with porters algorithm: ['oki', 'wan', 'meet', 'bishan', 'co', 'bishan', 'drive', 'todai']\n",
      "Tokenized sentence: ['xy', 'trying', 'smth', 'now', 'u', 'eat', 'already', 'we', 'havent']\n",
      "After stop words removal: ['xy', 'trying', 'smth', 'u', 'eat', 'already', 'havent']\n",
      "After stemming with porters algorithm: ['trying', 'smth', 'eat', 'alreadi', 'havent']\n",
      "Tokenized sentence: ['nobody', 'names', 'their', 'penis', 'a', 'girls', 'name', 'this', 'story', 'doesn', 't', 'add', 'up', 'at', 'all']\n",
      "After stop words removal: ['nobody', 'names', 'penis', 'girls', 'name', 'story', 'add']\n",
      "After stemming with porters algorithm: ['nobodi', 'name', 'peni', 'girl', 'name', 'stori', 'add']\n",
      "Tokenized sentence: ['i', 'will', 'cme', 'i', 'want', 'to', 'go', 'to', 'hos', 'morow', 'after', 'that', 'i', 'wil', 'cme', 'this', 'what', 'i', 'got', 'from', 'her', 'dear', 'what', 'to', 'do', 'she', 'didnt', 'say', 'any', 'time']\n",
      "After stop words removal: ['cme', 'want', 'go', 'hos', 'morow', 'wil', 'cme', 'got', 'dear', 'didnt', 'say', 'time']\n",
      "After stemming with porters algorithm: ['cme', 'want', 'ho', 'morow', 'wil', 'cme', 'got', 'dear', 'didnt', 'sai', 'time']\n",
      "Tokenized sentence: ['hi', 'where', 'you', 'you', 'in', 'home', 'or', 'calicut']\n",
      "After stop words removal: ['hi', 'home', 'calicut']\n",
      "After stemming with porters algorithm: ['home', 'calicut']\n",
      "Tokenized sentence: ['it', 's', 'a', 'taxt', 'massage', 'tie', 'pos', 'argh', 'ok', 'lool']\n",
      "After stop words removal: ['taxt', 'massage', 'tie', 'pos', 'argh', 'ok', 'lool']\n",
      "After stemming with porters algorithm: ['taxt', 'massag', 'tie', 'po', 'argh', 'lool']\n",
      "Tokenized sentence: ['except', 'theres', 'a', 'chick', 'with', 'huge', 'boobs']\n",
      "After stop words removal: ['except', 'theres', 'chick', 'huge', 'boobs']\n",
      "After stemming with porters algorithm: ['except', 'there', 'chick', 'huge', 'boob']\n",
      "Tokenized sentence: ['i', 've', 'been', 'barred', 'from', 'all', 'b', 'and', 'q', 'stores', 'for', 'life', 'this', 'twat', 'in', 'orange', 'dungerees', 'came', 'up', 'to', 'me', 'and', 'asked', 'if', 'i', 'wanted', 'decking', 'so', 'i', 'got', 'the', 'first', 'punch', 'in']\n",
      "After stop words removal: ['barred', 'b', 'q', 'stores', 'life', 'twat', 'orange', 'dungerees', 'came', 'asked', 'wanted', 'decking', 'got', 'first', 'punch']\n",
      "deck\n",
      "After stemming with porters algorithm: ['bar', 'store', 'life', 'twat', 'orang', 'dungere', 'came', 'as', 'wan', 'dec', 'got', 'first', 'punch']\n",
      "Tokenized sentence: ['just', 'so', 'that', 'you', 'know', 'yetunde', 'hasn', 't', 'sent', 'money', 'yet', 'i', 'just', 'sent', 'her', 'a', 'text', 'not', 'to', 'bother', 'sending', 'so', 'its', 'over', 'you', 'dont', 'have', 'to', 'involve', 'yourself', 'in', 'anything', 'i', 'shouldn', 't', 'have', 'imposed', 'anything', 'on', 'you', 'in', 'the', 'first', 'place', 'so', 'for', 'that', 'i', 'apologise']\n",
      "After stop words removal: ['know', 'yetunde', 'sent', 'money', 'yet', 'sent', 'text', 'bother', 'sending', 'dont', 'involve', 'anything', 'imposed', 'anything', 'first', 'place', 'apologise']\n",
      "send\n",
      "anyth\n",
      "anyth\n",
      "After stemming with porters algorithm: ['know', 'yetund', 'sent', 'monei', 'yet', 'sent', 'text', 'bother', 'sen', 'dont', 'involv', 'anyt', 'impos', 'anyt', 'first', 'place', 'apologis']\n",
      "Tokenized sentence: ['get', 'a', 'free', 'mobile', 'video', 'player', 'free', 'movie', 'to', 'collect', 'text', 'go', 'to', 'its', 'free', 'extra', 'films', 'can', 'be', 'ordered', 't', 's', 'and', 'c', 's', 'apply', 'yrs', 'only']\n",
      "After stop words removal: ['get', 'free', 'mobile', 'video', 'player', 'free', 'movie', 'collect', 'text', 'go', 'free', 'extra', 'films', 'ordered', 'c', 'apply', 'yrs']\n",
      "After stemming with porters algorithm: ['get', 'free', 'mobil', 'video', 'player', 'free', 'movi', 'collect', 'text', 'free', 'extra', 'film', 'order', 'appli', 'yr']\n",
      "Tokenized sentence: ['so', 'can', 'collect', 'ur', 'laptop']\n",
      "After stop words removal: ['collect', 'ur', 'laptop']\n",
      "After stemming with porters algorithm: ['collect', 'laptop']\n",
      "Tokenized sentence: ['i', 'm', 'home']\n",
      "After stop words removal: ['home']\n",
      "After stemming with porters algorithm: ['home']\n",
      "Tokenized sentence: ['you', 'have', 'received', 'your', 'mobile', 'content', 'enjoy']\n",
      "After stop words removal: ['received', 'mobile', 'content', 'enjoy']\n",
      "After stemming with porters algorithm: ['receiv', 'mobil', 'content', 'enjoi']\n",
      "Tokenized sentence: ['sorry', 'vikky', 'i', 'm', 'watching', 'olave', 'mandara', 'movie', 'kano', 'in', 'trishul', 'theatre', 'wit', 'my', 'frnds']\n",
      "After stop words removal: ['sorry', 'vikky', 'watching', 'olave', 'mandara', 'movie', 'kano', 'trishul', 'theatre', 'wit', 'frnds']\n",
      "watch\n",
      "After stemming with porters algorithm: ['sorri', 'vikki', 'watc', 'olav', 'mandara', 'movi', 'kano', 'trishul', 'theatr', 'wit', 'frnd']\n",
      "Tokenized sentence: ['now', 'u', 'sound', 'like', 'manky', 'scouse', 'boy', 'steve', 'like', 'i', 'is', 'travelling', 'on', 'da', 'bus', 'home', 'wot', 'has', 'u', 'inmind', 'recreation', 'dis', 'eve']\n",
      "After stop words removal: ['u', 'sound', 'like', 'manky', 'scouse', 'boy', 'steve', 'like', 'travelling', 'da', 'bus', 'home', 'wot', 'u', 'inmind', 'recreation', 'dis', 'eve']\n",
      "travell\n",
      "After stemming with porters algorithm: ['sound', 'like', 'manki', 'scous', 'boi', 'steve', 'like', 'travel', 'bu', 'home', 'wot', 'inmind', 'recreat', 'di', 'ev']\n",
      "Tokenized sentence: ['no', 'message', 'no', 'responce', 'what', 'happend']\n",
      "After stop words removal: ['message', 'responce', 'happend']\n",
      "After stemming with porters algorithm: ['messag', 'responc', 'happend']\n",
      "Tokenized sentence: ['one', 'day', 'a', 'crab', 'was', 'running', 'on', 'the', 'sea', 'shore', 'the', 'waves', 'came', 'n', 'cleared', 'the', 'footprints', 'of', 'the', 'crab', 'crab', 'asked', 'being', 'my', 'frnd', 'y', 'r', 'u', 'clearing', 'my', 'beautiful', 'footprints', 'waves', 'replied', 'a', 'fox', 'was', 'following', 'ur', 'footprints', 'to', 'catch', 'you', 'thats', 'y', 'i', 'cleared', 'it', 'off', 'frndsship', 'never', 'lets', 'u', 'dwn', 'gud', 'nyt']\n",
      "After stop words removal: ['one', 'day', 'crab', 'running', 'sea', 'shore', 'waves', 'came', 'n', 'cleared', 'footprints', 'crab', 'crab', 'asked', 'frnd', 'r', 'u', 'clearing', 'beautiful', 'footprints', 'waves', 'replied', 'fox', 'following', 'ur', 'footprints', 'catch', 'thats', 'cleared', 'frndsship', 'never', 'lets', 'u', 'dwn', 'gud', 'nyt']\n",
      "runn\n",
      "clear\n",
      "follow\n",
      "After stemming with porters algorithm: ['on', 'dai', 'crab', 'run', 'sea', 'shore', 'wave', 'came', 'clear', 'footprint', 'crab', 'crab', 'as', 'frnd', 'clear', 'beauti', 'footprint', 'wave', 'repli', 'fox', 'follow', 'footprint', 'catch', 'that', 'clear', 'frndsship', 'never', 'let', 'dwn', 'gud', 'nyt']\n",
      "Tokenized sentence: ['good', 'night', 'am', 'going', 'to', 'sleep']\n",
      "After stop words removal: ['good', 'night', 'going', 'sleep']\n",
      "go\n",
      "After stemming with porters algorithm: ['good', 'night', 'go', 'sleep']\n",
      "Tokenized sentence: ['network', 'operator', 'the', 'service', 'is', 'free', 'for', 't', 'c', 's', 'visit', 'biz']\n",
      "After stop words removal: ['network', 'operator', 'service', 'free', 'c', 'visit', 'biz']\n",
      "After stemming with porters algorithm: ['network', 'oper', 'servic', 'free', 'visit', 'biz']\n",
      "Tokenized sentence: ['i', 'll', 'leave', 'around', 'four', 'ok']\n",
      "After stop words removal: ['leave', 'around', 'four', 'ok']\n",
      "After stemming with porters algorithm: ['leav', 'around', 'four']\n",
      "Tokenized sentence: ['dear', 'where', 'you', 'will', 'be', 'when', 'i', 'reach', 'there']\n",
      "After stop words removal: ['dear', 'reach']\n",
      "After stemming with porters algorithm: ['dear', 'reach']\n",
      "Tokenized sentence: ['yeah', 'i', 'should', 'be', 'able', 'to', 'i', 'll', 'text', 'you', 'when', 'i', 'm', 'ready', 'to', 'meet', 'up']\n",
      "After stop words removal: ['yeah', 'able', 'text', 'ready', 'meet']\n",
      "After stemming with porters algorithm: ['yeah', 'abl', 'text', 'readi', 'meet']\n",
      "Tokenized sentence: ['oh', 'you', 'got', 'many', 'responsibilities']\n",
      "After stop words removal: ['oh', 'got', 'many', 'responsibilities']\n",
      "After stemming with porters algorithm: ['got', 'mani', 'respons']\n",
      "Tokenized sentence: ['alrite', 'hunny', 'wot', 'u', 'up', 'nite', 'didnt', 'end', 'up', 'goin', 'down', 'town', 'jus', 'da', 'pub', 'instead', 'jus', 'chillin', 'at', 'da', 'mo', 'in', 'me', 'bedroom', 'love', 'jen', 'xxx']\n",
      "After stop words removal: ['alrite', 'hunny', 'wot', 'u', 'nite', 'didnt', 'end', 'goin', 'town', 'jus', 'da', 'pub', 'instead', 'jus', 'chillin', 'da', 'mo', 'bedroom', 'love', 'jen', 'xxx']\n",
      "After stemming with porters algorithm: ['alrit', 'hunni', 'wot', 'nite', 'didnt', 'end', 'goin', 'town', 'ju', 'pub', 'instead', 'ju', 'chillin', 'bedroom', 'love', 'jen', 'xxx']\n",
      "Tokenized sentence: ['goodmorning', 'today', 'i', 'am', 'late', 'for', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['goodmorning', 'today', 'late', 'lt', 'gt', 'min']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['goodmor', 'todai', 'late', 'min']\n",
      "Tokenized sentence: ['your', 'opinion', 'about', 'me', 'over', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'not', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stop words removal: ['opinion', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stemming with porters algorithm: ['opinion', 'jada', 'kusruthi', 'lovab', 'silent', 'spl', 'charact', 'matur', 'stylish', 'simpl', 'pl', 'repli']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'a', 'meeting', 'call', 'me', 'later', 'at']\n",
      "After stop words removal: ['meeting', 'call', 'later']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'call', 'later']\n",
      "Tokenized sentence: ['just', 'finished', 'missing', 'you', 'plenty']\n",
      "After stop words removal: ['finished', 'missing', 'plenty']\n",
      "miss\n",
      "After stemming with porters algorithm: ['finis', 'miss', 'plenti']\n",
      "Tokenized sentence: ['pdate', 'now', 'double', 'mins', 'and', 'txts', 'on', 'orange', 'tariffs', 'latest', 'motorola', 'sonyericsson', 'nokia', 'bluetooth', 'free', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'yhl']\n",
      "After stop words removal: ['pdate', 'double', 'mins', 'txts', 'orange', 'tariffs', 'latest', 'motorola', 'sonyericsson', 'nokia', 'bluetooth', 'free', 'call', 'mobileupd', 'call', 'optout', 'yhl']\n",
      "After stemming with porters algorithm: ['pdate', 'doubl', 'min', 'txt', 'orang', 'tariff', 'latest', 'motorola', 'sonyericsson', 'nokia', 'bluetooth', 'free', 'call', 'mobileupd', 'call', 'optout', 'yhl']\n",
      "Tokenized sentence: ['sac', 'will', 'score', 'big', 'hundred', 'he', 'is', 'set', 'batsman']\n",
      "After stop words removal: ['sac', 'score', 'big', 'hundred', 'set', 'batsman']\n",
      "After stemming with porters algorithm: ['sac', 'score', 'big', 'hund', 'set', 'batsman']\n",
      "Tokenized sentence: ['ok', 'how', 'many', 'should', 'i', 'buy']\n",
      "After stop words removal: ['ok', 'many', 'buy']\n",
      "After stemming with porters algorithm: ['mani', 'bui']\n",
      "Tokenized sentence: ['i', 'like', 'cheap', 'but', 'i', 'm', 'happy', 'to', 'splash', 'out', 'on', 'the', 'wine', 'if', 'it', 'makes', 'you', 'feel', 'better']\n",
      "After stop words removal: ['like', 'cheap', 'happy', 'splash', 'wine', 'makes', 'feel', 'better']\n",
      "After stemming with porters algorithm: ['like', 'cheap', 'happi', 'splash', 'wine', 'make', 'feel', 'better']\n",
      "Tokenized sentence: ['have', 'a', 'good', 'evening', 'ttyl']\n",
      "After stop words removal: ['good', 'evening', 'ttyl']\n",
      "even\n",
      "After stemming with porters algorithm: ['good', 'even', 'ttyl']\n",
      "Tokenized sentence: ['heart', 'is', 'empty', 'without', 'love', 'mind', 'is', 'empty', 'without', 'wisdom', 'eyes', 'r', 'empty', 'without', 'dreams', 'amp', 'life', 'is', 'empty', 'without', 'frnds', 'so', 'alwys', 'be', 'in', 'touch', 'good', 'night', 'amp', 'sweet', 'dreams']\n",
      "After stop words removal: ['heart', 'empty', 'without', 'love', 'mind', 'empty', 'without', 'wisdom', 'eyes', 'r', 'empty', 'without', 'dreams', 'amp', 'life', 'empty', 'without', 'frnds', 'alwys', 'touch', 'good', 'night', 'amp', 'sweet', 'dreams']\n",
      "After stemming with porters algorithm: ['heart', 'empti', 'without', 'love', 'mind', 'empti', 'without', 'wisdom', 'ey', 'empti', 'without', 'dream', 'amp', 'life', 'empti', 'without', 'frnd', 'alwi', 'touch', 'good', 'night', 'amp', 'sweet', 'dream']\n",
      "Tokenized sentence: ['cool', 'text', 'me', 'when', 'you', 're', 'ready']\n",
      "After stop words removal: ['cool', 'text', 'ready']\n",
      "After stemming with porters algorithm: ['cool', 'text', 'readi']\n",
      "Tokenized sentence: ['usually', 'the', 'body', 'takes', 'care', 'of', 'it', 'buy', 'making', 'sure', 'it', 'doesnt', 'progress', 'can', 'we', 'pls', 'continue', 'this', 'talk', 'on', 'saturday']\n",
      "After stop words removal: ['usually', 'body', 'takes', 'care', 'buy', 'making', 'sure', 'doesnt', 'progress', 'pls', 'continue', 'talk', 'saturday']\n",
      "mak\n",
      "After stemming with porters algorithm: ['usual', 'bodi', 'take', 'care', 'bui', 'make', 'sure', 'doesnt', 'progress', 'pl', 'continu', 'talk', 'saturdai']\n",
      "Tokenized sentence: ['oh', 'only', 'outside', 'players', 'allowed', 'to', 'play', 'know']\n",
      "After stop words removal: ['oh', 'outside', 'players', 'allowed', 'play', 'know']\n",
      "After stemming with porters algorithm: ['outsid', 'player', 'allow', 'plai', 'know']\n",
      "Tokenized sentence: ['no', 'b', 'thursday']\n",
      "After stop words removal: ['b', 'thursday']\n",
      "After stemming with porters algorithm: ['thursdai']\n",
      "Tokenized sentence: ['haha', 'my', 'legs', 'and', 'neck', 'are', 'killing', 'me', 'and', 'my', 'amigos', 'are', 'hoping', 'to', 'end', 'the', 'night', 'with', 'a', 'burn', 'think', 'i', 'could', 'swing', 'by', 'in', 'like', 'an', 'hour']\n",
      "After stop words removal: ['haha', 'legs', 'neck', 'killing', 'amigos', 'hoping', 'end', 'night', 'burn', 'think', 'could', 'swing', 'like', 'hour']\n",
      "kill\n",
      "hop\n",
      "After stemming with porters algorithm: ['haha', 'leg', 'neck', 'kill', 'amigo', 'hope', 'end', 'night', 'burn', 'think', 'could', 'swing', 'like', 'hour']\n",
      "Tokenized sentence: ['someone', 'u', 'know', 'has', 'asked', 'our', 'dating', 'service', 'contact', 'you', 'cant', 'guess', 'who', 'call', 'now', 'all', 'will', 'be', 'revealed', 'po', 'box', 'm', 'wu']\n",
      "After stop words removal: ['someone', 'u', 'know', 'asked', 'dating', 'service', 'contact', 'cant', 'guess', 'call', 'revealed', 'po', 'box', 'wu']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someon', 'know', 'as', 'date', 'servic', 'contact', 'cant', 'guess', 'call', 'reveal', 'box']\n",
      "Tokenized sentence: ['si', 'si', 'i', 'think', 'ill', 'go', 'make', 'those', 'oreo', 'truffles']\n",
      "After stop words removal: ['si', 'si', 'think', 'ill', 'go', 'make', 'oreo', 'truffles']\n",
      "After stemming with porters algorithm: ['think', 'ill', 'make', 'oreo', 'truffl']\n",
      "Tokenized sentence: ['i', 'can', 't', 'describe', 'how', 'lucky', 'you', 'are', 'that', 'i', 'm', 'actually', 'awake', 'by', 'noon']\n",
      "After stop words removal: ['describe', 'lucky', 'actually', 'awake', 'noon']\n",
      "After stemming with porters algorithm: ['describ', 'lucki', 'actual', 'awak', 'noon']\n",
      "Tokenized sentence: ['as', 'per', 'your', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', 'to', 'copy', 'your', 'friends', 'callertune']\n",
      "After stop words removal: ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'callers', 'press', 'copy', 'friends', 'callertune']\n",
      "After stemming with porters algorithm: ['per', 'request', 'mell', 'mell', 'oru', 'minnaminungint', 'nurungu', 'vettam', 'set', 'callertun', 'caller', 'press', 'copi', 'friend', 'callertun']\n",
      "Tokenized sentence: ['can', 'you', 'say', 'what', 'happen']\n",
      "After stop words removal: ['say', 'happen']\n",
      "After stemming with porters algorithm: ['sai', 'happen']\n",
      "Tokenized sentence: ['so', 'check', 'your', 'errors', 'and', 'if', 'you', 'had', 'difficulties', 'do', 'correction']\n",
      "After stop words removal: ['check', 'errors', 'difficulties', 'correction']\n",
      "After stemming with porters algorithm: ['check', 'error', 'difficulti', 'correct']\n",
      "Tokenized sentence: ['ofcourse', 'i', 'also', 'upload', 'some', 'songs']\n",
      "After stop words removal: ['ofcourse', 'also', 'upload', 'songs']\n",
      "After stemming with porters algorithm: ['ofcours', 'also', 'upload', 'song']\n",
      "Tokenized sentence: ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n",
      "After stop words removal: ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say']\n",
      "After stemming with porters algorithm: ['dun', 'sai', 'earli', 'hor', 'alreadi', 'sai']\n",
      "Tokenized sentence: ['poyyarikatur', 'kolathupalayam', 'unjalur', 'post', 'erode', 'dis', 'lt', 'gt']\n",
      "After stop words removal: ['poyyarikatur', 'kolathupalayam', 'unjalur', 'post', 'erode', 'dis', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['poyyarikatur', 'kolathupalayam', 'unjalur', 'post', 'erod', 'di']\n",
      "Tokenized sentence: ['k', 'actually', 'can', 'you', 'guys', 'meet', 'me', 'at', 'the', 'sunoco', 'on', 'howard', 'it', 'should', 'be', 'right', 'on', 'the', 'way']\n",
      "After stop words removal: ['k', 'actually', 'guys', 'meet', 'sunoco', 'howard', 'right', 'way']\n",
      "After stemming with porters algorithm: ['actual', 'gui', 'meet', 'sunoco', 'howard', 'right', 'wai']\n",
      "Tokenized sentence: ['no', 'my', 'blankets', 'are', 'sufficient', 'thx']\n",
      "After stop words removal: ['blankets', 'sufficient', 'thx']\n",
      "After stemming with porters algorithm: ['blanket', 'suffici', 'thx']\n",
      "Tokenized sentence: ['you', 'are', 'being', 'contacted', 'by', 'our', 'dating', 'service', 'by', 'someone', 'you', 'know', 'to', 'find', 'out', 'who', 'it', 'is', 'call', 'from', 'your', 'mobile', 'or', 'landline', 'pobox', 'ldns']\n",
      "After stop words removal: ['contacted', 'dating', 'service', 'someone', 'know', 'find', 'call', 'mobile', 'landline', 'pobox', 'ldns']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['contac', 'date', 'servic', 'someon', 'know', 'find', 'call', 'mobil', 'landlin', 'pobox', 'ldn']\n",
      "Tokenized sentence: ['hi', 'darlin', 'im', 'on', 'helens', 'fone', 'im', 'gonna', 'b', 'up', 'the', 'princes', 'nite', 'please', 'come', 'up', 'tb', 'love', 'kate']\n",
      "After stop words removal: ['hi', 'darlin', 'im', 'helens', 'fone', 'im', 'gonna', 'b', 'princes', 'nite', 'please', 'come', 'tb', 'love', 'kate']\n",
      "After stemming with porters algorithm: ['darlin', 'helen', 'fone', 'gonna', 'princ', 'nite', 'pleas', 'come', 'love', 'kate']\n",
      "Tokenized sentence: ['ou', 'are', 'guaranteed', 'the', 'latest', 'nokia', 'phone', 'a', 'gb', 'ipod', 'mp', 'player', 'or', 'a', 'prize', 'txt', 'word', 'collect', 'to', 'no', 'ibhltd', 'ldnw', 'h', 'p', 'mtmsgrcvd']\n",
      "After stop words removal: ['ou', 'guaranteed', 'latest', 'nokia', 'phone', 'gb', 'ipod', 'mp', 'player', 'prize', 'txt', 'word', 'collect', 'ibhltd', 'ldnw', 'h', 'p', 'mtmsgrcvd']\n",
      "After stemming with porters algorithm: ['guaranteed', 'latest', 'nokia', 'phone', 'ipod', 'player', 'priz', 'txt', 'word', 'collect', 'ibhltd', 'ldnw', 'mtmsgrcvd']\n",
      "Tokenized sentence: ['dont', 'pick', 'up', 'd', 'call', 'when', 'something', 'important', 'is', 'there', 'to', 'tell', 'hrishi']\n",
      "After stop words removal: ['dont', 'pick', 'call', 'something', 'important', 'tell', 'hrishi']\n",
      "someth\n",
      "After stemming with porters algorithm: ['dont', 'pick', 'call', 'somet', 'import', 'tell', 'hrishi']\n",
      "Tokenized sentence: ['also', 'remember', 'to', 'get', 'dobby', 's', 'bowl', 'from', 'your', 'car']\n",
      "After stop words removal: ['also', 'remember', 'get', 'dobby', 'bowl', 'car']\n",
      "After stemming with porters algorithm: ['also', 'rememb', 'get', 'dobbi', 'bowl', 'car']\n",
      "Tokenized sentence: ['k', 'wen', 'ur', 'free', 'come', 'to', 'my', 'home', 'and', 'also', 'tel', 'vikky', 'i', 'hav', 'sent', 'mail', 'to', 'him', 'also', 'better', 'come', 'evening', 'il', 'be', 'free', 'today', 'aftr', 'pm']\n",
      "After stop words removal: ['k', 'wen', 'ur', 'free', 'come', 'home', 'also', 'tel', 'vikky', 'hav', 'sent', 'mail', 'also', 'better', 'come', 'evening', 'il', 'free', 'today', 'aftr', 'pm']\n",
      "even\n",
      "After stemming with porters algorithm: ['wen', 'free', 'come', 'home', 'also', 'tel', 'vikki', 'hav', 'sent', 'mail', 'also', 'better', 'come', 'even', 'free', 'todai', 'aftr']\n",
      "Tokenized sentence: ['dunno', 'cos', 'i', 'was', 'v', 'late', 'n', 'when', 'i', 'reach', 'they', 'inside', 'already', 'but', 'we', 'ate', 'spageddies', 'lor', 'it', 's', 'e', 'gals', 'who', 'r', 'laughing', 'at', 'me', 'lor']\n",
      "After stop words removal: ['dunno', 'cos', 'v', 'late', 'n', 'reach', 'inside', 'already', 'ate', 'spageddies', 'lor', 'e', 'gals', 'r', 'laughing', 'lor']\n",
      "laugh\n",
      "After stemming with porters algorithm: ['dunno', 'co', 'late', 'reach', 'insid', 'alreadi', 'at', 'spageddi', 'lor', 'gal', 'laug', 'lor']\n",
      "Tokenized sentence: ['yes', 'from', 'last', 'week', 'itself', 'i', 'm', 'taking', 'live', 'call']\n",
      "After stop words removal: ['yes', 'last', 'week', 'taking', 'live', 'call']\n",
      "tak\n",
      "After stemming with porters algorithm: ['ye', 'last', 'week', 'take', 'live', 'call']\n",
      "Tokenized sentence: ['ok', 'ill', 'tell', 'the', 'company']\n",
      "After stop words removal: ['ok', 'ill', 'tell', 'company']\n",
      "After stemming with porters algorithm: ['ill', 'tell', 'compani']\n",
      "Tokenized sentence: ['yup', 'from', 'what', 'i', 'remb', 'i', 'think', 'should', 'be', 'can', 'book']\n",
      "After stop words removal: ['yup', 'remb', 'think', 'book']\n",
      "After stemming with porters algorithm: ['yup', 'remb', 'think', 'book']\n",
      "Tokenized sentence: ['sms', 'services', 'for', 'your', 'inclusive', 'text', 'credits', 'pls', 'gotto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'with', 'stop', 'no', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stop words removal: ['sms', 'services', 'inclusive', 'text', 'credits', 'pls', 'gotto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'stop', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stemming with porters algorithm: ['sm', 'servic', 'inclus', 'text', 'credit', 'pl', 'gotto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscrib', 'stop', 'extra', 'charg', 'help', 'comuk']\n",
      "Tokenized sentence: ['tell', 'me', 'again', 'what', 'your', 'address', 'is']\n",
      "After stop words removal: ['tell', 'address']\n",
      "After stemming with porters algorithm: ['tell', 'address']\n",
      "Tokenized sentence: ['to', 'the', 'wonderful', 'okors', 'have', 'a', 'great', 'month', 'we', 'cherish', 'you', 'guys', 'and', 'wish', 'you', 'well', 'each', 'day', 'mojibiola']\n",
      "After stop words removal: ['wonderful', 'okors', 'great', 'month', 'cherish', 'guys', 'wish', 'well', 'day', 'mojibiola']\n",
      "After stemming with porters algorithm: ['wonder', 'okor', 'great', 'month', 'cherish', 'gui', 'wish', 'well', 'dai', 'mojibiola']\n",
      "Tokenized sentence: ['remember', 'all', 'those', 'whom', 'i', 'hurt', 'during', 'days', 'of', 'satanic', 'imposter', 'in', 'me', 'need', 'to', 'pay', 'a', 'price', 'so', 'be', 'it', 'may', 'destiny', 'keep', 'me', 'going', 'and', 'as', 'u', 'said', 'pray', 'that', 'i', 'get', 'the', 'mind', 'to', 'get', 'over', 'the', 'same']\n",
      "After stop words removal: ['remember', 'hurt', 'days', 'satanic', 'imposter', 'need', 'pay', 'price', 'may', 'destiny', 'keep', 'going', 'u', 'said', 'pray', 'get', 'mind', 'get']\n",
      "go\n",
      "After stemming with porters algorithm: ['rememb', 'hurt', 'dai', 'satan', 'impost', 'need', 'pai', 'price', 'mai', 'destini', 'keep', 'go', 'said', 'prai', 'get', 'mind', 'get']\n",
      "Tokenized sentence: ['kindly', 'send', 'some', 'one', 'to', 'our', 'flat', 'before', 'lt', 'decimal', 'gt', 'today']\n",
      "After stop words removal: ['kindly', 'send', 'one', 'flat', 'lt', 'decimal', 'gt', 'today']\n",
      "After stemming with porters algorithm: ['kindli', 'send', 'on', 'flat', 'decim', 'todai']\n",
      "Tokenized sentence: ['congratulations', 'ore', 'mo', 'owo', 're', 'wa', 'enjoy', 'it', 'and', 'i', 'wish', 'you', 'many', 'happy', 'moments', 'to', 'and', 'fro', 'wherever', 'you', 'go']\n",
      "After stop words removal: ['congratulations', 'ore', 'mo', 'owo', 'wa', 'enjoy', 'wish', 'many', 'happy', 'moments', 'fro', 'wherever', 'go']\n",
      "After stemming with porters algorithm: ['congratul', 'or', 'owo', 'enjoi', 'wish', 'mani', 'happi', 'moment', 'fro', 'wherev']\n",
      "Tokenized sentence: ['wait', 'i', 'will', 'msg', 'after', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['wait', 'msg', 'lt', 'gt', 'min']\n",
      "After stemming with porters algorithm: ['wait', 'msg', 'min']\n",
      "Tokenized sentence: ['pleassssssseeeeee', 'tel', 'me', 'v', 'avent', 'done', 'sportsx']\n",
      "After stop words removal: ['pleassssssseeeeee', 'tel', 'v', 'avent', 'done', 'sportsx']\n",
      "After stemming with porters algorithm: ['pleassssssseeeee', 'tel', 'avent', 'done', 'sportsx']\n",
      "Tokenized sentence: ['we', 'are', 'pleased', 'to', 'inform', 'that', 'your', 'application', 'for', 'airtel', 'broadband', 'is', 'processed', 'successfully', 'your', 'installation', 'will', 'happen', 'within', 'days']\n",
      "After stop words removal: ['pleased', 'inform', 'application', 'airtel', 'broadband', 'processed', 'successfully', 'installation', 'happen', 'within', 'days']\n",
      "After stemming with porters algorithm: ['pleas', 'inform', 'applic', 'airtel', 'broadband', 'process', 'successfulli', 'instal', 'happen', 'within', 'dai']\n",
      "Tokenized sentence: ['goodo', 'yes', 'we', 'must', 'speak', 'friday', 'egg', 'potato', 'ratio', 'for', 'tortilla', 'needed']\n",
      "After stop words removal: ['goodo', 'yes', 'must', 'speak', 'friday', 'egg', 'potato', 'ratio', 'tortilla', 'needed']\n",
      "After stemming with porters algorithm: ['goodo', 'ye', 'must', 'speak', 'fridai', 'egg', 'potato', 'ratio', 'tortilla', 'need']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'sent', 'lt', 'gt', 'mesages', 'today', 'thats', 'y', 'sorry', 'if', 'i', 'hurts']\n",
      "After stop words removal: ['want', 'sent', 'lt', 'gt', 'mesages', 'today', 'thats', 'sorry', 'hurts']\n",
      "After stemming with porters algorithm: ['want', 'sent', 'mesag', 'todai', 'that', 'sorri', 'hurt']\n",
      "Tokenized sentence: ['i', 'wont', 'do', 'anything', 'de']\n",
      "After stop words removal: ['wont', 'anything', 'de']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['wont', 'anyt']\n",
      "Tokenized sentence: ['that', 's', 'y', 'u', 'haf', 'keep', 'me', 'busy']\n",
      "After stop words removal: ['u', 'haf', 'keep', 'busy']\n",
      "After stemming with porters algorithm: ['haf', 'keep', 'busi']\n",
      "Tokenized sentence: ['hmph', 'go', 'head', 'big', 'baller']\n",
      "After stop words removal: ['hmph', 'go', 'head', 'big', 'baller']\n",
      "After stemming with porters algorithm: ['hmph', 'head', 'big', 'baller']\n",
      "Tokenized sentence: ['probably', 'earlier', 'than', 'that', 'if', 'the', 'station', 's', 'where', 'i', 'think', 'it', 'is']\n",
      "After stop words removal: ['probably', 'earlier', 'station', 'think']\n",
      "After stemming with porters algorithm: ['probab', 'earlier', 'stat', 'think']\n",
      "Tokenized sentence: ['so', 'pay', 'first', 'lar', 'then', 'when', 'is', 'da', 'stock', 'comin']\n",
      "After stop words removal: ['pay', 'first', 'lar', 'da', 'stock', 'comin']\n",
      "After stemming with porters algorithm: ['pai', 'first', 'lar', 'stock', 'comin']\n",
      "Tokenized sentence: ['s', 'reach', 'home', 'call', 'me']\n",
      "After stop words removal: ['reach', 'home', 'call']\n",
      "After stemming with porters algorithm: ['reach', 'home', 'call']\n",
      "Tokenized sentence: ['armand', 'says', 'get', 'your', 'ass', 'over', 'to', 'epsilon']\n",
      "After stop words removal: ['armand', 'says', 'get', 'ass', 'epsilon']\n",
      "After stemming with porters algorithm: ['armand', 'sai', 'get', 'ass', 'epsilon']\n",
      "Tokenized sentence: ['neshanth', 'tel', 'me', 'who', 'r', 'u']\n",
      "After stop words removal: ['neshanth', 'tel', 'r', 'u']\n",
      "After stemming with porters algorithm: ['neshanth', 'tel']\n",
      "Tokenized sentence: ['true', 'it', 'is', 'passable', 'and', 'if', 'you', 'get', 'a', 'high', 'score', 'and', 'apply', 'for', 'phd', 'you', 'get', 'years', 'of', 'salary', 'so', 'it', 'makes', 'life', 'easier']\n",
      "After stop words removal: ['true', 'passable', 'get', 'high', 'score', 'apply', 'phd', 'get', 'years', 'salary', 'makes', 'life', 'easier']\n",
      "After stemming with porters algorithm: ['true', 'passab', 'get', 'high', 'score', 'appli', 'phd', 'get', 'year', 'salari', 'make', 'life', 'easier']\n",
      "Tokenized sentence: ['he', 'says', 'hi', 'and', 'to', 'get', 'your', 'ass', 'back', 'to', 'south', 'tampa', 'preferably', 'at', 'a', 'kegger']\n",
      "After stop words removal: ['says', 'hi', 'get', 'ass', 'back', 'south', 'tampa', 'preferably', 'kegger']\n",
      "After stemming with porters algorithm: ['sai', 'get', 'ass', 'back', 'south', 'tampa', 'prefer', 'kegger']\n",
      "Tokenized sentence: ['ill', 'b', 'down', 'soon']\n",
      "After stop words removal: ['ill', 'b', 'soon']\n",
      "After stemming with porters algorithm: ['ill', 'soon']\n",
      "Tokenized sentence: ['i', 'guess', 'that', 's', 'why', 'you', 're', 'worried', 'you', 'must', 'know', 'that', 'there', 's', 'a', 'way', 'the', 'body', 'repairs', 'itself', 'and', 'i', 'm', 'quite', 'sure', 'you', 'shouldn', 't', 'worry', 'we', 'll', 'take', 'it', 'slow', 'first', 'the', 'tests', 'they', 'will', 'guide', 'when', 'your', 'ovulation', 'is', 'then', 'just', 'relax', 'nothing', 'you', 've', 'said', 'is', 'a', 'reason', 'to', 'worry', 'but', 'i', 'll', 'keep', 'on', 'followin', 'you', 'up']\n",
      "After stop words removal: ['guess', 'worried', 'must', 'know', 'way', 'body', 'repairs', 'quite', 'sure', 'worry', 'take', 'slow', 'first', 'tests', 'guide', 'ovulation', 'relax', 'nothing', 'said', 'reason', 'worry', 'keep', 'followin']\n",
      "noth\n",
      "After stemming with porters algorithm: ['guess', 'worri', 'must', 'know', 'wai', 'bodi', 'repair', 'quit', 'sure', 'worri', 'take', 'slow', 'first', 'test', 'guid', 'ovul', 'relax', 'not', 'said', 'reason', 'worri', 'keep', 'followin']\n",
      "Tokenized sentence: ['spook', 'up', 'your', 'mob', 'with', 'a', 'halloween', 'collection', 'of', 'a', 'logo', 'pic', 'message', 'plus', 'a', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'to', 'zed', 'p', 'per', 'logo', 'pic']\n",
      "After stop words removal: ['spook', 'mob', 'halloween', 'collection', 'logo', 'pic', 'message', 'plus', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'zed', 'p', 'per', 'logo', 'pic']\n",
      "After stemming with porters algorithm: ['spook', 'mob', 'halloween', 'collect', 'logo', 'pic', 'messag', 'plu', 'free', 'eeri', 'tone', 'txt', 'card', 'spook', 'zed', 'per', 'logo', 'pic']\n",
      "Tokenized sentence: ['ok', 'omw', 'now', 'you', 'at', 'castor']\n",
      "After stop words removal: ['ok', 'omw', 'castor']\n",
      "After stemming with porters algorithm: ['omw', 'castor']\n",
      "Tokenized sentence: ['i', 'thk', 'shd', 'be', 'ok', 'he', 'said', 'plus', 'minus', 'did', 'leave', 'a', 'line', 'in', 'between', 'paragraphs']\n",
      "After stop words removal: ['thk', 'shd', 'ok', 'said', 'plus', 'minus', 'leave', 'line', 'paragraphs']\n",
      "After stemming with porters algorithm: ['thk', 'shd', 'said', 'plu', 'minu', 'leav', 'line', 'paragraph']\n",
      "Tokenized sentence: ['i', 'don', 't', 'think', 'i', 'can', 'get', 'away', 'for', 'a', 'trek', 'that', 'long', 'with', 'family', 'in', 'town', 'sorry']\n",
      "After stop words removal: ['think', 'get', 'away', 'trek', 'long', 'family', 'town', 'sorry']\n",
      "After stemming with porters algorithm: ['think', 'get', 'awai', 'trek', 'long', 'famili', 'town', 'sorri']\n",
      "Tokenized sentence: ['yeah', 'get', 'the', 'unlimited']\n",
      "After stop words removal: ['yeah', 'get', 'unlimited']\n",
      "After stemming with porters algorithm: ['yeah', 'get', 'unlimit']\n",
      "Tokenized sentence: ['i', 'm', 'done', 'i', 'm', 'sorry', 'i', 'hope', 'your', 'next', 'space', 'gives', 'you', 'everything', 'you', 'want', 'remember', 'all', 'the', 'furniture', 'is', 'yours', 'if', 'i', 'm', 'not', 'around', 'when', 'you', 'move', 'it', 'just', 'lock', 'all', 'the', 'locks', 'and', 'leave', 'the', 'key', 'with', 'jenne']\n",
      "After stop words removal: ['done', 'sorry', 'hope', 'next', 'space', 'gives', 'everything', 'want', 'remember', 'furniture', 'around', 'move', 'lock', 'locks', 'leave', 'key', 'jenne']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['done', 'sorri', 'hope', 'next', 'space', 'give', 'everyt', 'want', 'rememb', 'furnitur', 'around', 'move', 'lock', 'lock', 'leav', 'kei', 'jenn']\n",
      "Tokenized sentence: ['is', 'ur', 'changes', 'da', 'report', 'big', 'cos', 'i', 've', 'already', 'made', 'changes', 'da', 'previous', 'report']\n",
      "After stop words removal: ['ur', 'changes', 'da', 'report', 'big', 'cos', 'already', 'made', 'changes', 'da', 'previous', 'report']\n",
      "After stemming with porters algorithm: ['chang', 'report', 'big', 'co', 'alreadi', 'made', 'chang', 'previou', 'report']\n",
      "Tokenized sentence: ['hey', 'is', 'rite', 'u', 'put', 'evey', 'mnth', 'is', 'that', 'all']\n",
      "After stop words removal: ['hey', 'rite', 'u', 'put', 'evey', 'mnth']\n",
      "After stemming with porters algorithm: ['hei', 'rite', 'put', 'evei', 'mnth']\n",
      "Tokenized sentence: ['well', 'its', 'not', 'like', 'you', 'actually', 'called', 'someone', 'a', 'punto', 'that', 'woulda', 'been', 'worse']\n",
      "After stop words removal: ['well', 'like', 'actually', 'called', 'someone', 'punto', 'woulda', 'worse']\n",
      "After stemming with porters algorithm: ['well', 'like', 'actual', 'call', 'someon', 'punto', 'woulda', 'wors']\n",
      "Tokenized sentence: ['life', 'alle', 'mone', 'eppolum', 'oru', 'pole', 'allalo']\n",
      "After stop words removal: ['life', 'alle', 'mone', 'eppolum', 'oru', 'pole', 'allalo']\n",
      "After stemming with porters algorithm: ['life', 'all', 'mone', 'eppolum', 'oru', 'pole', 'allalo']\n",
      "Tokenized sentence: ['hi', 'happy', 'new', 'year', 'i', 'dont', 'mean', 'to', 'intrude', 'but', 'can', 'you', 'pls', 'let', 'me', 'know', 'how', 'much', 'tuition', 'you', 'paid', 'last', 'semester', 'and', 'how', 'much', 'this', 'semester', 'is', 'thanks']\n",
      "After stop words removal: ['hi', 'happy', 'new', 'year', 'dont', 'mean', 'intrude', 'pls', 'let', 'know', 'much', 'tuition', 'paid', 'last', 'semester', 'much', 'semester', 'thanks']\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year', 'dont', 'mean', 'intrud', 'pl', 'let', 'know', 'much', 'tuit', 'paid', 'last', 'semest', 'much', 'semest', 'thank']\n",
      "Tokenized sentence: ['hmm', 'ill', 'have', 'to', 'think', 'about', 'it', 'ok', 'you', 're', 'forgiven', 'd']\n",
      "After stop words removal: ['hmm', 'ill', 'think', 'ok', 'forgiven']\n",
      "After stemming with porters algorithm: ['hmm', 'ill', 'think', 'forgiven']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'my', 'love', 'are', 'you', 'with', 'your', 'brother', 'time', 'to', 'talk', 'english', 'with', 'him', 'grins', 'say', 'hey', 'muhommad', 'penny', 'says', 'hello', 'from', 'across', 'the', 'sea']\n",
      "After stop words removal: ['love', 'brother', 'time', 'talk', 'english', 'grins', 'say', 'hey', 'muhommad', 'penny', 'says', 'hello', 'across', 'sea']\n",
      "After stemming with porters algorithm: ['love', 'brother', 'time', 'talk', 'english', 'grin', 'sai', 'hei', 'muhommad', 'penni', 'sai', 'hello', 'across', 'sea']\n",
      "Tokenized sentence: ['got', 'it', 'seventeen', 'pounds', 'for', 'seven', 'hundred', 'ml', 'hope', 'ok']\n",
      "After stop words removal: ['got', 'seventeen', 'pounds', 'seven', 'hundred', 'ml', 'hope', 'ok']\n",
      "After stemming with porters algorithm: ['got', 'seventeen', 'pound', 'seven', 'hund', 'hope']\n",
      "Tokenized sentence: ['its', 'too', 'late', 'but', 'its', 'k', 'wish', 'you', 'the', 'same']\n",
      "After stop words removal: ['late', 'k', 'wish']\n",
      "After stemming with porters algorithm: ['late', 'wish']\n",
      "Tokenized sentence: ['excellent', 'wish', 'we', 'were', 'together', 'right', 'now']\n",
      "After stop words removal: ['excellent', 'wish', 'together', 'right']\n",
      "After stemming with porters algorithm: ['excel', 'wish', 'togeth', 'right']\n",
      "Tokenized sentence: ['okie']\n",
      "After stop words removal: ['okie']\n",
      "After stemming with porters algorithm: ['oki']\n",
      "Tokenized sentence: ['cos', 'daddy', 'arranging', 'time', 'c', 'wat', 'time', 'fetch', 'mah']\n",
      "After stop words removal: ['cos', 'daddy', 'arranging', 'time', 'c', 'wat', 'time', 'fetch', 'mah']\n",
      "arrang\n",
      "After stemming with porters algorithm: ['co', 'daddi', 'arran', 'time', 'wat', 'time', 'fetch', 'mah']\n",
      "Tokenized sentence: ['aight', 'sorry', 'i', 'take', 'ten', 'years', 'to', 'shower', 'what', 's', 'the', 'plan']\n",
      "After stop words removal: ['aight', 'sorry', 'take', 'ten', 'years', 'shower', 'plan']\n",
      "After stemming with porters algorithm: ['aight', 'sorri', 'take', 'ten', 'year', 'shower', 'plan']\n",
      "Tokenized sentence: ['aight', 'i', 'll', 'text', 'you', 'when', 'i', 'm', 'back']\n",
      "After stop words removal: ['aight', 'text', 'back']\n",
      "After stemming with porters algorithm: ['aight', 'text', 'back']\n",
      "Tokenized sentence: ['free', 'polyphonic', 'ringtone', 'text', 'super', 'to', 'to', 'get', 'your', 'free', 'poly', 'tone', 'of', 'the', 'week', 'now', 'sn', 'pobox', 'nr', 'zs', 'subscription', 'pw']\n",
      "After stop words removal: ['free', 'polyphonic', 'ringtone', 'text', 'super', 'get', 'free', 'poly', 'tone', 'week', 'sn', 'pobox', 'nr', 'zs', 'subscription', 'pw']\n",
      "After stemming with porters algorithm: ['free', 'polyphon', 'rington', 'text', 'super', 'get', 'free', 'poli', 'tone', 'week', 'pobox', 'subscript']\n",
      "Tokenized sentence: ['they', 'released', 'another', 'italian', 'one', 'today', 'and', 'it', 'has', 'a', 'cosign', 'option']\n",
      "After stop words removal: ['released', 'another', 'italian', 'one', 'today', 'cosign', 'option']\n",
      "After stemming with porters algorithm: ['releas', 'anoth', 'italian', 'on', 'todai', 'cosign', 'opt']\n",
      "Tokenized sentence: ['what', 'you', 'thinked', 'about', 'me', 'first', 'time', 'you', 'saw', 'me', 'in', 'class']\n",
      "After stop words removal: ['thinked', 'first', 'time', 'saw', 'class']\n",
      "After stemming with porters algorithm: ['thin', 'first', 'time', 'saw', 'class']\n",
      "Tokenized sentence: ['she', 'just', 'broke', 'down', 'a', 'list', 'of', 'reasons', 'why', 'nobody', 's', 'in', 'town', 'and', 'i', 'can', 't', 'tell', 'if', 'she', 's', 'being', 'sarcastic', 'or', 'just', 'faggy']\n",
      "After stop words removal: ['broke', 'list', 'reasons', 'nobody', 'town', 'tell', 'sarcastic', 'faggy']\n",
      "After stemming with porters algorithm: ['broke', 'list', 'reason', 'nobodi', 'town', 'tell', 'sarcast', 'faggi']\n",
      "Tokenized sentence: ['all', 'boys', 'made', 'fun', 'of', 'me', 'today', 'ok', 'i', 'have', 'no', 'problem', 'i', 'just', 'sent', 'one', 'message', 'just', 'for', 'fun']\n",
      "After stop words removal: ['boys', 'made', 'fun', 'today', 'ok', 'problem', 'sent', 'one', 'message', 'fun']\n",
      "After stemming with porters algorithm: ['boi', 'made', 'fun', 'todai', 'problem', 'sent', 'on', 'messag', 'fun']\n",
      "Tokenized sentence: ['i', 'am', 'going', 'to', 'bed', 'now', 'prin']\n",
      "After stop words removal: ['going', 'bed', 'prin']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'bed', 'prin']\n",
      "Tokenized sentence: ['free', 'msg', 'we', 'billed', 'your', 'mobile', 'number', 'by', 'mistake', 'from', 'shortcode', 'please', 'call', 'to', 'have', 'charges', 'refunded', 'this', 'call', 'will', 'be', 'free', 'from', 'a', 'bt', 'landline']\n",
      "After stop words removal: ['free', 'msg', 'billed', 'mobile', 'number', 'mistake', 'shortcode', 'please', 'call', 'charges', 'refunded', 'call', 'free', 'bt', 'landline']\n",
      "After stemming with porters algorithm: ['free', 'msg', 'bill', 'mobil', 'number', 'mistak', 'shortcod', 'pleas', 'call', 'charg', 'refun', 'call', 'free', 'landlin']\n",
      "Tokenized sentence: ['recpt', 'you', 'have', 'ordered', 'a', 'ringtone', 'your', 'order', 'is', 'being', 'processed']\n",
      "After stop words removal: ['recpt', 'ordered', 'ringtone', 'order', 'processed']\n",
      "After stemming with porters algorithm: ['recpt', 'order', 'rington', 'order', 'process']\n",
      "Tokenized sentence: ['there', 'is', 'no', 'sense', 'in', 'my', 'foot', 'and', 'penis']\n",
      "After stop words removal: ['sense', 'foot', 'penis']\n",
      "After stemming with porters algorithm: ['sens', 'foot', 'peni']\n",
      "Tokenized sentence: ['yo', 'howz', 'u', 'girls', 'never', 'rang', 'after', 'india', 'l']\n",
      "After stop words removal: ['yo', 'howz', 'u', 'girls', 'never', 'rang', 'india', 'l']\n",
      "After stemming with porters algorithm: ['howz', 'girl', 'never', 'rang', 'india']\n",
      "Tokenized sentence: ['k', 'give', 'back', 'my', 'thanks']\n",
      "After stop words removal: ['k', 'give', 'back', 'thanks']\n",
      "After stemming with porters algorithm: ['give', 'back', 'thank']\n",
      "Tokenized sentence: ['urgent', 'please', 'call', 'your', 'abta', 'complimentary', 'spanish', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'box', 'po', 'ez', 'ppm']\n",
      "After stop words removal: ['urgent', 'please', 'call', 'abta', 'complimentary', 'spanish', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'box', 'po', 'ez', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'pleas', 'call', 'abta', 'complimentari', 'spanish', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['its', 'like', 'that', 'hotel', 'dusk', 'game', 'i', 'think', 'you', 'solve', 'puzzles', 'in', 'a', 'area', 'thing']\n",
      "After stop words removal: ['like', 'hotel', 'dusk', 'game', 'think', 'solve', 'puzzles', 'area', 'thing']\n",
      "After stemming with porters algorithm: ['like', 'hotel', 'dusk', 'game', 'think', 'solv', 'puzzl', 'area', 'thing']\n",
      "Tokenized sentence: ['a', 'networks', 'allow', 'companies', 'to', 'bill', 'for', 'sms', 'so', 'they', 'are', 'responsible', 'for', 'their', 'suppliers']\n",
      "After stop words removal: ['networks', 'allow', 'companies', 'bill', 'sms', 'responsible', 'suppliers']\n",
      "After stemming with porters algorithm: ['network', 'allow', 'compani', 'bill', 'sm', 'respons', 'supplier']\n",
      "Tokenized sentence: ['celebrate', 'my', 'b', 'day', 'y', 'else']\n",
      "After stop words removal: ['celebrate', 'b', 'day', 'else']\n",
      "After stemming with porters algorithm: ['celebr', 'dai', 'els']\n",
      "Tokenized sentence: ['well', 'boy', 'am', 'i', 'glad', 'g', 'wasted', 'all', 'night', 'at', 'applebees', 'for', 'nothing']\n",
      "After stop words removal: ['well', 'boy', 'glad', 'g', 'wasted', 'night', 'applebees', 'nothing']\n",
      "noth\n",
      "After stemming with porters algorithm: ['well', 'boi', 'glad', 'was', 'night', 'applebe', 'not']\n",
      "Tokenized sentence: ['tells', 'u', 'call', 'to', 'claim', 'prize', 'u', 'have', 'enter', 'all', 'ur', 'mobile', 'personal', 'details', 'the', 'prompts', 'careful']\n",
      "After stop words removal: ['tells', 'u', 'call', 'claim', 'prize', 'u', 'enter', 'ur', 'mobile', 'personal', 'details', 'prompts', 'careful']\n",
      "After stemming with porters algorithm: ['tell', 'call', 'claim', 'priz', 'enter', 'mobil', 'person', 'detail', 'prompt', 'care']\n",
      "Tokenized sentence: ['she', 'said', 'do', 'u', 'mind', 'if', 'i', 'go', 'into', 'the', 'bedroom', 'for', 'a', 'minute', 'ok', 'i', 'sed', 'in', 'a', 'sexy', 'mood', 'she', 'came', 'out', 'minuts', 'latr', 'wid', 'a', 'cake', 'n', 'my', 'wife']\n",
      "After stop words removal: ['said', 'u', 'mind', 'go', 'bedroom', 'minute', 'ok', 'sed', 'sexy', 'mood', 'came', 'minuts', 'latr', 'wid', 'cake', 'n', 'wife']\n",
      "After stemming with porters algorithm: ['said', 'mind', 'bedroom', 'minut', 'sed', 'sexi', 'mood', 'came', 'minut', 'latr', 'wid', 'cake', 'wife']\n",
      "Tokenized sentence: ['how', 's', 'my', 'loverboy', 'doing', 'what', 'does', 'he', 'do', 'that', 'keeps', 'him', 'from', 'coming', 'to', 'his', 'queen', 'hmmm', 'doesn', 't', 'he', 'ache', 'to', 'speak', 'to', 'me', 'miss', 'me', 'desparately']\n",
      "After stop words removal: ['loverboy', 'keeps', 'coming', 'queen', 'hmmm', 'ache', 'speak', 'miss', 'desparately']\n",
      "com\n",
      "After stemming with porters algorithm: ['loverboi', 'keep', 'come', 'queen', 'hmmm', 'ach', 'speak', 'miss', 'despar']\n",
      "Tokenized sentence: ['omg', 'i', 'want', 'to', 'scream', 'i', 'weighed', 'myself', 'and', 'i', 'lost', 'more', 'weight', 'woohoo']\n",
      "After stop words removal: ['omg', 'want', 'scream', 'weighed', 'lost', 'weight', 'woohoo']\n",
      "After stemming with porters algorithm: ['omg', 'want', 'scream', 'weig', 'lost', 'weight', 'woohoo']\n",
      "Tokenized sentence: ['your', 'chance', 'to', 'be', 'on', 'a', 'reality', 'fantasy', 'show', 'call', 'now', 'just', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'is', 'a', 'national', 'rate', 'call']\n",
      "After stop words removal: ['chance', 'reality', 'fantasy', 'show', 'call', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'national', 'rate', 'call']\n",
      "After stemming with porters algorithm: ['chanc', 'realiti', 'fantasi', 'show', 'call', 'per', 'min', 'ntt', 'ltd', 'box', 'croydon', 'nat', 'rate', 'call']\n",
      "Tokenized sentence: ['wife', 'how', 'she', 'knew', 'the', 'time', 'of', 'murder', 'exactly']\n",
      "After stop words removal: ['wife', 'knew', 'time', 'murder', 'exactly']\n",
      "After stemming with porters algorithm: ['wife', 'knew', 'time', 'murder', 'exactli']\n",
      "Tokenized sentence: ['a', 'link', 'to', 'your', 'picture', 'has', 'been', 'sent', 'you', 'can', 'also', 'use', 'http', 'alto', 'co', 'uk', 'wave', 'wave', 'asp', 'o']\n",
      "After stop words removal: ['link', 'picture', 'sent', 'also', 'use', 'http', 'alto', 'co', 'uk', 'wave', 'wave', 'asp']\n",
      "After stemming with porters algorithm: ['link', 'pictur', 'sent', 'also', 'us', 'http', 'alto', 'wave', 'wave', 'asp']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'bold', 'or', 'bb', 'torch']\n",
      "After stop words removal: ['want', 'bold', 'bb', 'torch']\n",
      "After stemming with porters algorithm: ['want', 'bold', 'torch']\n",
      "Tokenized sentence: ['wif', 'my', 'family', 'booking', 'tour', 'package']\n",
      "After stop words removal: ['wif', 'family', 'booking', 'tour', 'package']\n",
      "book\n",
      "After stemming with porters algorithm: ['wif', 'famili', 'book', 'tour', 'packag']\n",
      "Tokenized sentence: ['lol', 'enjoy', 'role', 'playing', 'much']\n",
      "After stop words removal: ['lol', 'enjoy', 'role', 'playing', 'much']\n",
      "play\n",
      "After stemming with porters algorithm: ['lol', 'enjoi', 'role', 'plai', 'much']\n",
      "Tokenized sentence: ['and', 'don', 't', 'worry', 'we', 'll', 'have', 'finished', 'by', 'march', 'ish']\n",
      "After stop words removal: ['worry', 'finished', 'march', 'ish']\n",
      "After stemming with porters algorithm: ['worri', 'finis', 'march', 'ish']\n",
      "Tokenized sentence: ['important', 'message', 'this', 'is', 'a', 'final', 'contact', 'attempt', 'you', 'have', 'important', 'messages', 'waiting', 'out', 'our', 'customer', 'claims', 'dept', 'expires', 'call', 'now']\n",
      "After stop words removal: ['important', 'message', 'final', 'contact', 'attempt', 'important', 'messages', 'waiting', 'customer', 'claims', 'dept', 'expires', 'call']\n",
      "wait\n",
      "After stemming with porters algorithm: ['import', 'messag', 'final', 'contact', 'attempt', 'import', 'messag', 'wait', 'custom', 'claim', 'dept', 'expir', 'call']\n",
      "Tokenized sentence: ['i', 'wont', 'touch', 'you', 'with', 'out', 'your', 'permission']\n",
      "After stop words removal: ['wont', 'touch', 'permission']\n",
      "After stemming with porters algorithm: ['wont', 'touch', 'permiss']\n",
      "Tokenized sentence: ['you', 'will', 'be', 'in', 'the', 'place', 'of', 'that', 'man']\n",
      "After stop words removal: ['place', 'man']\n",
      "After stemming with porters algorithm: ['place', 'man']\n",
      "Tokenized sentence: ['anything', 'is', 'valuable', 'in', 'only', 'situations', 'first', 'before', 'getting', 'it', 'second', 'after', 'loosing', 'it']\n",
      "After stop words removal: ['anything', 'valuable', 'situations', 'first', 'getting', 'second', 'loosing']\n",
      "anyth\n",
      "gett\n",
      "loos\n",
      "After stemming with porters algorithm: ['anyt', 'valuab', 'situat', 'first', 'get', 'second', 'loos']\n",
      "Tokenized sentence: ['i', 'm', 'serious', 'you', 'are', 'in', 'the', 'money', 'base']\n",
      "After stop words removal: ['serious', 'money', 'base']\n",
      "After stemming with porters algorithm: ['seriou', 'monei', 'base']\n",
      "Tokenized sentence: ['i', 'thought', 'we', 'were', 'doing', 'a', 'king', 'of', 'the', 'hill', 'thing', 'there']\n",
      "After stop words removal: ['thought', 'king', 'hill', 'thing']\n",
      "After stemming with porters algorithm: ['thought', 'king', 'hill', 'thing']\n",
      "Tokenized sentence: ['but', 'you', 'were', 'together', 'so', 'you', 'should', 'be', 'thinkin', 'about', 'him']\n",
      "After stop words removal: ['together', 'thinkin']\n",
      "After stemming with porters algorithm: ['togeth', 'thinkin']\n",
      "Tokenized sentence: ['hiya', 'had', 'a', 'good', 'day', 'have', 'you', 'spoken', 'to', 'since', 'the', 'weekend']\n",
      "After stop words removal: ['hiya', 'good', 'day', 'spoken', 'since', 'weekend']\n",
      "After stemming with porters algorithm: ['hiya', 'good', 'dai', 'spoken', 'sinc', 'weekend']\n",
      "Tokenized sentence: ['i', 'am', 'joining', 'today', 'formally', 'pls', 'keep', 'praying', 'will', 'talk', 'later']\n",
      "After stop words removal: ['joining', 'today', 'formally', 'pls', 'keep', 'praying', 'talk', 'later']\n",
      "join\n",
      "pray\n",
      "After stemming with porters algorithm: ['join', 'todai', 'formal', 'pl', 'keep', 'prai', 'talk', 'later']\n",
      "Tokenized sentence: ['i', 'know', 'complain', 'num', 'only', 'bettr', 'directly', 'go', 'to', 'bsnl', 'offc', 'nd', 'apply', 'for', 'it']\n",
      "After stop words removal: ['know', 'complain', 'num', 'bettr', 'directly', 'go', 'bsnl', 'offc', 'nd', 'apply']\n",
      "After stemming with porters algorithm: ['know', 'complain', 'num', 'bettr', 'directli', 'bsnl', 'offc', 'appli']\n",
      "Tokenized sentence: ['then', 'why', 'no', 'one', 'talking', 'to', 'me']\n",
      "After stop words removal: ['one', 'talking']\n",
      "talk\n",
      "After stemming with porters algorithm: ['on', 'tal']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'a', 'prize', 'to', 'claim', 'yr', 'prize', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'am', 'pm', 'cost', 'p']\n",
      "After stop words removal: ['guaranteed', 'cash', 'prize', 'claim', 'yr', 'prize', 'call', 'customer', 'service', 'representative', 'pm', 'cost', 'p']\n",
      "After stemming with porters algorithm: ['guaranteed', 'cash', 'priz', 'claim', 'priz', 'call', 'custom', 'servic', 'repres', 'cost']\n",
      "Tokenized sentence: ['you', 'have', 'to', 'pls', 'make', 'a', 'note', 'of', 'all', 'she', 's', 'exposed', 'to', 'also', 'find', 'out', 'from', 'her', 'school', 'if', 'anyone', 'else', 'was', 'vomiting', 'is', 'there', 'a', 'dog', 'or', 'cat', 'in', 'the', 'house', 'let', 'me', 'know', 'later']\n",
      "After stop words removal: ['pls', 'make', 'note', 'exposed', 'also', 'find', 'school', 'anyone', 'else', 'vomiting', 'dog', 'cat', 'house', 'let', 'know', 'later']\n",
      "vomit\n",
      "After stemming with porters algorithm: ['pl', 'make', 'note', 'expos', 'also', 'find', 'school', 'anyon', 'els', 'vomit', 'dog', 'cat', 'hous', 'let', 'know', 'later']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'a', 'prize', 'to', 'claim', 'yr', 'prize', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'am', 'pm', 'cost', 'p']\n",
      "After stop words removal: ['guaranteed', 'cash', 'prize', 'claim', 'yr', 'prize', 'call', 'customer', 'service', 'representative', 'pm', 'cost', 'p']\n",
      "After stemming with porters algorithm: ['guaranteed', 'cash', 'priz', 'claim', 'priz', 'call', 'custom', 'servic', 'repres', 'cost']\n",
      "Tokenized sentence: ['for', 'you', 'information', 'ikea', 'is', 'spelled', 'with', 'all', 'caps', 'that', 'is', 'not', 'yelling', 'when', 'you', 'thought', 'i', 'had', 'left', 'you', 'you', 'were', 'sitting', 'on', 'the', 'bed', 'among', 'the', 'mess', 'when', 'i', 'came', 'in', 'i', 'said', 'we', 'were', 'going', 'after', 'you', 'got', 'home', 'from', 'class', 'please', 'don', 't', 'try', 'and', 'bullshit', 'me', 'it', 'makes', 'me', 'want', 'to', 'listen', 'to', 'you', 'less']\n",
      "After stop words removal: ['information', 'ikea', 'spelled', 'caps', 'yelling', 'thought', 'left', 'sitting', 'bed', 'among', 'mess', 'came', 'said', 'going', 'got', 'home', 'class', 'please', 'try', 'bullshit', 'makes', 'want', 'listen', 'less']\n",
      "yell\n",
      "sitt\n",
      "go\n",
      "After stemming with porters algorithm: ['inform', 'ikea', 'spell', 'cap', 'yell', 'thought', 'left', 'sit', 'bed', 'among', 'mess', 'came', 'said', 'go', 'got', 'home', 'class', 'pleas', 'try', 'bullshit', 'make', 'want', 'listen', 'less']\n",
      "Tokenized sentence: ['sir', 'waiting', 'for', 'your', 'letter']\n",
      "After stop words removal: ['sir', 'waiting', 'letter']\n",
      "wait\n",
      "After stemming with porters algorithm: ['sir', 'wait', 'letter']\n",
      "Tokenized sentence: ['thats', 'cool', 'i', 'am', 'a', 'gentleman', 'and', 'will', 'treat', 'you', 'with', 'dignity', 'and', 'respect']\n",
      "After stop words removal: ['thats', 'cool', 'gentleman', 'treat', 'dignity', 'respect']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'gentleman', 'treat', 'digniti', 'respect']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'contact', 'you', 're', 'our', 'offer', 'of', 'new', 'video', 'phone', 'anytime', 'any', 'network', 'mins', 'half', 'price', 'rental', 'camcorder', 'call', 'or', 'reply', 'for', 'delivery', 'wed']\n",
      "After stop words removal: ['tried', 'contact', 'offer', 'new', 'video', 'phone', 'anytime', 'network', 'mins', 'half', 'price', 'rental', 'camcorder', 'call', 'reply', 'delivery', 'wed']\n",
      "After stemming with porters algorithm: ['tri', 'contact', 'offer', 'new', 'video', 'phone', 'anytim', 'network', 'min', 'half', 'price', 'rental', 'camcord', 'call', 'repli', 'deliveri', 'wed']\n",
      "Tokenized sentence: ['i', 'm', 'thinking', 'that', 'chennai', 'forgot', 'to', 'come', 'for', 'auction']\n",
      "After stop words removal: ['thinking', 'chennai', 'forgot', 'come', 'auction']\n",
      "think\n",
      "After stemming with porters algorithm: ['thin', 'chennai', 'forgot', 'come', 'auct']\n",
      "Tokenized sentence: ['nite']\n",
      "After stop words removal: ['nite']\n",
      "After stemming with porters algorithm: ['nite']\n",
      "Tokenized sentence: ['your', 'daily', 'text', 'from', 'me', 'a', 'favour', 'this', 'time']\n",
      "After stop words removal: ['daily', 'text', 'favour', 'time']\n",
      "After stemming with porters algorithm: ['daili', 'text', 'favour', 'time']\n",
      "Tokenized sentence: ['designation', 'is', 'software', 'developer', 'and', 'may', 'be', 'she', 'get', 'chennai']\n",
      "After stop words removal: ['designation', 'software', 'developer', 'may', 'get', 'chennai']\n",
      "After stemming with porters algorithm: ['design', 'softwar', 'develop', 'mai', 'get', 'chennai']\n",
      "Tokenized sentence: ['also', 'remember', 'the', 'beads', 'don', 't', 'come', 'off', 'ever']\n",
      "After stop words removal: ['also', 'remember', 'beads', 'come', 'ever']\n",
      "After stemming with porters algorithm: ['also', 'rememb', 'bead', 'come', 'ever']\n",
      "Tokenized sentence: ['wishing', 'you', 'a', 'beautiful', 'day', 'each', 'moment', 'revealing', 'even', 'more', 'things', 'to', 'keep', 'you', 'smiling', 'do', 'enjoy', 'it']\n",
      "After stop words removal: ['wishing', 'beautiful', 'day', 'moment', 'revealing', 'even', 'things', 'keep', 'smiling', 'enjoy']\n",
      "wish\n",
      "reveal\n",
      "smil\n",
      "After stemming with porters algorithm: ['wis', 'beauti', 'dai', 'moment', 'reveal', 'even', 'thing', 'keep', 'smile', 'enjoi']\n",
      "Tokenized sentence: ['customer', 'service', 'announcement', 'we', 'recently', 'tried', 'to', 'make', 'a', 'delivery', 'to', 'you', 'but', 'were', 'unable', 'to', 'do', 'so', 'please', 'call', 'to', 're', 'schedule', 'ref']\n",
      "After stop words removal: ['customer', 'service', 'announcement', 'recently', 'tried', 'make', 'delivery', 'unable', 'please', 'call', 'schedule', 'ref']\n",
      "After stemming with porters algorithm: ['custom', 'servic', 'announc', 'recent', 'tri', 'make', 'deliveri', 'unab', 'pleas', 'call', 'schedul', 'ref']\n",
      "Tokenized sentence: ['k', 'can', 'i', 'pick', 'up', 'another', 'th', 'when', 'you', 're', 'done']\n",
      "After stop words removal: ['k', 'pick', 'another', 'th', 'done']\n",
      "After stemming with porters algorithm: ['pick', 'anoth', 'done']\n",
      "Tokenized sentence: ['no', 'problem', 'talk', 'to', 'you', 'later']\n",
      "After stop words removal: ['problem', 'talk', 'later']\n",
      "After stemming with porters algorithm: ['problem', 'talk', 'later']\n",
      "Tokenized sentence: ['remind', 'me', 'how', 'to', 'get', 'there', 'and', 'i', 'shall', 'do', 'so']\n",
      "After stop words removal: ['remind', 'get', 'shall']\n",
      "After stemming with porters algorithm: ['remind', 'get', 'shall']\n",
      "Tokenized sentence: ['in', 'which', 'place', 'do', 'you', 'want', 'da']\n",
      "After stop words removal: ['place', 'want', 'da']\n",
      "After stemming with porters algorithm: ['place', 'want']\n",
      "Tokenized sentence: ['beerage']\n",
      "After stop words removal: ['beerage']\n",
      "After stemming with porters algorithm: ['beerag']\n",
      "Tokenized sentence: ['ha', 'both', 'of', 'us', 'doing', 'e', 'same', 'thing', 'but', 'i', 'got', 'tv', 'watch', 'u', 'can', 'thk', 'of', 'where', 'go', 'tonight', 'or', 'u', 'already', 'haf', 'smth', 'in', 'mind']\n",
      "After stop words removal: ['ha', 'us', 'e', 'thing', 'got', 'tv', 'watch', 'u', 'thk', 'go', 'tonight', 'u', 'already', 'haf', 'smth', 'mind']\n",
      "After stemming with porters algorithm: ['thing', 'got', 'watch', 'thk', 'tonight', 'alreadi', 'haf', 'smth', 'mind']\n",
      "Tokenized sentence: ['sweet', 'we', 'may', 'or', 'may', 'not', 'go', 'to', 'u', 'to', 'meet', 'carlos', 'so', 'gauge', 'patty', 's', 'interest', 'in', 'that']\n",
      "After stop words removal: ['sweet', 'may', 'may', 'go', 'u', 'meet', 'carlos', 'gauge', 'patty', 'interest']\n",
      "After stemming with porters algorithm: ['sweet', 'mai', 'mai', 'meet', 'carlo', 'gaug', 'patti', 'interest']\n",
      "Tokenized sentence: ['idea', 'will', 'soon', 'get', 'converted', 'to', 'live']\n",
      "After stop words removal: ['idea', 'soon', 'get', 'converted', 'live']\n",
      "After stemming with porters algorithm: ['idea', 'soon', 'get', 'conver', 'live']\n",
      "Tokenized sentence: ['unlimited', 'texts', 'limited', 'minutes']\n",
      "After stop words removal: ['unlimited', 'texts', 'limited', 'minutes']\n",
      "After stemming with porters algorithm: ['unlimit', 'text', 'limit', 'minut']\n",
      "Tokenized sentence: ['what', 'happen', 'to', 'her', 'tell', 'the', 'truth']\n",
      "After stop words removal: ['happen', 'tell', 'truth']\n",
      "After stemming with porters algorithm: ['happen', 'tell', 'truth']\n",
      "Tokenized sentence: ['only', 'once', 'then', 'after', 'ill', 'obey', 'all', 'yours']\n",
      "After stop words removal: ['ill', 'obey']\n",
      "After stemming with porters algorithm: ['ill', 'obei']\n",
      "Tokenized sentence: ['that', 's', 'y', 'we', 'haf', 'to', 'combine', 'n', 'c', 'how', 'lor']\n",
      "After stop words removal: ['haf', 'combine', 'n', 'c', 'lor']\n",
      "After stemming with porters algorithm: ['haf', 'combin', 'lor']\n",
      "Tokenized sentence: ['thats', 'cool', 'where', 'should', 'i', 'cum', 'on', 'you', 'or', 'in', 'you']\n",
      "After stop words removal: ['thats', 'cool', 'cum']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'cum']\n",
      "Tokenized sentence: ['did', 'he', 'say', 'how', 'fantastic', 'i', 'am', 'by', 'any', 'chance', 'or', 'anything', 'need', 'a', 'bigger', 'life', 'lift', 'as', 'losing', 'the', 'will', 'live', 'do', 'you', 'think', 'i', 'would', 'be', 'the', 'first', 'person', 'die', 'from', 'n', 'v', 'q']\n",
      "After stop words removal: ['say', 'fantastic', 'chance', 'anything', 'need', 'bigger', 'life', 'lift', 'losing', 'live', 'think', 'would', 'first', 'person', 'die', 'n', 'v', 'q']\n",
      "anyth\n",
      "los\n",
      "After stemming with porters algorithm: ['sai', 'fantast', 'chanc', 'anyt', 'need', 'bigger', 'life', 'lift', 'lose', 'live', 'think', 'would', 'first', 'person', 'die']\n",
      "Tokenized sentence: ['message', 'some', 'text', 'missing', 'sender', 'name', 'missing', 'number', 'missing', 'sent', 'date', 'missing', 'missing', 'u', 'a', 'lot', 'thats', 'y', 'everything', 'is', 'missing', 'sent', 'via', 'fullonsms', 'com']\n",
      "After stop words removal: ['message', 'text', 'missing', 'sender', 'name', 'missing', 'number', 'missing', 'sent', 'date', 'missing', 'missing', 'u', 'lot', 'thats', 'everything', 'missing', 'sent', 'via', 'fullonsms', 'com']\n",
      "miss\n",
      "miss\n",
      "miss\n",
      "miss\n",
      "miss\n",
      "everyth\n",
      "miss\n",
      "After stemming with porters algorithm: ['messag', 'text', 'miss', 'sender', 'name', 'miss', 'number', 'miss', 'sent', 'date', 'miss', 'miss', 'lot', 'that', 'everyt', 'miss', 'sent', 'via', 'fullonsm', 'com']\n",
      "Tokenized sentence: ['no', 'i', 'dont', 'want', 'to', 'hear', 'anything']\n",
      "After stop words removal: ['dont', 'want', 'hear', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['dont', 'want', 'hear', 'anyt']\n",
      "Tokenized sentence: ['so', 'i', 'could', 'kiss', 'and', 'feel', 'you', 'next', 'to', 'me']\n",
      "After stop words removal: ['could', 'kiss', 'feel', 'next']\n",
      "After stemming with porters algorithm: ['could', 'kiss', 'feel', 'next']\n",
      "Tokenized sentence: ['oops', 'i', 'thk', 'i', 'dun', 'haf', 'enuff', 'i', 'go', 'check', 'then', 'tell']\n",
      "After stop words removal: ['oops', 'thk', 'dun', 'haf', 'enuff', 'go', 'check', 'tell']\n",
      "After stemming with porters algorithm: ['oop', 'thk', 'dun', 'haf', 'enuff', 'check', 'tell']\n",
      "Tokenized sentence: ['call', 'him', 'and', 'say', 'you', 'not', 'coming', 'today', 'ok', 'and', 'tell', 'them', 'not', 'to', 'fool', 'me', 'like', 'this', 'ok']\n",
      "After stop words removal: ['call', 'say', 'coming', 'today', 'ok', 'tell', 'fool', 'like', 'ok']\n",
      "com\n",
      "After stemming with porters algorithm: ['call', 'sai', 'come', 'todai', 'tell', 'fool', 'like']\n",
      "Tokenized sentence: ['future', 'is', 'not', 'what', 'we', 'planned', 'for', 'tomorrow', 'it', 'is', 'the', 'result', 'of', 'what', 'we', 'do', 'today', 'do', 'the', 'best', 'in', 'present', 'enjoy', 'the', 'future']\n",
      "After stop words removal: ['future', 'planned', 'tomorrow', 'result', 'today', 'best', 'present', 'enjoy', 'future']\n",
      "After stemming with porters algorithm: ['futur', 'plan', 'tomorrow', 'result', 'todai', 'best', 'present', 'enjoi', 'futur']\n",
      "Tokenized sentence: ['do', 'you', 'realize', 'that', 'in', 'about', 'years', 'we', 'll', 'have', 'thousands', 'of', 'old', 'ladies', 'running', 'around', 'with', 'tattoos']\n",
      "After stop words removal: ['realize', 'years', 'thousands', 'old', 'ladies', 'running', 'around', 'tattoos']\n",
      "runn\n",
      "After stemming with porters algorithm: ['realiz', 'year', 'thousand', 'old', 'ladi', 'run', 'around', 'tattoo']\n",
      "Tokenized sentence: ['happy', 'birthday', 'may', 'u', 'find', 'ur', 'prince', 'charming', 'soon', 'n', 'dun', 'work', 'too', 'hard']\n",
      "After stop words removal: ['happy', 'birthday', 'may', 'u', 'find', 'ur', 'prince', 'charming', 'soon', 'n', 'dun', 'work', 'hard']\n",
      "charm\n",
      "After stemming with porters algorithm: ['happi', 'birthdai', 'mai', 'find', 'princ', 'char', 'soon', 'dun', 'work', 'hard']\n",
      "Tokenized sentence: ['hello', 'how', 'r', 'u', 'im', 'bored', 'inever', 'thought', 'id', 'get', 'bored', 'with', 'the', 'tv', 'but', 'i', 'am', 'tell', 'me', 'something', 'exciting', 'has', 'happened', 'there', 'anything']\n",
      "After stop words removal: ['hello', 'r', 'u', 'im', 'bored', 'inever', 'thought', 'id', 'get', 'bored', 'tv', 'tell', 'something', 'exciting', 'happened', 'anything']\n",
      "someth\n",
      "excit\n",
      "anyth\n",
      "After stemming with porters algorithm: ['hello', 'bore', 'inev', 'thought', 'get', 'bore', 'tell', 'somet', 'excit', 'happen', 'anyt']\n",
      "Tokenized sentence: ['i', 'bought', 'the', 'test', 'yesterday', 'its', 'something', 'that', 'lets', 'you', 'know', 'the', 'exact', 'day', 'u', 'ovulate', 'when', 'will', 'get', 'u', 'in', 'about', 'to', 'wks', 'but', 'pls', 'pls', 'dont', 'fret', 'i', 'know', 'u', 'r', 'worried', 'pls', 'relax', 'also', 'is', 'there', 'anything', 'in', 'ur', 'past', 'history', 'u', 'need', 'to', 'tell', 'me']\n",
      "After stop words removal: ['bought', 'test', 'yesterday', 'something', 'lets', 'know', 'exact', 'day', 'u', 'ovulate', 'get', 'u', 'wks', 'pls', 'pls', 'dont', 'fret', 'know', 'u', 'r', 'worried', 'pls', 'relax', 'also', 'anything', 'ur', 'past', 'history', 'u', 'need', 'tell']\n",
      "someth\n",
      "anyth\n",
      "After stemming with porters algorithm: ['bought', 'test', 'yesterdai', 'somet', 'let', 'know', 'exact', 'dai', 'ovul', 'get', 'wk', 'pl', 'pl', 'dont', 'fret', 'know', 'worri', 'pl', 'relax', 'also', 'anyt', 'past', 'histori', 'need', 'tell']\n",
      "Tokenized sentence: ['someone', 'has', 'contacted', 'our', 'dating', 'service', 'and', 'entered', 'your', 'phone', 'becausethey', 'fancy', 'you', 'to', 'find', 'out', 'who', 'it', 'is', 'call', 'from', 'a', 'landline', 'pobox', 'w', 'rg', 'p']\n",
      "After stop words removal: ['someone', 'contacted', 'dating', 'service', 'entered', 'phone', 'becausethey', 'fancy', 'find', 'call', 'landline', 'pobox', 'w', 'rg', 'p']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someon', 'contac', 'date', 'servic', 'enter', 'phone', 'becausethei', 'fanci', 'find', 'call', 'landlin', 'pobox']\n",
      "Tokenized sentence: ['that', 's', 'cause', 'your', 'old', 'i', 'live', 'to', 'be', 'high']\n",
      "After stop words removal: ['cause', 'old', 'live', 'high']\n",
      "After stemming with porters algorithm: ['caus', 'old', 'live', 'high']\n",
      "Tokenized sentence: ['haha', 'my', 'friend', 'tyler', 'literally', 'just', 'asked', 'if', 'you', 'could', 'get', 'him', 'a', 'dubsack']\n",
      "After stop words removal: ['haha', 'friend', 'tyler', 'literally', 'asked', 'could', 'get', 'dubsack']\n",
      "After stemming with porters algorithm: ['haha', 'friend', 'tyler', 'liter', 'as', 'could', 'get', 'dubsack']\n",
      "Tokenized sentence: ['just', 're', 'read', 'it', 'and', 'i', 'have', 'no', 'shame', 'but', 'tell', 'me', 'how', 'he', 'takes', 'it', 'and', 'if', 'he', 'runs', 'i', 'will', 'blame', 'u', 'ever', 'not', 'really', 'ever', 'just', 'a', 'long', 'time']\n",
      "After stop words removal: ['read', 'shame', 'tell', 'takes', 'runs', 'blame', 'u', 'ever', 'really', 'ever', 'long', 'time']\n",
      "After stemming with porters algorithm: ['read', 'shame', 'tell', 'take', 'run', 'blame', 'ever', 'realli', 'ever', 'long', 'time']\n",
      "Tokenized sentence: ['ya', 'i', 'm', 'referin', 'to', 'mei', 's', 'ex', 'wat', 'no', 'ah', 'waitin', 'u', 'to', 'treat', 'somebody', 'shld', 'b', 'rich', 'liao', 'so', 'gd', 'den', 'u', 'dun', 'have', 'to', 'work', 'frm', 'tmr', 'onwards']\n",
      "After stop words removal: ['ya', 'referin', 'mei', 'ex', 'wat', 'ah', 'waitin', 'u', 'treat', 'somebody', 'shld', 'b', 'rich', 'liao', 'gd', 'den', 'u', 'dun', 'work', 'frm', 'tmr', 'onwards']\n",
      "After stemming with porters algorithm: ['referin', 'mei', 'wat', 'waitin', 'treat', 'somebodi', 'shld', 'rich', 'liao', 'den', 'dun', 'work', 'frm', 'tmr', 'onward']\n",
      "Tokenized sentence: ['did', 'you', 'stitch', 'his', 'trouser']\n",
      "After stop words removal: ['stitch', 'trouser']\n",
      "After stemming with porters algorithm: ['stitch', 'trouser']\n",
      "Tokenized sentence: ['mm', 'feeling', 'sleepy', 'today', 'itself', 'i', 'shall', 'get', 'that', 'dear']\n",
      "After stop words removal: ['mm', 'feeling', 'sleepy', 'today', 'shall', 'get', 'dear']\n",
      "feel\n",
      "After stemming with porters algorithm: ['feel', 'sleepi', 'todai', 'shall', 'get', 'dear']\n",
      "Tokenized sentence: ['aft', 'i', 'finish', 'my', 'lunch', 'then', 'i', 'go', 'str', 'down', 'lor', 'ard', 'smth', 'lor', 'u', 'finish', 'ur', 'lunch', 'already']\n",
      "After stop words removal: ['aft', 'finish', 'lunch', 'go', 'str', 'lor', 'ard', 'smth', 'lor', 'u', 'finish', 'ur', 'lunch', 'already']\n",
      "After stemming with porters algorithm: ['aft', 'finish', 'lunch', 'str', 'lor', 'ard', 'smth', 'lor', 'finish', 'lunch', 'alreadi']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['yeah', 'there', 's', 'quite', 'a', 'bit', 'left', 'i', 'll', 'swing', 'by', 'tomorrow', 'when', 'i', 'get', 'up']\n",
      "After stop words removal: ['yeah', 'quite', 'bit', 'left', 'swing', 'tomorrow', 'get']\n",
      "After stemming with porters algorithm: ['yeah', 'quit', 'bit', 'left', 'swing', 'tomorrow', 'get']\n",
      "Tokenized sentence: ['free', 'camera', 'phones', 'with', 'linerental', 'from', 'month', 'with', 'cross', 'ntwk', 'mins', 'price', 'txt', 'bundle', 'deals', 'also', 'avble', 'call', 'or', 'call', 'optout', 'j', 'mf']\n",
      "After stop words removal: ['free', 'camera', 'phones', 'linerental', 'month', 'cross', 'ntwk', 'mins', 'price', 'txt', 'bundle', 'deals', 'also', 'avble', 'call', 'call', 'optout', 'j', 'mf']\n",
      "After stemming with porters algorithm: ['free', 'camera', 'phone', 'liner', 'month', 'cross', 'ntwk', 'min', 'price', 'txt', 'bundl', 'deal', 'also', 'avbl', 'call', 'call', 'optout']\n",
      "Tokenized sentence: ['oh', 'is', 'it', 'which', 'brand']\n",
      "After stop words removal: ['oh', 'brand']\n",
      "After stemming with porters algorithm: ['brand']\n",
      "Tokenized sentence: ['yeah', 'i', 'can', 'still', 'give', 'you', 'a', 'ride']\n",
      "After stop words removal: ['yeah', 'still', 'give', 'ride']\n",
      "After stemming with porters algorithm: ['yeah', 'still', 'give', 'ride']\n",
      "Tokenized sentence: ['that', 's', 'significant', 'but', 'dont', 'worry']\n",
      "After stop words removal: ['significant', 'dont', 'worry']\n",
      "After stemming with porters algorithm: ['signific', 'dont', 'worri']\n",
      "Tokenized sentence: ['no', 'got', 'new', 'job', 'at', 'bar', 'in', 'airport', 'on', 'satsgettin', 'per', 'hour', 'but', 'means', 'no', 'lie', 'in', 'keep', 'in', 'touch']\n",
      "After stop words removal: ['got', 'new', 'job', 'bar', 'airport', 'satsgettin', 'per', 'hour', 'means', 'lie', 'keep', 'touch']\n",
      "After stemming with porters algorithm: ['got', 'new', 'job', 'bar', 'airport', 'satsgettin', 'per', 'hour', 'mean', 'lie', 'keep', 'touch']\n",
      "Tokenized sentence: ['did', 'u', 'find', 'out', 'what', 'time', 'the', 'bus', 'is', 'at', 'coz', 'i', 'need', 'to', 'sort', 'some', 'stuff', 'out']\n",
      "After stop words removal: ['u', 'find', 'time', 'bus', 'coz', 'need', 'sort', 'stuff']\n",
      "After stemming with porters algorithm: ['find', 'time', 'bu', 'coz', 'need', 'sort', 'stuff']\n",
      "Tokenized sentence: ['cool', 'want', 'me', 'to', 'go', 'to', 'kappa', 'or', 'should', 'i', 'meet', 'you', 'outside', 'mu']\n",
      "After stop words removal: ['cool', 'want', 'go', 'kappa', 'meet', 'outside', 'mu']\n",
      "After stemming with porters algorithm: ['cool', 'want', 'kappa', 'meet', 'outsid']\n",
      "Tokenized sentence: ['i', 'feel', 'like', 'a', 'dick', 'because', 'i', 'keep', 'sleeping', 'through', 'your', 'texts', 'and', 'facebook', 'messages', 'sup', 'you', 'in', 'town']\n",
      "After stop words removal: ['feel', 'like', 'dick', 'keep', 'sleeping', 'texts', 'facebook', 'messages', 'sup', 'town']\n",
      "sleep\n",
      "After stemming with porters algorithm: ['feel', 'like', 'dick', 'keep', 'sleep', 'text', 'facebook', 'messag', 'sup', 'town']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'it', 'seriously', 'coz', 'being', 'angry', 'is', 'd', 'most', 'childish', 'n', 'true', 'way', 'of', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'have', 'nice', 'day', 'da']\n",
      "After stop words removal: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'seriously', 'coz', 'angry', 'childish', 'n', 'true', 'way', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'nice', 'day', 'da']\n",
      "show\n",
      "After stemming with porters algorithm: ['wen', 'lovab', 'bcum', 'angri', 'wid', 'dnt', 'take', 'serious', 'coz', 'angri', 'childish', 'true', 'wai', 'showe', 'deep', 'affect', 'care', 'luv', 'kettoda', 'manda', 'nice', 'dai']\n",
      "Tokenized sentence: ['at', 'esplanade', 'do', 'mind', 'giving', 'me', 'a', 'lift', 'cos', 'i', 'got', 'no', 'car', 'today']\n",
      "After stop words removal: ['esplanade', 'mind', 'giving', 'lift', 'cos', 'got', 'car', 'today']\n",
      "giv\n",
      "After stemming with porters algorithm: ['esplanad', 'mind', 'give', 'lift', 'co', 'got', 'car', 'todai']\n",
      "Tokenized sentence: ['u', 'gd', 'lor', 'go', 'shopping', 'i', 'got', 'stuff', 'to', 'do', 'u', 'wan', 'watch', 'infernal', 'affairs', 'a', 'not', 'come', 'lar']\n",
      "After stop words removal: ['u', 'gd', 'lor', 'go', 'shopping', 'got', 'stuff', 'u', 'wan', 'watch', 'infernal', 'affairs', 'come', 'lar']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['lor', 'shop', 'got', 'stuff', 'wan', 'watch', 'infern', 'affair', 'come', 'lar']\n",
      "Tokenized sentence: ['think', 'i', 'might', 'have', 'to', 'give', 'it', 'a', 'miss', 'am', 'teaching', 'til', 'twelve', 'then', 'have', 'lecture', 'at', 'two', 'damn', 'this', 'working', 'thing']\n",
      "After stop words removal: ['think', 'might', 'give', 'miss', 'teaching', 'til', 'twelve', 'lecture', 'two', 'damn', 'working', 'thing']\n",
      "teach\n",
      "work\n",
      "After stemming with porters algorithm: ['think', 'might', 'give', 'miss', 'teac', 'til', 'twelv', 'lectur', 'two', 'damn', 'wor', 'thing']\n",
      "Tokenized sentence: ['fuck', 'cedar', 'key', 'and', 'fuck', 'her', 'come', 'over', 'anyway', 'tho']\n",
      "After stop words removal: ['fuck', 'cedar', 'key', 'fuck', 'come', 'anyway', 'tho']\n",
      "After stemming with porters algorithm: ['fuck', 'cedar', 'kei', 'fuck', 'come', 'anywai', 'tho']\n",
      "Tokenized sentence: ['leaving', 'to', 'qatar', 'tonite', 'in', 'search', 'of', 'an', 'opportunity', 'all', 'went', 'fast', 'pls', 'add', 'me', 'in', 'ur', 'prayers', 'dear', 'rakhesh']\n",
      "After stop words removal: ['leaving', 'qatar', 'tonite', 'search', 'opportunity', 'went', 'fast', 'pls', 'add', 'ur', 'prayers', 'dear', 'rakhesh']\n",
      "leav\n",
      "After stemming with porters algorithm: ['leav', 'qatar', 'tonit', 'search', 'opportun', 'went', 'fast', 'pl', 'add', 'prayer', 'dear', 'rakhesh']\n",
      "Tokenized sentence: ['as', 'if', 'i', 'wasn', 't', 'having', 'enough', 'trouble', 'sleeping']\n",
      "After stop words removal: ['enough', 'trouble', 'sleeping']\n",
      "sleep\n",
      "After stemming with porters algorithm: ['enough', 'troubl', 'sleep']\n",
      "Tokenized sentence: ['how', 'dare', 'you', 'change', 'my', 'ring']\n",
      "After stop words removal: ['dare', 'change', 'ring']\n",
      "After stemming with porters algorithm: ['dare', 'chang', 'ring']\n",
      "Tokenized sentence: ['dear', 'will', 'call', 'tmorrow', 'pls', 'accomodate']\n",
      "After stop words removal: ['dear', 'call', 'tmorrow', 'pls', 'accomodate']\n",
      "After stemming with porters algorithm: ['dear', 'call', 'tmorrow', 'pl', 'accomod']\n",
      "Tokenized sentence: ['i', 'liked', 'the', 'new', 'mobile']\n",
      "After stop words removal: ['liked', 'new', 'mobile']\n",
      "After stemming with porters algorithm: ['like', 'new', 'mobil']\n",
      "Tokenized sentence: ['mm', 'that', 'time', 'you', 'dont', 'like', 'fun']\n",
      "After stop words removal: ['mm', 'time', 'dont', 'like', 'fun']\n",
      "After stemming with porters algorithm: ['time', 'dont', 'like', 'fun']\n",
      "Tokenized sentence: ['call', 'freephone', 'now']\n",
      "After stop words removal: ['call', 'freephone']\n",
      "After stemming with porters algorithm: ['call', 'freephon']\n",
      "Tokenized sentence: ['k', 'then', 'any', 'other', 'special']\n",
      "After stop words removal: ['k', 'special']\n",
      "After stemming with porters algorithm: ['special']\n",
      "Tokenized sentence: ['here', 'is', 'my', 'new', 'address', 'apples', 'pairs', 'all', 'that', 'malarky']\n",
      "After stop words removal: ['new', 'address', 'apples', 'pairs', 'malarky']\n",
      "After stemming with porters algorithm: ['new', 'address', 'appl', 'pair', 'malarki']\n",
      "Tokenized sentence: ['i', 'wanted', 'to', 'ask', 'to', 'wait', 'me', 'to', 'finish', 'lect', 'cos', 'my', 'lect', 'finishes', 'in', 'an', 'hour', 'anyway']\n",
      "After stop words removal: ['wanted', 'ask', 'wait', 'finish', 'lect', 'cos', 'lect', 'finishes', 'hour', 'anyway']\n",
      "After stemming with porters algorithm: ['wan', 'ask', 'wait', 'finish', 'lect', 'co', 'lect', 'finish', 'hour', 'anywai']\n",
      "Tokenized sentence: ['so', 'many', 'people', 'seems', 'to', 'be', 'special', 'at', 'first', 'sight', 'but', 'only', 'very', 'few', 'will', 'remain', 'special', 'to', 'you', 'till', 'your', 'last', 'sight', 'maintain', 'them', 'till', 'life', 'ends', 'sh', 'jas']\n",
      "After stop words removal: ['many', 'people', 'seems', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'ends', 'sh', 'jas']\n",
      "After stemming with porters algorithm: ['mani', 'peopl', 'seem', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'end', 'ja']\n",
      "Tokenized sentence: ['sorry', 'da', 'thangam', 'it', 's', 'my', 'mistake']\n",
      "After stop words removal: ['sorry', 'da', 'thangam', 'mistake']\n",
      "After stemming with porters algorithm: ['sorri', 'thangam', 'mistak']\n",
      "Tokenized sentence: ['as', 'per', 'your', 'request', 'maangalyam', 'alaipayuthe', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', 'to', 'copy', 'your', 'friends', 'callertune']\n",
      "After stop words removal: ['per', 'request', 'maangalyam', 'alaipayuthe', 'set', 'callertune', 'callers', 'press', 'copy', 'friends', 'callertune']\n",
      "After stemming with porters algorithm: ['per', 'request', 'maangalyam', 'alaipayuth', 'set', 'callertun', 'caller', 'press', 'copi', 'friend', 'callertun']\n",
      "Tokenized sentence: ['yo', 'carlos', 'a', 'few', 'friends', 'are', 'already', 'asking', 'me', 'about', 'you', 'you', 'working', 'at', 'all', 'this', 'weekend']\n",
      "After stop words removal: ['yo', 'carlos', 'friends', 'already', 'asking', 'working', 'weekend']\n",
      "ask\n",
      "work\n",
      "After stemming with porters algorithm: ['carlo', 'friend', 'alreadi', 'as', 'wor', 'weekend']\n",
      "Tokenized sentence: ['its', 'a', 'valentine', 'game', 'send', 'dis', 'msg', 'to', 'all', 'ur', 'friends', 'if', 'answers', 'r', 'd', 'same', 'then', 'someone', 'really', 'loves', 'u', 'ques', 'which', 'colour', 'suits', 'me', 'the', 'best']\n",
      "After stop words removal: ['valentine', 'game', 'send', 'dis', 'msg', 'ur', 'friends', 'answers', 'r', 'someone', 'really', 'loves', 'u', 'ques', 'colour', 'suits', 'best']\n",
      "After stemming with porters algorithm: ['valentin', 'game', 'send', 'di', 'msg', 'friend', 'answer', 'someon', 'realli', 'love', 'que', 'colour', 'suit', 'best']\n",
      "Tokenized sentence: ['the', 'hair', 'cream', 'has', 'not', 'been', 'shipped']\n",
      "After stop words removal: ['hair', 'cream', 'shipped']\n",
      "After stemming with porters algorithm: ['hair', 'cream', 'ship']\n",
      "Tokenized sentence: ['hi', 'ibh', 'customer', 'loyalty', 'offer', 'the', 'new', 'nokia', 'mobile', 'from', 'only', 'at', 'txtauction', 'txt', 'word', 'start', 'to', 'no', 'get', 'yours', 'now', 't']\n",
      "After stop words removal: ['hi', 'ibh', 'customer', 'loyalty', 'offer', 'new', 'nokia', 'mobile', 'txtauction', 'txt', 'word', 'start', 'get']\n",
      "After stemming with porters algorithm: ['ibh', 'custom', 'loyalti', 'offer', 'new', 'nokia', 'mobil', 'txtauct', 'txt', 'word', 'start', 'get']\n",
      "Tokenized sentence: ['could', 'you', 'not', 'read', 'me', 'my', 'love', 'i', 'answered', 'you']\n",
      "After stop words removal: ['could', 'read', 'love', 'answered']\n",
      "After stemming with porters algorithm: ['could', 'read', 'love', 'answer']\n",
      "Tokenized sentence: ['no', 'problem', 'how', 'are', 'you', 'doing']\n",
      "After stop words removal: ['problem']\n",
      "After stemming with porters algorithm: ['problem']\n",
      "Tokenized sentence: ['i', 'm', 'on', 'the', 'bus', 'love', 'you']\n",
      "After stop words removal: ['bus', 'love']\n",
      "After stemming with porters algorithm: ['bu', 'love']\n",
      "Tokenized sentence: ['i', 'will', 'come', 'tomorrow', 'di']\n",
      "After stop words removal: ['come', 'tomorrow', 'di']\n",
      "After stemming with porters algorithm: ['come', 'tomorrow']\n",
      "Tokenized sentence: ['i', 'think', 'it', 's', 'all', 'still', 'in', 'my', 'car']\n",
      "After stop words removal: ['think', 'still', 'car']\n",
      "After stemming with porters algorithm: ['think', 'still', 'car']\n",
      "Tokenized sentence: ['hi', 'wkend', 'ok', 'but', 'journey', 'terrible', 'wk', 'not', 'good', 'as', 'have', 'huge', 'back', 'log', 'of', 'marking', 'to', 'do']\n",
      "After stop words removal: ['hi', 'wkend', 'ok', 'journey', 'terrible', 'wk', 'good', 'huge', 'back', 'log', 'marking']\n",
      "mark\n",
      "After stemming with porters algorithm: ['wkend', 'journei', 'terrib', 'good', 'huge', 'back', 'log', 'mar']\n",
      "Tokenized sentence: ['hi', 'this', 'is', 'yijue', 'can', 'i', 'meet', 'u', 'at', 'tmr']\n",
      "After stop words removal: ['hi', 'yijue', 'meet', 'u', 'tmr']\n",
      "After stemming with porters algorithm: ['yiju', 'meet', 'tmr']\n",
      "Tokenized sentence: ['howz', 'pain', 'it', 'will', 'come', 'down', 'today', 'do', 'as', 'i', 'said', 'ystrday', 'ice', 'and', 'medicine']\n",
      "After stop words removal: ['howz', 'pain', 'come', 'today', 'said', 'ystrday', 'ice', 'medicine']\n",
      "After stemming with porters algorithm: ['howz', 'pain', 'come', 'todai', 'said', 'ystrdai', 'ic', 'medicin']\n",
      "Tokenized sentence: ['omw', 'back', 'to', 'tampa', 'from', 'west', 'palm', 'you', 'hear', 'what', 'happened']\n",
      "After stop words removal: ['omw', 'back', 'tampa', 'west', 'palm', 'hear', 'happened']\n",
      "After stemming with porters algorithm: ['omw', 'back', 'tampa', 'west', 'palm', 'hear', 'happen']\n",
      "Tokenized sentence: ['ugh', 'gotta', 'drive', 'back', 'to', 'sd', 'from', 'la', 'my', 'butt', 'is', 'sore']\n",
      "After stop words removal: ['ugh', 'gotta', 'drive', 'back', 'sd', 'la', 'butt', 'sore']\n",
      "After stemming with porters algorithm: ['ugh', 'gotta', 'drive', 'back', 'butt', 'sore']\n",
      "Tokenized sentence: ['discussed', 'with', 'your', 'mother', 'ah']\n",
      "After stop words removal: ['discussed', 'mother', 'ah']\n",
      "After stemming with porters algorithm: ['discuss', 'mother']\n",
      "Tokenized sentence: ['save', 'money', 'on', 'wedding', 'lingerie', 'at', 'www', 'bridal', 'petticoatdreams', 'co', 'uk', 'choose', 'from', 'a', 'superb', 'selection', 'with', 'national', 'delivery', 'brought', 'to', 'you', 'by', 'weddingfriend']\n",
      "After stop words removal: ['save', 'money', 'wedding', 'lingerie', 'www', 'bridal', 'petticoatdreams', 'co', 'uk', 'choose', 'superb', 'selection', 'national', 'delivery', 'brought', 'weddingfriend']\n",
      "wedd\n",
      "After stemming with porters algorithm: ['save', 'monei', 'wed', 'lingeri', 'www', 'bridal', 'petticoatdream', 'choos', 'superb', 'select', 'nat', 'deliveri', 'brought', 'weddingfriend']\n",
      "Tokenized sentence: ['call', 'me', 'i', 'am', 'senthil', 'from', 'hsbc']\n",
      "After stop words removal: ['call', 'senthil', 'hsbc']\n",
      "After stemming with porters algorithm: ['call', 'senthil', 'hsbc']\n",
      "Tokenized sentence: ['also', 'tell', 'him', 'i', 'said', 'happy', 'birthday']\n",
      "After stop words removal: ['also', 'tell', 'said', 'happy', 'birthday']\n",
      "After stemming with porters algorithm: ['also', 'tell', 'said', 'happi', 'birthdai']\n",
      "Tokenized sentence: ['bought', 'one', 'ringtone', 'and', 'now', 'getting', 'texts', 'costing', 'pound', 'offering', 'more', 'tones', 'etc']\n",
      "After stop words removal: ['bought', 'one', 'ringtone', 'getting', 'texts', 'costing', 'pound', 'offering', 'tones', 'etc']\n",
      "gett\n",
      "cost\n",
      "offer\n",
      "After stemming with porters algorithm: ['bought', 'on', 'rington', 'get', 'text', 'cos', 'pound', 'offer', 'tone', 'etc']\n",
      "Tokenized sentence: ['will', 'do', 'have', 'a', 'good', 'day']\n",
      "After stop words removal: ['good', 'day']\n",
      "After stemming with porters algorithm: ['good', 'dai']\n",
      "Tokenized sentence: ['today', 'iz', 'yellow', 'rose', 'day', 'if', 'u', 'love', 'my', 'frndship', 'give', 'me', 'misscall', 'amp', 'send', 'this', 'to', 'ur', 'frndz', 'amp', 'see', 'how', 'many', 'miss', 'calls', 'u', 'get', 'if', 'u', 'get', 'missed', 'u', 'marry', 'ur', 'lover']\n",
      "After stop words removal: ['today', 'iz', 'yellow', 'rose', 'day', 'u', 'love', 'frndship', 'give', 'misscall', 'amp', 'send', 'ur', 'frndz', 'amp', 'see', 'many', 'miss', 'calls', 'u', 'get', 'u', 'get', 'missed', 'u', 'marry', 'ur', 'lover']\n",
      "After stemming with porters algorithm: ['todai', 'yellow', 'rose', 'dai', 'love', 'frndship', 'give', 'misscal', 'amp', 'send', 'frndz', 'amp', 'see', 'mani', 'miss', 'call', 'get', 'get', 'miss', 'marri', 'lover']\n",
      "Tokenized sentence: ['company', 'is', 'very', 'good', 'environment', 'is', 'terrific', 'and', 'food', 'is', 'really', 'nice']\n",
      "After stop words removal: ['company', 'good', 'environment', 'terrific', 'food', 'really', 'nice']\n",
      "After stemming with porters algorithm: ['compani', 'good', 'environ', 'terrif', 'food', 'realli', 'nice']\n",
      "Tokenized sentence: ['tomorrow', 'i', 'am', 'not', 'going', 'to', 'theatre', 'so', 'i', 'can', 'come', 'wherever', 'u', 'call', 'me', 'tell', 'me', 'where', 'and', 'when', 'to', 'come', 'tomorrow']\n",
      "After stop words removal: ['tomorrow', 'going', 'theatre', 'come', 'wherever', 'u', 'call', 'tell', 'come', 'tomorrow']\n",
      "go\n",
      "After stemming with porters algorithm: ['tomorrow', 'go', 'theatr', 'come', 'wherev', 'call', 'tell', 'come', 'tomorrow']\n",
      "Tokenized sentence: ['hi', 'happy', 'birthday', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi']\n",
      "After stop words removal: ['hi', 'happy', 'birthday', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi']\n",
      "After stemming with porters algorithm: ['happi', 'birthdai']\n",
      "Tokenized sentence: ['k', 'i', 'did', 't', 'see', 'you', 'k', 'where', 'are', 'you', 'now']\n",
      "After stop words removal: ['k', 'see', 'k']\n",
      "After stemming with porters algorithm: ['see']\n",
      "Tokenized sentence: ['i', 'will', 'come', 'to', 'ur', 'home', 'now']\n",
      "After stop words removal: ['come', 'ur', 'home']\n",
      "After stemming with porters algorithm: ['come', 'home']\n",
      "Tokenized sentence: ['okey', 'doke', 'i', 'm', 'at', 'home', 'but', 'not', 'dressed', 'cos', 'laying', 'around', 'ill', 'speak', 'to', 'you', 'later', 'bout', 'times', 'and', 'stuff']\n",
      "After stop words removal: ['okey', 'doke', 'home', 'dressed', 'cos', 'laying', 'around', 'ill', 'speak', 'later', 'bout', 'times', 'stuff']\n",
      "lay\n",
      "After stemming with porters algorithm: ['okei', 'doke', 'home', 'dress', 'co', 'lai', 'around', 'ill', 'speak', 'later', 'bout', 'time', 'stuff']\n",
      "Tokenized sentence: ['i', 'can', 't', 'speak', 'bcaz', 'mobile', 'have', 'problem', 'i', 'can', 'listen', 'you', 'but', 'you', 'cann', 't', 'listen', 'my', 'voice', 'so', 'i', 'calls', 'you', 'later']\n",
      "After stop words removal: ['speak', 'bcaz', 'mobile', 'problem', 'listen', 'cann', 'listen', 'voice', 'calls', 'later']\n",
      "After stemming with porters algorithm: ['speak', 'bcaz', 'mobil', 'problem', 'listen', 'cann', 'listen', 'voic', 'call', 'later']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'bus', 'on', 'the', 'way', 'to', 'calicut']\n",
      "After stop words removal: ['bus', 'way', 'calicut']\n",
      "After stemming with porters algorithm: ['bu', 'wai', 'calicut']\n",
      "Tokenized sentence: ['good', 'night', 'my', 'dear', 'sleepwell', 'amp', 'take', 'care']\n",
      "After stop words removal: ['good', 'night', 'dear', 'sleepwell', 'amp', 'take', 'care']\n",
      "After stemming with porters algorithm: ['good', 'night', 'dear', 'sleepwel', 'amp', 'take', 'care']\n",
      "Tokenized sentence: ['lol', 'i', 'know', 'they', 're', 'so', 'dramatic', 'schools', 'already', 'closed', 'for', 'tomorrow', 'apparently', 'we', 'can', 't', 'drive', 'in', 'the', 'inch', 'of', 'snow', 'were', 'supposed', 'to', 'get']\n",
      "After stop words removal: ['lol', 'know', 'dramatic', 'schools', 'already', 'closed', 'tomorrow', 'apparently', 'drive', 'inch', 'snow', 'supposed', 'get']\n",
      "After stemming with porters algorithm: ['lol', 'know', 'dramat', 'school', 'alreadi', 'close', 'tomorrow', 'appar', 'drive', 'inch', 'snow', 'suppos', 'get']\n",
      "Tokenized sentence: ['dear', 'got', 'bus', 'directly', 'to', 'calicut']\n",
      "After stop words removal: ['dear', 'got', 'bus', 'directly', 'calicut']\n",
      "After stemming with porters algorithm: ['dear', 'got', 'bu', 'directli', 'calicut']\n",
      "Tokenized sentence: ['piggy', 'r', 'u', 'awake', 'i', 'bet', 'u', 're', 'still', 'sleeping', 'i', 'm', 'going', 'lunch', 'now']\n",
      "After stop words removal: ['piggy', 'r', 'u', 'awake', 'bet', 'u', 'still', 'sleeping', 'going', 'lunch']\n",
      "sleep\n",
      "go\n",
      "After stemming with porters algorithm: ['piggi', 'awak', 'bet', 'still', 'sleep', 'go', 'lunch']\n",
      "Tokenized sentence: ['he', 's', 'in', 'lag', 'that', 's', 'just', 'the', 'sad', 'part', 'but', 'we', 'keep', 'in', 'touch', 'thanks', 'to', 'skype']\n",
      "After stop words removal: ['lag', 'sad', 'part', 'keep', 'touch', 'thanks', 'skype']\n",
      "After stemming with porters algorithm: ['lag', 'sad', 'part', 'keep', 'touch', 'thank', 'skype']\n",
      "Tokenized sentence: ['guess', 'he', 'wants', 'alone', 'time', 'we', 'could', 'just', 'show', 'up', 'and', 'watch', 'when', 'they', 'do']\n",
      "After stop words removal: ['guess', 'wants', 'alone', 'time', 'could', 'show', 'watch']\n",
      "After stemming with porters algorithm: ['guess', 'want', 'alon', 'time', 'could', 'show', 'watch']\n",
      "Tokenized sentence: ['have', 'you', 'not', 'finished', 'work', 'yet', 'or', 'something']\n",
      "After stop words removal: ['finished', 'work', 'yet', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['finis', 'work', 'yet', 'somet']\n",
      "Tokenized sentence: ['sorry', 'left', 'phone', 'upstairs', 'ok', 'might', 'be', 'hectic', 'but', 'would', 'be', 'all', 'my', 'birds', 'with', 'one', 'fell', 'swoop', 'it', 's', 'a', 'date']\n",
      "After stop words removal: ['sorry', 'left', 'phone', 'upstairs', 'ok', 'might', 'hectic', 'would', 'birds', 'one', 'fell', 'swoop', 'date']\n",
      "After stemming with porters algorithm: ['sorri', 'left', 'phone', 'upstair', 'might', 'hectic', 'would', 'bird', 'on', 'fell', 'swoop', 'date']\n",
      "Tokenized sentence: ['lemme', 'know', 'when', 'you', 're', 'here']\n",
      "After stop words removal: ['lemme', 'know']\n",
      "After stemming with porters algorithm: ['lemm', 'know']\n",
      "Tokenized sentence: ['do', 'you', 'think', 'i', 'can', 'move', 'lt', 'gt', 'in', 'a', 'week']\n",
      "After stop words removal: ['think', 'move', 'lt', 'gt', 'week']\n",
      "After stemming with porters algorithm: ['think', 'move', 'week']\n",
      "Tokenized sentence: ['infact', 'happy', 'new', 'year', 'how', 'are', 'you', 'where', 'are', 'you', 'when', 'are', 'we', 'seeing']\n",
      "After stop words removal: ['infact', 'happy', 'new', 'year', 'seeing']\n",
      "see\n",
      "After stemming with porters algorithm: ['infact', 'happi', 'new', 'year', 'see']\n",
      "Tokenized sentence: ['that', 's', 'good', 'lets', 'thank', 'god', 'please', 'complete', 'the', 'drug', 'have', 'lots', 'of', 'water', 'and', 'have', 'a', 'beautiful', 'day']\n",
      "After stop words removal: ['good', 'lets', 'thank', 'god', 'please', 'complete', 'drug', 'lots', 'water', 'beautiful', 'day']\n",
      "After stemming with porters algorithm: ['good', 'let', 'thank', 'god', 'pleas', 'complet', 'drug', 'lot', 'water', 'beauti', 'dai']\n",
      "Tokenized sentence: ['that', 'way', 'transport', 'is', 'less', 'problematic', 'than', 'on', 'sat', 'night', 'by', 'the', 'way', 'if', 'u', 'want', 'to', 'ask', 'n', 'to', 'join', 'my', 'bday', 'feel', 'free', 'but', 'need', 'to', 'know', 'definite', 'nos', 'as', 'booking', 'on', 'fri']\n",
      "After stop words removal: ['way', 'transport', 'less', 'problematic', 'sat', 'night', 'way', 'u', 'want', 'ask', 'n', 'join', 'bday', 'feel', 'free', 'need', 'know', 'definite', 'nos', 'booking', 'fri']\n",
      "book\n",
      "After stemming with porters algorithm: ['wai', 'transport', 'less', 'problemat', 'sat', 'night', 'wai', 'want', 'ask', 'join', 'bdai', 'feel', 'free', 'need', 'know', 'definit', 'no', 'book', 'fri']\n",
      "Tokenized sentence: ['yo', 'call', 'me', 'when', 'you', 'get', 'the', 'chance', 'a', 'friend', 'of', 'mine', 'wanted', 'me', 'to', 'ask', 'you', 'about', 'a', 'big', 'order']\n",
      "After stop words removal: ['yo', 'call', 'get', 'chance', 'friend', 'mine', 'wanted', 'ask', 'big', 'order']\n",
      "After stemming with porters algorithm: ['call', 'get', 'chanc', 'friend', 'mine', 'wan', 'ask', 'big', 'order']\n",
      "Tokenized sentence: ['went', 'fast', 'asleep', 'dear', 'take', 'care']\n",
      "After stop words removal: ['went', 'fast', 'asleep', 'dear', 'take', 'care']\n",
      "After stemming with porters algorithm: ['went', 'fast', 'asleep', 'dear', 'take', 'care']\n",
      "Tokenized sentence: ['promotion', 'number', 'ur', 'awarded', 'a', 'city', 'break', 'and', 'could', 'win', 'a', 'summer', 'shopping', 'spree', 'every', 'wk', 'txt', 'store', 'to', 'skilgme', 'tscs', 'winawk', 'age', 'perwksub']\n",
      "After stop words removal: ['promotion', 'number', 'ur', 'awarded', 'city', 'break', 'could', 'win', 'summer', 'shopping', 'spree', 'every', 'wk', 'txt', 'store', 'skilgme', 'tscs', 'winawk', 'age', 'perwksub']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['promot', 'number', 'awar', 'citi', 'break', 'could', 'win', 'summer', 'shop', 'spree', 'everi', 'txt', 'store', 'skilgm', 'tsc', 'winawk', 'ag', 'perwksub']\n",
      "Tokenized sentence: ['ok', 'be', 'careful', 'don', 't', 'text', 'and', 'drive']\n",
      "After stop words removal: ['ok', 'careful', 'text', 'drive']\n",
      "After stemming with porters algorithm: ['care', 'text', 'drive']\n",
      "Tokenized sentence: ['tell', 'them', 'u', 'have', 'a', 'headache', 'and', 'just', 'want', 'to', 'use', 'hour', 'of', 'sick', 'time']\n",
      "After stop words removal: ['tell', 'u', 'headache', 'want', 'use', 'hour', 'sick', 'time']\n",
      "After stemming with porters algorithm: ['tell', 'headach', 'want', 'us', 'hour', 'sick', 'time']\n",
      "Tokenized sentence: ['if', 'we', 'hit', 'it', 'off', 'you', 'can', 'move', 'in', 'with', 'me']\n",
      "After stop words removal: ['hit', 'move']\n",
      "After stemming with porters algorithm: ['hit', 'move']\n",
      "Tokenized sentence: ['how', 'much', 'would', 'it', 'cost', 'to', 'hire', 'a', 'hitman']\n",
      "After stop words removal: ['much', 'would', 'cost', 'hire', 'hitman']\n",
      "After stemming with porters algorithm: ['much', 'would', 'cost', 'hire', 'hitman']\n",
      "Tokenized sentence: ['hi', 'there', 'we', 'have', 'now', 'moved', 'in', 'our', 'pub', 'would', 'be', 'great', 'c', 'u', 'if', 'u', 'cud', 'come', 'up']\n",
      "After stop words removal: ['hi', 'moved', 'pub', 'would', 'great', 'c', 'u', 'u', 'cud', 'come']\n",
      "After stemming with porters algorithm: ['move', 'pub', 'would', 'great', 'cud', 'come']\n",
      "Tokenized sentence: ['i', 'm', 'awake', 'oh', 'what', 's', 'up']\n",
      "After stop words removal: ['awake', 'oh']\n",
      "After stemming with porters algorithm: ['awak']\n",
      "Tokenized sentence: ['sent', 'me', 'de', 'webadres', 'for', 'geting', 'salary', 'slip']\n",
      "After stop words removal: ['sent', 'de', 'webadres', 'geting', 'salary', 'slip']\n",
      "get\n",
      "After stemming with porters algorithm: ['sent', 'webadr', 'gete', 'salari', 'slip']\n",
      "Tokenized sentence: ['night', 'has', 'ended', 'for', 'another', 'day', 'morning', 'has', 'come', 'in', 'a', 'special', 'way', 'may', 'you', 'smile', 'like', 'the', 'sunny', 'rays', 'and', 'leaves', 'your', 'worries', 'at', 'the', 'blue', 'blue', 'bay', 'gud', 'mrng']\n",
      "After stop words removal: ['night', 'ended', 'another', 'day', 'morning', 'come', 'special', 'way', 'may', 'smile', 'like', 'sunny', 'rays', 'leaves', 'worries', 'blue', 'blue', 'bay', 'gud', 'mrng']\n",
      "morn\n",
      "After stemming with porters algorithm: ['night', 'en', 'anoth', 'dai', 'mor', 'come', 'special', 'wai', 'mai', 'smile', 'like', 'sunni', 'rai', 'leav', 'worri', 'blue', 'blue', 'bai', 'gud', 'mrng']\n",
      "Tokenized sentence: ['ya', 'but', 'it', 'cant', 'display', 'internal', 'subs', 'so', 'i', 'gotta', 'extract', 'them']\n",
      "After stop words removal: ['ya', 'cant', 'display', 'internal', 'subs', 'gotta', 'extract']\n",
      "After stemming with porters algorithm: ['cant', 'displai', 'intern', 'sub', 'gotta', 'extract']\n",
      "Tokenized sentence: ['i', 'm', 'okay', 'chasing', 'the', 'dream', 'what', 's', 'good', 'what', 'are', 'you', 'doing', 'next']\n",
      "After stop words removal: ['okay', 'chasing', 'dream', 'good', 'next']\n",
      "chas\n",
      "After stemming with porters algorithm: ['okai', 'chase', 'dream', 'good', 'next']\n",
      "Tokenized sentence: ['sorry', 'i', 'cant', 'take', 'your', 'call', 'right', 'now', 'it', 'so', 'happens', 'that', 'there', 'r', 'waxsto', 'do', 'wat', 'you', 'want', 'she', 'can', 'come', 'and', 'ill', 'get', 'her', 'medical', 'insurance', 'and', 'she', 'll', 'be', 'able', 'to', 'deliver', 'and', 'have', 'basic', 'care', 'i', 'm', 'currently', 'shopping', 'for', 'the', 'right', 'medical', 'insurance', 'for', 'her', 'so', 'just', 'give', 'me', 'til', 'friday', 'morning', 'thats', 'when', 'i', 'll', 'see', 'the', 'major', 'person', 'that', 'can', 'guide', 'me', 'to', 'the', 'right', 'insurance']\n",
      "After stop words removal: ['sorry', 'cant', 'take', 'call', 'right', 'happens', 'r', 'waxsto', 'wat', 'want', 'come', 'ill', 'get', 'medical', 'insurance', 'able', 'deliver', 'basic', 'care', 'currently', 'shopping', 'right', 'medical', 'insurance', 'give', 'til', 'friday', 'morning', 'thats', 'see', 'major', 'person', 'guide', 'right', 'insurance']\n",
      "shopp\n",
      "morn\n",
      "After stemming with porters algorithm: ['sorri', 'cant', 'take', 'call', 'right', 'happen', 'waxsto', 'wat', 'want', 'come', 'ill', 'get', 'medic', 'insur', 'abl', 'deliv', 'basic', 'care', 'current', 'shop', 'right', 'medic', 'insur', 'give', 'til', 'fridai', 'mor', 'that', 'see', 'major', 'person', 'guid', 'right', 'insur']\n",
      "Tokenized sentence: ['i', 'came', 'hostel', 'i', 'm', 'going', 'to', 'sleep', 'plz', 'call', 'me', 'up', 'before', 'class', 'hrishi']\n",
      "After stop words removal: ['came', 'hostel', 'going', 'sleep', 'plz', 'call', 'class', 'hrishi']\n",
      "go\n",
      "After stemming with porters algorithm: ['came', 'hostel', 'go', 'sleep', 'plz', 'call', 'class', 'hrishi']\n",
      "Tokenized sentence: ['you', 'tell', 'what', 'happen', 'dont', 'behave', 'like', 'this', 'to', 'me', 'ok', 'no', 'need', 'to', 'say']\n",
      "After stop words removal: ['tell', 'happen', 'dont', 'behave', 'like', 'ok', 'need', 'say']\n",
      "After stemming with porters algorithm: ['tell', 'happen', 'dont', 'behav', 'like', 'need', 'sai']\n",
      "Tokenized sentence: ['don', 't', 'think', 'about', 'what', 'u', 'have', 'got', 'think', 'about', 'how', 'to', 'use', 'it', 'that', 'you', 'have', 'got', 'good', 'ni']\n",
      "After stop words removal: ['think', 'u', 'got', 'think', 'use', 'got', 'good', 'ni']\n",
      "After stemming with porters algorithm: ['think', 'got', 'think', 'us', 'got', 'good']\n",
      "Tokenized sentence: ['what', 'today', 'sunday', 'sunday', 'is', 'holiday', 'so', 'no', 'work']\n",
      "After stop words removal: ['today', 'sunday', 'sunday', 'holiday', 'work']\n",
      "After stemming with porters algorithm: ['todai', 'sundai', 'sundai', 'holidai', 'work']\n",
      "Tokenized sentence: ['why', 'de', 'you', 'looking', 'good', 'only']\n",
      "After stop words removal: ['de', 'looking', 'good']\n",
      "look\n",
      "After stemming with porters algorithm: ['look', 'good']\n",
      "Tokenized sentence: ['do', 'u', 'hav', 'any', 'frnd', 'by', 'name', 'ashwini', 'in', 'ur', 'college']\n",
      "After stop words removal: ['u', 'hav', 'frnd', 'name', 'ashwini', 'ur', 'college']\n",
      "After stemming with porters algorithm: ['hav', 'frnd', 'name', 'ashwini', 'colleg']\n",
      "Tokenized sentence: ['does', 'uncle', 'timi', 'help', 'in', 'clearing', 'cars']\n",
      "After stop words removal: ['uncle', 'timi', 'help', 'clearing', 'cars']\n",
      "clear\n",
      "After stemming with porters algorithm: ['uncl', 'timi', 'help', 'clear', 'car']\n",
      "Tokenized sentence: ['just', 'send', 'a', 'text', 'we', 'll', 'skype', 'later']\n",
      "After stop words removal: ['send', 'text', 'skype', 'later']\n",
      "After stemming with porters algorithm: ['send', 'text', 'skype', 'later']\n",
      "Tokenized sentence: ['sir', 'i', 'am', 'waiting', 'for', 'your', 'mail']\n",
      "After stop words removal: ['sir', 'waiting', 'mail']\n",
      "wait\n",
      "After stemming with porters algorithm: ['sir', 'wait', 'mail']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'doing', 'how', 's', 'the', 'queen', 'are', 'you', 'going', 'for', 'the', 'royal', 'wedding']\n",
      "After stop words removal: ['queen', 'going', 'royal', 'wedding']\n",
      "go\n",
      "wedd\n",
      "After stemming with porters algorithm: ['queen', 'go', 'royal', 'wed']\n",
      "Tokenized sentence: ['haha', 'really', 'oh', 'no', 'how', 'then', 'will', 'they', 'deduct', 'your', 'lesson', 'tmr']\n",
      "After stop words removal: ['haha', 'really', 'oh', 'deduct', 'lesson', 'tmr']\n",
      "After stemming with porters algorithm: ['haha', 'realli', 'deduct', 'lesson', 'tmr']\n",
      "Tokenized sentence: ['genius', 'what', 's', 'up', 'how', 'your', 'brother', 'pls', 'send', 'his', 'number', 'to', 'my', 'skype']\n",
      "After stop words removal: ['genius', 'brother', 'pls', 'send', 'number', 'skype']\n",
      "After stemming with porters algorithm: ['geniu', 'brother', 'pl', 'send', 'number', 'skype']\n",
      "Tokenized sentence: ['idk', 'i', 'm', 'sitting', 'here', 'in', 'a', 'stop', 'and', 'shop', 'parking', 'lot', 'right', 'now', 'bawling', 'my', 'eyes', 'out', 'because', 'i', 'feel', 'like', 'i', 'm', 'a', 'failure', 'in', 'everything', 'nobody', 'wants', 'me', 'and', 'now', 'i', 'feel', 'like', 'i', 'm', 'failing', 'you']\n",
      "After stop words removal: ['idk', 'sitting', 'stop', 'shop', 'parking', 'lot', 'right', 'bawling', 'eyes', 'feel', 'like', 'failure', 'everything', 'nobody', 'wants', 'feel', 'like', 'failing']\n",
      "sitt\n",
      "park\n",
      "bawl\n",
      "everyth\n",
      "fail\n",
      "After stemming with porters algorithm: ['idk', 'sit', 'stop', 'shop', 'par', 'lot', 'right', 'bawl', 'ey', 'feel', 'like', 'failur', 'everyt', 'nobodi', 'want', 'feel', 'like', 'fail']\n",
      "Tokenized sentence: ['free', 'ringtone', 'reply', 'real', 'or', 'poly', 'eg', 'real', 'pushbutton', 'dontcha', 'babygoodbye', 'golddigger', 'webeburnin', 'st', 'tone', 'free', 'and', 'more', 'when', 'u', 'join', 'for', 'wk']\n",
      "After stop words removal: ['free', 'ringtone', 'reply', 'real', 'poly', 'eg', 'real', 'pushbutton', 'dontcha', 'babygoodbye', 'golddigger', 'webeburnin', 'st', 'tone', 'free', 'u', 'join', 'wk']\n",
      "After stemming with porters algorithm: ['free', 'rington', 'repli', 'real', 'poli', 'real', 'pushbutton', 'dontcha', 'babygoodby', 'golddigg', 'webeburnin', 'tone', 'free', 'join']\n",
      "Tokenized sentence: ['smith', 'waste', 'da', 'i', 'wanna', 'gayle']\n",
      "After stop words removal: ['smith', 'waste', 'da', 'wanna', 'gayle']\n",
      "After stemming with porters algorithm: ['smith', 'wast', 'wanna', 'gayl']\n",
      "Tokenized sentence: ['can', 'u', 'get', 'pic', 'msgs', 'to', 'your', 'phone']\n",
      "After stop words removal: ['u', 'get', 'pic', 'msgs', 'phone']\n",
      "After stemming with porters algorithm: ['get', 'pic', 'msg', 'phone']\n",
      "Tokenized sentence: ['what', 'is', 'this', 'hex', 'place', 'you', 'talk', 'of', 'explain']\n",
      "After stop words removal: ['hex', 'place', 'talk', 'explain']\n",
      "After stemming with porters algorithm: ['hex', 'place', 'talk', 'explain']\n",
      "Tokenized sentence: ['how', 'long', 'does', 'it', 'take', 'to', 'get', 'it']\n",
      "After stop words removal: ['long', 'take', 'get']\n",
      "After stemming with porters algorithm: ['long', 'take', 'get']\n",
      "Tokenized sentence: ['reason', 'is', 'if', 'the', 'team', 'budget', 'is', 'available', 'at', 'last', 'they', 'buy', 'the', 'unsold', 'players', 'for', 'at', 'base', 'rate']\n",
      "After stop words removal: ['reason', 'team', 'budget', 'available', 'last', 'buy', 'unsold', 'players', 'base', 'rate']\n",
      "After stemming with porters algorithm: ['reason', 'team', 'budget', 'avail', 'last', 'bui', 'unsold', 'player', 'base', 'rate']\n",
      "Tokenized sentence: ['no', 'thank', 'you', 'you', 've', 'been', 'wonderful']\n",
      "After stop words removal: ['thank', 'wonderful']\n",
      "After stemming with porters algorithm: ['thank', 'wonder']\n",
      "Tokenized sentence: ['thats', 'cool', 'i', 'want', 'to', 'please', 'you']\n",
      "After stop words removal: ['thats', 'cool', 'want', 'please']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'want', 'pleas']\n",
      "Tokenized sentence: ['yeah', 'we', 'can', 'probably', 'swing', 'by', 'once', 'my', 'roommate', 'finishes', 'up', 'with', 'his', 'girl']\n",
      "After stop words removal: ['yeah', 'probably', 'swing', 'roommate', 'finishes', 'girl']\n",
      "After stemming with porters algorithm: ['yeah', 'probab', 'swing', 'roommat', 'finish', 'girl']\n",
      "Tokenized sentence: ['nah', 'wednesday', 'when', 'should', 'i', 'bring', 'the', 'mini', 'cheetos', 'bag', 'over']\n",
      "After stop words removal: ['nah', 'wednesday', 'bring', 'mini', 'cheetos', 'bag']\n",
      "After stemming with porters algorithm: ['nah', 'wednesdai', 'bring', 'mini', 'cheeto', 'bag']\n",
      "Tokenized sentence: ['you', 'were', 'supposed', 'to', 'wake', 'me', 'up', 'gt']\n",
      "After stop words removal: ['supposed', 'wake', 'gt']\n",
      "After stemming with porters algorithm: ['suppos', 'wake']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'office', 'now', 'i', 'will', 'call', 'you', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['office', 'call', 'lt', 'gt', 'min']\n",
      "After stemming with porters algorithm: ['offic', 'call', 'min']\n",
      "Tokenized sentence: ['so', 'is', 'th', 'gower', 'mate', 'which', 'is', 'where', 'i', 'am', 'how', 'r', 'u', 'man', 'all', 'is', 'good', 'in', 'wales', 'ill', 'b', 'back', 'morrow', 'c', 'u', 'this', 'wk', 'who', 'was', 'the', 'msg', 'random']\n",
      "After stop words removal: ['th', 'gower', 'mate', 'r', 'u', 'man', 'good', 'wales', 'ill', 'b', 'back', 'morrow', 'c', 'u', 'wk', 'msg', 'random']\n",
      "After stemming with porters algorithm: ['gower', 'mate', 'man', 'good', 'wale', 'ill', 'back', 'morrow', 'msg', 'random']\n",
      "Tokenized sentence: ['what', 'happened', 'in', 'interview']\n",
      "After stop words removal: ['happened', 'interview']\n",
      "After stemming with porters algorithm: ['happen', 'interview']\n",
      "Tokenized sentence: ['congratulations', 'ur', 'awarded', 'of', 'cd', 'vouchers', 'or', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music', 'to', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stop words removal: ['congratulations', 'ur', 'awarded', 'cd', 'vouchers', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stemming with porters algorithm: ['congratul', 'awar', 'voucher', 'gift', 'guaranteed', 'free', 'entri', 'wkly', 'draw', 'txt', 'music', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'ag']\n",
      "Tokenized sentence: ['no', 'da', 'today', 'also', 'i', 'forgot']\n",
      "After stop words removal: ['da', 'today', 'also', 'forgot']\n",
      "After stemming with porters algorithm: ['todai', 'also', 'forgot']\n",
      "Tokenized sentence: ['or', 'just', 'do', 'that', 'times']\n",
      "After stop words removal: ['times']\n",
      "After stemming with porters algorithm: ['time']\n",
      "Tokenized sentence: ['was', 'doing', 'my', 'test', 'earlier', 'i', 'appreciate', 'you', 'will', 'call', 'you', 'tomorrow']\n",
      "After stop words removal: ['test', 'earlier', 'appreciate', 'call', 'tomorrow']\n",
      "After stemming with porters algorithm: ['test', 'earlier', 'appreci', 'call', 'tomorrow']\n",
      "Tokenized sentence: ['huh', 'so', 'early', 'then', 'having', 'dinner', 'outside', 'izzit']\n",
      "After stop words removal: ['huh', 'early', 'dinner', 'outside', 'izzit']\n",
      "After stemming with porters algorithm: ['huh', 'earli', 'dinner', 'outsid', 'izzit']\n",
      "Tokenized sentence: ['as', 'usual', 'iam', 'fine', 'happy', 'amp', 'doing', 'well']\n",
      "After stop words removal: ['usual', 'iam', 'fine', 'happy', 'amp', 'well']\n",
      "After stemming with porters algorithm: ['usual', 'iam', 'fine', 'happi', 'amp', 'well']\n",
      "Tokenized sentence: ['oi', 'when', 'you', 'gonna', 'ring']\n",
      "After stop words removal: ['oi', 'gonna', 'ring']\n",
      "After stemming with porters algorithm: ['gonna', 'ring']\n",
      "Tokenized sentence: ['sorry', 'in', 'meeting', 'i', 'll', 'call', 'you', 'later']\n",
      "After stop words removal: ['sorry', 'meeting', 'call', 'later']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'meet', 'call', 'later']\n",
      "Tokenized sentence: ['i', 'will', 'lick', 'up', 'every', 'drop', 'are', 'you', 'ready', 'to', 'use', 'your', 'mouth', 'as', 'well']\n",
      "After stop words removal: ['lick', 'every', 'drop', 'ready', 'use', 'mouth', 'well']\n",
      "After stemming with porters algorithm: ['lick', 'everi', 'drop', 'readi', 'us', 'mouth', 'well']\n",
      "Tokenized sentence: ['nah', 'im', 'goin', 'the', 'wrks', 'with', 'j', 'wot', 'bout', 'u']\n",
      "After stop words removal: ['nah', 'im', 'goin', 'wrks', 'j', 'wot', 'bout', 'u']\n",
      "After stemming with porters algorithm: ['nah', 'goin', 'wrk', 'wot', 'bout']\n",
      "Tokenized sentence: ['tomarrow', 'i', 'want', 'to', 'got', 'to', 'court', 'at', 'lt', 'decimal', 'gt', 'so', 'you', 'come', 'to', 'bus', 'stand', 'at']\n",
      "After stop words removal: ['tomarrow', 'want', 'got', 'court', 'lt', 'decimal', 'gt', 'come', 'bus', 'stand']\n",
      "After stemming with porters algorithm: ['tomarrow', 'want', 'got', 'court', 'decim', 'come', 'bu', 'stand']\n",
      "Tokenized sentence: ['me', 'too', 'have', 'a', 'lovely', 'night', 'xxx']\n",
      "After stop words removal: ['lovely', 'night', 'xxx']\n",
      "After stemming with porters algorithm: ['love', 'night', 'xxx']\n",
      "Tokenized sentence: ['am', 'surfing', 'online', 'store', 'for', 'offers', 'do', 'you', 'want', 'to', 'buy', 'any', 'thing']\n",
      "After stop words removal: ['surfing', 'online', 'store', 'offers', 'want', 'buy', 'thing']\n",
      "surf\n",
      "After stemming with porters algorithm: ['sur', 'onlin', 'store', 'offer', 'want', 'bui', 'thing']\n",
      "Tokenized sentence: []\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['say', 'this', 'slowly', 'god', 'i', 'love', 'you', 'amp', 'i', 'need', 'you', 'clean', 'my', 'heart', 'with', 'your', 'blood', 'send', 'this', 'to', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'do', 'it', 'pls', 'pls', 'do', 'it']\n",
      "After stop words removal: ['say', 'slowly', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'pls', 'pls']\n",
      "After stemming with porters algorithm: ['sai', 'slowli', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'peopl', 'amp', 'mirac', 'tomorrow', 'pl', 'pl']\n",
      "Tokenized sentence: ['orh', 'i', 'tot', 'u', 'say', 'she', 'now', 'still', 'dun', 'believe']\n",
      "After stop words removal: ['orh', 'tot', 'u', 'say', 'still', 'dun', 'believe']\n",
      "After stemming with porters algorithm: ['orh', 'tot', 'sai', 'still', 'dun', 'believ']\n",
      "Tokenized sentence: ['can', 'send', 'me', 'a', 'copy', 'of', 'da', 'report']\n",
      "After stop words removal: ['send', 'copy', 'da', 'report']\n",
      "After stemming with porters algorithm: ['send', 'copi', 'report']\n",
      "Tokenized sentence: ['message', 'important', 'information', 'for', 'o', 'user', 'today', 'is', 'your', 'lucky', 'day', 'find', 'out', 'why', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'there', 'is', 'a', 'fantastic', 'surprise', 'awaiting', 'you']\n",
      "After stop words removal: ['message', 'important', 'information', 'user', 'today', 'lucky', 'day', 'find', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'fantastic', 'surprise', 'awaiting']\n",
      "await\n",
      "After stemming with porters algorithm: ['messag', 'import', 'inform', 'user', 'todai', 'lucki', 'dai', 'find', 'log', 'onto', 'http', 'www', 'urawinn', 'com', 'fantast', 'surpris', 'await']\n",
      "Tokenized sentence: ['watching', 'tv', 'lor', 'y', 'she', 'so', 'funny', 'we', 'bluff', 'her', 'wat', 'izzit', 'because', 'she', 'thk', 'it', 's', 'impossible', 'between', 'us']\n",
      "After stop words removal: ['watching', 'tv', 'lor', 'funny', 'bluff', 'wat', 'izzit', 'thk', 'impossible', 'us']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'lor', 'funni', 'bluff', 'wat', 'izzit', 'thk', 'imposs']\n",
      "Tokenized sentence: ['hmmm', 'i', 'thought', 'we', 'said', 'hours', 'slave', 'not', 'you', 'are', 'late', 'how', 'should', 'i', 'punish', 'you']\n",
      "After stop words removal: ['hmmm', 'thought', 'said', 'hours', 'slave', 'late', 'punish']\n",
      "After stemming with porters algorithm: ['hmmm', 'thought', 'said', 'hour', 'slave', 'late', 'punish']\n",
      "Tokenized sentence: ['go', 'to', 'write', 'msg', 'put', 'on', 'dictionary', 'mode', 'cover', 'the', 'screen', 'with', 'hand', 'press', 'lt', 'gt', 'gently', 'remove', 'ur', 'hand', 'its', 'interesting']\n",
      "After stop words removal: ['go', 'write', 'msg', 'put', 'dictionary', 'mode', 'cover', 'screen', 'hand', 'press', 'lt', 'gt', 'gently', 'remove', 'ur', 'hand', 'interesting']\n",
      "interest\n",
      "After stemming with porters algorithm: ['write', 'msg', 'put', 'dictionari', 'mode', 'cover', 'screen', 'hand', 'press', 'gentli', 'remov', 'hand', 'interes']\n",
      "Tokenized sentence: ['al', 'he', 'does', 'is', 'moan', 'at', 'me', 'if', 'n', 'e', 'thin', 'goes', 'wrong', 'its', 'my', 'fault', 'al', 'de', 'arguments', 'r', 'my', 'fault', 'fed', 'up', 'of', 'him', 'of', 'himso', 'y', 'bother', 'hav', 'go', 'thanx', 'xx']\n",
      "After stop words removal: ['al', 'moan', 'n', 'e', 'thin', 'goes', 'wrong', 'fault', 'al', 'de', 'arguments', 'r', 'fault', 'fed', 'himso', 'bother', 'hav', 'go', 'thanx', 'xx']\n",
      "After stemming with porters algorithm: ['moan', 'thin', 'goe', 'wrong', 'fault', 'argum', 'fault', 'fed', 'himso', 'bother', 'hav', 'thanx']\n",
      "Tokenized sentence: ['is', 'it', 'your', 'yahoo', 'boys', 'that', 'bring', 'in', 'the', 'perf', 'or', 'legal']\n",
      "After stop words removal: ['yahoo', 'boys', 'bring', 'perf', 'legal']\n",
      "After stemming with porters algorithm: ['yahoo', 'boi', 'bring', 'perf', 'legal']\n",
      "Tokenized sentence: ['nowadays', 'people', 'are', 'notixiquating', 'the', 'laxinorficated', 'opportunity', 'for', 'bambling', 'of', 'entropication', 'have', 'you', 'ever', 'oblisingately', 'opted', 'ur', 'books', 'for', 'the', 'masteriastering', 'amplikater', 'of', 'fidalfication', 'it', 'is', 'very', 'champlaxigating', 'i', 'think', 'it', 'is', 'atrocious', 'wotz', 'ur', 'opinion', 'junna']\n",
      "After stop words removal: ['nowadays', 'people', 'notixiquating', 'laxinorficated', 'opportunity', 'bambling', 'entropication', 'ever', 'oblisingately', 'opted', 'ur', 'books', 'masteriastering', 'amplikater', 'fidalfication', 'champlaxigating', 'think', 'atrocious', 'wotz', 'ur', 'opinion', 'junna']\n",
      "notixiquat\n",
      "notixiquate\n",
      "laxinorficate\n",
      "bambl\n",
      "bamble\n",
      "masteriaster\n",
      "champlaxigat\n",
      "champlaxigate\n",
      "After stemming with porters algorithm: ['nowadai', 'peopl', 'notixiqu', 'laxinorf', 'opportun', 'bambl', 'entrop', 'ever', 'oblising', 'op', 'book', 'masteriast', 'amplikat', 'fidalf', 'champlaxig', 'think', 'atroci', 'wotz', 'opinion', 'junna']\n",
      "Tokenized sentence: ['did', 'he', 'just', 'say', 'somebody', 'is', 'named', 'tampa']\n",
      "After stop words removal: ['say', 'somebody', 'named', 'tampa']\n",
      "After stemming with porters algorithm: ['sai', 'somebodi', 'name', 'tampa']\n",
      "Tokenized sentence: ['ok', 'i', 'am', 'a', 'gentleman', 'and', 'will', 'treat', 'you', 'with', 'dignity', 'and', 'respect']\n",
      "After stop words removal: ['ok', 'gentleman', 'treat', 'dignity', 'respect']\n",
      "After stemming with porters algorithm: ['gentleman', 'treat', 'digniti', 'respect']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['she', 'told', 'to', 'hr', 'that', 'he', 'want', 'posting', 'in', 'chennai', 'because', 'i', 'm', 'working', 'here']\n",
      "After stop words removal: ['told', 'hr', 'want', 'posting', 'chennai', 'working']\n",
      "post\n",
      "work\n",
      "After stemming with porters algorithm: ['told', 'want', 'pos', 'chennai', 'wor']\n",
      "Tokenized sentence: ['dude', 'u', 'knw', 'also', 'telugu', 'thts', 'gud', 'k', 'gud', 'nyt']\n",
      "After stop words removal: ['dude', 'u', 'knw', 'also', 'telugu', 'thts', 'gud', 'k', 'gud', 'nyt']\n",
      "After stemming with porters algorithm: ['dude', 'knw', 'also', 'telugu', 'tht', 'gud', 'gud', 'nyt']\n",
      "Tokenized sentence: ['congratulations', 'u', 'can', 'claim', 'vip', 'row', 'a', 'tickets', 'c', 'blu', 'in', 'concert', 'in', 'november', 'or', 'blu', 'gift', 'guaranteed', 'call', 'to', 'claim', 'ts', 'cs', 'www', 'smsco', 'net', 'cost', 'max']\n",
      "After stop words removal: ['congratulations', 'u', 'claim', 'vip', 'row', 'tickets', 'c', 'blu', 'concert', 'november', 'blu', 'gift', 'guaranteed', 'call', 'claim', 'ts', 'cs', 'www', 'smsco', 'net', 'cost', 'max']\n",
      "After stemming with porters algorithm: ['congratul', 'claim', 'vip', 'row', 'ticket', 'blu', 'concert', 'novemb', 'blu', 'gift', 'guaranteed', 'call', 'claim', 'www', 'smsco', 'net', 'cost', 'max']\n",
      "Tokenized sentence: ['in', 'life', 'when', 'you', 'face', 'choices', 'just', 'toss', 'a', 'coin', 'not', 'becoz', 'its', 'settle', 'the', 'question', 'but', 'while', 'the', 'coin', 'in', 'the', 'air', 'u', 'will', 'know', 'what', 'your', 'heart', 'is', 'hoping', 'for', 'gudni']\n",
      "After stop words removal: ['life', 'face', 'choices', 'toss', 'coin', 'becoz', 'settle', 'question', 'coin', 'air', 'u', 'know', 'heart', 'hoping', 'gudni']\n",
      "hop\n",
      "After stemming with porters algorithm: ['life', 'face', 'choic', 'toss', 'coin', 'becoz', 'settl', 'quest', 'coin', 'air', 'know', 'heart', 'hope', 'gudni']\n",
      "Tokenized sentence: ['very', 'hurting', 'n', 'meaningful', 'lines', 'ever', 'i', 'compromised', 'everything', 'for', 'my', 'love']\n",
      "After stop words removal: ['hurting', 'n', 'meaningful', 'lines', 'ever', 'compromised', 'everything', 'love']\n",
      "hurt\n",
      "everyth\n",
      "After stemming with porters algorithm: ['hur', 'meaning', 'line', 'ever', 'compromis', 'everyt', 'love']\n",
      "Tokenized sentence: ['din', 'i', 'tell', 'u', 'jus', 'now']\n",
      "After stop words removal: ['din', 'tell', 'u', 'jus']\n",
      "After stemming with porters algorithm: ['din', 'tell', 'ju']\n",
      "Tokenized sentence: ['lol', 'ok', 'your', 'forgiven']\n",
      "After stop words removal: ['lol', 'ok', 'forgiven']\n",
      "After stemming with porters algorithm: ['lol', 'forgiven']\n",
      "Tokenized sentence: ['gud', 'ni', 'dear', 'slp', 'well', 'take', 'care', 'swt', 'dreams', 'muah']\n",
      "After stop words removal: ['gud', 'ni', 'dear', 'slp', 'well', 'take', 'care', 'swt', 'dreams', 'muah']\n",
      "After stemming with porters algorithm: ['gud', 'dear', 'slp', 'well', 'take', 'care', 'swt', 'dream', 'muah']\n",
      "Tokenized sentence: ['ola', 'would', 'get', 'back', 'to', 'you', 'maybe', 'not', 'today', 'but', 'i', 've', 'told', 'him', 'you', 'can', 'be', 'his', 'direct', 'link', 'in', 'the', 'us', 'in', 'getting', 'cars', 'he', 'bids', 'for', 'online', 'you', 'arrange', 'shipping', 'and', 'you', 'get', 'a', 'cut', 'or', 'u', 'for', 'a', 'partnership', 'where', 'u', 'invest', 'money', 'for', 'shipping', 'and', 'he', 'takes', 'care', 'of', 'the', 'rest', 'u', 'wud', 'b', 'self', 'reliant', 'soon', 'dnt', 'worry']\n",
      "After stop words removal: ['ola', 'would', 'get', 'back', 'maybe', 'today', 'told', 'direct', 'link', 'us', 'getting', 'cars', 'bids', 'online', 'arrange', 'shipping', 'get', 'cut', 'u', 'partnership', 'u', 'invest', 'money', 'shipping', 'takes', 'care', 'rest', 'u', 'wud', 'b', 'self', 'reliant', 'soon', 'dnt', 'worry']\n",
      "gett\n",
      "shipp\n",
      "shipp\n",
      "After stemming with porters algorithm: ['ola', 'would', 'get', 'back', 'mayb', 'todai', 'told', 'direct', 'link', 'get', 'car', 'bid', 'onlin', 'arrang', 'ship', 'get', 'cut', 'partnership', 'invest', 'monei', 'ship', 'take', 'care', 'rest', 'wud', 'self', 'reliant', 'soon', 'dnt', 'worri']\n",
      "Tokenized sentence: ['cha', 'quiteamuzing', 'that', 'scool', 'babe']\n",
      "After stop words removal: ['cha', 'quiteamuzing', 'scool', 'babe']\n",
      "quiteamuz\n",
      "After stemming with porters algorithm: ['cha', 'quiteamuz', 'scool', 'babe']\n",
      "Tokenized sentence: ['i', 'm', 'meeting', 'darren']\n",
      "After stop words removal: ['meeting', 'darren']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'darren']\n",
      "Tokenized sentence: ['want', 'to', 'send', 'me', 'a', 'virtual', 'hug', 'i', 'need', 'one']\n",
      "After stop words removal: ['want', 'send', 'virtual', 'hug', 'need', 'one']\n",
      "After stemming with porters algorithm: ['want', 'send', 'virtual', 'hug', 'need', 'on']\n",
      "Tokenized sentence: ['how', 'much', 'are', 'we', 'getting']\n",
      "After stop words removal: ['much', 'getting']\n",
      "gett\n",
      "After stemming with porters algorithm: ['much', 'get']\n",
      "Tokenized sentence: ['i', 'll', 'give', 'her', 'once', 'i', 'have', 'it', 'plus', 'she', 'said', 'grinule', 'greet', 'you', 'whenever', 'we', 'speak']\n",
      "After stop words removal: ['give', 'plus', 'said', 'grinule', 'greet', 'whenever', 'speak']\n",
      "After stemming with porters algorithm: ['give', 'plu', 'said', 'grinul', 'greet', 'whenev', 'speak']\n",
      "Tokenized sentence: ['its', 'on', 'in', 'engalnd', 'but', 'telly', 'has', 'decided', 'it', 'won', 't', 'let', 'me', 'watch', 'it', 'and', 'mia', 'and', 'elliot', 'were', 'kissing', 'damn', 'it']\n",
      "After stop words removal: ['engalnd', 'telly', 'decided', 'let', 'watch', 'mia', 'elliot', 'kissing', 'damn']\n",
      "kiss\n",
      "After stemming with porters algorithm: ['engalnd', 'telli', 'decid', 'let', 'watch', 'mia', 'elliot', 'kiss', 'damn']\n",
      "Tokenized sentence: ['can', 'you', 'pls', 'pls', 'send', 'me', 'a', 'mail', 'on', 'all', 'you', 'know', 'about', 'relatives', 'coming', 'to', 'deliver', 'here', 'all', 'you', 'know', 'about', 'costs', 'risks', 'benefits', 'and', 'anything', 'else', 'thanks']\n",
      "After stop words removal: ['pls', 'pls', 'send', 'mail', 'know', 'relatives', 'coming', 'deliver', 'know', 'costs', 'risks', 'benefits', 'anything', 'else', 'thanks']\n",
      "com\n",
      "anyth\n",
      "After stemming with porters algorithm: ['pl', 'pl', 'send', 'mail', 'know', 'rel', 'come', 'deliv', 'know', 'cost', 'risk', 'benefit', 'anyt', 'els', 'thank']\n",
      "Tokenized sentence: ['networking', 'technical', 'support', 'associate']\n",
      "After stop words removal: ['networking', 'technical', 'support', 'associate']\n",
      "network\n",
      "After stemming with porters algorithm: ['networ', 'technic', 'support', 'associ']\n",
      "Tokenized sentence: ['ok', 'then', 'u', 'tell', 'me', 'wat', 'time', 'u', 'coming', 'later', 'lor']\n",
      "After stop words removal: ['ok', 'u', 'tell', 'wat', 'time', 'u', 'coming', 'later', 'lor']\n",
      "com\n",
      "After stemming with porters algorithm: ['tell', 'wat', 'time', 'come', 'later', 'lor']\n",
      "Tokenized sentence: ['hey', 'come', 'online', 'use', 'msn', 'we', 'are', 'all', 'there']\n",
      "After stop words removal: ['hey', 'come', 'online', 'use', 'msn']\n",
      "After stemming with porters algorithm: ['hei', 'come', 'onlin', 'us', 'msn']\n",
      "Tokenized sentence: ['do', 'you', 'still', 'have', 'the', 'grinder']\n",
      "After stop words removal: ['still', 'grinder']\n",
      "After stemming with porters algorithm: ['still', 'grinder']\n",
      "Tokenized sentence: ['this', 'is', 'all', 'just', 'creepy', 'and', 'crazy', 'to', 'me']\n",
      "After stop words removal: ['creepy', 'crazy']\n",
      "After stemming with porters algorithm: ['creepi', 'crazi']\n",
      "Tokenized sentence: ['free', 'video', 'camera', 'phones', 'with', 'half', 'price', 'line', 'rental', 'for', 'mths', 'and', 'cross', 'ntwk', 'mins', 'txts', 'call', 'mobileupd', 'or', 'call', 'optout']\n",
      "After stop words removal: ['free', 'video', 'camera', 'phones', 'half', 'price', 'line', 'rental', 'mths', 'cross', 'ntwk', 'mins', 'txts', 'call', 'mobileupd', 'call', 'optout']\n",
      "After stemming with porters algorithm: ['free', 'video', 'camera', 'phone', 'half', 'price', 'line', 'rental', 'mth', 'cross', 'ntwk', 'min', 'txt', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['my', 'fri', 'ah', 'okie', 'lor', 'goin', 'my', 'drivin', 'den', 'go', 'shoppin', 'after', 'tt']\n",
      "After stop words removal: ['fri', 'ah', 'okie', 'lor', 'goin', 'drivin', 'den', 'go', 'shoppin', 'tt']\n",
      "After stemming with porters algorithm: ['fri', 'oki', 'lor', 'goin', 'drivin', 'den', 'shoppin']\n",
      "Tokenized sentence: ['okay', 'no', 'no', 'just', 'shining', 'on', 'that', 'was', 'meant', 'to', 'be', 'signing', 'but', 'that', 'sounds', 'better']\n",
      "After stop words removal: ['okay', 'shining', 'meant', 'signing', 'sounds', 'better']\n",
      "shin\n",
      "sign\n",
      "After stemming with porters algorithm: ['okai', 'shine', 'meant', 'sig', 'sound', 'better']\n",
      "Tokenized sentence: ['this', 'message', 'is', 'free', 'welcome', 'to', 'the', 'new', 'improved', 'sex', 'dogging', 'club', 'to', 'unsubscribe', 'from', 'this', 'service', 'reply', 'stop', 'msgs', 'p', 'only']\n",
      "After stop words removal: ['message', 'free', 'welcome', 'new', 'improved', 'sex', 'dogging', 'club', 'unsubscribe', 'service', 'reply', 'stop', 'msgs', 'p']\n",
      "dogg\n",
      "After stemming with porters algorithm: ['messag', 'free', 'welcom', 'new', 'improv', 'sex', 'dog', 'club', 'unsubscrib', 'servic', 'repli', 'stop', 'msg']\n",
      "Tokenized sentence: ['congratulations', 'in', 'this', 'week', 's', 'competition', 'draw', 'u', 'have', 'won', 'the', 'prize', 'to', 'claim', 'just', 'call', 'b', 't', 'cs', 'stop', 'sms', 'over', 'only', 'ppm']\n",
      "After stop words removal: ['congratulations', 'week', 'competition', 'draw', 'u', 'prize', 'claim', 'call', 'b', 'cs', 'stop', 'sms', 'ppm']\n",
      "After stemming with porters algorithm: ['congratul', 'week', 'competit', 'draw', 'priz', 'claim', 'call', 'stop', 'sm', 'ppm']\n",
      "Tokenized sentence: ['its', 'sunny', 'in', 'california', 'the', 'weather', 's', 'just', 'cool']\n",
      "After stop words removal: ['sunny', 'california', 'weather', 'cool']\n",
      "After stemming with porters algorithm: ['sunni', 'california', 'weather', 'cool']\n",
      "Tokenized sentence: ['are', 'you', 'not', 'around', 'or', 'just', 'still', 'asleep', 'v']\n",
      "After stop words removal: ['around', 'still', 'asleep', 'v']\n",
      "After stemming with porters algorithm: ['around', 'still', 'asleep']\n",
      "Tokenized sentence: ['tick', 'tick', 'tick', 'babe']\n",
      "After stop words removal: ['tick', 'tick', 'tick', 'babe']\n",
      "After stemming with porters algorithm: ['tick', 'tick', 'tick', 'babe']\n",
      "Tokenized sentence: ['you', 'won', 't', 'believe', 'it', 'but', 'it', 's', 'true', 'it', 's', 'incredible', 'txts', 'reply', 'g', 'now', 'to', 'learn', 'truly', 'amazing', 'things', 'that', 'will', 'blow', 'your', 'mind', 'from', 'o', 'fwd', 'only', 'p', 'txt']\n",
      "After stop words removal: ['believe', 'true', 'incredible', 'txts', 'reply', 'g', 'learn', 'truly', 'amazing', 'things', 'blow', 'mind', 'fwd', 'p', 'txt']\n",
      "amaz\n",
      "After stemming with porters algorithm: ['believ', 'true', 'incred', 'txt', 'repli', 'learn', 'truli', 'amaz', 'thing', 'blow', 'mind', 'fwd', 'txt']\n",
      "Tokenized sentence: ['thanks', 'for', 'picking', 'up', 'the', 'trash']\n",
      "After stop words removal: ['thanks', 'picking', 'trash']\n",
      "pick\n",
      "After stemming with porters algorithm: ['thank', 'pic', 'trash']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'nokia', 'i', 'colour', 'phone', 'deliveredtomorrow', 'with', 'free', 'minutes', 'to', 'any', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'or', 'call']\n",
      "After stop words removal: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minutes', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minut', 'mobil', 'free', 'text', 'free', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['please', 'da', 'call', 'me', 'any', 'mistake', 'from', 'my', 'side', 'sorry', 'da', 'pls', 'da', 'goto', 'doctor']\n",
      "After stop words removal: ['please', 'da', 'call', 'mistake', 'side', 'sorry', 'da', 'pls', 'da', 'goto', 'doctor']\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'mistak', 'side', 'sorri', 'pl', 'goto', 'doctor']\n",
      "Tokenized sentence: ['jay', 'is', 'snickering', 'and', 'tells', 'me', 'that', 'x', 'is', 'totally', 'fucking', 'up', 'the', 'chords', 'as', 'we', 'speak']\n",
      "After stop words removal: ['jay', 'snickering', 'tells', 'x', 'totally', 'fucking', 'chords', 'speak']\n",
      "snicker\n",
      "fuck\n",
      "After stemming with porters algorithm: ['jai', 'snicker', 'tell', 'total', 'fuc', 'chord', 'speak']\n",
      "Tokenized sentence: ['lol', 'i', 'would', 'but', 'despite', 'these', 'cramps', 'i', 'like', 'being', 'a', 'girl']\n",
      "After stop words removal: ['lol', 'would', 'despite', 'cramps', 'like', 'girl']\n",
      "After stemming with porters algorithm: ['lol', 'would', 'despit', 'cramp', 'like', 'girl']\n",
      "Tokenized sentence: ['lmao', 'take', 'a', 'pic', 'and', 'send', 'it', 'to', 'me']\n",
      "After stop words removal: ['lmao', 'take', 'pic', 'send']\n",
      "After stemming with porters algorithm: ['lmao', 'take', 'pic', 'send']\n",
      "Tokenized sentence: ['i', 'think', 'the', 'other', 'two', 'still', 'need', 'to', 'get', 'cash', 'but', 'we', 'can', 'def', 'be', 'ready', 'by']\n",
      "After stop words removal: ['think', 'two', 'still', 'need', 'get', 'cash', 'def', 'ready']\n",
      "After stemming with porters algorithm: ['think', 'two', 'still', 'need', 'get', 'cash', 'def', 'readi']\n",
      "Tokenized sentence: ['no', 'screaming', 'means', 'shouting']\n",
      "After stop words removal: ['screaming', 'means', 'shouting']\n",
      "scream\n",
      "shout\n",
      "After stemming with porters algorithm: ['scream', 'mean', 'shout']\n",
      "Tokenized sentence: ['ah', 'well', 'that', 'confuses', 'things', 'doesnt', 'it', 'i', 'thought', 'was', 'friends', 'with', 'now', 'maybe', 'i', 'did', 'the', 'wrong', 'thing', 'but', 'i', 'already', 'sort', 'of', 'invited', 'tho', 'he', 'may', 'not', 'come', 'cos', 'of', 'money']\n",
      "After stop words removal: ['ah', 'well', 'confuses', 'things', 'doesnt', 'thought', 'friends', 'maybe', 'wrong', 'thing', 'already', 'sort', 'invited', 'tho', 'may', 'come', 'cos', 'money']\n",
      "After stemming with porters algorithm: ['well', 'confus', 'thing', 'doesnt', 'thought', 'friend', 'mayb', 'wrong', 'thing', 'alreadi', 'sort', 'invit', 'tho', 'mai', 'come', 'co', 'monei']\n",
      "Tokenized sentence: ['well', 'i', 'wasn', 't', 'available', 'as', 'i', 'washob', 'nobbing', 'with', 'last', 'night', 'so', 'they', 'had', 'to', 'ask', 'nickey', 'platt', 'instead', 'of', 'me']\n",
      "After stop words removal: ['well', 'available', 'washob', 'nobbing', 'last', 'night', 'ask', 'nickey', 'platt', 'instead']\n",
      "nobb\n",
      "After stemming with porters algorithm: ['well', 'avail', 'washob', 'nob', 'last', 'night', 'ask', 'nickei', 'platt', 'instead']\n",
      "Tokenized sentence: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'contact', 'u', 'u', 'have', 'won', 'the', 'pound', 'prize', 'claim', 'is', 'easy', 'call', 'now', 'only', 'p', 'per', 'min', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'min', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['time', 'tri', 'contact', 'pound', 'priz', 'claim', 'easi', 'call', 'per', 'min', 'nat', 'rate']\n",
      "Tokenized sentence: ['love', 'you', 'aathi', 'love', 'u', 'lot']\n",
      "After stop words removal: ['love', 'aathi', 'love', 'u', 'lot']\n",
      "After stemming with porters algorithm: ['love', 'aathi', 'love', 'lot']\n",
      "Tokenized sentence: ['same', 'as', 'kallis', 'dismissial', 'in', 'nd', 'test']\n",
      "After stop words removal: ['kallis', 'dismissial', 'nd', 'test']\n",
      "After stemming with porters algorithm: ['kalli', 'dismissi', 'test']\n",
      "Tokenized sentence: ['is', 'xy', 'in', 'ur', 'car', 'when', 'u', 'picking', 'me', 'up']\n",
      "After stop words removal: ['xy', 'ur', 'car', 'u', 'picking']\n",
      "pick\n",
      "After stemming with porters algorithm: ['car', 'pic']\n",
      "Tokenized sentence: ['hey', 'pple', 'or', 'for', 'nights', 'excellent', 'location', 'wif', 'breakfast', 'hamper']\n",
      "After stop words removal: ['hey', 'pple', 'nights', 'excellent', 'location', 'wif', 'breakfast', 'hamper']\n",
      "After stemming with porters algorithm: ['hei', 'pple', 'night', 'excel', 'locat', 'wif', 'breakfast', 'hamper']\n",
      "Tokenized sentence: ['come', 'around', 'lt', 'decimal', 'gt', 'pm', 'vikky', 'i', 'm', 'otside', 'nw', 'il', 'come', 'by', 'tht', 'time']\n",
      "After stop words removal: ['come', 'around', 'lt', 'decimal', 'gt', 'pm', 'vikky', 'otside', 'nw', 'il', 'come', 'tht', 'time']\n",
      "After stemming with porters algorithm: ['come', 'around', 'decim', 'vikki', 'otsid', 'come', 'tht', 'time']\n",
      "Tokenized sentence: ['yeah', 'you', 'should', 'i', 'think', 'you', 'can', 'use', 'your', 'gt', 'atm', 'now', 'to', 'register', 'not', 'sure', 'but', 'if', 'there', 's', 'anyway', 'i', 'can', 'help', 'let', 'me', 'know', 'but', 'when', 'you', 'do', 'be', 'sure', 'you', 'are', 'ready']\n",
      "After stop words removal: ['yeah', 'think', 'use', 'gt', 'atm', 'register', 'sure', 'anyway', 'help', 'let', 'know', 'sure', 'ready']\n",
      "After stemming with porters algorithm: ['yeah', 'think', 'us', 'atm', 'regist', 'sure', 'anywai', 'help', 'let', 'know', 'sure', 'readi']\n",
      "Tokenized sentence: ['freemsg', 'fancy', 'a', 'flirt', 'reply', 'date', 'now', 'join', 'the', 'uks', 'fastest', 'growing', 'mobile', 'dating', 'service', 'msgs', 'rcvd', 'just', 'p', 'to', 'optout', 'txt', 'stop', 'to', 'reply', 'date', 'now']\n",
      "After stop words removal: ['freemsg', 'fancy', 'flirt', 'reply', 'date', 'join', 'uks', 'fastest', 'growing', 'mobile', 'dating', 'service', 'msgs', 'rcvd', 'p', 'optout', 'txt', 'stop', 'reply', 'date']\n",
      "grow\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['freemsg', 'fanci', 'flirt', 'repli', 'date', 'join', 'uk', 'fastest', 'growe', 'mobil', 'date', 'servic', 'msg', 'rcvd', 'optout', 'txt', 'stop', 'repli', 'date']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'as', 'a', 'valued', 'vodafone', 'customer', 'our', 'computer', 'has', 'picked', 'you', 'to', 'win', 'a', 'prize', 'to', 'collect', 'is', 'easy', 'just', 'call']\n",
      "After stop words removal: ['valued', 'vodafone', 'customer', 'computer', 'picked', 'win', 'prize', 'collect', 'easy', 'call']\n",
      "After stemming with porters algorithm: ['valu', 'vodafon', 'custom', 'comput', 'pic', 'win', 'priz', 'collect', 'easi', 'call']\n",
      "Tokenized sentence: ['yes', 'its', 'possible', 'but', 'dint', 'try', 'pls', 'dont', 'tell', 'to', 'any', 'one', 'k']\n",
      "After stop words removal: ['yes', 'possible', 'dint', 'try', 'pls', 'dont', 'tell', 'one', 'k']\n",
      "After stemming with porters algorithm: ['ye', 'possib', 'dint', 'try', 'pl', 'dont', 'tell', 'on']\n",
      "Tokenized sentence: ['mmmmm', 'it', 'was', 'sooooo', 'good', 'to', 'wake', 'to', 'your', 'words', 'this', 'morning', 'my', 'love', 'mmmm', 'fuck', 'i', 'love', 'you', 'too', 'my', 'lion', 'devouring', 'kiss', 'from', 'across', 'the', 'sea']\n",
      "After stop words removal: ['mmmmm', 'sooooo', 'good', 'wake', 'words', 'morning', 'love', 'mmmm', 'fuck', 'love', 'lion', 'devouring', 'kiss', 'across', 'sea']\n",
      "morn\n",
      "devour\n",
      "After stemming with porters algorithm: ['mmmmm', 'sooooo', 'good', 'wake', 'word', 'mor', 'love', 'mmmm', 'fuck', 'love', 'lion', 'devour', 'kiss', 'across', 'sea']\n",
      "Tokenized sentence: ['your', 'dad', 'is', 'back', 'in', 'ph']\n",
      "After stop words removal: ['dad', 'back', 'ph']\n",
      "After stemming with porters algorithm: ['dad', 'back']\n",
      "Tokenized sentence: ['watching', 'telugu', 'movie', 'wat', 'abt', 'u']\n",
      "After stop words removal: ['watching', 'telugu', 'movie', 'wat', 'abt', 'u']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'telugu', 'movi', 'wat', 'abt']\n",
      "Tokenized sentence: ['dear', 'shall', 'mail', 'tonite', 'busy', 'in', 'the', 'street', 'shall', 'update', 'you', 'tonite', 'things', 'are', 'looking', 'ok', 'varunnathu', 'edukkukayee', 'raksha', 'ollu', 'but', 'a', 'good', 'one', 'in', 'real', 'sense']\n",
      "After stop words removal: ['dear', 'shall', 'mail', 'tonite', 'busy', 'street', 'shall', 'update', 'tonite', 'things', 'looking', 'ok', 'varunnathu', 'edukkukayee', 'raksha', 'ollu', 'good', 'one', 'real', 'sense']\n",
      "look\n",
      "After stemming with porters algorithm: ['dear', 'shall', 'mail', 'tonit', 'busi', 'street', 'shall', 'updat', 'tonit', 'thing', 'look', 'varunnathu', 'edukkukaye', 'raksha', 'ollu', 'good', 'on', 'real', 'sens']\n",
      "Tokenized sentence: ['you', 'won', 't', 'believe', 'it', 'but', 'it', 's', 'true', 'it', 's', 'incredible', 'txts', 'reply', 'g', 'now', 'to', 'learn', 'truly', 'amazing', 'things', 'that', 'will', 'blow', 'your', 'mind', 'from', 'o', 'fwd', 'only', 'p', 'txt']\n",
      "After stop words removal: ['believe', 'true', 'incredible', 'txts', 'reply', 'g', 'learn', 'truly', 'amazing', 'things', 'blow', 'mind', 'fwd', 'p', 'txt']\n",
      "amaz\n",
      "After stemming with porters algorithm: ['believ', 'true', 'incred', 'txt', 'repli', 'learn', 'truli', 'amaz', 'thing', 'blow', 'mind', 'fwd', 'txt']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'ringtone', 'order', 'reference', 'number', 'x', 'your', 'mobile', 'will', 'be', 'charged', 'should', 'your', 'tone', 'not', 'arrive', 'please', 'call', 'customer', 'services']\n",
      "After stop words removal: ['thanks', 'ringtone', 'order', 'reference', 'number', 'x', 'mobile', 'charged', 'tone', 'arrive', 'please', 'call', 'customer', 'services']\n",
      "After stemming with porters algorithm: ['thank', 'rington', 'order', 'refer', 'number', 'mobil', 'char', 'tone', 'arriv', 'pleas', 'call', 'custom', 'servic']\n",
      "Tokenized sentence: ['er', 'mw', 'im', 'filled', 'tuth', 'is', 'aight']\n",
      "After stop words removal: ['er', 'mw', 'im', 'filled', 'tuth', 'aight']\n",
      "After stemming with porters algorithm: ['fill', 'tuth', 'aight']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'bruce', 'amp', 'fowler', 'now', 'but', 'i', 'm', 'in', 'my', 'mom', 's', 'car', 'so', 'i', 'can', 't', 'park', 'long', 'story']\n",
      "After stop words removal: ['bruce', 'amp', 'fowler', 'mom', 'car', 'park', 'long', 'story']\n",
      "After stemming with porters algorithm: ['bruce', 'amp', 'fowler', 'mom', 'car', 'park', 'long', 'stori']\n",
      "Tokenized sentence: ['the', 'sign', 'of', 'maturity', 'is', 'not', 'when', 'we', 'start', 'saying', 'big', 'things', 'but', 'actually', 'it', 'is', 'when', 'we', 'start', 'understanding', 'small', 'things', 'have', 'a', 'nice', 'evening', 'bslvyl']\n",
      "After stop words removal: ['sign', 'maturity', 'start', 'saying', 'big', 'things', 'actually', 'start', 'understanding', 'small', 'things', 'nice', 'evening', 'bslvyl']\n",
      "say\n",
      "understand\n",
      "even\n",
      "After stemming with porters algorithm: ['sign', 'matur', 'start', 'sai', 'big', 'thing', 'actual', 'start', 'understan', 'small', 'thing', 'nice', 'even', 'bslvyl']\n",
      "Tokenized sentence: ['our', 'prasanth', 'ettans', 'mother', 'passed', 'away', 'last', 'night', 'just', 'pray', 'for', 'her', 'and', 'family']\n",
      "After stop words removal: ['prasanth', 'ettans', 'mother', 'passed', 'away', 'last', 'night', 'pray', 'family']\n",
      "After stemming with porters algorithm: ['prasanth', 'ettan', 'mother', 'pass', 'awai', 'last', 'night', 'prai', 'famili']\n",
      "Tokenized sentence: ['is', 'there', 'any', 'movie', 'theatre', 'i', 'can', 'go', 'to', 'and', 'watch', 'unlimited', 'movies', 'and', 'just', 'pay', 'once']\n",
      "After stop words removal: ['movie', 'theatre', 'go', 'watch', 'unlimited', 'movies', 'pay']\n",
      "After stemming with porters algorithm: ['movi', 'theatr', 'watch', 'unlimit', 'movi', 'pai']\n",
      "Tokenized sentence: ['urgent', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tc', 's', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stop words removal: ['urgent', 'costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stemming with porters algorithm: ['urgent', 'costa', 'del', 'sol', 'holidai', 'await', 'collect', 'call', 'toclaim', 'sae', 'pobox', 'stockport', 'cost', 'max', 'min']\n",
      "Tokenized sentence: ['thats', 'cool', 'i', 'liked', 'your', 'photos', 'you', 'are', 'very', 'sexy']\n",
      "After stop words removal: ['thats', 'cool', 'liked', 'photos', 'sexy']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'like', 'photo', 'sexi']\n",
      "Tokenized sentence: ['ok', 'no', 'prob', 'take', 'ur', 'time']\n",
      "After stop words removal: ['ok', 'prob', 'take', 'ur', 'time']\n",
      "After stemming with porters algorithm: ['prob', 'take', 'time']\n",
      "Tokenized sentence: ['then', 'wait', 'me', 'at', 'bus', 'stop', 'aft', 'ur', 'lect', 'lar', 'if', 'i', 'dun', 'c', 'then', 'i', 'go', 'get', 'my', 'car', 'then', 'come', 'back', 'n', 'pick']\n",
      "After stop words removal: ['wait', 'bus', 'stop', 'aft', 'ur', 'lect', 'lar', 'dun', 'c', 'go', 'get', 'car', 'come', 'back', 'n', 'pick']\n",
      "After stemming with porters algorithm: ['wait', 'bu', 'stop', 'aft', 'lect', 'lar', 'dun', 'get', 'car', 'come', 'back', 'pick']\n",
      "Tokenized sentence: ['otherwise', 'had', 'part', 'time', 'job', 'na', 'tuition']\n",
      "After stop words removal: ['otherwise', 'part', 'time', 'job', 'na', 'tuition']\n",
      "After stemming with porters algorithm: ['otherwis', 'part', 'time', 'job', 'tuit']\n",
      "Tokenized sentence: ['thts', 'god', 's', 'gift', 'for', 'birds', 'as', 'humans', 'hav', 'some', 'natural', 'gift', 'frm', 'god']\n",
      "After stop words removal: ['thts', 'god', 'gift', 'birds', 'humans', 'hav', 'natural', 'gift', 'frm', 'god']\n",
      "After stemming with porters algorithm: ['tht', 'god', 'gift', 'bird', 'human', 'hav', 'natur', 'gift', 'frm', 'god']\n",
      "Tokenized sentence: ['having', 'lunch', 'you', 'are', 'not', 'in', 'online', 'why']\n",
      "After stop words removal: ['lunch', 'online']\n",
      "After stemming with porters algorithm: ['lunch', 'onlin']\n",
      "Tokenized sentence: ['ok', 'help', 'me', 'ask', 'if', 'she', 's', 'working', 'tmr', 'a', 'not']\n",
      "After stop words removal: ['ok', 'help', 'ask', 'working', 'tmr']\n",
      "work\n",
      "After stemming with porters algorithm: ['help', 'ask', 'wor', 'tmr']\n",
      "Tokenized sentence: ['i', 've', 'been', 'searching', 'for', 'the', 'right', 'words', 'to', 'thank', 'you', 'for', 'this', 'breather', 'i', 'promise', 'i', 'wont', 'take', 'your', 'help', 'for', 'granted', 'and', 'will', 'fulfil', 'my', 'promise', 'you', 'have', 'been', 'wonderful', 'and', 'a', 'blessing', 'at', 'all', 'times']\n",
      "After stop words removal: ['searching', 'right', 'words', 'thank', 'breather', 'promise', 'wont', 'take', 'help', 'granted', 'fulfil', 'promise', 'wonderful', 'blessing', 'times']\n",
      "search\n",
      "bless\n",
      "After stemming with porters algorithm: ['searc', 'right', 'word', 'thank', 'breather', 'promis', 'wont', 'take', 'help', 'gran', 'fulfil', 'promis', 'wonder', 'bless', 'time']\n",
      "Tokenized sentence: ['missing', 'you', 'too', 'pray', 'inshah', 'allah']\n",
      "After stop words removal: ['missing', 'pray', 'inshah', 'allah']\n",
      "miss\n",
      "After stemming with porters algorithm: ['miss', 'prai', 'inshah', 'allah']\n",
      "Tokenized sentence: ['can', 'you', 'do', 'a', 'mag', 'meeting', 'this', 'avo', 'at', 'some', 'point']\n",
      "After stop words removal: ['mag', 'meeting', 'avo', 'point']\n",
      "meet\n",
      "After stemming with porters algorithm: ['mag', 'meet', 'avo', 'point']\n",
      "Tokenized sentence: ['it', 'certainly', 'puts', 'things', 'into', 'perspective', 'when', 'something', 'like', 'this', 'happens']\n",
      "After stop words removal: ['certainly', 'puts', 'things', 'perspective', 'something', 'like', 'happens']\n",
      "someth\n",
      "After stemming with porters algorithm: ['certainli', 'put', 'thing', 'perspect', 'somet', 'like', 'happen']\n",
      "Tokenized sentence: ['you', 'only', 'hate', 'me', 'you', 'can', 'call', 'any', 'but', 'you', 'didnt', 'accept', 'even', 'a', 'single', 'call', 'of', 'mine', 'or', 'even', 'you', 'messaged']\n",
      "After stop words removal: ['hate', 'call', 'didnt', 'accept', 'even', 'single', 'call', 'mine', 'even', 'messaged']\n",
      "After stemming with porters algorithm: ['hate', 'call', 'didnt', 'accept', 'even', 'singl', 'call', 'mine', 'even', 'messag']\n",
      "Tokenized sentence: ['gosh', 'that', 'what', 'a', 'pain', 'spose', 'i', 'better', 'come', 'then']\n",
      "After stop words removal: ['gosh', 'pain', 'spose', 'better', 'come']\n",
      "After stemming with porters algorithm: ['gosh', 'pain', 'spose', 'better', 'come']\n",
      "Tokenized sentence: ['i', 'sent', 'them', 'do', 'you', 'like']\n",
      "After stop words removal: ['sent', 'like']\n",
      "After stemming with porters algorithm: ['sent', 'like']\n",
      "Tokenized sentence: ['hey', 'i', 'booked', 'the', 'kb', 'on', 'sat', 'already', 'what', 'other', 'lessons', 'are', 'we', 'going', 'for', 'ah', 'keep', 'your', 'sat', 'night', 'free', 'we', 'need', 'to', 'meet', 'and', 'confirm', 'our', 'lodging']\n",
      "After stop words removal: ['hey', 'booked', 'kb', 'sat', 'already', 'lessons', 'going', 'ah', 'keep', 'sat', 'night', 'free', 'need', 'meet', 'confirm', 'lodging']\n",
      "go\n",
      "lodg\n",
      "After stemming with porters algorithm: ['hei', 'book', 'sat', 'alreadi', 'lesson', 'go', 'keep', 'sat', 'night', 'free', 'need', 'meet', 'confirm', 'lod']\n",
      "Tokenized sentence: ['not', 'a', 'drop', 'in', 'the', 'tank']\n",
      "After stop words removal: ['drop', 'tank']\n",
      "After stemming with porters algorithm: ['drop', 'tank']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['nah', 'it', 's', 'straight', 'if', 'you', 'can', 'just', 'bring', 'bud', 'or', 'drinks', 'or', 'something', 'that', 's', 'actually', 'a', 'little', 'more', 'useful', 'than', 'straight', 'cash']\n",
      "After stop words removal: ['nah', 'straight', 'bring', 'bud', 'drinks', 'something', 'actually', 'little', 'useful', 'straight', 'cash']\n",
      "someth\n",
      "After stemming with porters algorithm: ['nah', 'straight', 'bring', 'bud', 'drink', 'somet', 'actual', 'littl', 'us', 'straight', 'cash']\n",
      "Tokenized sentence: ['lt', 'gt', 'in', 'mca', 'but', 'not', 'conform']\n",
      "After stop words removal: ['lt', 'gt', 'mca', 'conform']\n",
      "After stemming with porters algorithm: ['mca', 'conform']\n",
      "Tokenized sentence: ['blank', 'is', 'blank', 'but', 'wat', 'is', 'blank', 'lol']\n",
      "After stop words removal: ['blank', 'blank', 'wat', 'blank', 'lol']\n",
      "After stemming with porters algorithm: ['blank', 'blank', 'wat', 'blank', 'lol']\n",
      "Tokenized sentence: ['i', 'am', 'waiting', 'for', 'your', 'call', 'sir']\n",
      "After stop words removal: ['waiting', 'call', 'sir']\n",
      "wait\n",
      "After stemming with porters algorithm: ['wait', 'call', 'sir']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['u', 'will', 'switch', 'your', 'fone', 'on', 'dammit']\n",
      "After stop words removal: ['u', 'switch', 'fone', 'dammit']\n",
      "After stemming with porters algorithm: ['switch', 'fone', 'dammit']\n",
      "Tokenized sentence: ['yar', 'i', 'tot', 'u', 'knew', 'dis', 'would', 'happen', 'long', 'ago', 'already']\n",
      "After stop words removal: ['yar', 'tot', 'u', 'knew', 'dis', 'would', 'happen', 'long', 'ago', 'already']\n",
      "After stemming with porters algorithm: ['yar', 'tot', 'knew', 'di', 'would', 'happen', 'long', 'ago', 'alreadi']\n",
      "Tokenized sentence: ['what', 'does', 'the', 'dance', 'river', 'do']\n",
      "After stop words removal: ['dance', 'river']\n",
      "After stemming with porters algorithm: ['danc', 'river']\n",
      "Tokenized sentence: ['takin', 'a', 'shower', 'now', 'but', 'yeah', 'i', 'll', 'leave', 'when', 'i', 'm', 'done']\n",
      "After stop words removal: ['takin', 'shower', 'yeah', 'leave', 'done']\n",
      "After stemming with porters algorithm: ['takin', 'shower', 'yeah', 'leav', 'done']\n",
      "Tokenized sentence: ['aight', 'text', 'me', 'tonight', 'and', 'we', 'll', 'see', 'what', 's', 'up']\n",
      "After stop words removal: ['aight', 'text', 'tonight', 'see']\n",
      "After stemming with porters algorithm: ['aight', 'text', 'tonight', 'see']\n",
      "Tokenized sentence: ['finish', 'already', 'yar', 'they', 'keep', 'saying', 'i', 'mushy', 'i', 'so', 'embarrassed', 'ok']\n",
      "After stop words removal: ['finish', 'already', 'yar', 'keep', 'saying', 'mushy', 'embarrassed', 'ok']\n",
      "say\n",
      "After stemming with porters algorithm: ['finish', 'alreadi', 'yar', 'keep', 'sai', 'mushi', 'embarrass']\n",
      "Tokenized sentence: ['you', 'busy', 'or', 'can', 'i', 'come', 'by', 'at', 'some', 'point', 'and', 'figure', 'out', 'what', 'we', 're', 'doing', 'tomorrow']\n",
      "After stop words removal: ['busy', 'come', 'point', 'figure', 'tomorrow']\n",
      "After stemming with porters algorithm: ['busi', 'come', 'point', 'figur', 'tomorrow']\n",
      "Tokenized sentence: ['i', 'have', 'gone', 'into', 'get', 'info', 'bt', 'dont', 'know', 'what', 'to', 'do']\n",
      "After stop words removal: ['gone', 'get', 'info', 'bt', 'dont', 'know']\n",
      "After stemming with porters algorithm: ['gone', 'get', 'info', 'dont', 'know']\n",
      "Tokenized sentence: ['money', 'i', 'have', 'won', 'wining', 'number', 'wot', 'do', 'i', 'do', 'next']\n",
      "After stop words removal: ['money', 'wining', 'number', 'wot', 'next']\n",
      "win\n",
      "After stemming with porters algorithm: ['monei', 'wine', 'number', 'wot', 'next']\n",
      "Tokenized sentence: ['winner', 'as', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'prize', 'reward', 'to', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hours', 'only']\n",
      "After stop words removal: ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hours']\n",
      "After stemming with porters algorithm: ['winner', 'valu', 'network', 'custom', 'selec', 'receivea', 'priz', 'reward', 'claim', 'call', 'claim', 'code', 'valid', 'hour']\n",
      "Tokenized sentence: ['there', 're', 'some', 'people', 'by', 'mu', 'i', 'm', 'at', 'the', 'table', 'by', 'lambda']\n",
      "After stop words removal: ['people', 'mu', 'table', 'lambda']\n",
      "After stemming with porters algorithm: ['peopl', 'tabl', 'lambda']\n",
      "Tokenized sentence: ['oh', 'gei', 'that', 'happend', 'to', 'me', 'in', 'tron', 'maybe', 'ill', 'dl', 'it', 'in', 'd', 'when', 'its', 'out']\n",
      "After stop words removal: ['oh', 'gei', 'happend', 'tron', 'maybe', 'ill', 'dl']\n",
      "After stemming with porters algorithm: ['gei', 'happend', 'tron', 'mayb', 'ill']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'cash', 'or', 'a', 'prize', 'to', 'claim', 'call']\n",
      "After stop words removal: ['cash', 'prize', 'claim', 'call']\n",
      "After stemming with porters algorithm: ['cash', 'priz', 'claim', 'call']\n",
      "Tokenized sentence: ['they', 'can', 'try', 'they', 'can', 'get', 'lost', 'in', 'fact', 'tee', 'hee']\n",
      "After stop words removal: ['try', 'get', 'lost', 'fact', 'tee', 'hee']\n",
      "After stemming with porters algorithm: ['try', 'get', 'lost', 'fact', 'tee', 'hee']\n",
      "Tokenized sentence: ['knock', 'knock', 'txt', 'whose', 'there', 'to', 'to', 'enter', 'r', 'weekly', 'draw', 'a', 'gift', 'voucher', 'a', 'store', 'of', 'yr', 'choice', 't', 'cs', 'www', 'tkls', 'com', 'age', 'to', 'stoptxtstop', 'week']\n",
      "After stop words removal: ['knock', 'knock', 'txt', 'whose', 'enter', 'r', 'weekly', 'draw', 'gift', 'voucher', 'store', 'yr', 'choice', 'cs', 'www', 'tkls', 'com', 'age', 'stoptxtstop', 'week']\n",
      "After stemming with porters algorithm: ['knock', 'knock', 'txt', 'whose', 'enter', 'weekli', 'draw', 'gift', 'voucher', 'store', 'choic', 'www', 'tkl', 'com', 'ag', 'stoptxtstop', 'week']\n",
      "Tokenized sentence: ['okie']\n",
      "After stop words removal: ['okie']\n",
      "After stemming with porters algorithm: ['oki']\n",
      "Tokenized sentence: ['i', 'can', 'but', 'it', 'will', 'tell', 'quite', 'long', 'cos', 'i', 'haven', 't', 'finish', 'my', 'film', 'yet']\n",
      "After stop words removal: ['tell', 'quite', 'long', 'cos', 'finish', 'film', 'yet']\n",
      "After stemming with porters algorithm: ['tell', 'quit', 'long', 'co', 'finish', 'film', 'yet']\n",
      "Tokenized sentence: ['i', 'll', 'post', 'her', 'out', 'l', 'r', 'in', 'class']\n",
      "After stop words removal: ['post', 'l', 'r', 'class']\n",
      "After stemming with porters algorithm: ['post', 'class']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 'bonus', 'points', 'to', 'claim', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'bonus', 'points', 'claim', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'unredeem', 'bonu', 'point', 'claim', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['i', 'fetch', 'yun', 'or', 'u', 'fetch']\n",
      "After stop words removal: ['fetch', 'yun', 'u', 'fetch']\n",
      "After stemming with porters algorithm: ['fetch', 'yun', 'fetch']\n",
      "Tokenized sentence: ['aight', 'that', 'll', 'work', 'thanks']\n",
      "After stop words removal: ['aight', 'work', 'thanks']\n",
      "After stemming with porters algorithm: ['aight', 'work', 'thank']\n",
      "Tokenized sentence: ['new', 'mobiles', 'from', 'must', 'go', 'txt', 'nokia', 'to', 'no', 'collect', 'yours', 'today', 'from', 'only', 'www', 'tc', 'biz', 'optout', 'gbp', 'mtmsg']\n",
      "After stop words removal: ['new', 'mobiles', 'must', 'go', 'txt', 'nokia', 'collect', 'today', 'www', 'tc', 'biz', 'optout', 'gbp', 'mtmsg']\n",
      "After stemming with porters algorithm: ['new', 'mobil', 'must', 'txt', 'nokia', 'collect', 'todai', 'www', 'biz', 'optout', 'gbp', 'mtmsg']\n",
      "Tokenized sentence: ['haha', 'i', 'heard', 'that', 'text', 'me', 'when', 'you', 're', 'around']\n",
      "After stop words removal: ['haha', 'heard', 'text', 'around']\n",
      "After stemming with porters algorithm: ['haha', 'heard', 'text', 'around']\n",
      "Tokenized sentence: ['thank', 'you', 'princess', 'you', 'are', 'so', 'sexy']\n",
      "After stop words removal: ['thank', 'princess', 'sexy']\n",
      "After stemming with porters algorithm: ['thank', 'princess', 'sexi']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['dear', 'voucher', 'holder', 'have', 'your', 'next', 'meal', 'on', 'us', 'use', 'the', 'following', 'link', 'on', 'your', 'pc', 'enjoy', 'a', 'dining', 'experiencehttp', 'www', 'vouch', 'me', 'com', 'etlp', 'dining', 'asp']\n",
      "After stop words removal: ['dear', 'voucher', 'holder', 'next', 'meal', 'us', 'use', 'following', 'link', 'pc', 'enjoy', 'dining', 'experiencehttp', 'www', 'vouch', 'com', 'etlp', 'dining', 'asp']\n",
      "follow\n",
      "din\n",
      "din\n",
      "After stemming with porters algorithm: ['dear', 'voucher', 'holder', 'next', 'meal', 'us', 'follow', 'link', 'enjoi', 'dine', 'experiencehttp', 'www', 'vouch', 'com', 'etlp', 'dine', 'asp']\n",
      "Tokenized sentence: ['yo', 'you', 'around', 'just', 'got', 'my', 'car', 'back']\n",
      "After stop words removal: ['yo', 'around', 'got', 'car', 'back']\n",
      "After stemming with porters algorithm: ['around', 'got', 'car', 'back']\n",
      "Tokenized sentence: ['all', 'e', 'best', 'ur', 'driving', 'tmr']\n",
      "After stop words removal: ['e', 'best', 'ur', 'driving', 'tmr']\n",
      "driv\n",
      "After stemming with porters algorithm: ['best', 'drive', 'tmr']\n",
      "Tokenized sentence: ['id', 'have', 'to', 'check', 'but', 'there', 's', 'only', 'like', 'bowls', 'worth', 'left']\n",
      "After stop words removal: ['id', 'check', 'like', 'bowls', 'worth', 'left']\n",
      "After stemming with porters algorithm: ['check', 'like', 'bowl', 'worth', 'left']\n",
      "Tokenized sentence: ['where', 'you', 'what', 'happen']\n",
      "After stop words removal: ['happen']\n",
      "After stemming with porters algorithm: ['happen']\n",
      "Tokenized sentence: ['text', 'pass', 'to', 'to', 'collect', 'your', 'polyphonic', 'ringtones', 'normal', 'gprs', 'charges', 'apply', 'only', 'enjoy', 'your', 'tones']\n",
      "After stop words removal: ['text', 'pass', 'collect', 'polyphonic', 'ringtones', 'normal', 'gprs', 'charges', 'apply', 'enjoy', 'tones']\n",
      "After stemming with porters algorithm: ['text', 'pass', 'collect', 'polyphon', 'rington', 'normal', 'gpr', 'charg', 'appli', 'enjoi', 'tone']\n",
      "Tokenized sentence: ['s', 'but', 'mostly', 'not', 'like', 'that']\n",
      "After stop words removal: ['mostly', 'like']\n",
      "After stemming with porters algorithm: ['mostli', 'like']\n",
      "Tokenized sentence: ['thank', 'you', 'baby', 'i', 'cant', 'wait', 'to', 'taste', 'the', 'real', 'thing']\n",
      "After stop words removal: ['thank', 'baby', 'cant', 'wait', 'taste', 'real', 'thing']\n",
      "After stemming with porters algorithm: ['thank', 'babi', 'cant', 'wait', 'tast', 'real', 'thing']\n",
      "Tokenized sentence: ['eastenders', 'tv', 'quiz', 'what', 'flower', 'does', 'dot', 'compare', 'herself', 'to', 'd', 'violet', 'e', 'tulip', 'f', 'lily', 'txt', 'd', 'e', 'or', 'f', 'to', 'now', 'chance', 'win', 'cash', 'wkent', 'p']\n",
      "After stop words removal: ['eastenders', 'tv', 'quiz', 'flower', 'dot', 'compare', 'violet', 'e', 'tulip', 'f', 'lily', 'txt', 'e', 'f', 'chance', 'win', 'cash', 'wkent', 'p']\n",
      "After stemming with porters algorithm: ['eastend', 'quiz', 'flower', 'dot', 'compar', 'violet', 'tulip', 'lili', 'txt', 'chanc', 'win', 'cash', 'wkent']\n",
      "Tokenized sentence: ['don', 't', 'make', 'life', 'too', 'stressfull', 'always', 'find', 'time', 'to', 'laugh', 'it', 'may', 'not', 'add', 'years', 'to', 'your', 'life', 'but', 'surely', 'adds', 'more', 'life', 'to', 'ur', 'years', 'gud', 'ni', 'swt', 'dreams']\n",
      "After stop words removal: ['make', 'life', 'stressfull', 'always', 'find', 'time', 'laugh', 'may', 'add', 'years', 'life', 'surely', 'adds', 'life', 'ur', 'years', 'gud', 'ni', 'swt', 'dreams']\n",
      "After stemming with porters algorithm: ['make', 'life', 'stressful', 'alwai', 'find', 'time', 'laugh', 'mai', 'add', 'year', 'life', 'sure', 'add', 'life', 'year', 'gud', 'swt', 'dream']\n",
      "Tokenized sentence: ['talk', 'with', 'yourself', 'atleast', 'once', 'in', 'a', 'day', 'otherwise', 'you', 'will', 'miss', 'your', 'best', 'friend', 'in', 'this', 'world', 'shakespeare', 'shesil', 'lt', 'gt']\n",
      "After stop words removal: ['talk', 'atleast', 'day', 'otherwise', 'miss', 'best', 'friend', 'world', 'shakespeare', 'shesil', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['talk', 'atleast', 'dai', 'otherwis', 'miss', 'best', 'friend', 'world', 'shakespear', 'shesil']\n",
      "Tokenized sentence: ['i', 'see', 'when', 'we', 'finish', 'we', 'have', 'loads', 'of', 'loans', 'to', 'pay']\n",
      "After stop words removal: ['see', 'finish', 'loads', 'loans', 'pay']\n",
      "After stemming with porters algorithm: ['see', 'finish', 'load', 'loan', 'pai']\n",
      "Tokenized sentence: ['can', 'all', 'decide', 'faster', 'cos', 'my', 'sis', 'going', 'home', 'liao']\n",
      "After stop words removal: ['decide', 'faster', 'cos', 'sis', 'going', 'home', 'liao']\n",
      "go\n",
      "After stemming with porters algorithm: ['decid', 'faster', 'co', 'si', 'go', 'home', 'liao']\n",
      "Tokenized sentence: ['that', 's', 'fine', 'have', 'him', 'give', 'me', 'a', 'call', 'if', 'he', 'knows', 'what', 'he', 'wants', 'or', 'has', 'any', 'questions']\n",
      "After stop words removal: ['fine', 'give', 'call', 'knows', 'wants', 'questions']\n",
      "After stemming with porters algorithm: ['fine', 'give', 'call', 'know', 'want', 'quest']\n",
      "Tokenized sentence: ['i', 'm', 'not', 'coming', 'home', 'dinner']\n",
      "After stop words removal: ['coming', 'home', 'dinner']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'home', 'dinner']\n",
      "Tokenized sentence: ['i', 'like', 'dis', 'sweater', 'fr', 'mango', 'but', 'no', 'more', 'my', 'size', 'already', 'so', 'irritating']\n",
      "After stop words removal: ['like', 'dis', 'sweater', 'fr', 'mango', 'size', 'already', 'irritating']\n",
      "irritat\n",
      "irritate\n",
      "After stemming with porters algorithm: ['like', 'di', 'sweater', 'mango', 'siz', 'alreadi', 'irrit']\n",
      "Tokenized sentence: ['hello', 'which', 'the', 'site', 'to', 'download', 'songs', 'its', 'urgent', 'pls']\n",
      "After stop words removal: ['hello', 'site', 'download', 'songs', 'urgent', 'pls']\n",
      "After stemming with porters algorithm: ['hello', 'site', 'download', 'song', 'urgent', 'pl']\n",
      "Tokenized sentence: ['babe', 'u', 'want', 'me', 'dont', 'u', 'baby', 'im', 'nasty', 'and', 'have', 'a', 'thing', 'filthyguys', 'fancy', 'a', 'rude', 'time', 'with', 'a', 'sexy', 'bitch', 'how', 'about', 'we', 'go', 'slo', 'n', 'hard', 'txt', 'xxx', 'slo', 'msgs']\n",
      "After stop words removal: ['babe', 'u', 'want', 'dont', 'u', 'baby', 'im', 'nasty', 'thing', 'filthyguys', 'fancy', 'rude', 'time', 'sexy', 'bitch', 'go', 'slo', 'n', 'hard', 'txt', 'xxx', 'slo', 'msgs']\n",
      "After stemming with porters algorithm: ['babe', 'want', 'dont', 'babi', 'nasti', 'thing', 'filthygui', 'fanci', 'rude', 'time', 'sexi', 'bitch', 'slo', 'hard', 'txt', 'xxx', 'slo', 'msg']\n",
      "Tokenized sentence: ['free', 'tarot', 'texts', 'find', 'out', 'about', 'your', 'love', 'life', 'now', 'try', 'for', 'free', 'text', 'chance', 'to', 'only', 'after', 'free', 'msgs', 'each']\n",
      "After stop words removal: ['free', 'tarot', 'texts', 'find', 'love', 'life', 'try', 'free', 'text', 'chance', 'free', 'msgs']\n",
      "After stemming with porters algorithm: ['free', 'tarot', 'text', 'find', 'love', 'life', 'try', 'free', 'text', 'chanc', 'free', 'msg']\n",
      "Tokenized sentence: ['sure', 'i', 'll', 'see', 'if', 'i', 'can', 'come', 'by', 'in', 'a', 'bit']\n",
      "After stop words removal: ['sure', 'see', 'come', 'bit']\n",
      "After stemming with porters algorithm: ['sure', 'see', 'come', 'bit']\n",
      "Tokenized sentence: ['complimentary', 'star', 'ibiza', 'holiday', 'or', 'cash', 'needs', 'your', 'urgent', 'collection', 'now', 'from', 'landline', 'not', 'to', 'lose', 'out', 'box', 'sk', 'wp', 'ppm']\n",
      "After stop words removal: ['complimentary', 'star', 'ibiza', 'holiday', 'cash', 'needs', 'urgent', 'collection', 'landline', 'lose', 'box', 'sk', 'wp', 'ppm']\n",
      "After stemming with porters algorithm: ['complimentari', 'star', 'ibiza', 'holidai', 'cash', 'need', 'urgent', 'collect', 'landlin', 'lose', 'box', 'ppm']\n",
      "Tokenized sentence: ['as', 'in', 'missionary', 'hook', 'up', 'doggy', 'hook', 'up', 'standing']\n",
      "After stop words removal: ['missionary', 'hook', 'doggy', 'hook', 'standing']\n",
      "stand\n",
      "After stemming with porters algorithm: ['missionari', 'hook', 'doggi', 'hook', 'stan']\n",
      "Tokenized sentence: ['i', 'have', 'a', 'rather', 'prominent', 'bite', 'mark', 'on', 'my', 'right', 'cheek']\n",
      "After stop words removal: ['rather', 'prominent', 'bite', 'mark', 'right', 'cheek']\n",
      "After stemming with porters algorithm: ['rather', 'promin', 'bite', 'mark', 'right', 'cheek']\n",
      "Tokenized sentence: ['a', 'boy', 'loved', 'a', 'gal', 'he', 'propsd', 'bt', 'she', 'didnt', 'mind', 'he', 'gv', 'lv', 'lttrs', 'bt', 'her', 'frnds', 'threw', 'thm', 'again', 'd', 'boy', 'decided', 'aproach', 'd', 'gal', 'dt', 'time', 'a', 'truck', 'was', 'speeding', 'towards', 'd', 'gal', 'wn', 'it', 'was', 'about', 'hit', 'd', 'girl', 'd', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'her', 'she', 'asked', 'hw', 'cn', 'u', 'run', 'so', 'fast', 'd', 'boy', 'replied', 'boost', 'is', 'd', 'secret', 'of', 'my', 'energy', 'n', 'instantly', 'd', 'girl', 'shouted', 'our', 'energy', 'n', 'thy', 'lived', 'happily', 'gthr', 'drinking', 'boost', 'evrydy', 'moral', 'of', 'd', 'story', 'i', 'hv', 'free', 'msgs', 'd', 'gud', 'ni']\n",
      "After stop words removal: ['boy', 'loved', 'gal', 'propsd', 'bt', 'didnt', 'mind', 'gv', 'lv', 'lttrs', 'bt', 'frnds', 'threw', 'thm', 'boy', 'decided', 'aproach', 'gal', 'dt', 'time', 'truck', 'speeding', 'towards', 'gal', 'wn', 'hit', 'girl', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'asked', 'hw', 'cn', 'u', 'run', 'fast', 'boy', 'replied', 'boost', 'secret', 'energy', 'n', 'instantly', 'girl', 'shouted', 'energy', 'n', 'thy', 'lived', 'happily', 'gthr', 'drinking', 'boost', 'evrydy', 'moral', 'story', 'hv', 'free', 'msgs', 'gud', 'ni']\n",
      "speed\n",
      "drink\n",
      "After stemming with porters algorithm: ['boi', 'love', 'gal', 'propsd', 'didnt', 'mind', 'lttr', 'frnd', 'threw', 'thm', 'boi', 'decid', 'aproach', 'gal', 'time', 'truck', 'speed', 'toward', 'gal', 'hit', 'girl', 'boi', 'ran', 'like', 'hell', 'save', 'as', 'run', 'fast', 'boi', 'repli', 'boost', 'secret', 'energi', 'instantli', 'girl', 'shout', 'energi', 'thy', 'live', 'happili', 'gthr', 'drin', 'boost', 'evrydi', 'moral', 'stori', 'free', 'msg', 'gud']\n",
      "Tokenized sentence: ['well', 'the', 'weather', 'in', 'cali', 's', 'great', 'but', 'its', 'complexities', 'are', 'great', 'you', 'need', 'a', 'car', 'to', 'move', 'freely', 'its', 'taxes', 'are', 'outrageous', 'but', 'all', 'in', 'all', 'its', 'a', 'great', 'place', 'the', 'sad', 'part', 'is', 'i', 'missing', 'home']\n",
      "After stop words removal: ['well', 'weather', 'cali', 'great', 'complexities', 'great', 'need', 'car', 'move', 'freely', 'taxes', 'outrageous', 'great', 'place', 'sad', 'part', 'missing', 'home']\n",
      "miss\n",
      "After stemming with porters algorithm: ['well', 'weather', 'cali', 'great', 'complex', 'great', 'need', 'car', 'move', 'freeli', 'tax', 'outrag', 'great', 'place', 'sad', 'part', 'miss', 'home']\n",
      "Tokenized sentence: ['i', 'wonder', 'if', 'you', 'll', 'get', 'this', 'text']\n",
      "After stop words removal: ['wonder', 'get', 'text']\n",
      "After stemming with porters algorithm: ['wonder', 'get', 'text']\n",
      "Tokenized sentence: ['will', 'you', 'come', 'online', 'today', 'night']\n",
      "After stop words removal: ['come', 'online', 'today', 'night']\n",
      "After stemming with porters algorithm: ['come', 'onlin', 'todai', 'night']\n",
      "Tokenized sentence: ['ew', 'are', 'you', 'one', 'of', 'them']\n",
      "After stop words removal: ['ew', 'one']\n",
      "After stemming with porters algorithm: ['on']\n",
      "Tokenized sentence: ['shall', 'i', 'come', 'to', 'get', 'pickle']\n",
      "After stop words removal: ['shall', 'come', 'get', 'pickle']\n",
      "After stemming with porters algorithm: ['shall', 'come', 'get', 'pickl']\n",
      "Tokenized sentence: ['babe', 'how', 'goes', 'that', 'day', 'what', 'are', 'you', 'up', 'to', 'i', 'miss', 'you', 'already', 'my', 'love', 'loving', 'kiss', 'i', 'hope', 'everything', 'goes', 'well']\n",
      "After stop words removal: ['babe', 'goes', 'day', 'miss', 'already', 'love', 'loving', 'kiss', 'hope', 'everything', 'goes', 'well']\n",
      "lov\n",
      "everyth\n",
      "After stemming with porters algorithm: ['babe', 'goe', 'dai', 'miss', 'alreadi', 'love', 'love', 'kiss', 'hope', 'everyt', 'goe', 'well']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'a', 'movie', 'collect', 'car', 'oredi']\n",
      "After stop words removal: ['movie', 'collect', 'car', 'oredi']\n",
      "After stemming with porters algorithm: ['movi', 'collect', 'car', 'oredi']\n",
      "Tokenized sentence: ['hi', 'there', 'nights', 'ur', 'lucky', 'night', 'uve', 'been', 'invited', 'xchat', 'the', 'uks', 'wildest', 'chat', 'txt', 'chat', 'to', 'now', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stop words removal: ['hi', 'nights', 'ur', 'lucky', 'night', 'uve', 'invited', 'xchat', 'uks', 'wildest', 'chat', 'txt', 'chat', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stemming with porters algorithm: ['night', 'lucki', 'night', 'uv', 'invit', 'xchat', 'uk', 'wildest', 'chat', 'txt', 'chat', 'msgrcvdhg', 'suit', 'land', 'row', 'ldn', 'yr']\n",
      "Tokenized sentence: ['no', 'chikku', 'nt', 'yet', 'ya', 'i', 'm', 'free']\n",
      "After stop words removal: ['chikku', 'nt', 'yet', 'ya', 'free']\n",
      "After stemming with porters algorithm: ['chikku', 'yet', 'free']\n",
      "Tokenized sentence: ['hi', 'hun', 'im', 'not', 'comin', 'nite', 'tell', 'every', 'im', 'sorry', 'me', 'hope', 'u', 'ava', 'goodtime', 'oli', 'rang', 'melnite', 'ifink', 'it', 'mite', 'b', 'sorted', 'but', 'il', 'explain', 'everythin', 'on', 'mon', 'l', 'rs', 'x']\n",
      "After stop words removal: ['hi', 'hun', 'im', 'comin', 'nite', 'tell', 'every', 'im', 'sorry', 'hope', 'u', 'ava', 'goodtime', 'oli', 'rang', 'melnite', 'ifink', 'mite', 'b', 'sorted', 'il', 'explain', 'everythin', 'mon', 'l', 'rs', 'x']\n",
      "After stemming with porters algorithm: ['hun', 'comin', 'nite', 'tell', 'everi', 'sorri', 'hope', 'ava', 'goodtim', 'oli', 'rang', 'melnit', 'ifink', 'mite', 'sor', 'explain', 'everythin', 'mon']\n",
      "Tokenized sentence: ['kind', 'of', 'just', 'missed', 'train', 'cos', 'of', 'asthma', 'attack', 'nxt', 'one', 'in', 'half', 'hr', 'so', 'driving', 'in', 'not', 'sure', 'where', 'to', 'park']\n",
      "After stop words removal: ['kind', 'missed', 'train', 'cos', 'asthma', 'attack', 'nxt', 'one', 'half', 'hr', 'driving', 'sure', 'park']\n",
      "driv\n",
      "After stemming with porters algorithm: ['kind', 'miss', 'train', 'co', 'asthma', 'attack', 'nxt', 'on', 'half', 'drive', 'sure', 'park']\n",
      "Tokenized sentence: ['dear', 'where', 'you', 'call', 'me']\n",
      "After stop words removal: ['dear', 'call']\n",
      "After stemming with porters algorithm: ['dear', 'call']\n",
      "Tokenized sentence: ['moji', 'i', 'love', 'you', 'more', 'than', 'words', 'have', 'a', 'rich', 'day']\n",
      "After stop words removal: ['moji', 'love', 'words', 'rich', 'day']\n",
      "After stemming with porters algorithm: ['moji', 'love', 'word', 'rich', 'dai']\n",
      "Tokenized sentence: ['yes', 'but', 'i', 'dont', 'care', 'i', 'need', 'you', 'bad', 'princess']\n",
      "After stop words removal: ['yes', 'dont', 'care', 'need', 'bad', 'princess']\n",
      "After stemming with porters algorithm: ['ye', 'dont', 'care', 'need', 'bad', 'princess']\n",
      "Tokenized sentence: ['not', 'tonight', 'mate', 'catching', 'up', 'on', 'some', 'sleep', 'this', 'is', 'my', 'new', 'number', 'by', 'the', 'way']\n",
      "After stop words removal: ['tonight', 'mate', 'catching', 'sleep', 'new', 'number', 'way']\n",
      "catch\n",
      "After stemming with porters algorithm: ['tonight', 'mate', 'catc', 'sleep', 'new', 'number', 'wai']\n",
      "Tokenized sentence: ['fantasy', 'football', 'is', 'back', 'on', 'your', 'tv', 'go', 'to', 'sky', 'gamestar', 'on', 'sky', 'active', 'and', 'play', 'k', 'dream', 'team', 'scoring', 'starts', 'on', 'saturday', 'so', 'register', 'now', 'sky', 'opt', 'out', 'to']\n",
      "After stop words removal: ['fantasy', 'football', 'back', 'tv', 'go', 'sky', 'gamestar', 'sky', 'active', 'play', 'k', 'dream', 'team', 'scoring', 'starts', 'saturday', 'register', 'sky', 'opt']\n",
      "scor\n",
      "After stemming with porters algorithm: ['fantasi', 'footbal', 'back', 'sky', 'gamestar', 'sky', 'activ', 'plai', 'dream', 'team', 'score', 'start', 'saturdai', 'regist', 'sky', 'opt']\n",
      "Tokenized sentence: ['free', 'msg', 'single', 'find', 'a', 'partner', 'in', 'your', 'area', 's', 'of', 'real', 'people', 'are', 'waiting', 'to', 'chat', 'now', 'send', 'chat', 'to', 'cncl', 'send', 'stopcs', 'per', 'msg']\n",
      "After stop words removal: ['free', 'msg', 'single', 'find', 'partner', 'area', 'real', 'people', 'waiting', 'chat', 'send', 'chat', 'cncl', 'send', 'stopcs', 'per', 'msg']\n",
      "wait\n",
      "After stemming with porters algorithm: ['free', 'msg', 'singl', 'find', 'partner', 'area', 'real', 'peopl', 'wait', 'chat', 'send', 'chat', 'cncl', 'send', 'stopc', 'per', 'msg']\n",
      "Tokenized sentence: ['also', 'where', 's', 'the', 'piece']\n",
      "After stop words removal: ['also', 'piece']\n",
      "After stemming with porters algorithm: ['also', 'piec']\n",
      "Tokenized sentence: ['hope', 'this', 'text', 'meets', 'you', 'smiling', 'if', 'not', 'then', 'let', 'this', 'text', 'give', 'you', 'a', 'reason', 'to', 'smile', 'have', 'a', 'beautiful', 'day']\n",
      "After stop words removal: ['hope', 'text', 'meets', 'smiling', 'let', 'text', 'give', 'reason', 'smile', 'beautiful', 'day']\n",
      "smil\n",
      "After stemming with porters algorithm: ['hope', 'text', 'meet', 'smile', 'let', 'text', 'give', 'reason', 'smile', 'beauti', 'dai']\n",
      "Tokenized sentence: ['ringtone', 'club', 'gr', 'new', 'polys', 'direct', 'to', 'your', 'mobile', 'every', 'week']\n",
      "After stop words removal: ['ringtone', 'club', 'gr', 'new', 'polys', 'direct', 'mobile', 'every', 'week']\n",
      "After stemming with porters algorithm: ['rington', 'club', 'new', 'poli', 'direct', 'mobil', 'everi', 'week']\n",
      "Tokenized sentence: ['k', 'i', 'll', 'head', 'out', 'in', 'a', 'few', 'mins', 'see', 'you', 'there']\n",
      "After stop words removal: ['k', 'head', 'mins', 'see']\n",
      "After stemming with porters algorithm: ['head', 'min', 'see']\n",
      "Tokenized sentence: ['spoke', 'with', 'uncle', 'john', 'today', 'he', 'strongly', 'feels', 'that', 'you', 'need', 'to', 'sacrifice', 'to', 'keep', 'me', 'here', 'he', 's', 'going', 'to', 'call', 'you', 'when', 'he', 'does', 'i', 'beg', 'you', 'to', 'just', 'listen', 'dont', 'make', 'any', 'promises', 'or', 'make', 'it', 'clear', 'things', 'are', 'not', 'easy', 'and', 'i', 'need', 'you', 'to', 'please', 'let', 'us', 'work', 'things', 'out', 'as', 'long', 'as', 'i', 'keep', 'expecting', 'help', 'my', 'creativity', 'will', 'be', 'stifled', 'so', 'pls', 'just', 'keep', 'him', 'happy', 'no', 'promises', 'on', 'your', 'part']\n",
      "After stop words removal: ['spoke', 'uncle', 'john', 'today', 'strongly', 'feels', 'need', 'sacrifice', 'keep', 'going', 'call', 'beg', 'listen', 'dont', 'make', 'promises', 'make', 'clear', 'things', 'easy', 'need', 'please', 'let', 'us', 'work', 'things', 'long', 'keep', 'expecting', 'help', 'creativity', 'stifled', 'pls', 'keep', 'happy', 'promises', 'part']\n",
      "go\n",
      "expect\n",
      "After stemming with porters algorithm: ['spoke', 'uncl', 'john', 'todai', 'strongli', 'feel', 'need', 'sacrific', 'keep', 'go', 'call', 'beg', 'listen', 'dont', 'make', 'promis', 'make', 'clear', 'thing', 'easi', 'need', 'pleas', 'let', 'work', 'thing', 'long', 'keep', 'expec', 'help', 'creativ', 'stifl', 'pl', 'keep', 'happi', 'promis', 'part']\n",
      "Tokenized sentence: ['super', 'da', 'good', 'replacement', 'for', 'murali']\n",
      "After stop words removal: ['super', 'da', 'good', 'replacement', 'murali']\n",
      "After stemming with porters algorithm: ['super', 'good', 'replac', 'murali']\n",
      "Tokenized sentence: ['i', 'thank', 'you', 'so', 'much', 'for', 'all', 'you', 'do', 'with', 'selflessness', 'i', 'love', 'you', 'plenty']\n",
      "After stop words removal: ['thank', 'much', 'selflessness', 'love', 'plenty']\n",
      "After stemming with porters algorithm: ['thank', 'much', 'selfless', 'love', 'plenti']\n",
      "Tokenized sentence: ['that', 's', 'the', 'way', 'you', 'should', 'stay', 'oh']\n",
      "After stop words removal: ['way', 'stay', 'oh']\n",
      "After stemming with porters algorithm: ['wai', 'stai']\n",
      "Tokenized sentence: ['you', 'stayin', 'out', 'of', 'trouble', 'stranger', 'saw', 'dave', 'the', 'other', 'day', 'he', 's', 'sorted', 'now', 'still', 'with', 'me', 'bloke', 'when', 'u', 'gona', 'get', 'a', 'girl', 'mr', 'ur', 'mum', 'still', 'thinks', 'we', 'will', 'get', 'getha']\n",
      "After stop words removal: ['stayin', 'trouble', 'stranger', 'saw', 'dave', 'day', 'sorted', 'still', 'bloke', 'u', 'gona', 'get', 'girl', 'mr', 'ur', 'mum', 'still', 'thinks', 'get', 'getha']\n",
      "After stemming with porters algorithm: ['stayin', 'troubl', 'stranger', 'saw', 'dave', 'dai', 'sor', 'still', 'bloke', 'gona', 'get', 'girl', 'mum', 'still', 'think', 'get', 'getha']\n",
      "Tokenized sentence: ['yes', 'i', 'have', 'so', 'that', 's', 'why', 'u', 'texted', 'pshew', 'missing', 'you', 'so', 'much']\n",
      "After stop words removal: ['yes', 'u', 'texted', 'pshew', 'missing', 'much']\n",
      "miss\n",
      "After stemming with porters algorithm: ['ye', 'tex', 'pshew', 'miss', 'much']\n",
      "Tokenized sentence: ['not', 'a', 'lot', 'has', 'happened', 'here', 'feels', 'very', 'quiet', 'beth', 'is', 'at', 'her', 'aunts', 'and', 'charlie', 'is', 'working', 'lots', 'just', 'me', 'and', 'helen', 'in', 'at', 'the', 'mo', 'how', 'have', 'you', 'been']\n",
      "After stop words removal: ['lot', 'happened', 'feels', 'quiet', 'beth', 'aunts', 'charlie', 'working', 'lots', 'helen', 'mo']\n",
      "work\n",
      "After stemming with porters algorithm: ['lot', 'happen', 'feel', 'quiet', 'beth', 'aunt', 'charli', 'wor', 'lot', 'helen']\n",
      "Tokenized sentence: ['great', 'i', 'shoot', 'big', 'loads', 'so', 'get', 'ready']\n",
      "After stop words removal: ['great', 'shoot', 'big', 'loads', 'get', 'ready']\n",
      "After stemming with porters algorithm: ['great', 'shoot', 'big', 'load', 'get', 'readi']\n",
      "Tokenized sentence: ['natalja', 'f', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'www', 'sms', 'ac', 'u', 'nat', 'stop', 'send', 'stop', 'frnd', 'to']\n",
      "After stop words removal: ['natalja', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'www', 'sms', 'ac', 'u', 'nat', 'stop', 'send', 'stop', 'frnd']\n",
      "invit\n",
      "After stemming with porters algorithm: ['natalja', 'invit', 'friend', 'repli', 'ye', 'see', 'www', 'sm', 'nat', 'stop', 'send', 'stop', 'frnd']\n",
      "Tokenized sentence: ['he', 's', 'an', 'adult', 'and', 'would', 'learn', 'from', 'the', 'experience', 'there', 's', 'no', 'real', 'danger', 'i', 'just', 'dont', 'like', 'peeps', 'using', 'drugs', 'they', 'dont', 'need', 'but', 'no', 'comment']\n",
      "After stop words removal: ['adult', 'would', 'learn', 'experience', 'real', 'danger', 'dont', 'like', 'peeps', 'using', 'drugs', 'dont', 'need', 'comment']\n",
      "us\n",
      "After stemming with porters algorithm: ['adult', 'would', 'learn', 'experi', 'real', 'danger', 'dont', 'like', 'peep', 'us', 'drug', 'dont', 'need', 'comment']\n",
      "Tokenized sentence: ['no', 'way', 'i', 'm', 'going', 'back', 'there']\n",
      "After stop words removal: ['way', 'going', 'back']\n",
      "go\n",
      "After stemming with porters algorithm: ['wai', 'go', 'back']\n",
      "Tokenized sentence: ['i', 'love', 'ya', 'too', 'but', 'try', 'and', 'budget', 'your', 'money', 'better', 'babe', 'gary', 'would', 'freak', 'on', 'me', 'if', 'he', 'knew']\n",
      "After stop words removal: ['love', 'ya', 'try', 'budget', 'money', 'better', 'babe', 'gary', 'would', 'freak', 'knew']\n",
      "After stemming with porters algorithm: ['love', 'try', 'budget', 'monei', 'better', 'babe', 'gari', 'would', 'freak', 'knew']\n",
      "Tokenized sentence: ['no', 'need', 'to', 'ke', 'qi', 'too', 'bored', 'izzit', 'y', 'suddenly', 'thk', 'of', 'this']\n",
      "After stop words removal: ['need', 'ke', 'qi', 'bored', 'izzit', 'suddenly', 'thk']\n",
      "After stemming with porters algorithm: ['need', 'bore', 'izzit', 'suddenli', 'thk']\n",
      "Tokenized sentence: ['no', 'calls', 'messages', 'missed', 'calls']\n",
      "After stop words removal: ['calls', 'messages', 'missed', 'calls']\n",
      "After stemming with porters algorithm: ['call', 'messag', 'miss', 'call']\n",
      "Tokenized sentence: ['were', 'somewhere', 'on', 'fredericksburg']\n",
      "After stop words removal: ['somewhere', 'fredericksburg']\n",
      "After stemming with porters algorithm: ['somewher', 'fredericksburg']\n",
      "Tokenized sentence: ['now', 'got', 'tv', 'watch', 'meh', 'u', 'no', 'work', 'today']\n",
      "After stop words removal: ['got', 'tv', 'watch', 'meh', 'u', 'work', 'today']\n",
      "After stemming with porters algorithm: ['got', 'watch', 'meh', 'work', 'todai']\n",
      "Tokenized sentence: ['i', 'just', 'really', 'need', 'shit', 'before', 'tomorrow', 'and', 'i', 'know', 'you', 'won', 't', 'be', 'awake', 'before', 'like']\n",
      "After stop words removal: ['really', 'need', 'shit', 'tomorrow', 'know', 'awake', 'like']\n",
      "After stemming with porters algorithm: ['realli', 'need', 'shit', 'tomorrow', 'know', 'awak', 'like']\n",
      "Tokenized sentence: ['thank', 'you', 'and', 'by', 'the', 'way', 'i', 'just', 'lost']\n",
      "After stop words removal: ['thank', 'way', 'lost']\n",
      "After stemming with porters algorithm: ['thank', 'wai', 'lost']\n",
      "Tokenized sentence: ['you', 'd', 'like', 'that', 'wouldn', 't', 'you', 'jerk']\n",
      "After stop words removal: ['like', 'jerk']\n",
      "After stemming with porters algorithm: ['like', 'jerk']\n",
      "Tokenized sentence: ['sounds', 'like', 'a', 'plan', 'cardiff', 'is', 'still', 'here', 'and', 'still', 'cold', 'i', 'm', 'sitting', 'on', 'the', 'radiator']\n",
      "After stop words removal: ['sounds', 'like', 'plan', 'cardiff', 'still', 'still', 'cold', 'sitting', 'radiator']\n",
      "sitt\n",
      "After stemming with porters algorithm: ['sound', 'like', 'plan', 'cardiff', 'still', 'still', 'cold', 'sit', 'radiat']\n",
      "Tokenized sentence: ['as', 'one', 'of', 'our', 'registered', 'subscribers', 'u', 'can', 'enter', 'the', 'draw', 'a', 'g', 'b', 'gift', 'voucher', 'by', 'replying', 'with', 'enter', 'to', 'unsubscribe', 'text', 'stop']\n",
      "After stop words removal: ['one', 'registered', 'subscribers', 'u', 'enter', 'draw', 'g', 'b', 'gift', 'voucher', 'replying', 'enter', 'unsubscribe', 'text', 'stop']\n",
      "reply\n",
      "After stemming with porters algorithm: ['on', 'regist', 'subscrib', 'enter', 'draw', 'gift', 'voucher', 'repl', 'enter', 'unsubscrib', 'text', 'stop']\n",
      "Tokenized sentence: ['hey', 'now', 'am', 'free', 'you', 'can', 'call', 'me']\n",
      "After stop words removal: ['hey', 'free', 'call']\n",
      "After stemming with porters algorithm: ['hei', 'free', 'call']\n",
      "Tokenized sentence: ['i', 'm', 'going', 'orchard', 'now', 'laready', 'me', 'reaching', 'soon', 'u', 'reaching']\n",
      "After stop words removal: ['going', 'orchard', 'laready', 'reaching', 'soon', 'u', 'reaching']\n",
      "go\n",
      "reach\n",
      "reach\n",
      "After stemming with porters algorithm: ['go', 'orchard', 'lareadi', 'reac', 'soon', 'reac']\n",
      "Tokenized sentence: ['no', 'gifts', 'you', 'trying', 'to', 'get', 'me', 'to', 'throw', 'myself', 'off', 'a', 'cliff', 'or', 'something']\n",
      "After stop words removal: ['gifts', 'trying', 'get', 'throw', 'cliff', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['gift', 'trying', 'get', 'throw', 'cliff', 'somet']\n",
      "Tokenized sentence: ['yeah', 'just', 'open', 'chat', 'and', 'click', 'friend', 'lists', 'then', 'make', 'the', 'list', 'easy', 'as', 'pie']\n",
      "After stop words removal: ['yeah', 'open', 'chat', 'click', 'friend', 'lists', 'make', 'list', 'easy', 'pie']\n",
      "After stemming with porters algorithm: ['yeah', 'open', 'chat', 'click', 'friend', 'list', 'make', 'list', 'easi', 'pie']\n",
      "Tokenized sentence: ['gettin', 'rdy', 'to', 'ship', 'comp']\n",
      "After stop words removal: ['gettin', 'rdy', 'ship', 'comp']\n",
      "After stemming with porters algorithm: ['gettin', 'rdy', 'ship', 'comp']\n",
      "Tokenized sentence: ['huh', 'y', 'lei']\n",
      "After stop words removal: ['huh', 'lei']\n",
      "After stemming with porters algorithm: ['huh', 'lei']\n",
      "Tokenized sentence: ['i', 'get', 'out', 'of', 'class', 'in', 'bsn', 'in', 'like', 'lt', 'gt', 'minutes', 'you', 'know', 'where', 'advising', 'is']\n",
      "After stop words removal: ['get', 'class', 'bsn', 'like', 'lt', 'gt', 'minutes', 'know', 'advising']\n",
      "advis\n",
      "After stemming with porters algorithm: ['get', 'class', 'bsn', 'like', 'minut', 'know', 'advis']\n",
      "Tokenized sentence: ['hey', 'what', 'happen', 'de', 'are', 'you', 'alright']\n",
      "After stop words removal: ['hey', 'happen', 'de', 'alright']\n",
      "After stemming with porters algorithm: ['hei', 'happen', 'alright']\n",
      "Tokenized sentence: ['miss', 'ya', 'need', 'ya', 'want', 'ya', 'love', 'ya']\n",
      "After stop words removal: ['miss', 'ya', 'need', 'ya', 'want', 'ya', 'love', 'ya']\n",
      "After stemming with porters algorithm: ['miss', 'need', 'want', 'love']\n",
      "Tokenized sentence: ['oooh', 'bed', 'ridden', 'ey', 'what', 'are', 'you', 'thinking', 'of']\n",
      "After stop words removal: ['oooh', 'bed', 'ridden', 'ey', 'thinking']\n",
      "think\n",
      "After stemming with porters algorithm: ['oooh', 'bed', 'ridden', 'thin']\n",
      "Tokenized sentence: ['i', 'though', 'we', 'shd', 'go', 'out', 'n', 'have', 'some', 'fun', 'so', 'bar', 'in', 'town', 'or', 'something', 'sound', 'ok']\n",
      "After stop words removal: ['though', 'shd', 'go', 'n', 'fun', 'bar', 'town', 'something', 'sound', 'ok']\n",
      "someth\n",
      "After stemming with porters algorithm: ['though', 'shd', 'fun', 'bar', 'town', 'somet', 'sound']\n",
      "Tokenized sentence: ['u', 'too']\n",
      "After stop words removal: ['u']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['which', 'is', 'why', 'i', 'never', 'wanted', 'to', 'tell', 'you', 'any', 'of', 'this', 'which', 'is', 'why', 'i', 'm', 'so', 'short', 'with', 'you', 'and', 'on', 'edge', 'as', 'of', 'late']\n",
      "After stop words removal: ['never', 'wanted', 'tell', 'short', 'edge', 'late']\n",
      "After stemming with porters algorithm: ['never', 'wan', 'tell', 'short', 'edg', 'late']\n",
      "Tokenized sentence: ['are', 'you', 'there', 'in', 'room']\n",
      "After stop words removal: ['room']\n",
      "After stemming with porters algorithm: ['room']\n",
      "Tokenized sentence: ['asking', 'do', 'u', 'knw', 'them', 'or', 'nt', 'may', 'be', 'ur', 'frnds', 'or', 'classmates']\n",
      "After stop words removal: ['asking', 'u', 'knw', 'nt', 'may', 'ur', 'frnds', 'classmates']\n",
      "ask\n",
      "After stemming with porters algorithm: ['as', 'knw', 'mai', 'frnd', 'classmat']\n",
      "Tokenized sentence: ['more', 'people', 'are', 'dogging', 'in', 'your', 'area', 'now', 'call', 'and', 'join', 'like', 'minded', 'guys', 'why', 'not', 'arrange', 'yourself', 'there', 's', 'this', 'evening', 'a', 'minapn', 'ls', 'bb']\n",
      "After stop words removal: ['people', 'dogging', 'area', 'call', 'join', 'like', 'minded', 'guys', 'arrange', 'evening', 'minapn', 'ls', 'bb']\n",
      "dogg\n",
      "even\n",
      "After stemming with porters algorithm: ['peopl', 'dog', 'area', 'call', 'join', 'like', 'min', 'gui', 'arrang', 'even', 'minapn']\n",
      "Tokenized sentence: ['ok', 'i', 'go', 'change', 'also']\n",
      "After stop words removal: ['ok', 'go', 'change', 'also']\n",
      "After stemming with porters algorithm: ['chang', 'also']\n",
      "Tokenized sentence: ['me', 'too', 'watching', 'surya', 'movie', 'only', 'after', 'pm', 'vijay', 'movie', 'pokkiri']\n",
      "After stop words removal: ['watching', 'surya', 'movie', 'pm', 'vijay', 'movie', 'pokkiri']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'surya', 'movi', 'vijai', 'movi', 'pokkiri']\n",
      "Tokenized sentence: ['but', 'pls', 'dont', 'play', 'in', 'others', 'life']\n",
      "After stop words removal: ['pls', 'dont', 'play', 'others', 'life']\n",
      "After stemming with porters algorithm: ['pl', 'dont', 'plai', 'other', 'life']\n",
      "Tokenized sentence: ['no', 'it', 'was', 'cancelled', 'yeah', 'baby', 'well', 'that', 'sounds', 'important', 'so', 'i', 'understand', 'my', 'darlin', 'give', 'me', 'a', 'ring', 'later', 'on', 'this', 'fone', 'love', 'kate', 'x']\n",
      "After stop words removal: ['cancelled', 'yeah', 'baby', 'well', 'sounds', 'important', 'understand', 'darlin', 'give', 'ring', 'later', 'fone', 'love', 'kate', 'x']\n",
      "After stemming with porters algorithm: ['cancel', 'yeah', 'babi', 'well', 'sound', 'import', 'understand', 'darlin', 'give', 'ring', 'later', 'fone', 'love', 'kate']\n",
      "Tokenized sentence: ['just', 'chill', 'for', 'another', 'hrs', 'if', 'you', 'could', 'sleep', 'the', 'pain', 'is', 'not', 'a', 'surgical', 'emergency', 'so', 'see', 'how', 'it', 'unfolds', 'okay']\n",
      "After stop words removal: ['chill', 'another', 'hrs', 'could', 'sleep', 'pain', 'surgical', 'emergency', 'see', 'unfolds', 'okay']\n",
      "After stemming with porters algorithm: ['chill', 'anoth', 'hr', 'could', 'sleep', 'pain', 'surgic', 'emerg', 'see', 'unfold', 'okai']\n",
      "Tokenized sentence: ['pick', 'you', 'up', 'bout', 'ish', 'what', 'time', 'are', 'and', 'that', 'going']\n",
      "After stop words removal: ['pick', 'bout', 'ish', 'time', 'going']\n",
      "go\n",
      "After stemming with porters algorithm: ['pick', 'bout', 'ish', 'time', 'go']\n",
      "Tokenized sentence: ['freemsg', 'feelin', 'kinda', 'lnly', 'hope', 'u', 'like', 'keep', 'me', 'company', 'jst', 'got', 'a', 'cam', 'moby', 'wanna', 'c', 'my', 'pic', 'txt', 'or', 'reply', 'date', 'to', 'msg', 'p', 'rcv', 'hlp', 'stop', 'to']\n",
      "After stop words removal: ['freemsg', 'feelin', 'kinda', 'lnly', 'hope', 'u', 'like', 'keep', 'company', 'jst', 'got', 'cam', 'moby', 'wanna', 'c', 'pic', 'txt', 'reply', 'date', 'msg', 'p', 'rcv', 'hlp', 'stop']\n",
      "After stemming with porters algorithm: ['freemsg', 'feelin', 'kinda', 'lnly', 'hope', 'like', 'keep', 'compani', 'jst', 'got', 'cam', 'mobi', 'wanna', 'pic', 'txt', 'repli', 'date', 'msg', 'rcv', 'hlp', 'stop']\n",
      "Tokenized sentence: ['ok', 'im', 'not', 'sure', 'what', 'time', 'i', 'finish', 'tomorrow', 'but', 'i', 'wanna', 'spend', 'the', 'evening', 'with', 'you', 'cos', 'that', 'would', 'be', 'vewy', 'vewy', 'lubly', 'love', 'me', 'xxx']\n",
      "After stop words removal: ['ok', 'im', 'sure', 'time', 'finish', 'tomorrow', 'wanna', 'spend', 'evening', 'cos', 'would', 'vewy', 'vewy', 'lubly', 'love', 'xxx']\n",
      "even\n",
      "After stemming with porters algorithm: ['sure', 'time', 'finish', 'tomorrow', 'wanna', 'spend', 'even', 'co', 'would', 'vewi', 'vewi', 'lubli', 'love', 'xxx']\n",
      "Tokenized sentence: ['great', 'so', 'should', 'i', 'send', 'you', 'my', 'account', 'number']\n",
      "After stop words removal: ['great', 'send', 'account', 'number']\n",
      "After stemming with porters algorithm: ['great', 'send', 'account', 'number']\n",
      "Tokenized sentence: ['free', 'message', 'thanks', 'for', 'using', 'the', 'auction', 'subscription', 'service', 'p', 'msgrcvd', 'skip', 'an', 'auction', 'txt', 'out', 'unsubscribe', 'txt', 'stop', 'customercare']\n",
      "After stop words removal: ['free', 'message', 'thanks', 'using', 'auction', 'subscription', 'service', 'p', 'msgrcvd', 'skip', 'auction', 'txt', 'unsubscribe', 'txt', 'stop', 'customercare']\n",
      "us\n",
      "After stemming with porters algorithm: ['free', 'messag', 'thank', 'us', 'auct', 'subscript', 'servic', 'msgrcvd', 'skip', 'auct', 'txt', 'unsubscrib', 'txt', 'stop', 'customercar']\n",
      "Tokenized sentence: ['wylie', 'update', 'my', 'weed', 'dealer', 'carlos', 'went', 'to', 'freedom', 'and', 'had', 'a', 'class', 'with', 'lunsford']\n",
      "After stop words removal: ['wylie', 'update', 'weed', 'dealer', 'carlos', 'went', 'freedom', 'class', 'lunsford']\n",
      "After stemming with porters algorithm: ['wylie', 'updat', 'weed', 'dealer', 'carlo', 'went', 'freedom', 'class', 'lunsford']\n",
      "Tokenized sentence: ['maybe', 'if', 'you', 'woke', 'up', 'before', 'fucking', 'this', 'wouldn', 't', 'be', 'a', 'problem']\n",
      "After stop words removal: ['maybe', 'woke', 'fucking', 'problem']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['mayb', 'woke', 'fuc', 'problem']\n",
      "Tokenized sentence: ['much', 'better', 'now', 'thanks', 'lol']\n",
      "After stop words removal: ['much', 'better', 'thanks', 'lol']\n",
      "After stemming with porters algorithm: ['much', 'better', 'thank', 'lol']\n",
      "Tokenized sentence: ['aiyo', 'please', 'got', 'time', 'meh']\n",
      "After stop words removal: ['aiyo', 'please', 'got', 'time', 'meh']\n",
      "After stemming with porters algorithm: ['aiyo', 'pleas', 'got', 'time', 'meh']\n",
      "Tokenized sentence: ['aiyo', 'her', 'lesson', 'so', 'early', 'i', 'm', 'still', 'sleepin', 'haha', 'okie', 'u', 'go', 'home', 'liao', 'den', 'confirm', 'w', 'me', 'lor']\n",
      "After stop words removal: ['aiyo', 'lesson', 'early', 'still', 'sleepin', 'haha', 'okie', 'u', 'go', 'home', 'liao', 'den', 'confirm', 'w', 'lor']\n",
      "After stemming with porters algorithm: ['aiyo', 'lesson', 'earli', 'still', 'sleepin', 'haha', 'oki', 'home', 'liao', 'den', 'confirm', 'lor']\n",
      "Tokenized sentence: ['oh', 'yeah', 'and', 'my', 'diet', 'just', 'flew', 'out', 'the', 'window']\n",
      "After stop words removal: ['oh', 'yeah', 'diet', 'flew', 'window']\n",
      "After stemming with porters algorithm: ['yeah', 'diet', 'flew', 'window']\n",
      "Tokenized sentence: ['come', 'aftr', 'lt', 'decimal', 'gt', 'now', 'i', 'm', 'cleaning', 'the', 'house']\n",
      "After stop words removal: ['come', 'aftr', 'lt', 'decimal', 'gt', 'cleaning', 'house']\n",
      "clean\n",
      "After stemming with porters algorithm: ['come', 'aftr', 'decim', 'clean', 'hous']\n",
      "Tokenized sentence: ['does', 'she', 'usually', 'take', 'fifteen', 'fucking', 'minutes', 'to', 'respond', 'to', 'a', 'yes', 'or', 'no', 'question']\n",
      "After stop words removal: ['usually', 'take', 'fifteen', 'fucking', 'minutes', 'respond', 'yes', 'question']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['usual', 'take', 'fifteen', 'fuc', 'minut', 'respond', 'ye', 'quest']\n",
      "Tokenized sentence: ['then', 'ur', 'sis', 'how']\n",
      "After stop words removal: ['ur', 'sis']\n",
      "After stemming with porters algorithm: ['si']\n",
      "Tokenized sentence: ['hey', 'hun', 'onbus', 'goin', 'meet', 'him', 'he', 'wants', 'go', 'out', 'a', 'meal', 'but', 'i', 'donyt', 'feel', 'like', 'it', 'cuz', 'have', 'get', 'last', 'bus', 'home', 'but', 'hes', 'sweet', 'latelyxxx']\n",
      "After stop words removal: ['hey', 'hun', 'onbus', 'goin', 'meet', 'wants', 'go', 'meal', 'donyt', 'feel', 'like', 'cuz', 'get', 'last', 'bus', 'home', 'hes', 'sweet', 'latelyxxx']\n",
      "After stemming with porters algorithm: ['hei', 'hun', 'onbu', 'goin', 'meet', 'want', 'meal', 'donyt', 'feel', 'like', 'cuz', 'get', 'last', 'bu', 'home', 'he', 'sweet', 'latelyxxx']\n",
      "Tokenized sentence: ['shant', 'disturb', 'u', 'anymore', 'jia', 'you']\n",
      "After stop words removal: ['shant', 'disturb', 'u', 'anymore', 'jia']\n",
      "After stemming with porters algorithm: ['shant', 'disturb', 'anymor', 'jia']\n",
      "Tokenized sentence: ['yar', 'he', 'quite', 'clever', 'but', 'aft', 'many', 'guesses', 'lor', 'he', 'got', 'ask', 'me', 'bring', 'but', 'i', 'thk', 'darren', 'not', 'so', 'willing', 'go', 'aiya', 'they', 'thk', 'leona', 'still', 'not', 'attach', 'wat']\n",
      "After stop words removal: ['yar', 'quite', 'clever', 'aft', 'many', 'guesses', 'lor', 'got', 'ask', 'bring', 'thk', 'darren', 'willing', 'go', 'aiya', 'thk', 'leona', 'still', 'attach', 'wat']\n",
      "will\n",
      "After stemming with porters algorithm: ['yar', 'quit', 'clever', 'aft', 'mani', 'guess', 'lor', 'got', 'ask', 'bring', 'thk', 'darren', 'will', 'aiya', 'thk', 'leona', 'still', 'attach', 'wat']\n",
      "Tokenized sentence: ['hey', 'gorgeous', 'man', 'my', 'work', 'mobile', 'number', 'is', 'have', 'a', 'good', 'one', 'babe', 'squishy', 'mwahs']\n",
      "After stop words removal: ['hey', 'gorgeous', 'man', 'work', 'mobile', 'number', 'good', 'one', 'babe', 'squishy', 'mwahs']\n",
      "After stemming with porters algorithm: ['hei', 'gorgeou', 'man', 'work', 'mobil', 'number', 'good', 'on', 'babe', 'squishi', 'mwah']\n",
      "Tokenized sentence: ['p', 'alfie', 'moon', 's', 'children', 'in', 'need', 'song', 'on', 'ur', 'mob', 'tell', 'ur', 'm', 's', 'txt', 'tone', 'charity', 'to', 'for', 'nokias', 'or', 'poly', 'charity', 'for', 'polys', 'zed', 'profit', 'charity']\n",
      "After stop words removal: ['p', 'alfie', 'moon', 'children', 'need', 'song', 'ur', 'mob', 'tell', 'ur', 'txt', 'tone', 'charity', 'nokias', 'poly', 'charity', 'polys', 'zed', 'profit', 'charity']\n",
      "After stemming with porters algorithm: ['alfi', 'moon', 'children', 'need', 'song', 'mob', 'tell', 'txt', 'tone', 'chariti', 'nokia', 'poli', 'chariti', 'poli', 'zed', 'profit', 'chariti']\n",
      "Tokenized sentence: ['yo', 'im', 'right', 'by', 'yo', 'work']\n",
      "After stop words removal: ['yo', 'im', 'right', 'yo', 'work']\n",
      "After stemming with porters algorithm: ['right', 'work']\n",
      "Tokenized sentence: ['sorry', 'my', 'battery', 'died', 'i', 'can', 'come', 'by', 'but', 'i', 'm', 'only', 'getting', 'a', 'gram', 'for', 'now', 'where', 's', 'your', 'place']\n",
      "After stop words removal: ['sorry', 'battery', 'died', 'come', 'getting', 'gram', 'place']\n",
      "gett\n",
      "After stemming with porters algorithm: ['sorri', 'batteri', 'di', 'come', 'get', 'gram', 'place']\n",
      "Tokenized sentence: ['hello', 'we', 'need', 'some', 'posh', 'birds', 'and', 'chaps', 'to', 'user', 'trial', 'prods', 'for', 'champneys', 'can', 'i', 'put', 'you', 'down', 'i', 'need', 'your', 'address', 'and', 'dob', 'asap', 'ta', 'r']\n",
      "After stop words removal: ['hello', 'need', 'posh', 'birds', 'chaps', 'user', 'trial', 'prods', 'champneys', 'put', 'need', 'address', 'dob', 'asap', 'ta', 'r']\n",
      "After stemming with porters algorithm: ['hello', 'need', 'posh', 'bird', 'chap', 'user', 'trial', 'prod', 'champnei', 'put', 'need', 'address', 'dob', 'asap']\n",
      "Tokenized sentence: ['ok', 'i', 'only', 'ask', 'abt', 'e', 'movie', 'u', 'wan', 'ktv', 'oso']\n",
      "After stop words removal: ['ok', 'ask', 'abt', 'e', 'movie', 'u', 'wan', 'ktv', 'oso']\n",
      "After stemming with porters algorithm: ['ask', 'abt', 'movi', 'wan', 'ktv', 'oso']\n",
      "Tokenized sentence: ['hey', 'whats', 'up', 'u', 'sleeping', 'all', 'morning']\n",
      "After stop words removal: ['hey', 'whats', 'u', 'sleeping', 'morning']\n",
      "sleep\n",
      "morn\n",
      "After stemming with porters algorithm: ['hei', 'what', 'sleep', 'mor']\n",
      "Tokenized sentence: ['for', 'ur', 'chance', 'to', 'win', 'a', 'wkly', 'shopping', 'spree', 'txt', 'shop', 'to', 't', 's', 'c', 's', 'www', 'txt', 'shop', 'com', 'custcare', 'x', 'p', 'wk']\n",
      "After stop words removal: ['ur', 'chance', 'win', 'wkly', 'shopping', 'spree', 'txt', 'shop', 'c', 'www', 'txt', 'shop', 'com', 'custcare', 'x', 'p', 'wk']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['chanc', 'win', 'wkly', 'shop', 'spree', 'txt', 'shop', 'www', 'txt', 'shop', 'com', 'custcar']\n",
      "Tokenized sentence: ['hi', 'this', 'is', 'yijue', 'it', 's', 'regarding', 'the', 'textbook', 'it', 's', 'intro', 'to', 'algorithms', 'second', 'edition', 'i', 'm', 'selling', 'it', 'for']\n",
      "After stop words removal: ['hi', 'yijue', 'regarding', 'textbook', 'intro', 'algorithms', 'second', 'edition', 'selling']\n",
      "regard\n",
      "sell\n",
      "After stemming with porters algorithm: ['yiju', 'regar', 'textbook', 'intro', 'algorithm', 'second', 'edit', 'sell']\n",
      "Tokenized sentence: ['book', 'which', 'lesson', 'then', 'you', 'msg', 'me', 'i', 'will', 'call', 'up', 'after', 'work', 'or', 'sth', 'i', 'm', 'going', 'to', 'get', 'specs', 'my', 'membership', 'is', 'px']\n",
      "After stop words removal: ['book', 'lesson', 'msg', 'call', 'work', 'sth', 'going', 'get', 'specs', 'membership', 'px']\n",
      "go\n",
      "After stemming with porters algorithm: ['book', 'lesson', 'msg', 'call', 'work', 'sth', 'go', 'get', 'spec', 'membership']\n",
      "Tokenized sentence: ['ok', 'both', 'our', 'days', 'so', 'what', 'are', 'you', 'making', 'for', 'dinner', 'tonite', 'am', 'i', 'invited']\n",
      "After stop words removal: ['ok', 'days', 'making', 'dinner', 'tonite', 'invited']\n",
      "mak\n",
      "After stemming with porters algorithm: ['dai', 'make', 'dinner', 'tonit', 'invit']\n",
      "Tokenized sentence: ['waqt', 'se', 'pehle', 'or', 'naseeb', 'se', 'zyada', 'kisi', 'ko', 'kuch', 'nahi', 'milta', 'zindgi', 'wo', 'nahi', 'he', 'jo', 'hum', 'sochte', 'hai', 'zindgi', 'wo', 'hai', 'jo', 'ham', 'jeetey', 'hai']\n",
      "After stop words removal: ['waqt', 'se', 'pehle', 'naseeb', 'se', 'zyada', 'kisi', 'ko', 'kuch', 'nahi', 'milta', 'zindgi', 'wo', 'nahi', 'jo', 'hum', 'sochte', 'hai', 'zindgi', 'wo', 'hai', 'jo', 'ham', 'jeetey', 'hai']\n",
      "After stemming with porters algorithm: ['waqt', 'pehl', 'naseeb', 'zyada', 'kisi', 'kuch', 'nahi', 'milta', 'zindgi', 'nahi', 'hum', 'socht', 'hai', 'zindgi', 'hai', 'ham', 'jeetei', 'hai']\n",
      "Tokenized sentence: ['sir', 'good', 'morning', 'hope', 'you', 'had', 'a', 'good', 'weekend', 'i', 'called', 'to', 'let', 'you', 'know', 'that', 'i', 'was', 'able', 'to', 'raise', 'lt', 'gt', 'from', 'my', 'dad', 'he', 'however', 'said', 'he', 'would', 'make', 'the', 'rest', 'available', 'by', 'mid', 'feb', 'this', 'amount', 'is', 'still', 'quite', 'short', 'and', 'i', 'was', 'hoping', 'you', 'would', 'help', 'do', 'have', 'a', 'good', 'day', 'abiola']\n",
      "After stop words removal: ['sir', 'good', 'morning', 'hope', 'good', 'weekend', 'called', 'let', 'know', 'able', 'raise', 'lt', 'gt', 'dad', 'however', 'said', 'would', 'make', 'rest', 'available', 'mid', 'feb', 'amount', 'still', 'quite', 'short', 'hoping', 'would', 'help', 'good', 'day', 'abiola']\n",
      "morn\n",
      "hop\n",
      "After stemming with porters algorithm: ['sir', 'good', 'mor', 'hope', 'good', 'weekend', 'call', 'let', 'know', 'abl', 'rais', 'dad', 'howev', 'said', 'would', 'make', 'rest', 'avail', 'mid', 'feb', 'amount', 'still', 'quit', 'short', 'hope', 'would', 'help', 'good', 'dai', 'abiola']\n",
      "Tokenized sentence: ['happy', 'new', 'year', 'princess']\n",
      "After stop words removal: ['happy', 'new', 'year', 'princess']\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year', 'princess']\n",
      "Tokenized sentence: ['headin', 'towards', 'busetop']\n",
      "After stop words removal: ['headin', 'towards', 'busetop']\n",
      "After stemming with porters algorithm: ['headin', 'toward', 'busetop']\n",
      "Tokenized sentence: ['now', 'only', 'i', 'reached', 'home', 'i', 'am', 'very', 'tired', 'now', 'i', 'will', 'come', 'tomorro']\n",
      "After stop words removal: ['reached', 'home', 'tired', 'come', 'tomorro']\n",
      "After stemming with porters algorithm: ['reac', 'home', 'tire', 'come', 'tomorro']\n",
      "Tokenized sentence: ['hi', 'princess', 'thank', 'you', 'for', 'the', 'pics', 'you', 'are', 'very', 'pretty', 'how', 'are', 'you']\n",
      "After stop words removal: ['hi', 'princess', 'thank', 'pics', 'pretty']\n",
      "After stemming with porters algorithm: ['princess', 'thank', 'pic', 'pretti']\n",
      "Tokenized sentence: ['but', 'we', 'havent', 'got', 'da', 'topic', 'yet', 'rite']\n",
      "After stop words removal: ['havent', 'got', 'da', 'topic', 'yet', 'rite']\n",
      "After stemming with porters algorithm: ['havent', 'got', 'topic', 'yet', 'rite']\n",
      "Tokenized sentence: ['oh', 'k', 'i', 'm', 'watching', 'here']\n",
      "After stop words removal: ['oh', 'k', 'watching']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc']\n",
      "Tokenized sentence: ['i', 'just', 'lov', 'this', 'line', 'hurt', 'me', 'with', 'the', 'truth']\n",
      "After stop words removal: ['lov', 'line', 'hurt', 'truth']\n",
      "After stemming with porters algorithm: ['lov', 'line', 'hurt', 'truth']\n",
      "Tokenized sentence: ['cool', 'i', 'll', 'text', 'you', 'in', 'a', 'few']\n",
      "After stop words removal: ['cool', 'text']\n",
      "After stemming with porters algorithm: ['cool', 'text']\n",
      "Tokenized sentence: ['i', 'don', 't', 'know', 'but', 'i', 'm', 'raping', 'dudes', 'at', 'poker']\n",
      "After stop words removal: ['know', 'raping', 'dudes', 'poker']\n",
      "rap\n",
      "After stemming with porters algorithm: ['know', 'rape', 'dude', 'poker']\n",
      "Tokenized sentence: ['some', 'of', 'them', 'told', 'accenture', 'is', 'not', 'confirm', 'is', 'it', 'true']\n",
      "After stop words removal: ['told', 'accenture', 'confirm', 'true']\n",
      "After stemming with porters algorithm: ['told', 'accentur', 'confirm', 'true']\n",
      "Tokenized sentence: ['she', 's', 'fine', 'i', 'have', 'had', 'difficulties', 'with', 'her', 'phone', 'it', 'works', 'with', 'mine', 'can', 'you', 'pls', 'send', 'her', 'another', 'friend', 'request']\n",
      "After stop words removal: ['fine', 'difficulties', 'phone', 'works', 'mine', 'pls', 'send', 'another', 'friend', 'request']\n",
      "After stemming with porters algorithm: ['fine', 'difficulti', 'phone', 'work', 'mine', 'pl', 'send', 'anoth', 'friend', 'request']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['raviyog', 'peripherals', 'bhayandar', 'east']\n",
      "After stop words removal: ['raviyog', 'peripherals', 'bhayandar', 'east']\n",
      "After stemming with porters algorithm: ['raviyog', 'periph', 'bhayandar', 'east']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['umma', 'my', 'life', 'and', 'vava', 'umma', 'love', 'you', 'lot', 'dear']\n",
      "After stop words removal: ['umma', 'life', 'vava', 'umma', 'love', 'lot', 'dear']\n",
      "After stemming with porters algorithm: ['umma', 'life', 'vava', 'umma', 'love', 'lot', 'dear']\n",
      "Tokenized sentence: ['he', 's', 'really', 'into', 'skateboarding', 'now', 'despite', 'the', 'fact', 'that', 'he', 'gets', 'thrown', 'off', 'of', 'it', 'and', 'winds', 'up', 'with', 'bandages', 'and', 'shit', 'all', 'over', 'his', 'arms', 'every', 'five', 'minutes']\n",
      "After stop words removal: ['really', 'skateboarding', 'despite', 'fact', 'gets', 'thrown', 'winds', 'bandages', 'shit', 'arms', 'every', 'five', 'minutes']\n",
      "skateboard\n",
      "After stemming with porters algorithm: ['realli', 'skateboar', 'despit', 'fact', 'get', 'thrown', 'wind', 'bandag', 'shit', 'arm', 'everi', 'five', 'minut']\n",
      "Tokenized sentence: ['okay', 'we', 'wait', 'ah']\n",
      "After stop words removal: ['okay', 'wait', 'ah']\n",
      "After stemming with porters algorithm: ['okai', 'wait']\n",
      "Tokenized sentence: ['ron', 'say', 'fri', 'leh', 'n', 'he', 'said', 'ding', 'tai', 'feng', 'cant', 'make', 'reservations', 'but', 'he', 'said', 'wait', 'lor']\n",
      "After stop words removal: ['ron', 'say', 'fri', 'leh', 'n', 'said', 'ding', 'tai', 'feng', 'cant', 'make', 'reservations', 'said', 'wait', 'lor']\n",
      "After stemming with porters algorithm: ['ron', 'sai', 'fri', 'leh', 'said', 'ding', 'tai', 'feng', 'cant', 'make', 'reserv', 'said', 'wait', 'lor']\n",
      "Tokenized sentence: ['not', 'for', 'possession', 'especially', 'not', 'first', 'offense']\n",
      "After stop words removal: ['possession', 'especially', 'first', 'offense']\n",
      "After stemming with porters algorithm: ['possess', 'especi', 'first', 'offens']\n",
      "Tokenized sentence: ['ok', 'so', 'april', 'cant', 'wait']\n",
      "After stop words removal: ['ok', 'april', 'cant', 'wait']\n",
      "After stemming with porters algorithm: ['april', 'cant', 'wait']\n",
      "Tokenized sentence: ['no', 'we', 'put', 'party', 'days', 'a', 'week', 'and', 'study', 'lightly', 'i', 'think', 'we', 'need', 'to', 'draw', 'in', 'some', 'custom', 'checkboxes', 'so', 'they', 'know', 'we', 're', 'hardcore']\n",
      "After stop words removal: ['put', 'party', 'days', 'week', 'study', 'lightly', 'think', 'need', 'draw', 'custom', 'checkboxes', 'know', 'hardcore']\n",
      "After stemming with porters algorithm: ['put', 'parti', 'dai', 'week', 'studi', 'lightli', 'think', 'need', 'draw', 'custom', 'checkbox', 'know', 'hardcor']\n",
      "Tokenized sentence: ['i', 'guess', 'you', 'could', 'be', 'as', 'good', 'an', 'excuse', 'as', 'any', 'lol']\n",
      "After stop words removal: ['guess', 'could', 'good', 'excuse', 'lol']\n",
      "After stemming with porters algorithm: ['guess', 'could', 'good', 'excus', 'lol']\n",
      "Tokenized sentence: ['if', 'you', 'want', 'to', 'mapquest', 'it', 'or', 'something', 'look', 'up', 'usf', 'dogwood', 'drive']\n",
      "After stop words removal: ['want', 'mapquest', 'something', 'look', 'usf', 'dogwood', 'drive']\n",
      "someth\n",
      "After stemming with porters algorithm: ['want', 'mapquest', 'somet', 'look', 'usf', 'dogwood', 'drive']\n",
      "Tokenized sentence: ['thats', 'cool', 'princess', 'i', 'will', 'cover', 'your', 'face', 'in', 'hot', 'sticky', 'cum']\n",
      "After stop words removal: ['thats', 'cool', 'princess', 'cover', 'face', 'hot', 'sticky', 'cum']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'princess', 'cover', 'face', 'hot', 'sticki', 'cum']\n",
      "Tokenized sentence: ['exactly', 'anyways', 'how', 'far', 'is', 'jide', 'her', 'to', 'study', 'or', 'just', 'visiting']\n",
      "After stop words removal: ['exactly', 'anyways', 'far', 'jide', 'study', 'visiting']\n",
      "visit\n",
      "After stemming with porters algorithm: ['exactli', 'anywai', 'far', 'jide', 'studi', 'visit']\n",
      "Tokenized sentence: ['the', 'last', 'thing', 'i', 'ever', 'wanted', 'to', 'do', 'was', 'hurt', 'you', 'and', 'i', 'didn', 't', 'think', 'it', 'would', 'have', 'you', 'd', 'laugh', 'be', 'embarassed', 'delete', 'the', 'tag', 'and', 'keep', 'going', 'but', 'as', 'far', 'as', 'i', 'knew', 'it', 'wasn', 't', 'even', 'up', 'the', 'fact', 'that', 'you', 'even', 'felt', 'like', 'i', 'would', 'do', 'it', 'to', 'hurt', 'you', 'shows', 'you', 'really', 'don', 't', 'know', 'me', 'at', 'all', 'it', 'was', 'messy', 'wednesday', 'but', 'it', 'wasn', 't', 'bad', 'the', 'problem', 'i', 'have', 'with', 'it', 'is', 'you', 'have', 'the', 'time', 'to', 'clean', 'it', 'but', 'you', 'choose', 'not', 'to', 'you', 'skype', 'you', 'take', 'pictures', 'you', 'sleep', 'you', 'want', 'to', 'go', 'out', 'i', 'don', 't', 'mind', 'a', 'few', 'things', 'here', 'and', 'there', 'but', 'when', 'you', 'don', 't', 'make', 'the', 'bed', 'when', 'you', 'throw', 'laundry', 'on', 'top', 'of', 'it', 'when', 'i', 'can', 't', 'have', 'a', 'friend', 'in', 'the', 'house', 'because', 'i', 'm', 'embarassed', 'that', 'there', 's', 'underwear', 'and', 'bras', 'strewn', 'on', 'the', 'bed', 'pillows', 'on', 'the', 'floor', 'that', 's', 'something', 'else', 'you', 'used', 'to', 'be', 'good', 'about', 'at', 'least', 'making', 'the', 'bed']\n",
      "After stop words removal: ['last', 'thing', 'ever', 'wanted', 'hurt', 'think', 'would', 'laugh', 'embarassed', 'delete', 'tag', 'keep', 'going', 'far', 'knew', 'even', 'fact', 'even', 'felt', 'like', 'would', 'hurt', 'shows', 'really', 'know', 'messy', 'wednesday', 'bad', 'problem', 'time', 'clean', 'choose', 'skype', 'take', 'pictures', 'sleep', 'want', 'go', 'mind', 'things', 'make', 'bed', 'throw', 'laundry', 'top', 'friend', 'house', 'embarassed', 'underwear', 'bras', 'strewn', 'bed', 'pillows', 'floor', 'something', 'else', 'used', 'good', 'least', 'making', 'bed']\n",
      "go\n",
      "someth\n",
      "mak\n",
      "After stemming with porters algorithm: ['last', 'thing', 'ever', 'wan', 'hurt', 'think', 'would', 'laugh', 'embarass', 'delet', 'tag', 'keep', 'go', 'far', 'knew', 'even', 'fact', 'even', 'felt', 'like', 'would', 'hurt', 'show', 'realli', 'know', 'messi', 'wednesdai', 'bad', 'problem', 'time', 'clean', 'choos', 'skype', 'take', 'pictur', 'sleep', 'want', 'mind', 'thing', 'make', 'bed', 'throw', 'laundri', 'top', 'friend', 'hous', 'embarass', 'underwear', 'bra', 'strewn', 'bed', 'pillow', 'floor', 'somet', 'els', 'us', 'good', 'least', 'make', 'bed']\n",
      "Tokenized sentence: ['you', 'have', 'been', 'selected', 'to', 'stay', 'in', 'of', 'top', 'british', 'hotels', 'for', 'nothing', 'holiday', 'worth', 'to', 'claim', 'call', 'london', 'bx', 'sw', 'ss']\n",
      "After stop words removal: ['selected', 'stay', 'top', 'british', 'hotels', 'nothing', 'holiday', 'worth', 'claim', 'call', 'london', 'bx', 'sw', 'ss']\n",
      "noth\n",
      "After stemming with porters algorithm: ['selec', 'stai', 'top', 'british', 'hotel', 'not', 'holidai', 'worth', 'claim', 'call', 'london']\n",
      "Tokenized sentence: ['what', 's', 'ur', 'pin']\n",
      "After stop words removal: ['ur', 'pin']\n",
      "After stemming with porters algorithm: ['pin']\n",
      "Tokenized sentence: ['i', 'place', 'all', 'ur', 'points', 'on', 'e', 'cultures', 'module', 'already']\n",
      "After stop words removal: ['place', 'ur', 'points', 'e', 'cultures', 'module', 'already']\n",
      "After stemming with porters algorithm: ['place', 'point', 'cultur', 'modul', 'alreadi']\n",
      "Tokenized sentence: ['dont', 'talk', 'to', 'him', 'ever', 'ok', 'its', 'my', 'word']\n",
      "After stop words removal: ['dont', 'talk', 'ever', 'ok', 'word']\n",
      "After stemming with porters algorithm: ['dont', 'talk', 'ever', 'word']\n",
      "Tokenized sentence: ['ok', 'good', 'then', 'i', 'later', 'come', 'find', 'c', 'lucky', 'i', 'told', 'to', 'go', 'earlier', 'later', 'pple', 'take', 'finish', 'no', 'more', 'again']\n",
      "After stop words removal: ['ok', 'good', 'later', 'come', 'find', 'c', 'lucky', 'told', 'go', 'earlier', 'later', 'pple', 'take', 'finish']\n",
      "After stemming with porters algorithm: ['good', 'later', 'come', 'find', 'lucki', 'told', 'earlier', 'later', 'pple', 'take', 'finish']\n",
      "Tokenized sentence: ['if', 'you', 'don', 't', 'respond', 'imma', 'assume', 'you', 're', 'still', 'asleep', 'and', 'imma', 'start', 'calling', 'n', 'shit']\n",
      "After stop words removal: ['respond', 'imma', 'assume', 'still', 'asleep', 'imma', 'start', 'calling', 'n', 'shit']\n",
      "call\n",
      "After stemming with porters algorithm: ['respond', 'imma', 'assum', 'still', 'asleep', 'imma', 'start', 'call', 'shit']\n",
      "Tokenized sentence: ['hmm', 'well', 'night', 'night']\n",
      "After stop words removal: ['hmm', 'well', 'night', 'night']\n",
      "After stemming with porters algorithm: ['hmm', 'well', 'night', 'night']\n",
      "Tokenized sentence: ['i', 'wont', 'so', 'wat', 's', 'wit', 'the', 'guys']\n",
      "After stop words removal: ['wont', 'wat', 'wit', 'guys']\n",
      "After stemming with porters algorithm: ['wont', 'wat', 'wit', 'gui']\n",
      "Tokenized sentence: ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n",
      "After stop words removal: ['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n",
      "After stemming with porters algorithm: ['jurong', 'point', 'crazi', 'avail', 'bugi', 'great', 'world', 'buffet', 'cine', 'got', 'amor', 'wat']\n",
      "Tokenized sentence: ['i', 'did', 'one', 'slice', 'and', 'one', 'breadstick', 'lol']\n",
      "After stop words removal: ['one', 'slice', 'one', 'breadstick', 'lol']\n",
      "After stemming with porters algorithm: ['on', 'slice', 'on', 'breadstick', 'lol']\n",
      "Tokenized sentence: ['no', 'it', 'will', 'reach', 'by', 'only', 'she', 'telling', 'she', 'will', 'be', 'there', 'i', 'dont', 'know']\n",
      "After stop words removal: ['reach', 'telling', 'dont', 'know']\n",
      "tell\n",
      "After stemming with porters algorithm: ['reach', 'tell', 'dont', 'know']\n",
      "Tokenized sentence: ['thanx', 'a', 'lot', 'ur', 'help']\n",
      "After stop words removal: ['thanx', 'lot', 'ur', 'help']\n",
      "After stemming with porters algorithm: ['thanx', 'lot', 'help']\n",
      "Tokenized sentence: ['no', 'but', 'the', 'bluray', 'player', 'can']\n",
      "After stop words removal: ['bluray', 'player']\n",
      "After stemming with porters algorithm: ['blurai', 'player']\n",
      "Tokenized sentence: ['tee', 'hee', 'off', 'to', 'lecture', 'cheery', 'bye', 'bye']\n",
      "After stop words removal: ['tee', 'hee', 'lecture', 'cheery', 'bye', 'bye']\n",
      "After stemming with porters algorithm: ['tee', 'hee', 'lectur', 'cheeri', 'bye', 'bye']\n",
      "Tokenized sentence: ['great', 'new', 'offer', 'double', 'mins', 'double', 'txt', 'on', 'best', 'orange', 'tariffs', 'and', 'get', 'latest', 'camera', 'phones', 'free', 'call', 'mobileupd', 'free', 'on', 'now', 'or', 'stoptxt', 't', 'cs']\n",
      "After stop words removal: ['great', 'new', 'offer', 'double', 'mins', 'double', 'txt', 'best', 'orange', 'tariffs', 'get', 'latest', 'camera', 'phones', 'free', 'call', 'mobileupd', 'free', 'stoptxt', 'cs']\n",
      "After stemming with porters algorithm: ['great', 'new', 'offer', 'doubl', 'min', 'doubl', 'txt', 'best', 'orang', 'tariff', 'get', 'latest', 'camera', 'phone', 'free', 'call', 'mobileupd', 'free', 'stoptxt']\n",
      "Tokenized sentence: ['please', 'tell', 'me', 'you', 'have', 'some', 'of', 'that', 'special', 'stock', 'you', 'were', 'talking', 'about']\n",
      "After stop words removal: ['please', 'tell', 'special', 'stock', 'talking']\n",
      "talk\n",
      "After stemming with porters algorithm: ['pleas', 'tell', 'special', 'stock', 'tal']\n",
      "Tokenized sentence: ['had', 'your', 'contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'all', 'free', 'double', 'mins', 'text', 'on', 'orange', 'tariffs', 'text', 'yes', 'for', 'callback', 'no', 'to', 'remove', 'from', 'records']\n",
      "After stop words removal: ['contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'free', 'double', 'mins', 'text', 'orange', 'tariffs', 'text', 'yes', 'callback', 'remove', 'records']\n",
      "After stemming with porters algorithm: ['contract', 'mobil', 'mnth', 'latest', 'motorola', 'nokia', 'etc', 'free', 'doubl', 'min', 'text', 'orang', 'tariff', 'text', 'ye', 'callback', 'remov', 'record']\n",
      "Tokenized sentence: ['god', 'asked', 'what', 'is', 'forgiveness', 'a', 'little', 'child', 'gave', 'lovely', 'reply']\n",
      "After stop words removal: ['god', 'asked', 'forgiveness', 'little', 'child', 'gave', 'lovely', 'reply']\n",
      "After stemming with porters algorithm: ['god', 'as', 'forgiv', 'littl', 'child', 'gave', 'love', 'repli']\n",
      "Tokenized sentence: ['does', 'not', 'operate', 'after', 'lt', 'gt', 'or', 'what']\n",
      "After stop words removal: ['operate', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['oper']\n",
      "Tokenized sentence: ['will', 'be', 'office', 'around', 'pm', 'now', 'i', 'am', 'going', 'hospital']\n",
      "After stop words removal: ['office', 'around', 'pm', 'going', 'hospital']\n",
      "go\n",
      "After stemming with porters algorithm: ['offic', 'around', 'go', 'hospit']\n",
      "Tokenized sentence: ['how', 'stupid', 'to', 'say', 'that', 'i', 'challenge', 'god', 'you', 'dont', 'think', 'at', 'all', 'on', 'what', 'i', 'write', 'instead', 'you', 'respond', 'immed']\n",
      "After stop words removal: ['stupid', 'say', 'challenge', 'god', 'dont', 'think', 'write', 'instead', 'respond', 'immed']\n",
      "After stemming with porters algorithm: ['stupid', 'sai', 'challeng', 'god', 'dont', 'think', 'write', 'instead', 'respond', 'im']\n",
      "Tokenized sentence: ['same', 'i', 'm', 'at', 'my', 'great', 'aunts', 'anniversary', 'party', 'in', 'tarpon', 'springs']\n",
      "After stop words removal: ['great', 'aunts', 'anniversary', 'party', 'tarpon', 'springs']\n",
      "After stemming with porters algorithm: ['great', 'aunt', 'anniversari', 'parti', 'tarpon', 'spring']\n",
      "Tokenized sentence: ['ee', 'msg', 'na', 'poortiyagi', 'odalebeku', 'hanumanji', 'name', 'hanuman', 'bajarangabali', 'maruti', 'pavanaputra', 'sankatmochan', 'ramaduth', 'mahaveer', 'ee', 'name', 'lt', 'gt', 'janarige', 'ivatte', 'kalisidare', 'next', 'saturday', 'olage', 'ondu', 'good', 'news', 'keluviri', 'maretare', 'inde', 'dodda', 'problum', 'nalli', 'siguviri', 'idu', 'matra', 'lt', 'gt', 'true', 'don', 't', 'neglet']\n",
      "After stop words removal: ['ee', 'msg', 'na', 'poortiyagi', 'odalebeku', 'hanumanji', 'name', 'hanuman', 'bajarangabali', 'maruti', 'pavanaputra', 'sankatmochan', 'ramaduth', 'mahaveer', 'ee', 'name', 'lt', 'gt', 'janarige', 'ivatte', 'kalisidare', 'next', 'saturday', 'olage', 'ondu', 'good', 'news', 'keluviri', 'maretare', 'inde', 'dodda', 'problum', 'nalli', 'siguviri', 'idu', 'matra', 'lt', 'gt', 'true', 'neglet']\n",
      "After stemming with porters algorithm: ['msg', 'poortiyagi', 'odalebeku', 'hanumanji', 'name', 'hanuman', 'bajarangabali', 'maruti', 'pavanaputra', 'sankatmochan', 'ramaduth', 'mahav', 'name', 'janarig', 'ivatt', 'kalisidar', 'next', 'saturdai', 'olag', 'ondu', 'good', 'new', 'keluviri', 'maretar', 'ind', 'dodda', 'problum', 'nalli', 'siguviri', 'idu', 'matra', 'true', 'neglet']\n",
      "Tokenized sentence: ['please', 'charge', 'my', 'mobile', 'when', 'you', 'get', 'up', 'in', 'morning']\n",
      "After stop words removal: ['please', 'charge', 'mobile', 'get', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['pleas', 'charg', 'mobil', 'get', 'mor']\n",
      "Tokenized sentence: ['xclusive', 'clubsaisai', 'morow', 'soiree', 'speciale', 'zouk', 'with', 'nichols', 'from', 'paris', 'free', 'roses', 'all', 'ladies', 'info']\n",
      "After stop words removal: ['xclusive', 'clubsaisai', 'morow', 'soiree', 'speciale', 'zouk', 'nichols', 'paris', 'free', 'roses', 'ladies', 'info']\n",
      "After stemming with porters algorithm: ['xclusiv', 'clubsaisai', 'morow', 'soire', 'special', 'zouk', 'nichol', 'pari', 'free', 'rose', 'ladi', 'info']\n",
      "Tokenized sentence: ['then', 'cant', 'get', 'da', 'laptop', 'my', 'matric', 'card', 'wif', 'lei']\n",
      "After stop words removal: ['cant', 'get', 'da', 'laptop', 'matric', 'card', 'wif', 'lei']\n",
      "After stemming with porters algorithm: ['cant', 'get', 'laptop', 'matric', 'card', 'wif', 'lei']\n",
      "Tokenized sentence: ['fantasy', 'football', 'is', 'back', 'on', 'your', 'tv', 'go', 'to', 'sky', 'gamestar', 'on', 'sky', 'active', 'and', 'play', 'k', 'dream', 'team', 'scoring', 'starts', 'on', 'saturday', 'so', 'register', 'now', 'sky', 'opt', 'out', 'to']\n",
      "After stop words removal: ['fantasy', 'football', 'back', 'tv', 'go', 'sky', 'gamestar', 'sky', 'active', 'play', 'k', 'dream', 'team', 'scoring', 'starts', 'saturday', 'register', 'sky', 'opt']\n",
      "scor\n",
      "After stemming with porters algorithm: ['fantasi', 'footbal', 'back', 'sky', 'gamestar', 'sky', 'activ', 'plai', 'dream', 'team', 'score', 'start', 'saturdai', 'regist', 'sky', 'opt']\n",
      "Tokenized sentence: ['its', 'not', 'that', 'time', 'of', 'the', 'month', 'nor', 'mid', 'of', 'the', 'time']\n",
      "After stop words removal: ['time', 'month', 'mid', 'time']\n",
      "After stemming with porters algorithm: ['time', 'month', 'mid', 'time']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['daddy', 'shu', 'shu', 'is', 'looking', 'u', 'u', 'wan', 'me', 'tell', 'him', 'u', 're', 'not', 'in', 'singapore', 'or', 'wat']\n",
      "After stop words removal: ['daddy', 'shu', 'shu', 'looking', 'u', 'u', 'wan', 'tell', 'u', 'singapore', 'wat']\n",
      "look\n",
      "After stemming with porters algorithm: ['daddi', 'shu', 'shu', 'look', 'wan', 'tell', 'singapor', 'wat']\n",
      "Tokenized sentence: ['me', 'i', 'm', 'not', 'workin', 'once', 'i', 'get', 'job']\n",
      "After stop words removal: ['workin', 'get', 'job']\n",
      "After stemming with porters algorithm: ['workin', 'get', 'job']\n",
      "Tokenized sentence: ['watching', 'tv', 'lor']\n",
      "After stop words removal: ['watching', 'tv', 'lor']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'lor']\n",
      "Tokenized sentence: ['ok', 'then', 'i', 'will', 'come', 'to', 'ur', 'home', 'after', 'half', 'an', 'hour']\n",
      "After stop words removal: ['ok', 'come', 'ur', 'home', 'half', 'hour']\n",
      "After stemming with porters algorithm: ['come', 'home', 'half', 'hour']\n",
      "Tokenized sentence: ['wewa', 'is', 'iriver', 'all', 'mb']\n",
      "After stop words removal: ['wewa', 'iriver', 'mb']\n",
      "After stemming with porters algorithm: ['wewa', 'iriv']\n",
      "Tokenized sentence: ['important', 'information', 'orange', 'user', 'today', 'is', 'your', 'lucky', 'day', 'find', 'out', 'why', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'there', 's', 'a', 'fantastic', 'surprise', 'awaiting', 'you']\n",
      "After stop words removal: ['important', 'information', 'orange', 'user', 'today', 'lucky', 'day', 'find', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'fantastic', 'surprise', 'awaiting']\n",
      "await\n",
      "After stemming with porters algorithm: ['import', 'inform', 'orang', 'user', 'todai', 'lucki', 'dai', 'find', 'log', 'onto', 'http', 'www', 'urawinn', 'com', 'fantast', 'surpris', 'await']\n",
      "Tokenized sentence: ['long', 'after', 'i', 'quit', 'i', 'get', 'on', 'only', 'like', 'minutes', 'a', 'day', 'as', 'it', 'is']\n",
      "After stop words removal: ['long', 'quit', 'get', 'like', 'minutes', 'day']\n",
      "After stemming with porters algorithm: ['long', 'quit', 'get', 'like', 'minut', 'dai']\n",
      "Tokenized sentence: ['yar', 'i', 'wanted', 'scold', 'u', 'yest', 'but', 'late', 'already', 'i', 'where', 'got', 'zhong', 'se', 'qing', 'you', 'if', 'u', 'ask', 'me', 'b', 'he', 'ask', 'me', 'then', 'i', 'll', 'go', 'out', 'w', 'u', 'all', 'lor', 'n', 'u', 'still', 'can', 'act', 'so', 'real']\n",
      "After stop words removal: ['yar', 'wanted', 'scold', 'u', 'yest', 'late', 'already', 'got', 'zhong', 'se', 'qing', 'u', 'ask', 'b', 'ask', 'go', 'w', 'u', 'lor', 'n', 'u', 'still', 'act', 'real']\n",
      "After stemming with porters algorithm: ['yar', 'wan', 'scold', 'yest', 'late', 'alreadi', 'got', 'zhong', 'qing', 'ask', 'ask', 'lor', 'still', 'act', 'real']\n",
      "Tokenized sentence: ['wife', 'how', 'she', 'knew', 'the', 'time', 'of', 'murder', 'exactly']\n",
      "After stop words removal: ['wife', 'knew', 'time', 'murder', 'exactly']\n",
      "After stemming with porters algorithm: ['wife', 'knew', 'time', 'murder', 'exactli']\n",
      "Tokenized sentence: ['or', 'maybe', 'my', 'fat', 'fingers', 'just', 'press', 'all', 'these', 'buttons', 'and', 'it', 'doesn', 't', 'know', 'what', 'to', 'do']\n",
      "After stop words removal: ['maybe', 'fat', 'fingers', 'press', 'buttons', 'know']\n",
      "After stemming with porters algorithm: ['mayb', 'fat', 'finger', 'press', 'button', 'know']\n",
      "Tokenized sentence: ['check', 'with', 'nuerologist']\n",
      "After stop words removal: ['check', 'nuerologist']\n",
      "After stemming with porters algorithm: ['check', 'nuerologist']\n",
      "Tokenized sentence: ['apps', 'class', 'varaya', 'elaya']\n",
      "After stop words removal: ['apps', 'class', 'varaya', 'elaya']\n",
      "After stemming with porters algorithm: ['app', 'class', 'varaya', 'elaya']\n",
      "Tokenized sentence: ['sad', 'story', 'of', 'a', 'man', 'last', 'week', 'was', 'my', 'b', 'day', 'my', 'wife', 'did', 'nt', 'wish', 'me', 'my', 'parents', 'forgot', 'n', 'so', 'did', 'my', 'kids', 'i', 'went', 'to', 'work', 'even', 'my', 'colleagues', 'did', 'not', 'wish', 'as', 'i', 'entered', 'my', 'cabin', 'my', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'i', 'felt', 'special', 'she', 'askd', 'me', 'lunch', 'after', 'lunch', 'she', 'invited', 'me', 'to', 'her', 'apartment', 'we', 'went', 'there', 'she', 'said', 'do', 'u', 'mind', 'if', 'i', 'go', 'into', 'the', 'bedroom', 'for', 'a', 'minute', 'ok', 'i', 'sed', 'in', 'a', 'sexy', 'mood', 'she', 'came', 'out', 'minuts', 'latr', 'wid', 'a', 'cake', 'n', 'my', 'wife', 'my', 'parents', 'my', 'kidz', 'my', 'friends', 'n', 'my', 'colleagues', 'all', 'screaming', 'surprise', 'and', 'i', 'was', 'waiting', 'on', 'the', 'sofa', 'naked']\n",
      "After stop words removal: ['sad', 'story', 'man', 'last', 'week', 'b', 'day', 'wife', 'nt', 'wish', 'parents', 'forgot', 'n', 'kids', 'went', 'work', 'even', 'colleagues', 'wish', 'entered', 'cabin', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invited', 'apartment', 'went', 'said', 'u', 'mind', 'go', 'bedroom', 'minute', 'ok', 'sed', 'sexy', 'mood', 'came', 'minuts', 'latr', 'wid', 'cake', 'n', 'wife', 'parents', 'kidz', 'friends', 'n', 'colleagues', 'screaming', 'surprise', 'waiting', 'sofa', 'naked']\n",
      "scream\n",
      "wait\n",
      "After stemming with porters algorithm: ['sad', 'stori', 'man', 'last', 'week', 'dai', 'wife', 'wish', 'parent', 'forgot', 'kid', 'went', 'work', 'even', 'colleagu', 'wish', 'enter', 'cabin', 'said', 'happi', 'dai', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invit', 'apart', 'went', 'said', 'mind', 'bedroom', 'minut', 'sed', 'sexi', 'mood', 'came', 'minut', 'latr', 'wid', 'cake', 'wife', 'parent', 'kidz', 'friend', 'colleagu', 'scream', 'surpris', 'wait', 'sofa', 'nake']\n",
      "Tokenized sentence: ['its', 'so', 'common', 'hearin', 'how', 'r', 'u', 'wat', 'r', 'u', 'doing', 'how', 'was', 'ur', 'day', 'so', 'let', 'me', 'ask', 'u', 'something', 'different', 'did', 'u', 'smile', 'today', 'if', 'not', 'do', 'it', 'now', 'gud', 'evng']\n",
      "After stop words removal: ['common', 'hearin', 'r', 'u', 'wat', 'r', 'u', 'ur', 'day', 'let', 'ask', 'u', 'something', 'different', 'u', 'smile', 'today', 'gud', 'evng']\n",
      "someth\n",
      "After stemming with porters algorithm: ['common', 'hearin', 'wat', 'dai', 'let', 'ask', 'somet', 'differ', 'smile', 'todai', 'gud', 'evng']\n",
      "Tokenized sentence: ['hi', 'darlin', 'how', 'was', 'work', 'did', 'u', 'get', 'into', 'trouble', 'ijust', 'talked', 'to', 'your', 'mum', 'all', 'morning', 'i', 'had', 'a', 'really', 'good', 'time', 'last', 'night', 'im', 'goin', 'out', 'soon', 'but', 'call', 'me', 'if', 'u', 'can']\n",
      "After stop words removal: ['hi', 'darlin', 'work', 'u', 'get', 'trouble', 'ijust', 'talked', 'mum', 'morning', 'really', 'good', 'time', 'last', 'night', 'im', 'goin', 'soon', 'call', 'u']\n",
      "morn\n",
      "After stemming with porters algorithm: ['darlin', 'work', 'get', 'troubl', 'ijust', 'tal', 'mum', 'mor', 'realli', 'good', 'time', 'last', 'night', 'goin', 'soon', 'call']\n",
      "Tokenized sentence: ['u', 'meet', 'other', 'fren', 'dun', 'wan', 'meet', 'me', 'ah', 'muz', 'b', 'a', 'guy', 'rite']\n",
      "After stop words removal: ['u', 'meet', 'fren', 'dun', 'wan', 'meet', 'ah', 'muz', 'b', 'guy', 'rite']\n",
      "After stemming with porters algorithm: ['meet', 'fren', 'dun', 'wan', 'meet', 'muz', 'gui', 'rite']\n",
      "Tokenized sentence: ['message', 'from', 'i', 'am', 'at', 'truro', 'hospital', 'on', 'ext', 'you', 'can', 'phone', 'me', 'here', 'as', 'i', 'have', 'a', 'phone', 'by', 'my', 'side']\n",
      "After stop words removal: ['message', 'truro', 'hospital', 'ext', 'phone', 'phone', 'side']\n",
      "After stemming with porters algorithm: ['messag', 'truro', 'hospit', 'ext', 'phone', 'phone', 'side']\n",
      "Tokenized sentence: ['pls', 'clarify', 'back', 'if', 'an', 'open', 'return', 'ticket', 'that', 'i', 'have', 'can', 'be', 'preponed', 'for', 'me', 'to', 'go', 'back', 'to', 'kerala']\n",
      "After stop words removal: ['pls', 'clarify', 'back', 'open', 'return', 'ticket', 'preponed', 'go', 'back', 'kerala']\n",
      "After stemming with porters algorithm: ['pl', 'clarifi', 'back', 'open', 'return', 'ticket', 'prepon', 'back', 'kerala']\n",
      "Tokenized sentence: ['lookatme', 'thanks', 'for', 'your', 'purchase', 'of', 'a', 'video', 'clip', 'from', 'lookatme', 'you', 've', 'been', 'charged', 'p', 'think', 'you', 'can', 'do', 'better', 'why', 'not', 'send', 'a', 'video', 'in', 'a', 'mmsto']\n",
      "After stop words removal: ['lookatme', 'thanks', 'purchase', 'video', 'clip', 'lookatme', 'charged', 'p', 'think', 'better', 'send', 'video', 'mmsto']\n",
      "After stemming with porters algorithm: ['lookatm', 'thank', 'purchas', 'video', 'clip', 'lookatm', 'char', 'think', 'better', 'send', 'video', 'mmsto']\n",
      "Tokenized sentence: ['ard', 'lor']\n",
      "After stop words removal: ['ard', 'lor']\n",
      "After stemming with porters algorithm: ['ard', 'lor']\n",
      "Tokenized sentence: ['still', 'i', 'have', 'not', 'checked', 'it', 'da']\n",
      "After stop words removal: ['still', 'checked', 'da']\n",
      "After stemming with porters algorithm: ['still', 'chec']\n",
      "Tokenized sentence: ['huh', 'means', 'computational', 'science', 'y', 'they', 'like', 'dat', 'one', 'push', 'here', 'n', 'there']\n",
      "After stop words removal: ['huh', 'means', 'computational', 'science', 'like', 'dat', 'one', 'push', 'n']\n",
      "After stemming with porters algorithm: ['huh', 'mean', 'comput', 'scienc', 'like', 'dat', 'on', 'push']\n",
      "Tokenized sentence: ['sorry', 'my', 'roommates', 'took', 'forever', 'it', 'ok', 'if', 'i', 'come', 'by', 'now']\n",
      "After stop words removal: ['sorry', 'roommates', 'took', 'forever', 'ok', 'come']\n",
      "After stemming with porters algorithm: ['sorri', 'roommat', 'took', 'forev', 'come']\n",
      "Tokenized sentence: ['oh', 'really', 'did', 'you', 'make', 'it', 'on', 'air', 'what', 's', 'your', 'talent']\n",
      "After stop words removal: ['oh', 'really', 'make', 'air', 'talent']\n",
      "After stemming with porters algorithm: ['realli', 'make', 'air', 'talent']\n",
      "Tokenized sentence: ['yup', 'not', 'comin']\n",
      "After stop words removal: ['yup', 'comin']\n",
      "After stemming with porters algorithm: ['yup', 'comin']\n",
      "Tokenized sentence: ['merry', 'christmas', 'to', 'you', 'too', 'babe', 'i', 'love', 'ya', 'kisses']\n",
      "After stop words removal: ['merry', 'christmas', 'babe', 'love', 'ya', 'kisses']\n",
      "After stemming with porters algorithm: ['merri', 'christma', 'babe', 'love', 'kiss']\n",
      "Tokenized sentence: ['send', 'me', 'your', 'resume']\n",
      "After stop words removal: ['send', 'resume']\n",
      "After stemming with porters algorithm: ['send', 'resum']\n",
      "Tokenized sentence: ['congratulations', 'you', 've', 'won', 'you', 're', 'a', 'winner', 'in', 'our', 'august', 'prize', 'draw', 'call', 'now', 'prize', 'code']\n",
      "After stop words removal: ['congratulations', 'winner', 'august', 'prize', 'draw', 'call', 'prize', 'code']\n",
      "After stemming with porters algorithm: ['congratul', 'winner', 'august', 'priz', 'draw', 'call', 'priz', 'code']\n",
      "Tokenized sentence: ['the', 'sign', 'of', 'maturity', 'is', 'not', 'when', 'we', 'start', 'saying', 'big', 'things', 'but', 'actually', 'it', 'is', 'when', 'we', 'start', 'understanding', 'small', 'things', 'have', 'a', 'nice', 'evening', 'bslvyl']\n",
      "After stop words removal: ['sign', 'maturity', 'start', 'saying', 'big', 'things', 'actually', 'start', 'understanding', 'small', 'things', 'nice', 'evening', 'bslvyl']\n",
      "say\n",
      "understand\n",
      "even\n",
      "After stemming with porters algorithm: ['sign', 'matur', 'start', 'sai', 'big', 'thing', 'actual', 'start', 'understan', 'small', 'thing', 'nice', 'even', 'bslvyl']\n",
      "Tokenized sentence: ['are', 'you', 'unique', 'enough', 'find', 'out', 'from', 'th', 'august', 'www', 'areyouunique', 'co', 'uk']\n",
      "After stop words removal: ['unique', 'enough', 'find', 'th', 'august', 'www', 'areyouunique', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['uniqu', 'enough', 'find', 'august', 'www', 'areyouuniqu']\n",
      "Tokenized sentence: ['cashbin', 'co', 'uk', 'get', 'lots', 'of', 'cash', 'this', 'weekend', 'www', 'cashbin', 'co', 'uk', 'dear', 'welcome', 'to', 'the', 'weekend', 'we', 'have', 'got', 'our', 'biggest', 'and', 'best', 'ever', 'cash', 'give', 'away', 'these']\n",
      "After stop words removal: ['cashbin', 'co', 'uk', 'get', 'lots', 'cash', 'weekend', 'www', 'cashbin', 'co', 'uk', 'dear', 'welcome', 'weekend', 'got', 'biggest', 'best', 'ever', 'cash', 'give', 'away']\n",
      "After stemming with porters algorithm: ['cashbin', 'get', 'lot', 'cash', 'weekend', 'www', 'cashbin', 'dear', 'welcom', 'weekend', 'got', 'biggest', 'best', 'ever', 'cash', 'give', 'awai']\n",
      "Tokenized sentence: ['lol', 'please', 'do', 'actually', 'send', 'a', 'pic', 'of', 'yourself', 'right', 'now', 'i', 'wanna', 'see', 'pose', 'with', 'a', 'comb', 'and', 'hair', 'dryer', 'or', 'something']\n",
      "After stop words removal: ['lol', 'please', 'actually', 'send', 'pic', 'right', 'wanna', 'see', 'pose', 'comb', 'hair', 'dryer', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['lol', 'pleas', 'actual', 'send', 'pic', 'right', 'wanna', 'see', 'pose', 'comb', 'hair', 'dryer', 'somet']\n",
      "Tokenized sentence: ['my', 'computer', 'just', 'fried', 'the', 'only', 'essential', 'part', 'we', 'don', 't', 'keep', 'spares', 'of', 'because', 'my', 'fucking', 'idiot', 'roommates', 'looovvve', 'leaving', 'the', 'thing', 'running', 'on', 'full', 'lt', 'gt']\n",
      "After stop words removal: ['computer', 'fried', 'essential', 'part', 'keep', 'spares', 'fucking', 'idiot', 'roommates', 'looovvve', 'leaving', 'thing', 'running', 'full', 'lt', 'gt']\n",
      "fuck\n",
      "leav\n",
      "runn\n",
      "After stemming with porters algorithm: ['comput', 'fri', 'essenti', 'part', 'keep', 'spare', 'fuc', 'idiot', 'roommat', 'looovvv', 'leav', 'thing', 'run', 'full']\n",
      "Tokenized sentence: ['how', 'much', 'for', 'an', 'eighth']\n",
      "After stop words removal: ['much', 'eighth']\n",
      "After stemming with porters algorithm: ['much', 'eighth']\n",
      "Tokenized sentence: ['well', 'done', 'and', 'luv', 'ya', 'all']\n",
      "After stop words removal: ['well', 'done', 'luv', 'ya']\n",
      "After stemming with porters algorithm: ['well', 'done', 'luv']\n",
      "Tokenized sentence: ['dai', 'what', 'this', 'da', 'can', 'i', 'send', 'my', 'resume', 'to', 'this', 'id']\n",
      "After stop words removal: ['dai', 'da', 'send', 'resume', 'id']\n",
      "After stemming with porters algorithm: ['dai', 'send', 'resum']\n",
      "Tokenized sentence: ['lt', 'gt', 'am', 'i', 'think', 'should', 'say', 'on', 'syllabus']\n",
      "After stop words removal: ['lt', 'gt', 'think', 'say', 'syllabus']\n",
      "After stemming with porters algorithm: ['think', 'sai', 'syllabu']\n",
      "Tokenized sentence: ['from', 'to', 'only', 'my', 'work', 'timing']\n",
      "After stop words removal: ['work', 'timing']\n",
      "tim\n",
      "After stemming with porters algorithm: ['work', 'time']\n",
      "Tokenized sentence: ['what', 'happen', 'dear', 'tell', 'me']\n",
      "After stop words removal: ['happen', 'dear', 'tell']\n",
      "After stemming with porters algorithm: ['happen', 'dear', 'tell']\n",
      "Tokenized sentence: ['alex', 'knows', 'a', 'guy', 'who', 'sells', 'mids', 'but', 'he', 's', 'down', 'in', 'south', 'tampa', 'and', 'i', 'don', 't', 'think', 'i', 'could', 'set', 'it', 'up', 'before', 'like']\n",
      "After stop words removal: ['alex', 'knows', 'guy', 'sells', 'mids', 'south', 'tampa', 'think', 'could', 'set', 'like']\n",
      "After stemming with porters algorithm: ['alex', 'know', 'gui', 'sell', 'mid', 'south', 'tampa', 'think', 'could', 'set', 'like']\n",
      "Tokenized sentence: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'contact', 'u', 'u', 'have', 'won', 'the', 'pound', 'prize', 'claim', 'is', 'easy', 'call', 'now', 'only', 'p', 'per', 'min', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'min', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['time', 'tri', 'contact', 'pound', 'priz', 'claim', 'easi', 'call', 'per', 'min', 'nat', 'rate']\n",
      "Tokenized sentence: ['when', 're', 'you', 'guys', 'getting', 'back', 'g', 'said', 'you', 'were', 'thinking', 'about', 'not', 'staying', 'for', 'mcr']\n",
      "After stop words removal: ['guys', 'getting', 'back', 'g', 'said', 'thinking', 'staying', 'mcr']\n",
      "gett\n",
      "think\n",
      "stay\n",
      "After stemming with porters algorithm: ['gui', 'get', 'back', 'said', 'thin', 'stai', 'mcr']\n",
      "Tokenized sentence: ['is', 'avatar', 'supposed', 'to', 'have', 'subtoitles']\n",
      "After stop words removal: ['avatar', 'supposed', 'subtoitles']\n",
      "After stemming with porters algorithm: ['avatar', 'suppos', 'subtoit']\n",
      "Tokenized sentence: ['so', 'there', 's', 'a', 'ring', 'that', 'comes', 'with', 'the', 'guys', 'costumes', 'it', 's', 'there', 'so', 'they', 'can', 'gift', 'their', 'future', 'yowifes', 'hint', 'hint']\n",
      "After stop words removal: ['ring', 'comes', 'guys', 'costumes', 'gift', 'future', 'yowifes', 'hint', 'hint']\n",
      "After stemming with porters algorithm: ['ring', 'come', 'gui', 'costum', 'gift', 'futur', 'yowif', 'hint', 'hint']\n",
      "Tokenized sentence: ['i', 'm', 'freezing', 'and', 'craving', 'ice', 'fml']\n",
      "After stop words removal: ['freezing', 'craving', 'ice', 'fml']\n",
      "freez\n",
      "crav\n",
      "After stemming with porters algorithm: ['freez', 'crave', 'ic', 'fml']\n",
      "Tokenized sentence: ['good', 'afternoon', 'loverboy', 'how', 'goes', 'you', 'day', 'any', 'luck', 'come', 'your', 'way', 'i', 'think', 'of', 'you', 'sweetie', 'and', 'send', 'my', 'love', 'across', 'the', 'sea', 'to', 'make', 'you', 'smile', 'and', 'happy']\n",
      "After stop words removal: ['good', 'afternoon', 'loverboy', 'goes', 'day', 'luck', 'come', 'way', 'think', 'sweetie', 'send', 'love', 'across', 'sea', 'make', 'smile', 'happy']\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'loverboi', 'goe', 'dai', 'luck', 'come', 'wai', 'think', 'sweeti', 'send', 'love', 'across', 'sea', 'make', 'smile', 'happi']\n",
      "Tokenized sentence: ['thanks', 'for', 'the', 'temales', 'it', 'was', 'wonderful', 'thank', 'have', 'a', 'great', 'week']\n",
      "After stop words removal: ['thanks', 'temales', 'wonderful', 'thank', 'great', 'week']\n",
      "After stemming with porters algorithm: ['thank', 'temal', 'wonder', 'thank', 'great', 'week']\n",
      "Tokenized sentence: ['hui', 'xin', 'is', 'in', 'da', 'lib']\n",
      "After stop words removal: ['hui', 'xin', 'da', 'lib']\n",
      "After stemming with porters algorithm: ['hui', 'xin', 'lib']\n",
      "Tokenized sentence: ['u', 'attend', 'ur', 'driving', 'lesson', 'how', 'many', 'times', 'a', 'wk', 'n', 'which', 'day']\n",
      "After stop words removal: ['u', 'attend', 'ur', 'driving', 'lesson', 'many', 'times', 'wk', 'n', 'day']\n",
      "driv\n",
      "After stemming with porters algorithm: ['attend', 'drive', 'lesson', 'mani', 'time', 'dai']\n",
      "Tokenized sentence: ['shall', 'call', 'now', 'dear', 'having', 'food']\n",
      "After stop words removal: ['shall', 'call', 'dear', 'food']\n",
      "After stemming with porters algorithm: ['shall', 'call', 'dear', 'food']\n",
      "Tokenized sentence: ['mm', 'not', 'entirely', 'sure', 'i', 'understood', 'that', 'text', 'but', 'hey', 'ho', 'which', 'weekend']\n",
      "After stop words removal: ['mm', 'entirely', 'sure', 'understood', 'text', 'hey', 'ho', 'weekend']\n",
      "After stemming with porters algorithm: ['entir', 'sure', 'understood', 'text', 'hei', 'weekend']\n",
      "Tokenized sentence: ['dunno', 'lei', 'he', 'neva', 'say']\n",
      "After stop words removal: ['dunno', 'lei', 'neva', 'say']\n",
      "After stemming with porters algorithm: ['dunno', 'lei', 'neva', 'sai']\n",
      "Tokenized sentence: ['no', 'i', 'meant', 'the', 'calculation', 'is', 'the', 'same', 'that', 'lt', 'gt', 'units', 'at', 'lt', 'gt', 'this', 'school', 'is', 'really', 'expensive', 'have', 'you', 'started', 'practicing', 'your', 'accent', 'because', 'its', 'important', 'and', 'have', 'you', 'decided', 'if', 'you', 'are', 'doing', 'years', 'of', 'dental', 'school', 'or', 'if', 'you', 'll', 'just', 'do', 'the', 'nmde', 'exam']\n",
      "After stop words removal: ['meant', 'calculation', 'lt', 'gt', 'units', 'lt', 'gt', 'school', 'really', 'expensive', 'started', 'practicing', 'accent', 'important', 'decided', 'years', 'dental', 'school', 'nmde', 'exam']\n",
      "practic\n",
      "After stemming with porters algorithm: ['meant', 'calcul', 'unit', 'school', 'realli', 'expens', 'star', 'practic', 'accent', 'import', 'decid', 'year', 'dental', 'school', 'nmde', 'exam']\n",
      "Tokenized sentence: ['loan', 'for', 'any', 'purpose', 'homeowners', 'tenants', 'welcome', 'have', 'you', 'been', 'previously', 'refused', 'we', 'can', 'still', 'help', 'call', 'free', 'or', 'text', 'back', 'help']\n",
      "After stop words removal: ['loan', 'purpose', 'homeowners', 'tenants', 'welcome', 'previously', 'refused', 'still', 'help', 'call', 'free', 'text', 'back', 'help']\n",
      "After stemming with porters algorithm: ['loan', 'purpos', 'homeown', 'tenant', 'welcom', 'previous', 'refus', 'still', 'help', 'call', 'free', 'text', 'back', 'help']\n",
      "Tokenized sentence: ['yup', 'i', 'm', 'free']\n",
      "After stop words removal: ['yup', 'free']\n",
      "After stemming with porters algorithm: ['yup', 'free']\n",
      "Tokenized sentence: ['just', 'normal', 'only', 'here']\n",
      "After stop words removal: ['normal']\n",
      "After stemming with porters algorithm: ['normal']\n",
      "Tokenized sentence: ['idc', 'get', 'over', 'here', 'you', 'are', 'not', 'weaseling', 'your', 'way', 'out', 'of', 'this', 'shit', 'twice', 'in', 'a', 'row']\n",
      "After stop words removal: ['idc', 'get', 'weaseling', 'way', 'shit', 'twice', 'row']\n",
      "weasel\n",
      "After stemming with porters algorithm: ['idc', 'get', 'weasel', 'wai', 'shit', 'twice', 'row']\n",
      "Tokenized sentence: ['mmm', 'thats', 'better', 'now', 'i', 'got', 'a', 'roast', 'down', 'me', 'i', 'd', 'b', 'better', 'if', 'i', 'had', 'a', 'few', 'drinks', 'down', 'me', 'good', 'indian']\n",
      "After stop words removal: ['mmm', 'thats', 'better', 'got', 'roast', 'b', 'better', 'drinks', 'good', 'indian']\n",
      "After stemming with porters algorithm: ['mmm', 'that', 'better', 'got', 'roast', 'better', 'drink', 'good', 'indian']\n",
      "Tokenized sentence: ['lt', 'gt', 'great', 'loxahatchee', 'xmas', 'tree', 'burning', 'update', 'you', 'can', 'totally', 'see', 'stars', 'here']\n",
      "After stop words removal: ['lt', 'gt', 'great', 'loxahatchee', 'xmas', 'tree', 'burning', 'update', 'totally', 'see', 'stars']\n",
      "burn\n",
      "After stemming with porters algorithm: ['great', 'loxahatche', 'xma', 'tree', 'bur', 'updat', 'total', 'see', 'star']\n",
      "Tokenized sentence: ['wnevr', 'i', 'wana', 'fal', 'in', 'luv', 'vth', 'my', 'books', 'my', 'bed', 'fals', 'in', 'luv', 'vth', 'me', 'yen', 'madodu', 'nav', 'pretsorginta', 'nammanna', 'pretsovru', 'important', 'alwa', 'gud', 'eveb']\n",
      "After stop words removal: ['wnevr', 'wana', 'fal', 'luv', 'vth', 'books', 'bed', 'fals', 'luv', 'vth', 'yen', 'madodu', 'nav', 'pretsorginta', 'nammanna', 'pretsovru', 'important', 'alwa', 'gud', 'eveb']\n",
      "After stemming with porters algorithm: ['wnevr', 'wana', 'fal', 'luv', 'vth', 'book', 'bed', 'fal', 'luv', 'vth', 'yen', 'madodu', 'nav', 'pretsorginta', 'nammanna', 'pretsovru', 'import', 'alwa', 'gud', 'eveb']\n",
      "Tokenized sentence: ['its', 'not', 'the', 'same', 'here', 'still', 'looking', 'for', 'a', 'job', 'how', 'much', 'do', 'ta', 's', 'earn', 'there']\n",
      "After stop words removal: ['still', 'looking', 'job', 'much', 'ta', 'earn']\n",
      "look\n",
      "After stemming with porters algorithm: ['still', 'look', 'job', 'much', 'earn']\n",
      "Tokenized sentence: ['short', 'but', 'cute', 'be', 'a', 'good', 'person']\n",
      "After stop words removal: ['short', 'cute', 'good', 'person']\n",
      "After stemming with porters algorithm: ['short', 'cute', 'good', 'person']\n",
      "Tokenized sentence: ['so', 'when', 'you', 'gonna', 'get', 'rimac', 'access']\n",
      "After stop words removal: ['gonna', 'get', 'rimac', 'access']\n",
      "After stemming with porters algorithm: ['gonna', 'get', 'rimac', 'access']\n",
      "Tokenized sentence: ['alright', 'babe']\n",
      "After stop words removal: ['alright', 'babe']\n",
      "After stemming with porters algorithm: ['alright', 'babe']\n",
      "Tokenized sentence: ['hai', 'dear', 'friends', 'this', 'is', 'my', 'new', 'amp', 'present', 'number', 'by', 'rajitha', 'raj', 'ranju']\n",
      "After stop words removal: ['hai', 'dear', 'friends', 'new', 'amp', 'present', 'number', 'rajitha', 'raj', 'ranju']\n",
      "After stemming with porters algorithm: ['hai', 'dear', 'friend', 'new', 'amp', 'present', 'number', 'rajitha', 'raj', 'ranju']\n",
      "Tokenized sentence: ['it', 's', 'to', 'get', 'in', 'is', 'that', 'ok']\n",
      "After stop words removal: ['get', 'ok']\n",
      "After stemming with porters algorithm: ['get']\n",
      "Tokenized sentence: ['what', 'is', 'the', 'plural', 'of', 'the', 'noun', 'research']\n",
      "After stop words removal: ['plural', 'noun', 'research']\n",
      "After stemming with porters algorithm: ['plural', 'noun', 'research']\n",
      "Tokenized sentence: ['aight', 'do', 'you', 'still', 'want', 'to', 'get', 'money']\n",
      "After stop words removal: ['aight', 'still', 'want', 'get', 'money']\n",
      "After stemming with porters algorithm: ['aight', 'still', 'want', 'get', 'monei']\n",
      "Tokenized sentence: ['my', 'slave', 'i', 'want', 'you', 'to', 'take', 'or', 'pictures', 'of', 'yourself', 'today', 'in', 'bright', 'light', 'on', 'your', 'cell', 'phone', 'bright', 'light']\n",
      "After stop words removal: ['slave', 'want', 'take', 'pictures', 'today', 'bright', 'light', 'cell', 'phone', 'bright', 'light']\n",
      "After stemming with porters algorithm: ['slave', 'want', 'take', 'pictur', 'todai', 'bright', 'light', 'cell', 'phone', 'bright', 'light']\n",
      "Tokenized sentence: ['nice', 'talking', 'to', 'you', 'please', 'dont', 'forget', 'my', 'pix', 'i', 'want', 'to', 'see', 'all', 'of', 'you']\n",
      "After stop words removal: ['nice', 'talking', 'please', 'dont', 'forget', 'pix', 'want', 'see']\n",
      "talk\n",
      "After stemming with porters algorithm: ['nice', 'tal', 'pleas', 'dont', 'forget', 'pix', 'want', 'see']\n",
      "Tokenized sentence: ['yeah', 'give', 'me', 'a', 'call', 'if', 'you', 've', 'got', 'a', 'minute']\n",
      "After stop words removal: ['yeah', 'give', 'call', 'got', 'minute']\n",
      "After stemming with porters algorithm: ['yeah', 'give', 'call', 'got', 'minut']\n",
      "Tokenized sentence: ['nutter', 'cutter', 'ctter', 'cttergg', 'cttargg', 'ctargg', 'ctagg', 'ie', 'you']\n",
      "After stop words removal: ['nutter', 'cutter', 'ctter', 'cttergg', 'cttargg', 'ctargg', 'ctagg', 'ie']\n",
      "After stemming with porters algorithm: ['nutter', 'cutter', 'ctter', 'cttergg', 'cttargg', 'ctargg', 'ctagg']\n",
      "Tokenized sentence: ['ok', 'lor', 'or', 'u', 'wan', 'me', 'go', 'look', 'u']\n",
      "After stop words removal: ['ok', 'lor', 'u', 'wan', 'go', 'look', 'u']\n",
      "After stemming with porters algorithm: ['lor', 'wan', 'look']\n",
      "Tokenized sentence: ['dear', 'dave', 'this', 'is', 'your', 'final', 'notice', 'to', 'collect', 'your', 'tenerife', 'holiday', 'or', 'cash', 'award', 'call', 'from', 'landline', 'tcs', 'sae', 'box', 'cw', 'wx', 'ppm']\n",
      "After stop words removal: ['dear', 'dave', 'final', 'notice', 'collect', 'tenerife', 'holiday', 'cash', 'award', 'call', 'landline', 'tcs', 'sae', 'box', 'cw', 'wx', 'ppm']\n",
      "After stemming with porters algorithm: ['dear', 'dave', 'final', 'notic', 'collect', 'tenerif', 'holidai', 'cash', 'award', 'call', 'landlin', 'tc', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['aight', 'i', 'should', 'be', 'there', 'by', 'at', 'the', 'latest', 'probably', 'closer', 'to', 'are', 'jay', 'and', 'tyler', 'down', 'or', 'should', 'we', 'just', 'do', 'two', 'trips']\n",
      "After stop words removal: ['aight', 'latest', 'probably', 'closer', 'jay', 'tyler', 'two', 'trips']\n",
      "After stemming with porters algorithm: ['aight', 'latest', 'probab', 'closer', 'jai', 'tyler', 'two', 'trip']\n",
      "Tokenized sentence: ['prepare', 'to', 'be', 'pleasured']\n",
      "After stop words removal: ['prepare', 'pleasured']\n",
      "After stemming with porters algorithm: ['prepar', 'pleasur']\n",
      "Tokenized sentence: ['we', 'have', 'new', 'local', 'dates', 'in', 'your', 'area', 'lots', 'of', 'new', 'people', 'registered', 'in', 'your', 'area', 'reply', 'date', 'to', 'start', 'now', 'only', 'www', 'flirtparty', 'us', 'replys']\n",
      "After stop words removal: ['new', 'local', 'dates', 'area', 'lots', 'new', 'people', 'registered', 'area', 'reply', 'date', 'start', 'www', 'flirtparty', 'us', 'replys']\n",
      "After stemming with porters algorithm: ['new', 'local', 'date', 'area', 'lot', 'new', 'peopl', 'regist', 'area', 'repli', 'date', 'start', 'www', 'flirtparti', 'repli']\n",
      "Tokenized sentence: ['lol', 'nah', 'wasn', 't', 'too', 'bad', 'thanks', 'its', 'good', 'to', 'b', 'home', 'but', 'its', 'been', 'quite', 'a', 'reality', 'check', 'hows', 'ur', 'day', 'been', 'did', 'u', 'do', 'anything', 'with', 'website']\n",
      "After stop words removal: ['lol', 'nah', 'bad', 'thanks', 'good', 'b', 'home', 'quite', 'reality', 'check', 'hows', 'ur', 'day', 'u', 'anything', 'website']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['lol', 'nah', 'bad', 'thank', 'good', 'home', 'quit', 'realiti', 'check', 'how', 'dai', 'anyt', 'websit']\n",
      "Tokenized sentence: ['you', 've', 'won', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tc', 's', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stop words removal: ['costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stemming with porters algorithm: ['costa', 'del', 'sol', 'holidai', 'await', 'collect', 'call', 'toclaim', 'sae', 'pobox', 'stockport', 'cost', 'max', 'min']\n",
      "Tokenized sentence: ['dear', 'friends', 'sorry', 'for', 'the', 'late', 'information', 'today', 'is', 'the', 'birthday', 'of', 'our', 'loving', 'ar', 'praveesh', 'for', 'more', 'details', 'log', 'on', 'to', 'face', 'book', 'and', 'see', 'its', 'his', 'number', 'lt', 'gt', 'dont', 'miss', 'a', 'delicious', 'treat']\n",
      "After stop words removal: ['dear', 'friends', 'sorry', 'late', 'information', 'today', 'birthday', 'loving', 'ar', 'praveesh', 'details', 'log', 'face', 'book', 'see', 'number', 'lt', 'gt', 'dont', 'miss', 'delicious', 'treat']\n",
      "lov\n",
      "After stemming with porters algorithm: ['dear', 'friend', 'sorri', 'late', 'inform', 'todai', 'birthdai', 'love', 'praveesh', 'detail', 'log', 'face', 'book', 'see', 'number', 'dont', 'miss', 'delici', 'treat']\n",
      "Tokenized sentence: ['i', 'actually', 'did', 'for', 'the', 'first', 'time', 'in', 'a', 'while', 'i', 'went', 'to', 'bed', 'not', 'too', 'long', 'after', 'i', 'spoke', 'with', 'you', 'woke', 'up', 'at', 'how', 'was', 'your', 'night']\n",
      "After stop words removal: ['actually', 'first', 'time', 'went', 'bed', 'long', 'spoke', 'woke', 'night']\n",
      "After stemming with porters algorithm: ['actual', 'first', 'time', 'went', 'bed', 'long', 'spoke', 'woke', 'night']\n",
      "Tokenized sentence: ['science', 'tells', 'that', 'chocolate', 'will', 'melt', 'under', 'the', 'sunlight', 'please', 'don', 't', 'walk', 'under', 'the', 'sunlight', 'bcoz', 'i', 'don', 't', 'want', 'to', 'loss', 'a', 'sweet', 'friend']\n",
      "After stop words removal: ['science', 'tells', 'chocolate', 'melt', 'sunlight', 'please', 'walk', 'sunlight', 'bcoz', 'want', 'loss', 'sweet', 'friend']\n",
      "After stemming with porters algorithm: ['scienc', 'tell', 'chocol', 'melt', 'sunlight', 'pleas', 'walk', 'sunlight', 'bcoz', 'want', 'loss', 'sweet', 'friend']\n",
      "Tokenized sentence: ['no', 'need', 'lar', 'jus', 'testing', 'e', 'phone', 'card', 'dunno', 'network', 'not', 'gd', 'i', 'thk', 'me', 'waiting', 'my', 'sis', 'finish', 'bathing', 'so', 'i', 'can', 'bathe', 'dun', 'disturb', 'u', 'liao', 'u', 'cleaning', 'ur', 'room']\n",
      "After stop words removal: ['need', 'lar', 'jus', 'testing', 'e', 'phone', 'card', 'dunno', 'network', 'gd', 'thk', 'waiting', 'sis', 'finish', 'bathing', 'bathe', 'dun', 'disturb', 'u', 'liao', 'u', 'cleaning', 'ur', 'room']\n",
      "test\n",
      "wait\n",
      "bath\n",
      "clean\n",
      "After stemming with porters algorithm: ['need', 'lar', 'ju', 'tes', 'phone', 'card', 'dunno', 'network', 'thk', 'wait', 'si', 'finish', 'bat', 'bath', 'dun', 'disturb', 'liao', 'clean', 'room']\n",
      "Tokenized sentence: ['log', 'off', 'wat', 'it', 's', 'sdryb', 'i']\n",
      "After stop words removal: ['log', 'wat', 'sdryb']\n",
      "After stemming with porters algorithm: ['log', 'wat', 'sdryb']\n",
      "Tokenized sentence: ['how', 'abt', 'making', 'some', 'of', 'the', 'pics', 'bigger']\n",
      "After stop words removal: ['abt', 'making', 'pics', 'bigger']\n",
      "mak\n",
      "After stemming with porters algorithm: ['abt', 'make', 'pic', 'bigger']\n",
      "Tokenized sentence: ['trust', 'me', 'even', 'if', 'isn', 't', 'there', 'its', 'there']\n",
      "After stop words removal: ['trust', 'even']\n",
      "After stemming with porters algorithm: ['trust', 'even']\n",
      "Tokenized sentence: ['no', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'nok', 'to', 'st', 'tone', 'free', 'so', 'get', 'txtin', 'now', 'and', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stop words removal: ['nokia', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'nok', 'st', 'tone', 'free', 'get', 'txtin', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stemming with porters algorithm: ['nokia', 'tone', 'mob', 'everi', 'week', 'txt', 'nok', 'tone', 'free', 'get', 'txtin', 'tell', 'friend', 'tone', 'repli', 'info']\n",
      "Tokenized sentence: ['omg', 'it', 'could', 'snow', 'here', 'tonite']\n",
      "After stop words removal: ['omg', 'could', 'snow', 'tonite']\n",
      "After stemming with porters algorithm: ['omg', 'could', 'snow', 'tonit']\n",
      "Tokenized sentence: ['jesus', 'armand', 'really', 'is', 'trying', 'to', 'tell', 'everybody', 'he', 'can', 'find']\n",
      "After stop words removal: ['jesus', 'armand', 'really', 'trying', 'tell', 'everybody', 'find']\n",
      "After stemming with porters algorithm: ['jesu', 'armand', 'realli', 'trying', 'tell', 'everybodi', 'find']\n",
      "Tokenized sentence: ['anytime']\n",
      "After stop words removal: ['anytime']\n",
      "After stemming with porters algorithm: ['anytim']\n",
      "Tokenized sentence: ['i', 'will', 'cal', 'you', 'sir', 'in', 'meeting']\n",
      "After stop words removal: ['cal', 'sir', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['cal', 'sir', 'meet']\n",
      "Tokenized sentence: ['dude', 'ive', 'been', 'seeing', 'a', 'lotta', 'corvettes', 'lately']\n",
      "After stop words removal: ['dude', 'ive', 'seeing', 'lotta', 'corvettes', 'lately']\n",
      "see\n",
      "After stemming with porters algorithm: ['dude', 'iv', 'see', 'lotta', 'corvett', 'late']\n",
      "Tokenized sentence: ['i', 'come', 'n', 'pick', 'up', 'come', 'out', 'immediately', 'aft', 'ur', 'lesson']\n",
      "After stop words removal: ['come', 'n', 'pick', 'come', 'immediately', 'aft', 'ur', 'lesson']\n",
      "After stemming with porters algorithm: ['come', 'pick', 'come', 'immedi', 'aft', 'lesson']\n",
      "Tokenized sentence: ['how', 's', 'it', 'feel', 'mr', 'your', 'not', 'my', 'real', 'valentine', 'just', 'my', 'yo', 'valentine', 'even', 'tho', 'u', 'hardly', 'play']\n",
      "After stop words removal: ['feel', 'mr', 'real', 'valentine', 'yo', 'valentine', 'even', 'tho', 'u', 'hardly', 'play']\n",
      "After stemming with porters algorithm: ['feel', 'real', 'valentin', 'valentin', 'even', 'tho', 'hardli', 'plai']\n",
      "Tokenized sentence: ['and', 'do', 'you', 'have', 'any', 'one', 'that', 'can', 'teach', 'me', 'how', 'to', 'ship', 'cars']\n",
      "After stop words removal: ['one', 'teach', 'ship', 'cars']\n",
      "After stemming with porters algorithm: ['on', 'teach', 'ship', 'car']\n",
      "Tokenized sentence: ['huh', 'so', 'late', 'fr', 'dinner']\n",
      "After stop words removal: ['huh', 'late', 'fr', 'dinner']\n",
      "After stemming with porters algorithm: ['huh', 'late', 'dinner']\n",
      "Tokenized sentence: ['k', 'i', 'yan', 'jiu', 'liao', 'sat', 'we', 'can', 'go', 'bugis', 'vill', 'one', 'frm', 'to', 'den', 'hop', 'to', 'parco', 'nb', 'sun', 'can', 'go', 'cine', 'frm', 'to', 'den', 'hop', 'to', 'orc', 'mrt', 'hip', 'hop', 'at']\n",
      "After stop words removal: ['k', 'yan', 'jiu', 'liao', 'sat', 'go', 'bugis', 'vill', 'one', 'frm', 'den', 'hop', 'parco', 'nb', 'sun', 'go', 'cine', 'frm', 'den', 'hop', 'orc', 'mrt', 'hip', 'hop']\n",
      "After stemming with porters algorithm: ['yan', 'jiu', 'liao', 'sat', 'bugi', 'vill', 'on', 'frm', 'den', 'hop', 'parco', 'sun', 'cine', 'frm', 'den', 'hop', 'orc', 'mrt', 'hip', 'hop']\n",
      "Tokenized sentence: ['they', 'r', 'giving', 'a', 'second', 'chance', 'to', 'rahul', 'dengra']\n",
      "After stop words removal: ['r', 'giving', 'second', 'chance', 'rahul', 'dengra']\n",
      "giv\n",
      "After stemming with porters algorithm: ['give', 'second', 'chanc', 'rahul', 'dengra']\n",
      "Tokenized sentence: ['wat', 'time', 'wan', 'today']\n",
      "After stop words removal: ['wat', 'time', 'wan', 'today']\n",
      "After stemming with porters algorithm: ['wat', 'time', 'wan', 'todai']\n",
      "Tokenized sentence: ['ok', 'that', 's', 'great', 'thanx', 'a', 'lot']\n",
      "After stop words removal: ['ok', 'great', 'thanx', 'lot']\n",
      "After stemming with porters algorithm: ['great', 'thanx', 'lot']\n",
      "Tokenized sentence: ['yo', 'any', 'way', 'we', 'could', 'pick', 'something', 'up', 'tonight']\n",
      "After stop words removal: ['yo', 'way', 'could', 'pick', 'something', 'tonight']\n",
      "someth\n",
      "After stemming with porters algorithm: ['wai', 'could', 'pick', 'somet', 'tonight']\n",
      "Tokenized sentence: ['never', 'blame', 'a', 'day', 'in', 'ur', 'life', 'good', 'days', 'give', 'u', 'happiness', 'bad', 'days', 'give', 'u', 'experience', 'both', 'are', 'essential', 'in', 'life', 'all', 'are', 'gods', 'blessings', 'good', 'morning']\n",
      "After stop words removal: ['never', 'blame', 'day', 'ur', 'life', 'good', 'days', 'give', 'u', 'happiness', 'bad', 'days', 'give', 'u', 'experience', 'essential', 'life', 'gods', 'blessings', 'good', 'morning']\n",
      "bless\n",
      "morn\n",
      "After stemming with porters algorithm: ['never', 'blame', 'dai', 'life', 'good', 'dai', 'give', 'happi', 'bad', 'dai', 'give', 'experi', 'essenti', 'life', 'god', 'bless', 'good', 'mor']\n",
      "Tokenized sentence: ['night', 'has', 'ended', 'for', 'another', 'day', 'morning', 'has', 'come', 'in', 'a', 'special', 'way', 'may', 'you', 'smile', 'like', 'the', 'sunny', 'rays', 'and', 'leaves', 'your', 'worries', 'at', 'the', 'blue', 'blue', 'bay', 'gud', 'mrng']\n",
      "After stop words removal: ['night', 'ended', 'another', 'day', 'morning', 'come', 'special', 'way', 'may', 'smile', 'like', 'sunny', 'rays', 'leaves', 'worries', 'blue', 'blue', 'bay', 'gud', 'mrng']\n",
      "morn\n",
      "After stemming with porters algorithm: ['night', 'en', 'anoth', 'dai', 'mor', 'come', 'special', 'wai', 'mai', 'smile', 'like', 'sunni', 'rai', 'leav', 'worri', 'blue', 'blue', 'bai', 'gud', 'mrng']\n",
      "Tokenized sentence: ['hey', 'you', 'around', 'i', 've', 'got', 'enough', 'for', 'a', 'half', 'the', 'ten', 'i', 'owe', 'you']\n",
      "After stop words removal: ['hey', 'around', 'got', 'enough', 'half', 'ten', 'owe']\n",
      "After stemming with porters algorithm: ['hei', 'around', 'got', 'enough', 'half', 'ten', 'ow']\n",
      "Tokenized sentence: ['double', 'mins', 'txts', 'on', 'orange', 'tariffs', 'latest', 'motorola', 'sonyericsson', 'nokia', 'with', 'bluetooth', 'free', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'hf']\n",
      "After stop words removal: ['double', 'mins', 'txts', 'orange', 'tariffs', 'latest', 'motorola', 'sonyericsson', 'nokia', 'bluetooth', 'free', 'call', 'mobileupd', 'call', 'optout', 'hf']\n",
      "After stemming with porters algorithm: ['doubl', 'min', 'txt', 'orang', 'tariff', 'latest', 'motorola', 'sonyericsson', 'nokia', 'bluetooth', 'free', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['if', 'you', 'were', 'are', 'free', 'i', 'can', 'give', 'otherwise', 'nalla', 'adi', 'entey', 'nattil', 'kittum']\n",
      "After stop words removal: ['free', 'give', 'otherwise', 'nalla', 'adi', 'entey', 'nattil', 'kittum']\n",
      "After stemming with porters algorithm: ['free', 'give', 'otherwis', 'nalla', 'adi', 'entei', 'nattil', 'kittum']\n",
      "Tokenized sentence: ['nothing', 'lor', 'a', 'bit', 'bored', 'too', 'then', 'y', 'dun', 'u', 'go', 'home', 'early', 'sleep', 'today']\n",
      "After stop words removal: ['nothing', 'lor', 'bit', 'bored', 'dun', 'u', 'go', 'home', 'early', 'sleep', 'today']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not', 'lor', 'bit', 'bore', 'dun', 'home', 'earli', 'sleep', 'todai']\n",
      "Tokenized sentence: ['pls', 'go', 'ahead', 'with', 'watts', 'i', 'just', 'wanted', 'to', 'be', 'sure', 'do', 'have', 'a', 'great', 'weekend', 'abiola']\n",
      "After stop words removal: ['pls', 'go', 'ahead', 'watts', 'wanted', 'sure', 'great', 'weekend', 'abiola']\n",
      "After stemming with porters algorithm: ['pl', 'ahead', 'watt', 'wan', 'sure', 'great', 'weekend', 'abiola']\n",
      "Tokenized sentence: ['guess', 'who', 'spent', 'all', 'last', 'night', 'phasing', 'in', 'and', 'out', 'of', 'the', 'fourth', 'dimension']\n",
      "After stop words removal: ['guess', 'spent', 'last', 'night', 'phasing', 'fourth', 'dimension']\n",
      "phas\n",
      "After stemming with porters algorithm: ['guess', 'spent', 'last', 'night', 'phase', 'fourth', 'dimens']\n",
      "Tokenized sentence: ['i', 'am', 'sorry', 'it', 'hurt', 'you']\n",
      "After stop words removal: ['sorry', 'hurt']\n",
      "After stemming with porters algorithm: ['sorri', 'hurt']\n",
      "Tokenized sentence: ['guess', 'who', 'am', 'i', 'this', 'is', 'the', 'first', 'time', 'i', 'created', 'a', 'web', 'page', 'www', 'asjesus', 'com', 'read', 'all', 'i', 'wrote', 'i', 'm', 'waiting', 'for', 'your', 'opinions', 'i', 'want', 'to', 'be', 'your', 'friend']\n",
      "After stop words removal: ['guess', 'first', 'time', 'created', 'web', 'page', 'www', 'asjesus', 'com', 'read', 'wrote', 'waiting', 'opinions', 'want', 'friend']\n",
      "create\n",
      "wait\n",
      "After stemming with porters algorithm: ['guess', 'first', 'time', 'creat', 'web', 'page', 'www', 'asjesu', 'com', 'read', 'wrote', 'wait', 'opinion', 'want', 'friend']\n",
      "Tokenized sentence: ['dear', 'u', 've', 'been', 'invited', 'to', 'xchat', 'this', 'is', 'our', 'final', 'attempt', 'to', 'contact', 'u', 'txt', 'chat', 'to']\n",
      "After stop words removal: ['dear', 'u', 'invited', 'xchat', 'final', 'attempt', 'contact', 'u', 'txt', 'chat']\n",
      "After stemming with porters algorithm: ['dear', 'invit', 'xchat', 'final', 'attempt', 'contact', 'txt', 'chat']\n",
      "Tokenized sentence: ['dont', 'forget', 'you', 'can', 'place', 'as', 'many', 'free', 'requests', 'with', 'stchoice', 'co', 'uk', 'as', 'you', 'wish', 'for', 'more', 'information', 'call']\n",
      "After stop words removal: ['dont', 'forget', 'place', 'many', 'free', 'requests', 'stchoice', 'co', 'uk', 'wish', 'information', 'call']\n",
      "After stemming with porters algorithm: ['dont', 'forget', 'place', 'mani', 'free', 'request', 'stchoic', 'wish', 'inform', 'call']\n",
      "Tokenized sentence: ['we', 'live', 'in', 'the', 'next', 'lt', 'gt', 'mins']\n",
      "After stop words removal: ['live', 'next', 'lt', 'gt', 'mins']\n",
      "After stemming with porters algorithm: ['live', 'next', 'min']\n",
      "Tokenized sentence: ['i', 'will', 'be', 'gentle', 'baby', 'soon', 'you', 'will', 'be', 'taking', 'all', 'lt', 'gt', 'inches', 'deep', 'inside', 'your', 'tight', 'pussy']\n",
      "After stop words removal: ['gentle', 'baby', 'soon', 'taking', 'lt', 'gt', 'inches', 'deep', 'inside', 'tight', 'pussy']\n",
      "tak\n",
      "After stemming with porters algorithm: ['gentl', 'babi', 'soon', 'take', 'inch', 'deep', 'insid', 'tight', 'pussi']\n",
      "Tokenized sentence: ['that', 'day', 'you', 'asked', 'about', 'anand', 'number', 'why']\n",
      "After stop words removal: ['day', 'asked', 'anand', 'number']\n",
      "After stemming with porters algorithm: ['dai', 'as', 'anand', 'number']\n",
      "Tokenized sentence: ['let', 's', 'pool', 'our', 'money', 'together', 'and', 'buy', 'a', 'bunch', 'of', 'lotto', 'tickets', 'if', 'we', 'win', 'i', 'get', 'lt', 'gt', 'u', 'get', 'lt', 'gt', 'deal']\n",
      "After stop words removal: ['let', 'pool', 'money', 'together', 'buy', 'bunch', 'lotto', 'tickets', 'win', 'get', 'lt', 'gt', 'u', 'get', 'lt', 'gt', 'deal']\n",
      "After stemming with porters algorithm: ['let', 'pool', 'monei', 'togeth', 'bui', 'bunch', 'lotto', 'ticket', 'win', 'get', 'get', 'deal']\n",
      "Tokenized sentence: ['i', 'meant', 'as', 'an', 'apology', 'from', 'me', 'for', 'texting', 'you', 'to', 'get', 'me', 'drugs', 'at', 'lt', 'gt', 'at', 'night']\n",
      "After stop words removal: ['meant', 'apology', 'texting', 'get', 'drugs', 'lt', 'gt', 'night']\n",
      "text\n",
      "After stemming with porters algorithm: ['meant', 'apologi', 'tex', 'get', 'drug', 'night']\n",
      "Tokenized sentence: ['k', 'i', 'll', 'call', 'you', 'when', 'i', 'm', 'close']\n",
      "After stop words removal: ['k', 'call', 'close']\n",
      "After stemming with porters algorithm: ['call', 'close']\n",
      "Tokenized sentence: ['great', 'escape', 'i', 'fancy', 'the', 'bridge', 'but', 'needs', 'her', 'lager', 'see', 'you', 'tomo']\n",
      "After stop words removal: ['great', 'escape', 'fancy', 'bridge', 'needs', 'lager', 'see', 'tomo']\n",
      "After stemming with porters algorithm: ['great', 'escap', 'fanci', 'bridg', 'need', 'lager', 'see', 'tomo']\n",
      "Tokenized sentence: ['wat', 'time', 'liao', 'where', 'still', 'got']\n",
      "After stop words removal: ['wat', 'time', 'liao', 'still', 'got']\n",
      "After stemming with porters algorithm: ['wat', 'time', 'liao', 'still', 'got']\n",
      "Tokenized sentence: ['not', 'enufcredeit', 'tocall', 'shall', 'ileave', 'uni', 'at', 'get', 'a', 'bus', 'to', 'yor', 'house']\n",
      "After stop words removal: ['enufcredeit', 'tocall', 'shall', 'ileave', 'uni', 'get', 'bus', 'yor', 'house']\n",
      "After stemming with porters algorithm: ['enufcredeit', 'tocal', 'shall', 'ileav', 'uni', 'get', 'bu', 'yor', 'hous']\n",
      "Tokenized sentence: ['it', 'has', 'issues', 'right', 'now', 'ill', 'fix', 'for', 'her', 'by', 'tomorrow']\n",
      "After stop words removal: ['issues', 'right', 'ill', 'fix', 'tomorrow']\n",
      "After stemming with porters algorithm: ['issu', 'right', 'ill', 'fix', 'tomorrow']\n",
      "Tokenized sentence: ['rodger', 'burns', 'msg', 'we', 'tried', 'to', 'call', 'you', 're', 'your', 'reply', 'to', 'our', 'sms', 'for', 'a', 'free', 'nokia', 'mobile', 'free', 'camcorder', 'please', 'call', 'now', 'for', 'delivery', 'tomorrow']\n",
      "After stop words removal: ['rodger', 'burns', 'msg', 'tried', 'call', 'reply', 'sms', 'free', 'nokia', 'mobile', 'free', 'camcorder', 'please', 'call', 'delivery', 'tomorrow']\n",
      "After stemming with porters algorithm: ['rodger', 'burn', 'msg', 'tri', 'call', 'repli', 'sm', 'free', 'nokia', 'mobil', 'free', 'camcord', 'pleas', 'call', 'deliveri', 'tomorrow']\n",
      "Tokenized sentence: ['once', 'free', 'call', 'me', 'sir', 'i', 'am', 'waiting', 'for', 'you']\n",
      "After stop words removal: ['free', 'call', 'sir', 'waiting']\n",
      "wait\n",
      "After stemming with porters algorithm: ['free', 'call', 'sir', 'wait']\n",
      "Tokenized sentence: ['geeeee', 'i', 'love', 'you', 'so', 'much', 'i', 'can', 'barely', 'stand', 'it']\n",
      "After stop words removal: ['geeeee', 'love', 'much', 'barely', 'stand']\n",
      "After stemming with porters algorithm: ['geeeee', 'love', 'much', 'bare', 'stand']\n",
      "Tokenized sentence: ['its', 'cool', 'but', 'tyler', 'had', 'to', 'take', 'off', 'so', 'we', 're', 'gonna', 'buy', 'for', 'him', 'and', 'drop', 'it', 'off', 'at', 'his', 'place', 'later', 'tonight', 'our', 'total', 'order', 'is', 'a', 'quarter', 'you', 'got', 'enough']\n",
      "After stop words removal: ['cool', 'tyler', 'take', 'gonna', 'buy', 'drop', 'place', 'later', 'tonight', 'total', 'order', 'quarter', 'got', 'enough']\n",
      "After stemming with porters algorithm: ['cool', 'tyler', 'take', 'gonna', 'bui', 'drop', 'place', 'later', 'tonight', 'total', 'order', 'quarter', 'got', 'enough']\n",
      "Tokenized sentence: ['i', 'just', 'got', 'home', 'babe', 'are', 'you', 'still', 'awake']\n",
      "After stop words removal: ['got', 'home', 'babe', 'still', 'awake']\n",
      "After stemming with porters algorithm: ['got', 'home', 'babe', 'still', 'awak']\n",
      "Tokenized sentence: ['no', 'that', 'just', 'means', 'you', 'have', 'a', 'fat', 'head']\n",
      "After stop words removal: ['means', 'fat', 'head']\n",
      "After stemming with porters algorithm: ['mean', 'fat', 'head']\n",
      "Tokenized sentence: ['received', 'understood', 'n', 'acted', 'upon']\n",
      "After stop words removal: ['received', 'understood', 'n', 'acted', 'upon']\n",
      "After stemming with porters algorithm: ['receiv', 'understood', 'ac', 'upon']\n",
      "Tokenized sentence: ['sorry', 'me', 'going', 'home', 'first', 'daddy', 'come', 'fetch', 'later']\n",
      "After stop words removal: ['sorry', 'going', 'home', 'first', 'daddy', 'come', 'fetch', 'later']\n",
      "go\n",
      "After stemming with porters algorithm: ['sorri', 'go', 'home', 'first', 'daddi', 'come', 'fetch', 'later']\n",
      "Tokenized sentence: ['i', 'm', 'fine', 'hope', 'you', 'are', 'also']\n",
      "After stop words removal: ['fine', 'hope', 'also']\n",
      "After stemming with porters algorithm: ['fine', 'hope', 'also']\n",
      "Tokenized sentence: ['hey', 'you', 'can', 'pay', 'with', 'salary', 'de', 'only', 'lt', 'gt']\n",
      "After stop words removal: ['hey', 'pay', 'salary', 'de', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['hei', 'pai', 'salari']\n",
      "Tokenized sentence: ['give', 'me', 'some', 'time', 'to', 'walk', 'there']\n",
      "After stop words removal: ['give', 'time', 'walk']\n",
      "After stemming with porters algorithm: ['give', 'time', 'walk']\n",
      "Tokenized sentence: ['we', 'can', 'make', 'a', 'baby', 'in', 'yo', 'tho']\n",
      "After stop words removal: ['make', 'baby', 'yo', 'tho']\n",
      "After stemming with porters algorithm: ['make', 'babi', 'tho']\n",
      "Tokenized sentence: ['also', 'fuck', 'you', 'and', 'your', 'family', 'for', 'going', 'to', 'rhode', 'island', 'or', 'wherever', 'the', 'fuck', 'and', 'leaving', 'me', 'all', 'alone', 'the', 'week', 'i', 'have', 'a', 'new', 'bong', 'gt']\n",
      "After stop words removal: ['also', 'fuck', 'family', 'going', 'rhode', 'island', 'wherever', 'fuck', 'leaving', 'alone', 'week', 'new', 'bong', 'gt']\n",
      "go\n",
      "leav\n",
      "After stemming with porters algorithm: ['also', 'fuck', 'famili', 'go', 'rhode', 'island', 'wherev', 'fuck', 'leav', 'alon', 'week', 'new', 'bong']\n",
      "Tokenized sentence: ['you', 'should', 'know', 'now', 'so', 'how', 's', 'anthony', 'are', 'you', 'bringing', 'money', 'i', 've', 'school', 'fees', 'to', 'pay', 'and', 'rent', 'and', 'stuff', 'like', 'that', 'thats', 'why', 'i', 'need', 'your', 'help', 'a', 'friend', 'in', 'need']\n",
      "After stop words removal: ['know', 'anthony', 'bringing', 'money', 'school', 'fees', 'pay', 'rent', 'stuff', 'like', 'thats', 'need', 'help', 'friend', 'need']\n",
      "bring\n",
      "After stemming with porters algorithm: ['know', 'anthoni', 'brin', 'monei', 'school', 'fee', 'pai', 'rent', 'stuff', 'like', 'that', 'need', 'help', 'friend', 'need']\n",
      "Tokenized sentence: ['mum', 'say', 'we', 'wan', 'to', 'go', 'then', 'go', 'then', 'she', 'can', 'shun', 'bian', 'watch', 'da', 'glass', 'exhibition']\n",
      "After stop words removal: ['mum', 'say', 'wan', 'go', 'go', 'shun', 'bian', 'watch', 'da', 'glass', 'exhibition']\n",
      "After stemming with porters algorithm: ['mum', 'sai', 'wan', 'shun', 'bian', 'watch', 'glass', 'exhibit']\n",
      "Tokenized sentence: ['dear', 'umma', 'she', 'called', 'me', 'now']\n",
      "After stop words removal: ['dear', 'umma', 'called']\n",
      "After stemming with porters algorithm: ['dear', 'umma', 'call']\n",
      "Tokenized sentence: ['not', 'getting', 'anywhere', 'with', 'this', 'damn', 'job', 'hunting', 'over', 'here']\n",
      "After stop words removal: ['getting', 'anywhere', 'damn', 'job', 'hunting']\n",
      "gett\n",
      "hunt\n",
      "After stemming with porters algorithm: ['get', 'anywher', 'damn', 'job', 'hun']\n",
      "Tokenized sentence: ['today', 'is', 'accept', 'day', 'u', 'accept', 'me', 'as', 'brother', 'sister', 'lover', 'dear', 'best', 'clos', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'no', 'rply', 'means', 'enemy']\n",
      "After stop words removal: ['today', 'accept', 'day', 'u', 'accept', 'brother', 'sister', 'lover', 'dear', 'best', 'clos', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'rply', 'means', 'enemy']\n",
      "After stemming with porters algorithm: ['todai', 'accept', 'dai', 'accept', 'brother', 'sister', 'lover', 'dear', 'best', 'clo', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'rply', 'mean', 'enemi']\n",
      "Tokenized sentence: ['takin', 'linear', 'algebra', 'today']\n",
      "After stop words removal: ['takin', 'linear', 'algebra', 'today']\n",
      "After stemming with porters algorithm: ['takin', 'linear', 'algebra', 'todai']\n",
      "Tokenized sentence: ['and', 'you', 'will', 'expect', 'you', 'whenever', 'you', 'text', 'hope', 'all', 'goes', 'well', 'tomo']\n",
      "After stop words removal: ['expect', 'whenever', 'text', 'hope', 'goes', 'well', 'tomo']\n",
      "After stemming with porters algorithm: ['expect', 'whenev', 'text', 'hope', 'goe', 'well', 'tomo']\n",
      "Tokenized sentence: ['welcome', 'to', 'uk', 'mobile', 'date', 'this', 'msg', 'is', 'free', 'giving', 'you', 'free', 'calling', 'to', 'future', 'mgs', 'billed', 'at', 'p', 'daily', 'to', 'cancel', 'send', 'go', 'stop', 'to']\n",
      "After stop words removal: ['welcome', 'uk', 'mobile', 'date', 'msg', 'free', 'giving', 'free', 'calling', 'future', 'mgs', 'billed', 'p', 'daily', 'cancel', 'send', 'go', 'stop']\n",
      "giv\n",
      "call\n",
      "After stemming with porters algorithm: ['welcom', 'mobil', 'date', 'msg', 'free', 'give', 'free', 'call', 'futur', 'mg', 'bill', 'daili', 'cancel', 'send', 'stop']\n",
      "Tokenized sentence: ['gud', 'mrng', 'dear', 'hav', 'a', 'nice', 'day']\n",
      "After stop words removal: ['gud', 'mrng', 'dear', 'hav', 'nice', 'day']\n",
      "After stemming with porters algorithm: ['gud', 'mrng', 'dear', 'hav', 'nice', 'dai']\n",
      "Tokenized sentence: ['your', 'account', 'has', 'been', 'refilled', 'successfully', 'by', 'inr', 'lt', 'decimal', 'gt', 'your', 'keralacircle', 'prepaid', 'account', 'balance', 'is', 'rs', 'lt', 'decimal', 'gt', 'your', 'transaction', 'id', 'is', 'kr', 'lt', 'gt']\n",
      "After stop words removal: ['account', 'refilled', 'successfully', 'inr', 'lt', 'decimal', 'gt', 'keralacircle', 'prepaid', 'account', 'balance', 'rs', 'lt', 'decimal', 'gt', 'transaction', 'id', 'kr', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['account', 'refil', 'successfulli', 'inr', 'decim', 'keralacirc', 'prepaid', 'account', 'balanc', 'decim', 'transact']\n",
      "Tokenized sentence: ['where', 'are', 'the', 'garage', 'keys', 'they', 'aren', 't', 'on', 'the', 'bookshelf']\n",
      "After stop words removal: ['garage', 'keys', 'bookshelf']\n",
      "After stemming with porters algorithm: ['garag', 'kei', 'bookshelf']\n",
      "Tokenized sentence: ['gud', 'mrng', 'dear', 'hav', 'a', 'nice', 'day']\n",
      "After stop words removal: ['gud', 'mrng', 'dear', 'hav', 'nice', 'day']\n",
      "After stemming with porters algorithm: ['gud', 'mrng', 'dear', 'hav', 'nice', 'dai']\n",
      "Tokenized sentence: ['k', 'i', 'll', 'work', 'something', 'out']\n",
      "After stop words removal: ['k', 'work', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['work', 'somet']\n",
      "Tokenized sentence: ['already', 'one', 'guy', 'loving', 'you']\n",
      "After stop words removal: ['already', 'one', 'guy', 'loving']\n",
      "lov\n",
      "After stemming with porters algorithm: ['alreadi', 'on', 'gui', 'love']\n",
      "Tokenized sentence: ['yes', 'i', 'know', 'the', 'cheesy', 'songs', 'from', 'frosty', 'the', 'snowman']\n",
      "After stop words removal: ['yes', 'know', 'cheesy', 'songs', 'frosty', 'snowman']\n",
      "After stemming with porters algorithm: ['ye', 'know', 'cheesi', 'song', 'frosti', 'snowman']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['no', 'i', 'am', 'not', 'having', 'not', 'any', 'movies', 'in', 'my', 'laptop']\n",
      "After stop words removal: ['movies', 'laptop']\n",
      "After stemming with porters algorithm: ['movi', 'laptop']\n",
      "Tokenized sentence: ['sometimes', 'we', 'put', 'walls', 'around', 'our', 'hearts', 'not', 'just', 'to', 'be', 'safe', 'from', 'getting', 'hurt', 'but', 'to', 'find', 'out', 'who', 'cares', 'enough', 'to', 'break', 'the', 'walls', 'amp', 'get', 'closer', 'goodnoon']\n",
      "After stop words removal: ['sometimes', 'put', 'walls', 'around', 'hearts', 'safe', 'getting', 'hurt', 'find', 'cares', 'enough', 'break', 'walls', 'amp', 'get', 'closer', 'goodnoon']\n",
      "gett\n",
      "After stemming with porters algorithm: ['sometim', 'put', 'wall', 'around', 'heart', 'safe', 'get', 'hurt', 'find', 'care', 'enough', 'break', 'wall', 'amp', 'get', 'closer', 'goodnoon']\n",
      "Tokenized sentence: ['senthil', 'group', 'company', 'apnt', 'pm']\n",
      "After stop words removal: ['senthil', 'group', 'company', 'apnt', 'pm']\n",
      "After stemming with porters algorithm: ['senthil', 'group', 'compani', 'apnt']\n",
      "Tokenized sentence: ['dad', 'wanted', 'to', 'talk', 'about', 'the', 'apartment', 'so', 'i', 'got', 'a', 'late', 'start', 'omw', 'now']\n",
      "After stop words removal: ['dad', 'wanted', 'talk', 'apartment', 'got', 'late', 'start', 'omw']\n",
      "After stemming with porters algorithm: ['dad', 'wan', 'talk', 'apart', 'got', 'late', 'start', 'omw']\n",
      "Tokenized sentence: []\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['after', 'the', 'drug', 'she', 'will', 'be', 'able', 'to', 'eat']\n",
      "After stop words removal: ['drug', 'able', 'eat']\n",
      "After stemming with porters algorithm: ['drug', 'abl', 'eat']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'babes', 'hope', 'your', 'doing', 'ok', 'i', 'had', 'a', 'shit', 'nights', 'sleep', 'i', 'fell', 'asleep', 'at', 'i', 'm', 'knackered', 'and', 'i', 'm', 'dreading', 'work', 'tonight', 'what', 'are', 'thou', 'upto', 'tonight', 'x']\n",
      "After stop words removal: ['babes', 'hope', 'ok', 'shit', 'nights', 'sleep', 'fell', 'asleep', 'knackered', 'dreading', 'work', 'tonight', 'thou', 'upto', 'tonight', 'x']\n",
      "dread\n",
      "After stemming with porters algorithm: ['babe', 'hope', 'shit', 'night', 'sleep', 'fell', 'asleep', 'knacker', 'dread', 'work', 'tonight', 'thou', 'upto', 'tonight']\n",
      "Tokenized sentence: ['who', 's', 'there', 'say', 'hi', 'to', 'our', 'drugdealer']\n",
      "After stop words removal: ['say', 'hi', 'drugdealer']\n",
      "After stemming with porters algorithm: ['sai', 'drugdeal']\n",
      "Tokenized sentence: ['i', 'm', 'always', 'on', 'yahoo', 'messenger', 'now', 'just', 'send', 'the', 'message', 'to', 'me', 'and', 'i', 'll', 'get', 'it', 'you', 'may', 'have', 'to', 'send', 'it', 'in', 'the', 'mobile', 'mode', 'sha', 'but', 'i', 'll', 'get', 'it', 'and', 'will', 'reply']\n",
      "After stop words removal: ['always', 'yahoo', 'messenger', 'send', 'message', 'get', 'may', 'send', 'mobile', 'mode', 'sha', 'get', 'reply']\n",
      "After stemming with porters algorithm: ['alwai', 'yahoo', 'messeng', 'send', 'messag', 'get', 'mai', 'send', 'mobil', 'mode', 'sha', 'get', 'repli']\n",
      "Tokenized sentence: ['this', 'is', 'one', 'of', 'the', 'days', 'you', 'have', 'a', 'billion', 'classes', 'right']\n",
      "After stop words removal: ['one', 'days', 'billion', 'classes', 'right']\n",
      "After stemming with porters algorithm: ['on', 'dai', 'billion', 'class', 'right']\n",
      "Tokenized sentence: ['huh', 'also', 'cannot', 'then', 'only', 'how', 'many', 'mistakes']\n",
      "After stop words removal: ['huh', 'also', 'cannot', 'many', 'mistakes']\n",
      "After stemming with porters algorithm: ['huh', 'also', 'cannot', 'mani', 'mistak']\n",
      "Tokenized sentence: ['that', 'day', 'say', 'cut', 'ur', 'hair', 'at', 'paragon', 'is', 'it', 'called', 'hair', 'sense', 'do', 'noe', 'how', 'much', 'is', 'a', 'hair', 'cut']\n",
      "After stop words removal: ['day', 'say', 'cut', 'ur', 'hair', 'paragon', 'called', 'hair', 'sense', 'noe', 'much', 'hair', 'cut']\n",
      "After stemming with porters algorithm: ['dai', 'sai', 'cut', 'hair', 'paragon', 'call', 'hair', 'sens', 'noe', 'much', 'hair', 'cut']\n",
      "Tokenized sentence: ['huh', 'so', 'slow', 'i', 'tot', 'u', 'reach', 'long', 'ago', 'liao', 'u', 'more', 'days', 'only', 'i', 'more', 'leh']\n",
      "After stop words removal: ['huh', 'slow', 'tot', 'u', 'reach', 'long', 'ago', 'liao', 'u', 'days', 'leh']\n",
      "After stemming with porters algorithm: ['huh', 'slow', 'tot', 'reach', 'long', 'ago', 'liao', 'dai', 'leh']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'ringtone', 'order', 'reference', 'number', 'x', 'your', 'mobile', 'will', 'be', 'charged', 'should', 'your', 'tone', 'not', 'arrive', 'please', 'call', 'customer', 'services']\n",
      "After stop words removal: ['thanks', 'ringtone', 'order', 'reference', 'number', 'x', 'mobile', 'charged', 'tone', 'arrive', 'please', 'call', 'customer', 'services']\n",
      "After stemming with porters algorithm: ['thank', 'rington', 'order', 'refer', 'number', 'mobil', 'char', 'tone', 'arriv', 'pleas', 'call', 'custom', 'servic']\n",
      "Tokenized sentence: ['okie']\n",
      "After stop words removal: ['okie']\n",
      "After stemming with porters algorithm: ['oki']\n",
      "Tokenized sentence: ['its', 'a', 'laptop', 'take', 'it', 'with', 'you']\n",
      "After stop words removal: ['laptop', 'take']\n",
      "After stemming with porters algorithm: ['laptop', 'take']\n",
      "Tokenized sentence: ['todays', 'vodafone', 'numbers', 'ending', 'with', 'my', 'last', 'four', 'digits', 'are', 'selected', 'to', 'received', 'a', 'award', 'if', 'your', 'number', 'matches', 'please', 'call', 'to', 'claim', 'your', 'award']\n",
      "After stop words removal: ['todays', 'vodafone', 'numbers', 'ending', 'last', 'four', 'digits', 'selected', 'received', 'award', 'number', 'matches', 'please', 'call', 'claim', 'award']\n",
      "end\n",
      "After stemming with porters algorithm: ['todai', 'vodafon', 'number', 'en', 'last', 'four', 'digit', 'selec', 'receiv', 'award', 'number', 'match', 'pleas', 'call', 'claim', 'award']\n",
      "Tokenized sentence: ['we', 'still', 'on', 'for', 'tonight']\n",
      "After stop words removal: ['still', 'tonight']\n",
      "After stemming with porters algorithm: ['still', 'tonight']\n",
      "Tokenized sentence: ['can', 'you', 'open', 'the', 'door']\n",
      "After stop words removal: ['open', 'door']\n",
      "After stemming with porters algorithm: ['open', 'door']\n",
      "Tokenized sentence: ['still', 'chance', 'there', 'if', 'you', 'search', 'hard', 'you', 'will', 'get', 'it', 'let', 'have', 'a', 'try']\n",
      "After stop words removal: ['still', 'chance', 'search', 'hard', 'get', 'let', 'try']\n",
      "After stemming with porters algorithm: ['still', 'chanc', 'search', 'hard', 'get', 'let', 'try']\n",
      "Tokenized sentence: ['s', 'da', 'al', 'r', 'above', 'lt', 'gt']\n",
      "After stop words removal: ['da', 'al', 'r', 'lt', 'gt']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['its', 'a', 'valentine', 'game', 'send', 'dis', 'msg', 'to', 'all', 'ur', 'friends', 'if', 'answers', 'r', 'd', 'same', 'then', 'someone', 'really', 'loves', 'u', 'ques', 'which', 'colour', 'suits', 'me', 'the', 'best', 'rply', 'me']\n",
      "After stop words removal: ['valentine', 'game', 'send', 'dis', 'msg', 'ur', 'friends', 'answers', 'r', 'someone', 'really', 'loves', 'u', 'ques', 'colour', 'suits', 'best', 'rply']\n",
      "After stemming with porters algorithm: ['valentin', 'game', 'send', 'di', 'msg', 'friend', 'answer', 'someon', 'realli', 'love', 'que', 'colour', 'suit', 'best', 'rply']\n",
      "Tokenized sentence: ['don', 't', 'look', 'back', 'at', 'the', 'building', 'because', 'you', 'have', 'no', 'coat', 'and', 'i', 'don', 't', 'want', 'you', 'to', 'get', 'more', 'sick', 'just', 'hurry', 'home', 'and', 'wear', 'a', 'coat', 'to', 'the', 'gym']\n",
      "After stop words removal: ['look', 'back', 'building', 'coat', 'want', 'get', 'sick', 'hurry', 'home', 'wear', 'coat', 'gym']\n",
      "build\n",
      "After stemming with porters algorithm: ['look', 'back', 'buil', 'coat', 'want', 'get', 'sick', 'hurri', 'home', 'wear', 'coat', 'gym']\n",
      "Tokenized sentence: ['oh', 'okie', 'lor', 'we', 'go', 'on', 'sat']\n",
      "After stop words removal: ['oh', 'okie', 'lor', 'go', 'sat']\n",
      "After stemming with porters algorithm: ['oki', 'lor', 'sat']\n",
      "Tokenized sentence: ['then', 'get', 'some', 'cash', 'together', 'and', 'i', 'll', 'text', 'jason']\n",
      "After stop words removal: ['get', 'cash', 'together', 'text', 'jason']\n",
      "After stemming with porters algorithm: ['get', 'cash', 'togeth', 'text', 'jason']\n",
      "Tokenized sentence: ['why', 'nothing', 'ok', 'anyway', 'give', 'me', 'treat']\n",
      "After stop words removal: ['nothing', 'ok', 'anyway', 'give', 'treat']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not', 'anywai', 'give', 'treat']\n",
      "Tokenized sentence: ['our', 'prashanthettan', 's', 'mother', 'passed', 'away', 'last', 'night', 'pray', 'for', 'her', 'and', 'family']\n",
      "After stop words removal: ['prashanthettan', 'mother', 'passed', 'away', 'last', 'night', 'pray', 'family']\n",
      "After stemming with porters algorithm: ['prashanthettan', 'mother', 'pass', 'awai', 'last', 'night', 'prai', 'famili']\n",
      "Tokenized sentence: ['mmmmm', 'i', 'loved', 'waking', 'to', 'your', 'words', 'this', 'morning', 'i', 'miss', 'you', 'too', 'my', 'love', 'i', 'hope', 'your', 'day', 'goes', 'well', 'and', 'you', 'are', 'happy', 'i', 'wait', 'for', 'us', 'to', 'be', 'together', 'again']\n",
      "After stop words removal: ['mmmmm', 'loved', 'waking', 'words', 'morning', 'miss', 'love', 'hope', 'day', 'goes', 'well', 'happy', 'wait', 'us', 'together']\n",
      "wak\n",
      "morn\n",
      "After stemming with porters algorithm: ['mmmmm', 'love', 'wake', 'word', 'mor', 'miss', 'love', 'hope', 'dai', 'goe', 'well', 'happi', 'wait', 'togeth']\n",
      "Tokenized sentence: ['in', 'other', 'news', 'after', 'hassling', 'me', 'to', 'get', 'him', 'weed', 'for', 'a', 'week', 'andres', 'has', 'no', 'money', 'haughaighgtujhyguj']\n",
      "After stop words removal: ['news', 'hassling', 'get', 'weed', 'week', 'andres', 'money', 'haughaighgtujhyguj']\n",
      "hassl\n",
      "After stemming with porters algorithm: ['new', 'hassl', 'get', 'weed', 'week', 'andr', 'monei', 'haughaighgtujhyguj']\n",
      "Tokenized sentence: ['please', 'leave', 'this', 'topic', 'sorry', 'for', 'telling', 'that']\n",
      "After stop words removal: ['please', 'leave', 'topic', 'sorry', 'telling']\n",
      "tell\n",
      "After stemming with porters algorithm: ['pleas', 'leav', 'topic', 'sorri', 'tell']\n",
      "Tokenized sentence: ['mystery', 'solved', 'just', 'opened', 'my', 'email', 'and', 'he', 's', 'sent', 'me', 'another', 'batch', 'isn', 't', 'he', 'a', 'sweetie']\n",
      "After stop words removal: ['mystery', 'solved', 'opened', 'email', 'sent', 'another', 'batch', 'sweetie']\n",
      "After stemming with porters algorithm: ['mysteri', 'sol', 'open', 'email', 'sent', 'anoth', 'batch', 'sweeti']\n",
      "Tokenized sentence: ['tap', 'spile', 'at', 'seven', 'is', 'that', 'pub', 'on', 'gas', 'st', 'off', 'broad', 'st', 'by', 'canal', 'ok']\n",
      "After stop words removal: ['tap', 'spile', 'seven', 'pub', 'gas', 'st', 'broad', 'st', 'canal', 'ok']\n",
      "After stemming with porters algorithm: ['tap', 'spile', 'seven', 'pub', 'ga', 'broad', 'canal']\n",
      "Tokenized sentence: ['v', 'ma', 'fan']\n",
      "After stop words removal: ['v', 'fan']\n",
      "After stemming with porters algorithm: ['fan']\n",
      "Tokenized sentence: ['said', 'kiss', 'kiss', 'i', 'can', 't', 'do', 'the', 'sound', 'effects', 'he', 'is', 'a', 'gorgeous', 'man', 'isn', 't', 'he', 'kind', 'of', 'person', 'who', 'needs', 'a', 'smile', 'to', 'brighten', 'his', 'day']\n",
      "After stop words removal: ['said', 'kiss', 'kiss', 'sound', 'effects', 'gorgeous', 'man', 'kind', 'person', 'needs', 'smile', 'brighten', 'day']\n",
      "After stemming with porters algorithm: ['said', 'kiss', 'kiss', 'sound', 'effect', 'gorgeou', 'man', 'kind', 'person', 'need', 'smile', 'brighten', 'dai']\n",
      "Tokenized sentence: ['hey', 'leave', 'it', 'not', 'a', 'big', 'deal', 'take', 'care']\n",
      "After stop words removal: ['hey', 'leave', 'big', 'deal', 'take', 'care']\n",
      "After stemming with porters algorithm: ['hei', 'leav', 'big', 'deal', 'take', 'care']\n",
      "Tokenized sentence: ['yup', 'how', 'noe', 'leh']\n",
      "After stop words removal: ['yup', 'noe', 'leh']\n",
      "After stemming with porters algorithm: ['yup', 'noe', 'leh']\n",
      "Tokenized sentence: ['don', 'know', 'i', 'did', 't', 'msg', 'him', 'recently']\n",
      "After stop words removal: ['know', 'msg', 'recently']\n",
      "After stemming with porters algorithm: ['know', 'msg', 'recent']\n",
      "Tokenized sentence: ['pansy', 'you', 've', 'been', 'living', 'in', 'a', 'jungle', 'for', 'two', 'years', 'its', 'my', 'driving', 'you', 'should', 'be', 'more', 'worried', 'about']\n",
      "After stop words removal: ['pansy', 'living', 'jungle', 'two', 'years', 'driving', 'worried']\n",
      "liv\n",
      "driv\n",
      "After stemming with porters algorithm: ['pansi', 'live', 'jungl', 'two', 'year', 'drive', 'worri']\n",
      "Tokenized sentence: ['oh', 'yeah', 'and', 'hav', 'a', 'great', 'time', 'in', 'newquay', 'send', 'me', 'a', 'postcard', 'look', 'after', 'all', 'the', 'girls', 'while', 'im', 'gone', 'u', 'know', 'the', 'im', 'talkin', 'bout', 'xx']\n",
      "After stop words removal: ['oh', 'yeah', 'hav', 'great', 'time', 'newquay', 'send', 'postcard', 'look', 'girls', 'im', 'gone', 'u', 'know', 'im', 'talkin', 'bout', 'xx']\n",
      "After stemming with porters algorithm: ['yeah', 'hav', 'great', 'time', 'newquai', 'send', 'postcard', 'look', 'girl', 'gone', 'know', 'talkin', 'bout']\n",
      "Tokenized sentence: ['haha', 'just', 'kidding', 'papa', 'needs', 'drugs']\n",
      "After stop words removal: ['haha', 'kidding', 'papa', 'needs', 'drugs']\n",
      "kidd\n",
      "After stemming with porters algorithm: ['haha', 'kid', 'papa', 'need', 'drug']\n",
      "Tokenized sentence: ['hahaha', 'use', 'your', 'brain', 'dear']\n",
      "After stop words removal: ['hahaha', 'use', 'brain', 'dear']\n",
      "After stemming with porters algorithm: ['hahaha', 'us', 'brain', 'dear']\n",
      "Tokenized sentence: ['watch', 'lor', 'i', 'saw', 'a', 'few', 'swatch', 'one', 'i', 'thk', 'quite', 'ok', 'ard', 'but', 'i', 'need', 'nd', 'opinion', 'leh']\n",
      "After stop words removal: ['watch', 'lor', 'saw', 'swatch', 'one', 'thk', 'quite', 'ok', 'ard', 'need', 'nd', 'opinion', 'leh']\n",
      "After stemming with porters algorithm: ['watch', 'lor', 'saw', 'swatch', 'on', 'thk', 'quit', 'ard', 'need', 'opinion', 'leh']\n",
      "Tokenized sentence: ['mths', 'half', 'price', 'orange', 'line', 'rental', 'latest', 'camera', 'phones', 'free', 'had', 'your', 'phone', 'mths', 'call', 'mobilesdirect', 'free', 'on', 'to', 'update', 'now', 'or', 'stoptxt']\n",
      "After stop words removal: ['mths', 'half', 'price', 'orange', 'line', 'rental', 'latest', 'camera', 'phones', 'free', 'phone', 'mths', 'call', 'mobilesdirect', 'free', 'update', 'stoptxt']\n",
      "After stemming with porters algorithm: ['mth', 'half', 'price', 'orang', 'line', 'rental', 'latest', 'camera', 'phone', 'free', 'phone', 'mth', 'call', 'mobilesdirect', 'free', 'updat', 'stoptxt']\n",
      "Tokenized sentence: ['dun', 'need', 'to', 'pick', 'ur', 'gf']\n",
      "After stop words removal: ['dun', 'need', 'pick', 'ur', 'gf']\n",
      "After stemming with porters algorithm: ['dun', 'need', 'pick']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'class', 'did', 'you', 'get', 'my', 'text']\n",
      "After stop words removal: ['class', 'get', 'text']\n",
      "After stemming with porters algorithm: ['class', 'get', 'text']\n",
      "Tokenized sentence: ['jay', 'wants', 'to', 'work', 'out', 'first', 'how', 's', 'sound']\n",
      "After stop words removal: ['jay', 'wants', 'work', 'first', 'sound']\n",
      "After stemming with porters algorithm: ['jai', 'want', 'work', 'first', 'sound']\n",
      "Tokenized sentence: ['and', 'that', 's', 'fine', 'i', 'got', 'enough', 'bud', 'to', 'last', 'most', 'of', 'the', 'night', 'at', 'least']\n",
      "After stop words removal: ['fine', 'got', 'enough', 'bud', 'last', 'night', 'least']\n",
      "After stemming with porters algorithm: ['fine', 'got', 'enough', 'bud', 'last', 'night', 'least']\n",
      "Tokenized sentence: ['what', 's', 'happening', 'with', 'you', 'have', 'you', 'gotten', 'a', 'job', 'and', 'have', 'you', 'begun', 'registration', 'for', 'permanent', 'residency']\n",
      "After stop words removal: ['happening', 'gotten', 'job', 'begun', 'registration', 'permanent', 'residency']\n",
      "happen\n",
      "After stemming with porters algorithm: ['happen', 'gotten', 'job', 'begun', 'registr', 'perman', 'resid']\n",
      "Tokenized sentence: ['ambrith', 'madurai', 'met', 'u', 'in', 'arun', 'dha', 'marrge', 'remembr']\n",
      "After stop words removal: ['ambrith', 'madurai', 'met', 'u', 'arun', 'dha', 'marrge', 'remembr']\n",
      "After stemming with porters algorithm: ['ambrith', 'madurai', 'met', 'arun', 'dha', 'marrg', 'remembr']\n",
      "Tokenized sentence: ['free', 'entry', 'into', 'our', 'weekly', 'competition', 'just', 'text', 'the', 'word', 'win', 'to', 'now', 't', 'c', 'www', 'txttowin', 'co', 'uk']\n",
      "After stop words removal: ['free', 'entry', 'weekly', 'competition', 'text', 'word', 'win', 'c', 'www', 'txttowin', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'weekli', 'competit', 'text', 'word', 'win', 'www', 'txttowin']\n",
      "Tokenized sentence: ['i', 'will', 'reach', 'ur', 'home', 'in', 'lt', 'gt', 'minutes']\n",
      "After stop words removal: ['reach', 'ur', 'home', 'lt', 'gt', 'minutes']\n",
      "After stemming with porters algorithm: ['reach', 'home', 'minut']\n",
      "Tokenized sentence: ['cramps', 'stopped', 'going', 'back', 'to', 'sleep']\n",
      "After stop words removal: ['cramps', 'stopped', 'going', 'back', 'sleep']\n",
      "go\n",
      "After stemming with porters algorithm: ['cramp', 'stop', 'go', 'back', 'sleep']\n",
      "Tokenized sentence: ['in', 'da', 'car', 'park']\n",
      "After stop words removal: ['da', 'car', 'park']\n",
      "After stemming with porters algorithm: ['car', 'park']\n",
      "Tokenized sentence: ['isn', 't', 'frnd', 'a', 'necesity', 'in', 'life', 'imagine', 'urself', 'witout', 'a', 'frnd', 'hw', 'd', 'u', 'feel', 'at', 'ur', 'colleg', 'wat', 'll', 'u', 'do', 'wth', 'ur', 'cell', 'wat', 'abt', 'functions', 'thnk', 'abt', 'events', 'espe', 'll', 'cared', 'missed', 'amp', 'irritated', 'u', 'wrd', 'it', 'to', 'all', 'those', 'dear', 'loving', 'frnds', 'wthout', 'whom', 'u', 'cant', 'live', 'i', 'jst', 'did', 'it', 'takecare', 'goodmorning']\n",
      "After stop words removal: ['frnd', 'necesity', 'life', 'imagine', 'urself', 'witout', 'frnd', 'hw', 'u', 'feel', 'ur', 'colleg', 'wat', 'u', 'wth', 'ur', 'cell', 'wat', 'abt', 'functions', 'thnk', 'abt', 'events', 'espe', 'cared', 'missed', 'amp', 'irritated', 'u', 'wrd', 'dear', 'loving', 'frnds', 'wthout', 'u', 'cant', 'live', 'jst', 'takecare', 'goodmorning']\n",
      "irritate\n",
      "lov\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['frnd', 'neces', 'life', 'imagin', 'urself', 'witout', 'frnd', 'feel', 'colleg', 'wat', 'wth', 'cell', 'wat', 'abt', 'funct', 'thnk', 'abt', 'event', 'esp', 'care', 'miss', 'amp', 'irrit', 'wrd', 'dear', 'love', 'frnd', 'wthout', 'cant', 'live', 'jst', 'takecar', 'goodmor']\n",
      "Tokenized sentence: ['me', 'babe', 'i', 'feel', 'the', 'same', 'lets', 'just', 'get', 'about', 'it', 'both', 'try', 'cheer', 'up', 'not', 'fit', 'soo', 'muchxxlove', 'u', 'locaxx']\n",
      "After stop words removal: ['babe', 'feel', 'lets', 'get', 'try', 'cheer', 'fit', 'soo', 'muchxxlove', 'u', 'locaxx']\n",
      "After stemming with porters algorithm: ['babe', 'feel', 'let', 'get', 'try', 'cheer', 'fit', 'soo', 'muchxxlov', 'locaxx']\n",
      "Tokenized sentence: ['mm', 'umma', 'ask', 'vava', 'also', 'to', 'come', 'tell', 'him', 'can', 'play', 'later', 'together']\n",
      "After stop words removal: ['mm', 'umma', 'ask', 'vava', 'also', 'come', 'tell', 'play', 'later', 'together']\n",
      "After stemming with porters algorithm: ['umma', 'ask', 'vava', 'also', 'come', 'tell', 'plai', 'later', 'togeth']\n",
      "Tokenized sentence: ['don', 'know', 'wait', 'i', 'will', 'check', 'it']\n",
      "After stop words removal: ['know', 'wait', 'check']\n",
      "After stemming with porters algorithm: ['know', 'wait', 'check']\n",
      "Tokenized sentence: ['yes', 'i', 'm', 'small', 'kid', 'and', 'boost', 'is', 'the', 'secret', 'of', 'my', 'energy']\n",
      "After stop words removal: ['yes', 'small', 'kid', 'boost', 'secret', 'energy']\n",
      "After stemming with porters algorithm: ['ye', 'small', 'kid', 'boost', 'secret', 'energi']\n",
      "Tokenized sentence: ['da', 'my', 'birthdate', 'in', 'certificate', 'is', 'in', 'april', 'but', 'real', 'date', 'is', 'today', 'but', 'dont', 'publish', 'it', 'i', 'shall', 'give', 'you', 'a', 'special', 'treat', 'if', 'you', 'keep', 'the', 'secret', 'any', 'way', 'thanks', 'for', 'the', 'wishes']\n",
      "After stop words removal: ['da', 'birthdate', 'certificate', 'april', 'real', 'date', 'today', 'dont', 'publish', 'shall', 'give', 'special', 'treat', 'keep', 'secret', 'way', 'thanks', 'wishes']\n",
      "After stemming with porters algorithm: ['birthdat', 'certif', 'april', 'real', 'date', 'todai', 'dont', 'publish', 'shall', 'give', 'special', 'treat', 'keep', 'secret', 'wai', 'thank', 'wish']\n",
      "Tokenized sentence: ['good', 'no', 'don', 't', 'need', 'any', 'receipts', 'well', 'done', 'yes', 'please', 'tell', 'what', 's', 'her', 'number', 'i', 'could', 'ring', 'her']\n",
      "After stop words removal: ['good', 'need', 'receipts', 'well', 'done', 'yes', 'please', 'tell', 'number', 'could', 'ring']\n",
      "After stemming with porters algorithm: ['good', 'need', 'receipt', 'well', 'done', 'ye', 'pleas', 'tell', 'number', 'could', 'ring']\n",
      "Tokenized sentence: ['as', 'i', 'entered', 'my', 'cabin', 'my', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'i', 'felt', 'special', 'she', 'askd', 'me', 'lunch', 'after', 'lunch', 'she', 'invited', 'me', 'to', 'her', 'apartment', 'we', 'went', 'there']\n",
      "After stop words removal: ['entered', 'cabin', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invited', 'apartment', 'went']\n",
      "After stemming with porters algorithm: ['enter', 'cabin', 'said', 'happi', 'dai', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invit', 'apart', 'went']\n",
      "Tokenized sentence: ['ok', 'but', 'i', 'finish', 'at']\n",
      "After stop words removal: ['ok', 'finish']\n",
      "After stemming with porters algorithm: ['finish']\n",
      "Tokenized sentence: ['tessy', 'pls', 'do', 'me', 'a', 'favor', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimya', 'pls', 'dnt', 'forget', 'it', 'today', 'is', 'her', 'birthday', 'shijas']\n",
      "After stop words removal: ['tessy', 'pls', 'favor', 'pls', 'convey', 'birthday', 'wishes', 'nimya', 'pls', 'dnt', 'forget', 'today', 'birthday', 'shijas']\n",
      "After stemming with porters algorithm: ['tessi', 'pl', 'favor', 'pl', 'convei', 'birthdai', 'wish', 'nimya', 'pl', 'dnt', 'forget', 'todai', 'birthdai', 'shija']\n",
      "Tokenized sentence: ['hey', 'tmr', 'maybe', 'can', 'meet', 'you', 'at', 'yck']\n",
      "After stop words removal: ['hey', 'tmr', 'maybe', 'meet', 'yck']\n",
      "After stemming with porters algorithm: ['hei', 'tmr', 'mayb', 'meet', 'yck']\n",
      "Tokenized sentence: ['i', 'had', 'askd', 'u', 'a', 'question', 'some', 'hours', 'before', 'its', 'answer']\n",
      "After stop words removal: ['askd', 'u', 'question', 'hours', 'answer']\n",
      "After stemming with porters algorithm: ['askd', 'quest', 'hour', 'answer']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'ringtone', 'order', 'reference', 'number', 'x', 'your', 'mobile', 'will', 'be', 'charged', 'should', 'your', 'tone', 'not', 'arrive', 'please', 'call', 'customer', 'services', 'from', 'colour', 'red', 'text', 'colour', 'txtstar']\n",
      "After stop words removal: ['thanks', 'ringtone', 'order', 'reference', 'number', 'x', 'mobile', 'charged', 'tone', 'arrive', 'please', 'call', 'customer', 'services', 'colour', 'red', 'text', 'colour', 'txtstar']\n",
      "After stemming with porters algorithm: ['thank', 'rington', 'order', 'refer', 'number', 'mobil', 'char', 'tone', 'arriv', 'pleas', 'call', 'custom', 'servic', 'colour', 'red', 'text', 'colour', 'txtstar']\n",
      "Tokenized sentence: ['height', 'of', 'confidence', 'all', 'the', 'aeronautics', 'professors', 'wer', 'calld', 'amp', 'they', 'wer', 'askd', 'sit', 'in', 'an', 'aeroplane', 'aftr', 'they', 'sat', 'they', 'wer', 'told', 'dat', 'the', 'plane', 'ws', 'made', 'by', 'their', 'students', 'dey', 'all', 'hurried', 'out', 'of', 'd', 'plane', 'bt', 'only', 'didnt', 'move', 'he', 'said', 'if', 'it', 'is', 'made', 'by', 'my', 'students']\n",
      "After stop words removal: ['height', 'confidence', 'aeronautics', 'professors', 'wer', 'calld', 'amp', 'wer', 'askd', 'sit', 'aeroplane', 'aftr', 'sat', 'wer', 'told', 'dat', 'plane', 'ws', 'made', 'students', 'dey', 'hurried', 'plane', 'bt', 'didnt', 'move', 'said', 'made', 'students']\n",
      "After stemming with porters algorithm: ['height', 'confid', 'aeronaut', 'professor', 'wer', 'calld', 'amp', 'wer', 'askd', 'sit', 'aeroplan', 'aftr', 'sat', 'wer', 'told', 'dat', 'plane', 'made', 'student', 'dei', 'hurri', 'plane', 'didnt', 'move', 'said', 'made', 'student']\n",
      "Tokenized sentence: ['edison', 'has', 'rightly', 'said', 'a', 'fool', 'can', 'ask', 'more', 'questions', 'than', 'a', 'wise', 'man', 'can', 'answer', 'now', 'you', 'know', 'why', 'all', 'of', 'us', 'are', 'speechless', 'during', 'viva', 'gm']\n",
      "After stop words removal: ['edison', 'rightly', 'said', 'fool', 'ask', 'questions', 'wise', 'man', 'answer', 'know', 'us', 'speechless', 'viva', 'gm']\n",
      "After stemming with porters algorithm: ['edison', 'rightli', 'said', 'fool', 'ask', 'quest', 'wise', 'man', 'answer', 'know', 'speechless', 'viva']\n",
      "Tokenized sentence: ['i', 'am', 'real', 'baby', 'i', 'want', 'to', 'bring', 'out', 'your', 'inner', 'tigress']\n",
      "After stop words removal: ['real', 'baby', 'want', 'bring', 'inner', 'tigress']\n",
      "After stemming with porters algorithm: ['real', 'babi', 'want', 'bring', 'inner', 'tigress']\n",
      "Tokenized sentence: ['did', 'u', 'got', 'that', 'persons', 'story']\n",
      "After stop words removal: ['u', 'got', 'persons', 'story']\n",
      "After stemming with porters algorithm: ['got', 'person', 'stori']\n",
      "Tokenized sentence: ['wat', 'would', 'u', 'like', 'ur', 'birthday']\n",
      "After stop words removal: ['wat', 'would', 'u', 'like', 'ur', 'birthday']\n",
      "After stemming with porters algorithm: ['wat', 'would', 'like', 'birthdai']\n",
      "Tokenized sentence: ['ya', 'going', 'for', 'restaurant']\n",
      "After stop words removal: ['ya', 'going', 'restaurant']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'restaur']\n",
      "Tokenized sentence: ['dear', 'matthew', 'please', 'call', 'from', 'a', 'landline', 'your', 'complimentary', 'lux', 'tenerife', 'holiday', 'or', 'cash', 'await', 'collection', 'ppm', 'sae', 't', 'cs', 'box', 'sk', 'xh']\n",
      "After stop words removal: ['dear', 'matthew', 'please', 'call', 'landline', 'complimentary', 'lux', 'tenerife', 'holiday', 'cash', 'await', 'collection', 'ppm', 'sae', 'cs', 'box', 'sk', 'xh']\n",
      "After stemming with porters algorithm: ['dear', 'matthew', 'pleas', 'call', 'landlin', 'complimentari', 'lux', 'tenerif', 'holidai', 'cash', 'await', 'collect', 'ppm', 'sae', 'box']\n",
      "Tokenized sentence: ['yup', 'song', 'bro', 'no', 'creative', 'neva', 'test', 'quality', 'he', 'said', 'check', 'review', 'online']\n",
      "After stop words removal: ['yup', 'song', 'bro', 'creative', 'neva', 'test', 'quality', 'said', 'check', 'review', 'online']\n",
      "After stemming with porters algorithm: ['yup', 'song', 'bro', 'creativ', 'neva', 'test', 'qualiti', 'said', 'check', 'review', 'onlin']\n",
      "Tokenized sentence: ['i', 'don', 't', 'run', 'away', 'frm', 'u', 'i', 'walk', 'slowly', 'amp', 'it', 'kills', 'me', 'that', 'u', 'don', 't', 'care', 'enough', 'to', 'stop', 'me']\n",
      "After stop words removal: ['run', 'away', 'frm', 'u', 'walk', 'slowly', 'amp', 'kills', 'u', 'care', 'enough', 'stop']\n",
      "After stemming with porters algorithm: ['run', 'awai', 'frm', 'walk', 'slowli', 'amp', 'kill', 'care', 'enough', 'stop']\n",
      "Tokenized sentence: ['my', 'trip', 'was', 'ok', 'but', 'quite', 'tiring', 'lor', 'uni', 'starts', 'today', 'but', 'it', 's', 'ok', 'me', 'cos', 'i', 'm', 'not', 'taking', 'any', 'modules', 'but', 'jus', 'concentrating', 'on', 'my', 'final', 'yr', 'project']\n",
      "After stop words removal: ['trip', 'ok', 'quite', 'tiring', 'lor', 'uni', 'starts', 'today', 'ok', 'cos', 'taking', 'modules', 'jus', 'concentrating', 'final', 'yr', 'project']\n",
      "tir\n",
      "tak\n",
      "concentrat\n",
      "concentrate\n",
      "After stemming with porters algorithm: ['trip', 'quit', 'tire', 'lor', 'uni', 'start', 'todai', 'co', 'take', 'modul', 'ju', 'concentr', 'final', 'project']\n",
      "Tokenized sentence: ['oh', 'just', 'getting', 'even', 'with', 'u', 'u']\n",
      "After stop words removal: ['oh', 'getting', 'even', 'u', 'u']\n",
      "gett\n",
      "After stemming with porters algorithm: ['get', 'even']\n",
      "Tokenized sentence: ['your', 'opinion', 'about', 'me', 'over', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'not', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stop words removal: ['opinion', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stemming with porters algorithm: ['opinion', 'jada', 'kusruthi', 'lovab', 'silent', 'spl', 'charact', 'matur', 'stylish', 'simpl', 'pl', 'repli']\n",
      "Tokenized sentence: ['i', 'love', 'you', 'both', 'too']\n",
      "After stop words removal: ['love']\n",
      "After stemming with porters algorithm: ['love']\n",
      "Tokenized sentence: ['wat', 'r', 'u', 'doing']\n",
      "After stop words removal: ['wat', 'r', 'u']\n",
      "After stemming with porters algorithm: ['wat']\n",
      "Tokenized sentence: ['hey', 'darlin', 'i', 'can', 'pick', 'u', 'up', 'at', 'college', 'if', 'u', 'tell', 'me', 'wen', 'where', 'mt', 'love', 'pete', 'xx']\n",
      "After stop words removal: ['hey', 'darlin', 'pick', 'u', 'college', 'u', 'tell', 'wen', 'mt', 'love', 'pete', 'xx']\n",
      "After stemming with porters algorithm: ['hei', 'darlin', 'pick', 'colleg', 'tell', 'wen', 'love', 'pete']\n",
      "Tokenized sentence: ['looks', 'like', 'u', 'wil', 'b', 'getting', 'a', 'headstart', 'im', 'leaving', 'here', 'bout', 'ish', 'but', 'if', 'u', 'r', 'desperate', 'for', 'my', 'company', 'i', 'could', 'head', 'in', 'earlier', 'we', 'were', 'goin', 'to', 'meet', 'in', 'rummer']\n",
      "After stop words removal: ['looks', 'like', 'u', 'wil', 'b', 'getting', 'headstart', 'im', 'leaving', 'bout', 'ish', 'u', 'r', 'desperate', 'company', 'could', 'head', 'earlier', 'goin', 'meet', 'rummer']\n",
      "gett\n",
      "leav\n",
      "After stemming with porters algorithm: ['look', 'like', 'wil', 'get', 'headstart', 'leav', 'bout', 'ish', 'desper', 'compani', 'could', 'head', 'earlier', 'goin', 'meet', 'rummer']\n",
      "Tokenized sentence: ['no', 'did', 'you', 'multimedia', 'message', 'them', 'or', 'e', 'mail']\n",
      "After stop words removal: ['multimedia', 'message', 'e', 'mail']\n",
      "After stemming with porters algorithm: ['multimedia', 'messag', 'mail']\n",
      "Tokenized sentence: ['the', 'xmas', 'story', 'is', 'peace', 'the', 'xmas', 'msg', 'is', 'love', 'the', 'xmas', 'miracle', 'is', 'jesus', 'hav', 'a', 'blessed', 'month', 'ahead', 'amp', 'wish', 'u', 'merry', 'xmas']\n",
      "After stop words removal: ['xmas', 'story', 'peace', 'xmas', 'msg', 'love', 'xmas', 'miracle', 'jesus', 'hav', 'blessed', 'month', 'ahead', 'amp', 'wish', 'u', 'merry', 'xmas']\n",
      "After stemming with porters algorithm: ['xma', 'stori', 'peac', 'xma', 'msg', 'love', 'xma', 'mirac', 'jesu', 'hav', 'bless', 'month', 'ahead', 'amp', 'wish', 'merri', 'xma']\n",
      "Tokenized sentence: ['wot', 'student', 'discount', 'can', 'u', 'get', 'on', 'books']\n",
      "After stop words removal: ['wot', 'student', 'discount', 'u', 'get', 'books']\n",
      "After stemming with porters algorithm: ['wot', 'student', 'discount', 'get', 'book']\n",
      "Tokenized sentence: ['do', 'thing', 'change', 'that', 'sentence', 'into', 'because', 'i', 'want', 'concentrate', 'in', 'my', 'educational', 'career', 'im', 'leaving', 'here']\n",
      "After stop words removal: ['thing', 'change', 'sentence', 'want', 'concentrate', 'educational', 'career', 'im', 'leaving']\n",
      "leav\n",
      "After stemming with porters algorithm: ['thing', 'chang', 'sentenc', 'want', 'concentr', 'educ', 'career', 'leav']\n",
      "Tokenized sentence: ['aight', 'well', 'keep', 'me', 'informed']\n",
      "After stop words removal: ['aight', 'well', 'keep', 'informed']\n",
      "After stemming with porters algorithm: ['aight', 'well', 'keep', 'infor']\n",
      "Tokenized sentence: ['don', 't', 'necessarily', 'expect', 'it', 'to', 'be', 'done', 'before', 'you', 'get', 'back', 'though', 'because', 'i', 'm', 'just', 'now', 'headin', 'out']\n",
      "After stop words removal: ['necessarily', 'expect', 'done', 'get', 'back', 'though', 'headin']\n",
      "After stemming with porters algorithm: ['necessarili', 'expect', 'done', 'get', 'back', 'though', 'headin']\n",
      "Tokenized sentence: ['i', 'll', 'meet', 'you', 'in', 'the', 'lobby']\n",
      "After stop words removal: ['meet', 'lobby']\n",
      "After stemming with porters algorithm: ['meet', 'lobbi']\n",
      "Tokenized sentence: ['im', 'on', 'the', 'snowboarding', 'trip', 'i', 'was', 'wondering', 'if', 'your', 'planning', 'to', 'get', 'everyone', 'together', 'befor', 'we', 'go', 'a', 'meet', 'and', 'greet', 'kind', 'of', 'affair', 'cheers']\n",
      "After stop words removal: ['im', 'snowboarding', 'trip', 'wondering', 'planning', 'get', 'everyone', 'together', 'befor', 'go', 'meet', 'greet', 'kind', 'affair', 'cheers']\n",
      "snowboard\n",
      "wonder\n",
      "plann\n",
      "After stemming with porters algorithm: ['snowboar', 'trip', 'wonder', 'plan', 'get', 'everyon', 'togeth', 'befor', 'meet', 'greet', 'kind', 'affair', 'cheer']\n",
      "Tokenized sentence: ['how', 'dare', 'you', 'stupid', 'i', 'wont', 'tell', 'anything', 'to', 'you', 'hear', 'after', 'i', 'wont', 'talk', 'to', 'you']\n",
      "After stop words removal: ['dare', 'stupid', 'wont', 'tell', 'anything', 'hear', 'wont', 'talk']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['dare', 'stupid', 'wont', 'tell', 'anyt', 'hear', 'wont', 'talk']\n",
      "Tokenized sentence: ['got', 'wat', 'to', 'buy', 'tell', 'us', 'then', 'no', 'need', 'to', 'come', 'in', 'again']\n",
      "After stop words removal: ['got', 'wat', 'buy', 'tell', 'us', 'need', 'come']\n",
      "After stemming with porters algorithm: ['got', 'wat', 'bui', 'tell', 'need', 'come']\n",
      "Tokenized sentence: ['k', 'fyi', 'x', 'has', 'a', 'ride', 'early', 'tomorrow', 'morning', 'but', 'he', 's', 'crashing', 'at', 'our', 'place', 'tonight']\n",
      "After stop words removal: ['k', 'fyi', 'x', 'ride', 'early', 'tomorrow', 'morning', 'crashing', 'place', 'tonight']\n",
      "morn\n",
      "crash\n",
      "After stemming with porters algorithm: ['fyi', 'ride', 'earli', 'tomorrow', 'mor', 'cras', 'place', 'tonight']\n",
      "Tokenized sentence: ['yes', 'but', 'can', 'we', 'meet', 'in', 'town', 'cos', 'will', 'go', 'to', 'gep', 'and', 'then', 'home', 'you', 'could', 'text', 'at', 'bus', 'stop', 'and', 'don', 't', 'worry', 'we', 'll', 'have', 'finished', 'by', 'march', 'ish']\n",
      "After stop words removal: ['yes', 'meet', 'town', 'cos', 'go', 'gep', 'home', 'could', 'text', 'bus', 'stop', 'worry', 'finished', 'march', 'ish']\n",
      "After stemming with porters algorithm: ['ye', 'meet', 'town', 'co', 'gep', 'home', 'could', 'text', 'bu', 'stop', 'worri', 'finis', 'march', 'ish']\n",
      "Tokenized sentence: ['maybe', 'you', 'should', 'find', 'something', 'else', 'to', 'do', 'instead']\n",
      "After stop words removal: ['maybe', 'find', 'something', 'else', 'instead']\n",
      "someth\n",
      "After stemming with porters algorithm: ['mayb', 'find', 'somet', 'els', 'instead']\n",
      "Tokenized sentence: ['k', 'k', 'any', 'special', 'today']\n",
      "After stop words removal: ['k', 'k', 'special', 'today']\n",
      "After stemming with porters algorithm: ['special', 'todai']\n",
      "Tokenized sentence: ['do', 'whatever', 'you', 'want', 'you', 'know', 'what', 'the', 'rules', 'are', 'we', 'had', 'a', 'talk', 'earlier', 'this', 'week', 'about', 'what', 'had', 'to', 'start', 'happening', 'you', 'showing', 'responsibility', 'yet', 'every', 'week', 'it', 's', 'can', 'i', 'bend', 'the', 'rule', 'this', 'way', 'what', 'about', 'that', 'way', 'do', 'whatever', 'i', 'm', 'tired', 'of', 'having', 'thia', 'same', 'argument', 'with', 'you', 'every', 'week', 'and', 'a', 'lt', 'gt', 'movie', 'doesnt', 'inlude', 'the', 'previews', 'you', 're', 'still', 'getting', 'in', 'after']\n",
      "After stop words removal: ['whatever', 'want', 'know', 'rules', 'talk', 'earlier', 'week', 'start', 'happening', 'showing', 'responsibility', 'yet', 'every', 'week', 'bend', 'rule', 'way', 'way', 'whatever', 'tired', 'thia', 'argument', 'every', 'week', 'lt', 'gt', 'movie', 'doesnt', 'inlude', 'previews', 'still', 'getting']\n",
      "happen\n",
      "show\n",
      "gett\n",
      "After stemming with porters algorithm: ['whatev', 'want', 'know', 'rule', 'talk', 'earlier', 'week', 'start', 'happen', 'showe', 'respons', 'yet', 'everi', 'week', 'bend', 'rule', 'wai', 'wai', 'whatev', 'tire', 'thia', 'argum', 'everi', 'week', 'movi', 'doesnt', 'inlud', 'preview', 'still', 'get']\n",
      "Tokenized sentence: ['early', 'bird', 'any', 'purchases', 'yet']\n",
      "After stop words removal: ['early', 'bird', 'purchases', 'yet']\n",
      "After stemming with porters algorithm: ['earli', 'bird', 'purchas', 'yet']\n",
      "Tokenized sentence: ['its', 'good', 'we', 'll', 'find', 'a', 'way']\n",
      "After stop words removal: ['good', 'find', 'way']\n",
      "After stemming with porters algorithm: ['good', 'find', 'wai']\n",
      "Tokenized sentence: ['nothing', 'but', 'we', 'jus', 'tot', 'u', 'would', 'ask', 'cos', 'u', 'ba', 'gua', 'but', 'we', 'went', 'mt', 'faber', 'yest', 'yest', 'jus', 'went', 'out', 'already', 'mah', 'so', 'today', 'not', 'going', 'out', 'jus', 'call', 'lor']\n",
      "After stop words removal: ['nothing', 'jus', 'tot', 'u', 'would', 'ask', 'cos', 'u', 'ba', 'gua', 'went', 'mt', 'faber', 'yest', 'yest', 'jus', 'went', 'already', 'mah', 'today', 'going', 'jus', 'call', 'lor']\n",
      "noth\n",
      "go\n",
      "After stemming with porters algorithm: ['not', 'ju', 'tot', 'would', 'ask', 'co', 'gua', 'went', 'faber', 'yest', 'yest', 'ju', 'went', 'alreadi', 'mah', 'todai', 'go', 'ju', 'call', 'lor']\n",
      "Tokenized sentence: ['rose', 'for', 'red', 'red', 'for', 'blood', 'blood', 'for', 'heart', 'heart', 'for', 'u', 'but', 'u', 'for', 'me', 'send', 'tis', 'to', 'all', 'ur', 'friends', 'including', 'me', 'if', 'u', 'like', 'me', 'if', 'u', 'get', 'back', 'u', 'r', 'poor', 'in', 'relation', 'u', 'need', 'some', 'to', 'support', 'u', 'r', 'frnd', 'many', 'some', 'luvs', 'u', 'some', 'is', 'praying', 'god', 'to', 'marry', 'u', 'try', 'it']\n",
      "After stop words removal: ['rose', 'red', 'red', 'blood', 'blood', 'heart', 'heart', 'u', 'u', 'send', 'tis', 'ur', 'friends', 'including', 'u', 'like', 'u', 'get', 'back', 'u', 'r', 'poor', 'relation', 'u', 'need', 'support', 'u', 'r', 'frnd', 'many', 'luvs', 'u', 'praying', 'god', 'marry', 'u', 'try']\n",
      "includ\n",
      "pray\n",
      "After stemming with porters algorithm: ['rose', 'red', 'red', 'blood', 'blood', 'heart', 'heart', 'send', 'ti', 'friend', 'includ', 'like', 'get', 'back', 'poor', 'relat', 'need', 'support', 'frnd', 'mani', 'luv', 'prai', 'god', 'marri', 'try']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'work', 'please', 'call']\n",
      "After stop words removal: ['work', 'please', 'call']\n",
      "After stemming with porters algorithm: ['work', 'pleas', 'call']\n",
      "Tokenized sentence: ['free', 'msg', 'sorry', 'a', 'service', 'you', 'ordered', 'from', 'could', 'not', 'be', 'delivered', 'as', 'you', 'do', 'not', 'have', 'sufficient', 'credit', 'please', 'top', 'up', 'to', 'receive', 'the', 'service']\n",
      "After stop words removal: ['free', 'msg', 'sorry', 'service', 'ordered', 'could', 'delivered', 'sufficient', 'credit', 'please', 'top', 'receive', 'service']\n",
      "After stemming with porters algorithm: ['free', 'msg', 'sorri', 'servic', 'order', 'could', 'deliv', 'suffici', 'credit', 'pleas', 'top', 'receiv', 'servic']\n",
      "Tokenized sentence: ['sorry', 'no', 'have', 'got', 'few', 'things', 'to', 'do', 'may', 'be', 'in', 'pub', 'later']\n",
      "After stop words removal: ['sorry', 'got', 'things', 'may', 'pub', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'got', 'thing', 'mai', 'pub', 'later']\n",
      "Tokenized sentence: ['s', 's', 'first', 'time', 'dhoni', 'rocks']\n",
      "After stop words removal: ['first', 'time', 'dhoni', 'rocks']\n",
      "After stemming with porters algorithm: ['first', 'time', 'dhoni', 'rock']\n",
      "Tokenized sentence: ['xmas', 'iscoming', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'r', 'weekly', 'draw', 'txt', 'music', 'to', 'tnc']\n",
      "After stop words removal: ['xmas', 'iscoming', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'r', 'weekly', 'draw', 'txt', 'music', 'tnc']\n",
      "iscom\n",
      "After stemming with porters algorithm: ['xma', 'iscom', 'awar', 'either', 'gift', 'voucher', 'free', 'entri', 'weekli', 'draw', 'txt', 'music', 'tnc']\n",
      "Tokenized sentence: ['bognor', 'it', 'is', 'should', 'be', 'splendid', 'at', 'this', 'time', 'of', 'year']\n",
      "After stop words removal: ['bognor', 'splendid', 'time', 'year']\n",
      "After stemming with porters algorithm: ['bognor', 'splendid', 'time', 'year']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'video', 'phone', 'anytime', 'any', 'network', 'mins', 'text', 'for', 'only', 'five', 'pounds', 'per', 'week', 'call', 'now', 'or', 'reply', 'for', 'delivery', 'tomorrow']\n",
      "After stop words removal: ['want', 'new', 'video', 'phone', 'anytime', 'network', 'mins', 'text', 'five', 'pounds', 'per', 'week', 'call', 'reply', 'delivery', 'tomorrow']\n",
      "After stemming with porters algorithm: ['want', 'new', 'video', 'phone', 'anytim', 'network', 'min', 'text', 'five', 'pound', 'per', 'week', 'call', 'repli', 'deliveri', 'tomorrow']\n",
      "Tokenized sentence: ['want', 'get', 'laid', 'tonight', 'want', 'real', 'dogging', 'locations', 'sent', 'direct', 'ur', 'mob', 'join', 'the', 'uk', 's', 'largest', 'dogging', 'network', 'by', 'txting', 'moan', 'to', 'nyt', 'ec', 'a', 'p', 'msg', 'p']\n",
      "After stop words removal: ['want', 'get', 'laid', 'tonight', 'want', 'real', 'dogging', 'locations', 'sent', 'direct', 'ur', 'mob', 'join', 'uk', 'largest', 'dogging', 'network', 'txting', 'moan', 'nyt', 'ec', 'p', 'msg', 'p']\n",
      "dogg\n",
      "dogg\n",
      "After stemming with porters algorithm: ['want', 'get', 'laid', 'tonight', 'want', 'real', 'dog', 'locat', 'sent', 'direct', 'mob', 'join', 'largest', 'dog', 'network', 'txting', 'moan', 'nyt', 'msg']\n",
      "Tokenized sentence: ['go', 'to', 'write', 'msg', 'put', 'on', 'dictionary', 'mode', 'cover', 'the', 'screen', 'with', 'hand', 'press', 'lt', 'gt', 'gently', 'remove', 'ur', 'hand', 'its', 'interesting']\n",
      "After stop words removal: ['go', 'write', 'msg', 'put', 'dictionary', 'mode', 'cover', 'screen', 'hand', 'press', 'lt', 'gt', 'gently', 'remove', 'ur', 'hand', 'interesting']\n",
      "interest\n",
      "After stemming with porters algorithm: ['write', 'msg', 'put', 'dictionari', 'mode', 'cover', 'screen', 'hand', 'press', 'gentli', 'remov', 'hand', 'interes']\n",
      "Tokenized sentence: ['good', 'morning', 'princess', 'how', 'are', 'you']\n",
      "After stop words removal: ['good', 'morning', 'princess']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'princess']\n",
      "Tokenized sentence: ['reckon', 'need', 'to', 'be', 'in', 'town', 'by', 'eightish', 'to', 'walk', 'from', 'carpark']\n",
      "After stop words removal: ['reckon', 'need', 'town', 'eightish', 'walk', 'carpark']\n",
      "After stemming with porters algorithm: ['reckon', 'need', 'town', 'eightish', 'walk', 'carpark']\n",
      "Tokenized sentence: ['still', 'at', 'west', 'coast', 'haiz', 'll', 'take', 'forever', 'to', 'come', 'back']\n",
      "After stop words removal: ['still', 'west', 'coast', 'haiz', 'take', 'forever', 'come', 'back']\n",
      "After stemming with porters algorithm: ['still', 'west', 'coast', 'haiz', 'take', 'forev', 'come', 'back']\n",
      "Tokenized sentence: ['where', 'in', 'abj', 'are', 'you', 'serving', 'are', 'you', 'staying', 'with', 'dad', 'or', 'alone']\n",
      "After stop words removal: ['abj', 'serving', 'staying', 'dad', 'alone']\n",
      "serv\n",
      "stay\n",
      "After stemming with porters algorithm: ['abj', 'ser', 'stai', 'dad', 'alon']\n",
      "Tokenized sentence: ['you', 've', 'won', 'tkts', 'to', 'the', 'euro', 'cup', 'final', 'or', 'cash', 'to', 'collect', 'call', 'b', 'pobox', 'ppm']\n",
      "After stop words removal: ['tkts', 'euro', 'cup', 'final', 'cash', 'collect', 'call', 'b', 'pobox', 'ppm']\n",
      "After stemming with porters algorithm: ['tkt', 'euro', 'cup', 'final', 'cash', 'collect', 'call', 'pobox', 'ppm']\n",
      "Tokenized sentence: ['i', 'm', 'leaving', 'my', 'house', 'now']\n",
      "After stop words removal: ['leaving', 'house']\n",
      "leav\n",
      "After stemming with porters algorithm: ['leav', 'hous']\n",
      "Tokenized sentence: ['house', 'maid', 'is', 'the', 'murderer', 'coz', 'the', 'man', 'was', 'murdered', 'on', 'lt', 'gt', 'th', 'january', 'as', 'public', 'holiday', 'all', 'govt', 'instituitions', 'are', 'closed', 'including', 'post', 'office', 'understand']\n",
      "After stop words removal: ['house', 'maid', 'murderer', 'coz', 'man', 'murdered', 'lt', 'gt', 'th', 'january', 'public', 'holiday', 'govt', 'instituitions', 'closed', 'including', 'post', 'office', 'understand']\n",
      "includ\n",
      "After stemming with porters algorithm: ['hous', 'maid', 'murder', 'coz', 'man', 'murder', 'januari', 'public', 'holidai', 'govt', 'instituit', 'close', 'includ', 'post', 'offic', 'understand']\n",
      "Tokenized sentence: ['no', 'am', 'working', 'on', 'the', 'ringing', 'u', 'thing', 'but', 'have', 'whole', 'houseful', 'of', 'screaming', 'brats', 'so', 'am', 'pulling', 'my', 'hair', 'out', 'loving', 'u']\n",
      "After stop words removal: ['working', 'ringing', 'u', 'thing', 'whole', 'houseful', 'screaming', 'brats', 'pulling', 'hair', 'loving', 'u']\n",
      "work\n",
      "ring\n",
      "scream\n",
      "pull\n",
      "lov\n",
      "After stemming with porters algorithm: ['wor', 'rin', 'thing', 'whole', 'hous', 'scream', 'brat', 'pull', 'hair', 'love']\n",
      "Tokenized sentence: ['well', 'if', 'i', 'm', 'that', 'desperate', 'i', 'll', 'just', 'call', 'armand', 'again']\n",
      "After stop words removal: ['well', 'desperate', 'call', 'armand']\n",
      "After stemming with porters algorithm: ['well', 'desper', 'call', 'armand']\n",
      "Tokenized sentence: ['daddy', 'will', 'take', 'good', 'care', 'of', 'you']\n",
      "After stop words removal: ['daddy', 'take', 'good', 'care']\n",
      "After stemming with porters algorithm: ['daddi', 'take', 'good', 'care']\n",
      "Tokenized sentence: ['honey', 'boo', 'i', 'm', 'missing', 'u']\n",
      "After stop words removal: ['honey', 'boo', 'missing', 'u']\n",
      "miss\n",
      "After stemming with porters algorithm: ['honei', 'boo', 'miss']\n",
      "Tokenized sentence: ['u', 'r', 'the', 'most', 'beautiful', 'girl', 'ive', 'ever', 'seen', 'u', 'r', 'my', 'baby', 'come', 'and', 'c', 'me', 'in', 'the', 'common', 'room']\n",
      "After stop words removal: ['u', 'r', 'beautiful', 'girl', 'ive', 'ever', 'seen', 'u', 'r', 'baby', 'come', 'c', 'common', 'room']\n",
      "After stemming with porters algorithm: ['beauti', 'girl', 'iv', 'ever', 'seen', 'babi', 'come', 'common', 'room']\n",
      "Tokenized sentence: ['boo', 'how', 's', 'things', 'i', 'm', 'back', 'at', 'home', 'and', 'a', 'little', 'bored', 'already']\n",
      "After stop words removal: ['boo', 'things', 'back', 'home', 'little', 'bored', 'already']\n",
      "After stemming with porters algorithm: ['boo', 'thing', 'back', 'home', 'littl', 'bore', 'alreadi']\n",
      "Tokenized sentence: ['lolnice', 'i', 'went', 'from', 'a', 'fish', 'to', 'water']\n",
      "After stop words removal: ['lolnice', 'went', 'fish', 'water']\n",
      "After stemming with porters algorithm: ['lolnic', 'went', 'fish', 'water']\n",
      "Tokenized sentence: ['you', 'have', 'registered', 'sinco', 'as', 'payee', 'log', 'in', 'at', 'icicibank', 'com', 'and', 'enter', 'urn', 'lt', 'gt', 'to', 'confirm', 'beware', 'of', 'frauds', 'do', 'not', 'share', 'or', 'disclose', 'urn', 'to', 'anyone']\n",
      "After stop words removal: ['registered', 'sinco', 'payee', 'log', 'icicibank', 'com', 'enter', 'urn', 'lt', 'gt', 'confirm', 'beware', 'frauds', 'share', 'disclose', 'urn', 'anyone']\n",
      "After stemming with porters algorithm: ['regist', 'sinco', 'paye', 'log', 'icicibank', 'com', 'enter', 'urn', 'confirm', 'bewar', 'fraud', 'share', 'disclos', 'urn', 'anyon']\n",
      "Tokenized sentence: ['heehee', 'that', 'was', 'so', 'funny', 'tho']\n",
      "After stop words removal: ['heehee', 'funny', 'tho']\n",
      "After stemming with porters algorithm: ['heehe', 'funni', 'tho']\n",
      "Tokenized sentence: ['what', 'do', 'u', 'want', 'when', 'i', 'come', 'back', 'a', 'beautiful', 'necklace', 'as', 'a', 'token', 'of', 'my', 'heart', 'for', 'you', 'thats', 'what', 'i', 'will', 'give', 'but', 'only', 'to', 'my', 'wife', 'of', 'my', 'liking', 'be', 'that', 'and', 'see', 'no', 'one', 'can', 'give', 'you', 'that', 'dont', 'call', 'me', 'i', 'will', 'wait', 'till', 'i', 'come']\n",
      "After stop words removal: ['u', 'want', 'come', 'back', 'beautiful', 'necklace', 'token', 'heart', 'thats', 'give', 'wife', 'liking', 'see', 'one', 'give', 'dont', 'call', 'wait', 'till', 'come']\n",
      "lik\n",
      "After stemming with porters algorithm: ['want', 'come', 'back', 'beauti', 'necklac', 'token', 'heart', 'that', 'give', 'wife', 'like', 'see', 'on', 'give', 'dont', 'call', 'wait', 'till', 'come']\n",
      "Tokenized sentence: ['yup', 'leaving', 'right', 'now', 'be', 'back', 'soon']\n",
      "After stop words removal: ['yup', 'leaving', 'right', 'back', 'soon']\n",
      "leav\n",
      "After stemming with porters algorithm: ['yup', 'leav', 'right', 'back', 'soon']\n",
      "Tokenized sentence: ['jay', 'says', 'he', 'll', 'put', 'in', 'lt', 'gt']\n",
      "After stop words removal: ['jay', 'says', 'put', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['jai', 'sai', 'put']\n",
      "Tokenized sentence: ['y', 'dun', 'cut', 'too', 'short', 'leh', 'u', 'dun', 'like', 'ah', 'she', 'failed', 'she', 's', 'quite', 'sad']\n",
      "After stop words removal: ['dun', 'cut', 'short', 'leh', 'u', 'dun', 'like', 'ah', 'failed', 'quite', 'sad']\n",
      "After stemming with porters algorithm: ['dun', 'cut', 'short', 'leh', 'dun', 'like', 'fail', 'quit', 'sad']\n",
      "Tokenized sentence: ['once', 'free', 'call', 'me', 'sir']\n",
      "After stop words removal: ['free', 'call', 'sir']\n",
      "After stemming with porters algorithm: ['free', 'call', 'sir']\n",
      "Tokenized sentence: ['ugh', 'y', 'can', 't', 'u', 'just', 'apologize', 'admit', 'u', 'were', 'wrong', 'and', 'ask', 'me', 'to', 'take', 'u', 'back']\n",
      "After stop words removal: ['ugh', 'u', 'apologize', 'admit', 'u', 'wrong', 'ask', 'take', 'u', 'back']\n",
      "After stemming with porters algorithm: ['ugh', 'apolog', 'admit', 'wrong', 'ask', 'take', 'back']\n",
      "Tokenized sentence: ['no', 'dude', 'its', 'not', 'fake', 'my', 'frnds', 'got', 'money', 'thts', 'y', 'i', 'm', 'reffering', 'u', 'if', 'u', 'member', 'wit', 'my', 'mail', 'link', 'u', 'vl', 'be', 'credited', 'lt', 'gt', 'rs', 'and', 'il', 'be', 'getiing', 'lt', 'gt', 'rs', 'i', 'can', 'draw', 'my', 'acc', 'wen', 'it', 'is', 'lt', 'gt', 'rs']\n",
      "After stop words removal: ['dude', 'fake', 'frnds', 'got', 'money', 'thts', 'reffering', 'u', 'u', 'member', 'wit', 'mail', 'link', 'u', 'vl', 'credited', 'lt', 'gt', 'rs', 'il', 'getiing', 'lt', 'gt', 'rs', 'draw', 'acc', 'wen', 'lt', 'gt', 'rs']\n",
      "reffer\n",
      "geti\n",
      "After stemming with porters algorithm: ['dude', 'fake', 'frnd', 'got', 'monei', 'tht', 'reffer', 'member', 'wit', 'mail', 'link', 'credit', 'geti', 'draw', 'acc', 'wen']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['i', 'knew', 'it', 'u', 'slept', 'v', 'late', 'yest', 'wake', 'up', 'so', 'late']\n",
      "After stop words removal: ['knew', 'u', 'slept', 'v', 'late', 'yest', 'wake', 'late']\n",
      "After stemming with porters algorithm: ['knew', 'slept', 'late', 'yest', 'wake', 'late']\n",
      "Tokenized sentence: ['i', 've', 'sent', 'my', 'wife', 'your', 'text', 'after', 'we', 'buy', 'them', 'she', 'll', 'tell', 'you', 'what', 'to', 'do', 'so', 'just', 'relax', 'we', 'should', 'go', 'get', 'them', 'this', 'wkend']\n",
      "After stop words removal: ['sent', 'wife', 'text', 'buy', 'tell', 'relax', 'go', 'get', 'wkend']\n",
      "After stemming with porters algorithm: ['sent', 'wife', 'text', 'bui', 'tell', 'relax', 'get', 'wkend']\n",
      "Tokenized sentence: ['oh', 'k', 'i', 'think', 'most', 'of', 'wi', 'and', 'nz', 'players', 'unsold']\n",
      "After stop words removal: ['oh', 'k', 'think', 'wi', 'nz', 'players', 'unsold']\n",
      "After stemming with porters algorithm: ['think', 'player', 'unsold']\n",
      "Tokenized sentence: ['going', 'to', 'take', 'your', 'babe', 'out']\n",
      "After stop words removal: ['going', 'take', 'babe']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'take', 'babe']\n",
      "Tokenized sentence: ['dear', 'voucher', 'holder', 'to', 'claim', 'this', 'weeks', 'offer', 'at', 'you', 'pc', 'please', 'go', 'to', 'http', 'www', 'e', 'tlp', 'co', 'uk', 'reward', 'ts', 'cs', 'apply']\n",
      "After stop words removal: ['dear', 'voucher', 'holder', 'claim', 'weeks', 'offer', 'pc', 'please', 'go', 'http', 'www', 'e', 'tlp', 'co', 'uk', 'reward', 'ts', 'cs', 'apply']\n",
      "After stemming with porters algorithm: ['dear', 'voucher', 'holder', 'claim', 'week', 'offer', 'pleas', 'http', 'www', 'tlp', 'reward', 'appli']\n",
      "Tokenized sentence: ['no', 'it', 's', 'not', 'pride', 'i', 'm', 'almost', 'lt', 'gt', 'years', 'old', 'and', 'shouldn', 't', 'be', 'takin', 'money', 'from', 'my', 'kid', 'you', 're', 'not', 'supposed', 'to', 'have', 'to', 'deal', 'with', 'this', 'stuff', 'this', 'is', 'grownup', 'stuff', 'why', 'i', 'don', 't', 'tell', 'you']\n",
      "After stop words removal: ['pride', 'almost', 'lt', 'gt', 'years', 'old', 'takin', 'money', 'kid', 'supposed', 'deal', 'stuff', 'grownup', 'stuff', 'tell']\n",
      "After stemming with porters algorithm: ['pride', 'almost', 'year', 'old', 'takin', 'monei', 'kid', 'suppos', 'deal', 'stuff', 'grownup', 'stuff', 'tell']\n",
      "Tokenized sentence: ['no', 'i', 'don', 't', 'have', 'cancer', 'moms', 'making', 'a', 'big', 'deal', 'out', 'of', 'a', 'regular', 'checkup', 'aka', 'pap', 'smear']\n",
      "After stop words removal: ['cancer', 'moms', 'making', 'big', 'deal', 'regular', 'checkup', 'aka', 'pap', 'smear']\n",
      "mak\n",
      "After stemming with porters algorithm: ['cancer', 'mom', 'make', 'big', 'deal', 'regular', 'checkup', 'aka', 'pap', 'smear']\n",
      "Tokenized sentence: ['night', 'sweet', 'sleep', 'well', 'i', 've', 'just', 'been', 'to', 'see', 'the', 'exorcism', 'of', 'emily', 'rose', 'and', 'may', 'never', 'sleep', 'again', 'hugs', 'and', 'snogs']\n",
      "After stop words removal: ['night', 'sweet', 'sleep', 'well', 'see', 'exorcism', 'emily', 'rose', 'may', 'never', 'sleep', 'hugs', 'snogs']\n",
      "After stemming with porters algorithm: ['night', 'sweet', 'sleep', 'well', 'see', 'exorc', 'emili', 'rose', 'mai', 'never', 'sleep', 'hug', 'snog']\n",
      "Tokenized sentence: ['latest', 'nokia', 'mobile', 'or', 'ipod', 'mp', 'player', 'proze', 'guaranteed', 'reply', 'with', 'win', 'to', 'now', 'norcorp', 'ltd', 'mtmsgrcvd']\n",
      "After stop words removal: ['latest', 'nokia', 'mobile', 'ipod', 'mp', 'player', 'proze', 'guaranteed', 'reply', 'win', 'norcorp', 'ltd', 'mtmsgrcvd']\n",
      "After stemming with porters algorithm: ['latest', 'nokia', 'mobil', 'ipod', 'player', 'proz', 'guaranteed', 'repli', 'win', 'norcorp', 'ltd', 'mtmsgrcvd']\n",
      "Tokenized sentence: ['it', 's', 'ok', 'at', 'least', 'armand', 's', 'still', 'around']\n",
      "After stop words removal: ['ok', 'least', 'armand', 'still', 'around']\n",
      "After stemming with porters algorithm: ['least', 'armand', 'still', 'around']\n",
      "Tokenized sentence: ['you', 'do', 'your', 'studies', 'alone', 'without', 'anyones', 'help', 'if', 'you', 'cant', 'no', 'need', 'to', 'study']\n",
      "After stop words removal: ['studies', 'alone', 'without', 'anyones', 'help', 'cant', 'need', 'study']\n",
      "After stemming with porters algorithm: ['studi', 'alon', 'without', 'anyon', 'help', 'cant', 'need', 'studi']\n",
      "Tokenized sentence: ['great', 'i', 'was', 'getting', 'worried', 'about', 'you', 'just', 'know', 'that', 'a', 'wonderful', 'and', 'caring', 'person', 'like', 'you', 'will', 'have', 'only', 'the', 'best', 'in', 'life', 'know', 'that', 'u', 'r', 'wonderful', 'and', 'god', 's', 'love', 'is', 'yours']\n",
      "After stop words removal: ['great', 'getting', 'worried', 'know', 'wonderful', 'caring', 'person', 'like', 'best', 'life', 'know', 'u', 'r', 'wonderful', 'god', 'love']\n",
      "gett\n",
      "car\n",
      "After stemming with porters algorithm: ['great', 'get', 'worri', 'know', 'wonder', 'care', 'person', 'like', 'best', 'life', 'know', 'wonder', 'god', 'love']\n",
      "Tokenized sentence: ['can', 'u', 'get', 'phone', 'now', 'i', 'wanna', 'chat', 'set', 'up', 'meet', 'call', 'me', 'now', 'on', 'u', 'can', 'cum', 'here', 'moro', 'luv', 'jane', 'xx', 'calls', 'minmoremobsemspobox', 'po', 'wa']\n",
      "After stop words removal: ['u', 'get', 'phone', 'wanna', 'chat', 'set', 'meet', 'call', 'u', 'cum', 'moro', 'luv', 'jane', 'xx', 'calls', 'minmoremobsemspobox', 'po', 'wa']\n",
      "After stemming with porters algorithm: ['get', 'phone', 'wanna', 'chat', 'set', 'meet', 'call', 'cum', 'moro', 'luv', 'jane', 'call', 'minmoremobsemspobox']\n",
      "Tokenized sentence: ['good', 'night', 'my', 'dear', 'sleepwell', 'amp', 'take', 'care']\n",
      "After stop words removal: ['good', 'night', 'dear', 'sleepwell', 'amp', 'take', 'care']\n",
      "After stemming with porters algorithm: ['good', 'night', 'dear', 'sleepwel', 'amp', 'take', 'care']\n",
      "Tokenized sentence: ['it', 'took', 'mr', 'owl', 'licks']\n",
      "After stop words removal: ['took', 'mr', 'owl', 'licks']\n",
      "After stemming with porters algorithm: ['took', 'owl', 'lick']\n",
      "Tokenized sentence: ['of', 'course', 'i', 'guess', 'god', 's', 'just', 'got', 'me', 'on', 'hold', 'right', 'now']\n",
      "After stop words removal: ['course', 'guess', 'god', 'got', 'hold', 'right']\n",
      "After stemming with porters algorithm: ['cours', 'guess', 'god', 'got', 'hold', 'right']\n",
      "Tokenized sentence: ['love', 'isn', 't', 'a', 'decision', 'it', 's', 'a', 'feeling', 'if', 'we', 'could', 'decide', 'who', 'to', 'love', 'then', 'life', 'would', 'be', 'much', 'simpler', 'but', 'then', 'less', 'magical']\n",
      "After stop words removal: ['love', 'decision', 'feeling', 'could', 'decide', 'love', 'life', 'would', 'much', 'simpler', 'less', 'magical']\n",
      "feel\n",
      "After stemming with porters algorithm: ['love', 'decis', 'feel', 'could', 'decid', 'love', 'life', 'would', 'much', 'simpler', 'less', 'magic']\n",
      "Tokenized sentence: ['rightio', 'it', 'is', 'then', 'well', 'arent', 'we', 'all', 'up', 'bright', 'and', 'early', 'this', 'morning']\n",
      "After stop words removal: ['rightio', 'well', 'arent', 'bright', 'early', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['rightio', 'well', 'arent', 'bright', 'earli', 'mor']\n",
      "Tokenized sentence: ['am', 'okay', 'will', 'soon', 'be', 'over', 'all', 'the', 'best']\n",
      "After stop words removal: ['okay', 'soon', 'best']\n",
      "After stemming with porters algorithm: ['okai', 'soon', 'best']\n",
      "Tokenized sentence: ['yeah', 'there', 's', 'barely', 'enough', 'room', 'for', 'the', 'two', 'of', 'us', 'x', 'has', 'too', 'many', 'fucking', 'shoes', 'sorry', 'man', 'see', 'you', 'later']\n",
      "After stop words removal: ['yeah', 'barely', 'enough', 'room', 'two', 'us', 'x', 'many', 'fucking', 'shoes', 'sorry', 'man', 'see', 'later']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['yeah', 'bare', 'enough', 'room', 'two', 'mani', 'fuc', 'shoe', 'sorri', 'man', 'see', 'later']\n",
      "Tokenized sentence: ['are', 'we', 'doing', 'the', 'norm', 'tomorrow', 'i', 'finish', 'just', 'a', 'cos', 'of', 'st', 'tests', 'need', 'to', 'sort', 'library', 'stuff', 'out', 'at', 'some', 'point', 'tomo', 'got', 'letter', 'from', 'today', 'access', 'til', 'end', 'march', 'so', 'i', 'better', 'get', 'move', 'on']\n",
      "After stop words removal: ['norm', 'tomorrow', 'finish', 'cos', 'st', 'tests', 'need', 'sort', 'library', 'stuff', 'point', 'tomo', 'got', 'letter', 'today', 'access', 'til', 'end', 'march', 'better', 'get', 'move']\n",
      "After stemming with porters algorithm: ['norm', 'tomorrow', 'finish', 'co', 'test', 'need', 'sort', 'librari', 'stuff', 'point', 'tomo', 'got', 'letter', 'todai', 'access', 'til', 'end', 'march', 'better', 'get', 'move']\n",
      "Tokenized sentence: ['well', 'done', 'england', 'get', 'the', 'official', 'poly', 'ringtone', 'or', 'colour', 'flag', 'on', 'yer', 'mobile', 'text', 'tone', 'or', 'flag', 'to', 'now', 'opt', 'out', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stop words removal: ['well', 'done', 'england', 'get', 'official', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'text', 'tone', 'flag', 'opt', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stemming with porters algorithm: ['well', 'done', 'england', 'get', 'offici', 'poli', 'rington', 'colour', 'flag', 'yer', 'mobil', 'text', 'tone', 'flag', 'opt', 'txt', 'eng', 'stop', 'box']\n",
      "Tokenized sentence: ['our', 'dating', 'service', 'has', 'been', 'asked', 'contact', 'u', 'by', 'someone', 'shy', 'call', 'now', 'all', 'will', 'be', 'revealed', 'pobox', 'm', 'uz', 'p']\n",
      "After stop words removal: ['dating', 'service', 'asked', 'contact', 'u', 'someone', 'shy', 'call', 'revealed', 'pobox', 'uz', 'p']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['date', 'servic', 'as', 'contact', 'someon', 'shy', 'call', 'reveal', 'pobox']\n",
      "Tokenized sentence: ['thankyou', 'so', 'much', 'for', 'the', 'call', 'i', 'appreciate', 'your', 'care']\n",
      "After stop words removal: ['thankyou', 'much', 'call', 'appreciate', 'care']\n",
      "After stemming with porters algorithm: ['thankyou', 'much', 'call', 'appreci', 'care']\n",
      "Tokenized sentence: ['oh', 'a', 'half', 'hour', 'is', 'much', 'longer', 'in', 'syria', 'than', 'canada', 'eh', 'wow', 'you', 'must', 'get', 'so', 'much', 'more', 'work', 'done', 'in', 'a', 'day', 'than', 'us', 'with', 'all', 'that', 'extra', 'time', 'grins']\n",
      "After stop words removal: ['oh', 'half', 'hour', 'much', 'longer', 'syria', 'canada', 'eh', 'wow', 'must', 'get', 'much', 'work', 'done', 'day', 'us', 'extra', 'time', 'grins']\n",
      "After stemming with porters algorithm: ['half', 'hour', 'much', 'longer', 'syria', 'canada', 'wow', 'must', 'get', 'much', 'work', 'done', 'dai', 'extra', 'time', 'grin']\n",
      "Tokenized sentence: ['honeybee', 'said', 'i', 'm', 'd', 'sweetest', 'in', 'd', 'world', 'god', 'laughed', 'amp', 'said', 'wait', 'u', 'havnt', 'met', 'd', 'person', 'reading', 'this', 'msg', 'moral', 'even', 'god', 'can', 'crack', 'jokes', 'gm', 'gn', 'ge', 'gn']\n",
      "After stop words removal: ['honeybee', 'said', 'sweetest', 'world', 'god', 'laughed', 'amp', 'said', 'wait', 'u', 'havnt', 'met', 'person', 'reading', 'msg', 'moral', 'even', 'god', 'crack', 'jokes', 'gm', 'gn', 'ge', 'gn']\n",
      "read\n",
      "After stemming with porters algorithm: ['honeybe', 'said', 'sweetest', 'world', 'god', 'laug', 'amp', 'said', 'wait', 'havnt', 'met', 'person', 'read', 'msg', 'moral', 'even', 'god', 'crack', 'joke']\n",
      "Tokenized sentence: ['aight', 'text', 'me', 'when', 'you', 're', 'back', 'at', 'mu', 'and', 'i', 'll', 'swing', 'by', 'need', 'somebody', 'to', 'get', 'the', 'door', 'for', 'me']\n",
      "After stop words removal: ['aight', 'text', 'back', 'mu', 'swing', 'need', 'somebody', 'get', 'door']\n",
      "After stemming with porters algorithm: ['aight', 'text', 'back', 'swing', 'need', 'somebodi', 'get', 'door']\n",
      "Tokenized sentence: ['jay', 's', 'getting', 'really', 'impatient', 'and', 'belligerent']\n",
      "After stop words removal: ['jay', 'getting', 'really', 'impatient', 'belligerent']\n",
      "gett\n",
      "After stemming with porters algorithm: ['jai', 'get', 'realli', 'impati', 'belliger']\n",
      "Tokenized sentence: ['well', 'thats', 'nice', 'too', 'bad', 'i', 'cant', 'eat', 'it']\n",
      "After stop words removal: ['well', 'thats', 'nice', 'bad', 'cant', 'eat']\n",
      "After stemming with porters algorithm: ['well', 'that', 'nice', 'bad', 'cant', 'eat']\n",
      "Tokenized sentence: ['come', 'round', 'it', 's']\n",
      "After stop words removal: ['come', 'round']\n",
      "After stemming with porters algorithm: ['come', 'round']\n",
      "Tokenized sentence: ['cheers', 'for', 'the', 'card', 'is', 'it', 'that', 'time', 'of', 'year', 'already']\n",
      "After stop words removal: ['cheers', 'card', 'time', 'year', 'already']\n",
      "After stemming with porters algorithm: ['cheer', 'card', 'time', 'year', 'alreadi']\n",
      "Tokenized sentence: ['booked', 'ticket', 'for', 'pongal']\n",
      "After stop words removal: ['booked', 'ticket', 'pongal']\n",
      "After stemming with porters algorithm: ['book', 'ticket', 'pongal']\n",
      "Tokenized sentence: ['no', 'sir', 'that', 's', 'why', 'i', 'had', 'an', 'hr', 'trip', 'on', 'the', 'bus', 'last', 'week', 'have', 'another', 'audition', 'next', 'wednesday', 'but', 'i', 'think', 'i', 'might', 'drive', 'this', 'time']\n",
      "After stop words removal: ['sir', 'hr', 'trip', 'bus', 'last', 'week', 'another', 'audition', 'next', 'wednesday', 'think', 'might', 'drive', 'time']\n",
      "After stemming with porters algorithm: ['sir', 'trip', 'bu', 'last', 'week', 'anoth', 'audit', 'next', 'wednesdai', 'think', 'might', 'drive', 'time']\n",
      "Tokenized sentence: ['ugh', 'fuck', 'it', 'i', 'm', 'resubbing', 'to', 'eve']\n",
      "After stop words removal: ['ugh', 'fuck', 'resubbing', 'eve']\n",
      "resubb\n",
      "After stemming with porters algorithm: ['ugh', 'fuck', 'resub', 'ev']\n",
      "Tokenized sentence: ['just', 'looked', 'it', 'up', 'and', 'addie', 'goes', 'back', 'monday', 'sucks', 'to', 'be', 'her']\n",
      "After stop words removal: ['looked', 'addie', 'goes', 'back', 'monday', 'sucks']\n",
      "After stemming with porters algorithm: ['look', 'addi', 'goe', 'back', 'mondai', 'suck']\n",
      "Tokenized sentence: ['just', 'curious', 'because', 'my', 'cuz', 'asked', 'what', 'i', 'was', 'up', 'to']\n",
      "After stop words removal: ['curious', 'cuz', 'asked']\n",
      "After stemming with porters algorithm: ['curiou', 'cuz', 'as']\n",
      "Tokenized sentence: ['remember', 'on', 'that', 'day']\n",
      "After stop words removal: ['remember', 'day']\n",
      "After stemming with porters algorithm: ['rememb', 'dai']\n",
      "Tokenized sentence: ['no', 'did', 'you', 'check', 'i', 'got', 'his', 'detailed', 'message', 'now']\n",
      "After stop words removal: ['check', 'got', 'detailed', 'message']\n",
      "After stemming with porters algorithm: ['check', 'got', 'detail', 'messag']\n",
      "Tokenized sentence: ['wanna', 'do', 'some', 'art', 'd']\n",
      "After stop words removal: ['wanna', 'art']\n",
      "After stemming with porters algorithm: ['wanna', 'art']\n",
      "Tokenized sentence: ['just', 'finished', 'eating', 'got', 'u', 'a', 'plate', 'not', 'leftovers', 'this', 'time']\n",
      "After stop words removal: ['finished', 'eating', 'got', 'u', 'plate', 'leftovers', 'time']\n",
      "eat\n",
      "eate\n",
      "After stemming with porters algorithm: ['finis', 'eat', 'got', 'plate', 'leftov', 'time']\n",
      "Tokenized sentence: ['haha', 'but', 'no', 'money', 'leh', 'later', 'got', 'to', 'go', 'for', 'tuition', 'haha', 'and', 'looking', 'for', 'empty', 'slots', 'for', 'driving', 'lessons']\n",
      "After stop words removal: ['haha', 'money', 'leh', 'later', 'got', 'go', 'tuition', 'haha', 'looking', 'empty', 'slots', 'driving', 'lessons']\n",
      "look\n",
      "driv\n",
      "After stemming with porters algorithm: ['haha', 'monei', 'leh', 'later', 'got', 'tuit', 'haha', 'look', 'empti', 'slot', 'drive', 'lesson']\n",
      "Tokenized sentence: ['captain', 'is', 'in', 'our', 'room']\n",
      "After stop words removal: ['captain', 'room']\n",
      "After stemming with porters algorithm: ['captain', 'room']\n",
      "Tokenized sentence: ['ugh', 'my', 'leg', 'hurts', 'musta', 'overdid', 'it', 'on', 'mon']\n",
      "After stop words removal: ['ugh', 'leg', 'hurts', 'musta', 'overdid', 'mon']\n",
      "After stemming with porters algorithm: ['ugh', 'leg', 'hurt', 'musta', 'overdid', 'mon']\n",
      "Tokenized sentence: ['yes', 'i', 'think', 'so', 'i', 'am', 'in', 'office', 'but', 'my', 'lap', 'is', 'in', 'room', 'i', 'think', 'thats', 'on', 'for', 'the', 'last', 'few', 'days', 'i', 'didnt', 'shut', 'that', 'down']\n",
      "After stop words removal: ['yes', 'think', 'office', 'lap', 'room', 'think', 'thats', 'last', 'days', 'didnt', 'shut']\n",
      "After stemming with porters algorithm: ['ye', 'think', 'offic', 'lap', 'room', 'think', 'that', 'last', 'dai', 'didnt', 'shut']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['nooooooo', 'i', 'm', 'gonna', 'be', 'bored', 'to', 'death', 'all', 'day', 'cable', 'and', 'internet', 'outage']\n",
      "After stop words removal: ['nooooooo', 'gonna', 'bored', 'death', 'day', 'cable', 'internet', 'outage']\n",
      "After stemming with porters algorithm: ['nooooooo', 'gonna', 'bore', 'death', 'dai', 'cabl', 'internet', 'outag']\n",
      "Tokenized sentence: ['ur', 'cash', 'balance', 'is', 'currently', 'pounds', 'to', 'maximize', 'ur', 'cash', 'in', 'now', 'send', 'collect', 'to', 'only', 'p', 'msg', 'cc', 'po', 'box', 'tcr', 'w']\n",
      "After stop words removal: ['ur', 'cash', 'balance', 'currently', 'pounds', 'maximize', 'ur', 'cash', 'send', 'collect', 'p', 'msg', 'cc', 'po', 'box', 'tcr', 'w']\n",
      "After stemming with porters algorithm: ['cash', 'balanc', 'current', 'pound', 'maxim', 'cash', 'send', 'collect', 'msg', 'box', 'tcr']\n",
      "Tokenized sentence: ['erm', 'woodland', 'avenue', 'somewhere', 'do', 'you', 'get', 'the', 'parish', 'magazine', 'his', 'telephone', 'number', 'will', 'be', 'in', 'there']\n",
      "After stop words removal: ['erm', 'woodland', 'avenue', 'somewhere', 'get', 'parish', 'magazine', 'telephone', 'number']\n",
      "After stemming with porters algorithm: ['erm', 'woodland', 'avenu', 'somewher', 'get', 'parish', 'magazin', 'telephon', 'number']\n",
      "Tokenized sentence: ['will', 'do', 'you', 'gonna', 'be', 'at', 'blake', 's', 'all', 'night', 'i', 'might', 'be', 'able', 'to', 'get', 'out', 'of', 'here', 'a', 'little', 'early']\n",
      "After stop words removal: ['gonna', 'blake', 'night', 'might', 'able', 'get', 'little', 'early']\n",
      "After stemming with porters algorithm: ['gonna', 'blake', 'night', 'might', 'abl', 'get', 'littl', 'earli']\n",
      "Tokenized sentence: ['should', 'i', 'head', 'straight', 'there', 'or', 'what']\n",
      "After stop words removal: ['head', 'straight']\n",
      "After stemming with porters algorithm: ['head', 'straight']\n",
      "Tokenized sentence: ['pete', 'is', 'this', 'your', 'phone', 'still', 'its', 'jenny', 'from', 'college', 'and', 'leanne', 'what', 'are', 'you', 'up', 'to', 'now']\n",
      "After stop words removal: ['pete', 'phone', 'still', 'jenny', 'college', 'leanne']\n",
      "After stemming with porters algorithm: ['pete', 'phone', 'still', 'jenni', 'colleg', 'leann']\n",
      "Tokenized sentence: ['k', 'k', 'what', 'are', 'detail', 'you', 'want', 'to', 'transfer', 'acc', 'no', 'enough']\n",
      "After stop words removal: ['k', 'k', 'detail', 'want', 'transfer', 'acc', 'enough']\n",
      "After stemming with porters algorithm: ['detail', 'want', 'transfer', 'acc', 'enough']\n",
      "Tokenized sentence: ['can', 'you', 'plz', 'tell', 'me', 'the', 'ans', 'bslvyl', 'sent', 'via', 'fullonsms', 'com']\n",
      "After stop words removal: ['plz', 'tell', 'ans', 'bslvyl', 'sent', 'via', 'fullonsms', 'com']\n",
      "After stemming with porters algorithm: ['plz', 'tell', 'an', 'bslvyl', 'sent', 'via', 'fullonsm', 'com']\n",
      "Tokenized sentence: ['r', 'u', 'here', 'yet', 'i', 'm', 'wearing', 'blue', 'shirt', 'n', 'black', 'pants']\n",
      "After stop words removal: ['r', 'u', 'yet', 'wearing', 'blue', 'shirt', 'n', 'black', 'pants']\n",
      "wear\n",
      "After stemming with porters algorithm: ['yet', 'wear', 'blue', 'shirt', 'black', 'pant']\n",
      "Tokenized sentence: ['i', 'have', 'had', 'two', 'more', 'letters', 'from', 'i', 'will', 'copy', 'them', 'for', 'you', 'cos', 'one', 'has', 'a', 'message', 'for', 'you', 'speak', 'soon']\n",
      "After stop words removal: ['two', 'letters', 'copy', 'cos', 'one', 'message', 'speak', 'soon']\n",
      "After stemming with porters algorithm: ['two', 'letter', 'copi', 'co', 'on', 'messag', 'speak', 'soon']\n",
      "Tokenized sentence: ['wn', 'u', 'r', 'hurt', 'by', 'd', 'prsn', 'who', 's', 'close', 'u', 'do', 'fight', 'wit', 'dem', 'coz', 'somtimes', 'dis', 'fight', 'saves', 'a', 'relation', 'bt', 'being', 'quiet', 'leaves', 'nothin', 'in', 'a', 'relation', 'gud', 'eveb']\n",
      "After stop words removal: ['wn', 'u', 'r', 'hurt', 'prsn', 'close', 'u', 'fight', 'wit', 'dem', 'coz', 'somtimes', 'dis', 'fight', 'saves', 'relation', 'bt', 'quiet', 'leaves', 'nothin', 'relation', 'gud', 'eveb']\n",
      "After stemming with porters algorithm: ['hurt', 'prsn', 'close', 'fight', 'wit', 'dem', 'coz', 'somtim', 'di', 'fight', 'save', 'relat', 'quiet', 'leav', 'nothin', 'relat', 'gud', 'eveb']\n",
      "Tokenized sentence: ['when', 'people', 'see', 'my', 'msgs', 'they', 'think', 'iam', 'addicted', 'to', 'msging', 'they', 'are', 'wrong', 'bcoz', 'they', 'don', 't', 'know', 'that', 'iam', 'addicted', 'to', 'my', 'sweet', 'friends', 'bslvyl']\n",
      "After stop words removal: ['people', 'see', 'msgs', 'think', 'iam', 'addicted', 'msging', 'wrong', 'bcoz', 'know', 'iam', 'addicted', 'sweet', 'friends', 'bslvyl']\n",
      "After stemming with porters algorithm: ['peopl', 'see', 'msg', 'think', 'iam', 'addic', 'msging', 'wrong', 'bcoz', 'know', 'iam', 'addic', 'sweet', 'friend', 'bslvyl']\n",
      "Tokenized sentence: ['if', 'u', 'dun', 'drive', 'then', 'how', 'i', 'go', 'sch']\n",
      "After stop words removal: ['u', 'dun', 'drive', 'go', 'sch']\n",
      "After stemming with porters algorithm: ['dun', 'drive', 'sch']\n",
      "Tokenized sentence: ['once', 'a', 'fishrman', 'woke', 'early', 'in', 'd', 'mrng', 'it', 'was', 'very', 'dark', 'he', 'waited', 'a', 'while', 'amp', 'found', 'a', 'sack', 'ful', 'of', 'stones', 'he', 'strtd', 'throwin', 'thm', 'in', 'd', 'sea', 'pass', 'time', 'atlast', 'he', 'had', 'jus', 'stone', 'sun', 'rose', 'up', 'amp', 'he', 'found', 'out', 'tht', 'those', 'r', 'nt', 'stones', 'those', 'were', 'diamonds', 'moral', 'dont', 'wake', 'up', 'early', 'in', 'd', 'mrng', 'good', 'night']\n",
      "After stop words removal: ['fishrman', 'woke', 'early', 'mrng', 'dark', 'waited', 'amp', 'found', 'sack', 'ful', 'stones', 'strtd', 'throwin', 'thm', 'sea', 'pass', 'time', 'atlast', 'jus', 'stone', 'sun', 'rose', 'amp', 'found', 'tht', 'r', 'nt', 'stones', 'diamonds', 'moral', 'dont', 'wake', 'early', 'mrng', 'good', 'night']\n",
      "After stemming with porters algorithm: ['fishrman', 'woke', 'earli', 'mrng', 'dark', 'wait', 'amp', 'found', 'sack', 'ful', 'stone', 'strtd', 'throwin', 'thm', 'sea', 'pass', 'time', 'atlast', 'ju', 'stone', 'sun', 'rose', 'amp', 'found', 'tht', 'stone', 'diamond', 'moral', 'dont', 'wake', 'earli', 'mrng', 'good', 'night']\n",
      "Tokenized sentence: ['ha', 'then', 'we', 'must', 'walk', 'to', 'everywhere', 'cannot', 'take', 'tram', 'my', 'cousin', 'said', 'can', 'walk', 'to', 'vic', 'market', 'from', 'our', 'hotel']\n",
      "After stop words removal: ['ha', 'must', 'walk', 'everywhere', 'cannot', 'take', 'tram', 'cousin', 'said', 'walk', 'vic', 'market', 'hotel']\n",
      "After stemming with porters algorithm: ['must', 'walk', 'everywher', 'cannot', 'take', 'tram', 'cousin', 'said', 'walk', 'vic', 'market', 'hotel']\n",
      "Tokenized sentence: ['so', 'its', 'to', 'be', 'poking', 'man', 'everyday', 'that', 'they', 'teach', 'you', 'in', 'canada', 'abi', 'how', 'are', 'you', 'just', 'saying', 'hi']\n",
      "After stop words removal: ['poking', 'man', 'everyday', 'teach', 'canada', 'abi', 'saying', 'hi']\n",
      "pok\n",
      "say\n",
      "After stemming with porters algorithm: ['poke', 'man', 'everydai', 'teach', 'canada', 'abi', 'sai']\n",
      "Tokenized sentence: ['jus', 'finish', 'blowing', 'my', 'hair', 'u', 'finish', 'dinner', 'already']\n",
      "After stop words removal: ['jus', 'finish', 'blowing', 'hair', 'u', 'finish', 'dinner', 'already']\n",
      "blow\n",
      "After stemming with porters algorithm: ['ju', 'finish', 'blowe', 'hair', 'finish', 'dinner', 'alreadi']\n",
      "Tokenized sentence: ['wat', 's', 'da', 'model', 'num', 'of', 'ur', 'phone']\n",
      "After stop words removal: ['wat', 'da', 'model', 'num', 'ur', 'phone']\n",
      "After stemming with porters algorithm: ['wat', 'model', 'num', 'phone']\n",
      "Tokenized sentence: ['yes', 'they', 'replied', 'my', 'mail', 'i', 'm', 'going', 'to', 'the', 'management', 'office', 'later', 'plus', 'will', 'in', 'to', 'bank', 'later', 'also', 'or', 'on', 'wednesday']\n",
      "After stop words removal: ['yes', 'replied', 'mail', 'going', 'management', 'office', 'later', 'plus', 'bank', 'later', 'also', 'wednesday']\n",
      "go\n",
      "After stemming with porters algorithm: ['ye', 'repli', 'mail', 'go', 'manag', 'offic', 'later', 'plu', 'bank', 'later', 'also', 'wednesdai']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for']\n",
      "After stop words removal: ['private', 'account', 'statement']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem']\n",
      "Tokenized sentence: ['actually', 'nvm', 'got', 'hella', 'cash', 'we', 'still', 'on', 'for', 'lt', 'gt', 'ish']\n",
      "After stop words removal: ['actually', 'nvm', 'got', 'hella', 'cash', 'still', 'lt', 'gt', 'ish']\n",
      "After stemming with porters algorithm: ['actual', 'nvm', 'got', 'hella', 'cash', 'still', 'ish']\n",
      "Tokenized sentence: ['hello', 'my', 'love', 'what', 'are', 'you', 'doing', 'did', 'you', 'get', 'to', 'that', 'interview', 'today', 'are', 'you', 'you', 'happy', 'are', 'you', 'being', 'a', 'good', 'boy', 'do', 'you', 'think', 'of', 'me', 'are', 'you', 'missing', 'me']\n",
      "After stop words removal: ['hello', 'love', 'get', 'interview', 'today', 'happy', 'good', 'boy', 'think', 'missing']\n",
      "miss\n",
      "After stemming with porters algorithm: ['hello', 'love', 'get', 'interview', 'todai', 'happi', 'good', 'boi', 'think', 'miss']\n",
      "Tokenized sentence: ['just', 'woke', 'up', 'yeesh', 'its', 'late', 'but', 'i', 'didn', 't', 'fall', 'asleep', 'til', 'lt', 'gt', 'am']\n",
      "After stop words removal: ['woke', 'yeesh', 'late', 'fall', 'asleep', 'til', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['woke', 'yeesh', 'late', 'fall', 'asleep', 'til']\n",
      "Tokenized sentence: ['when', 'did', 'dad', 'get', 'back']\n",
      "After stop words removal: ['dad', 'get', 'back']\n",
      "After stemming with porters algorithm: ['dad', 'get', 'back']\n",
      "Tokenized sentence: ['that', 'sucks', 'i', 'll', 'go', 'over', 'so', 'u', 'can', 'do', 'my', 'hair', 'you', 'll', 'do', 'it', 'free', 'right']\n",
      "After stop words removal: ['sucks', 'go', 'u', 'hair', 'free', 'right']\n",
      "After stemming with porters algorithm: ['suck', 'hair', 'free', 'right']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'solihull', 'do', 'you', 'want', 'anything']\n",
      "After stop words removal: ['solihull', 'want', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['solihul', 'want', 'anyt']\n",
      "Tokenized sentence: ['eerie', 'nokia', 'tones', 'u', 'rply', 'tone', 'title', 'to', 'eg', 'tone', 'dracula', 'to', 'titles', 'ghost', 'addamsfa', 'munsters', 'exorcist', 'twilight', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'p']\n",
      "After stop words removal: ['eerie', 'nokia', 'tones', 'u', 'rply', 'tone', 'title', 'eg', 'tone', 'dracula', 'titles', 'ghost', 'addamsfa', 'munsters', 'exorcist', 'twilight', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'p']\n",
      "After stemming with porters algorithm: ['eeri', 'nokia', 'tone', 'rply', 'tone', 'titl', 'tone', 'dracula', 'titl', 'ghost', 'addamsfa', 'munster', 'exorcist', 'twilight', 'www', 'getz', 'pobox']\n",
      "Tokenized sentence: ['yeah', 'we', 'do', 'totes', 'when', 'u', 'wanna']\n",
      "After stop words removal: ['yeah', 'totes', 'u', 'wanna']\n",
      "After stemming with porters algorithm: ['yeah', 'tote', 'wanna']\n",
      "Tokenized sentence: ['ok', 'lor', 'i', 'ned', 'go', 'toa', 'payoh', 'a', 'while', 'return', 'smth', 'u', 'wan', 'send', 'me', 'there', 'or', 'wat']\n",
      "After stop words removal: ['ok', 'lor', 'ned', 'go', 'toa', 'payoh', 'return', 'smth', 'u', 'wan', 'send', 'wat']\n",
      "After stemming with porters algorithm: ['lor', 'ned', 'toa', 'payoh', 'return', 'smth', 'wan', 'send', 'wat']\n",
      "Tokenized sentence: ['congratulations', 'ur', 'awarded', 'of', 'cd', 'vouchers', 'or', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music', 'to']\n",
      "After stop words removal: ['congratulations', 'ur', 'awarded', 'cd', 'vouchers', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music']\n",
      "After stemming with porters algorithm: ['congratul', 'awar', 'voucher', 'gift', 'guaranteed', 'free', 'entri', 'wkly', 'draw', 'txt', 'music']\n",
      "Tokenized sentence: ['i', 'keep', 'seeing', 'weird', 'shit', 'and', 'bein', 'all', 'woah', 'then', 'realising', 'it', 's', 'actually', 'reasonable', 'and', 'i', 'm', 'all', 'oh']\n",
      "After stop words removal: ['keep', 'seeing', 'weird', 'shit', 'bein', 'woah', 'realising', 'actually', 'reasonable', 'oh']\n",
      "see\n",
      "realis\n",
      "After stemming with porters algorithm: ['keep', 'see', 'weird', 'shit', 'bein', 'woah', 'realis', 'actual', 'reason']\n",
      "Tokenized sentence: ['hey', 'sorry', 'i', 'didntgive', 'ya', 'a', 'a', 'bellearlier', 'hunny']\n",
      "After stop words removal: ['hey', 'sorry', 'didntgive', 'ya', 'bellearlier', 'hunny']\n",
      "After stemming with porters algorithm: ['hei', 'sorri', 'didntgiv', 'bellearli', 'hunni']\n",
      "Tokenized sentence: ['monthly', 'password', 'for', 'wap', 'mobsi', 'com', 'is', 'use', 'your', 'wap', 'phone', 'not', 'pc']\n",
      "After stop words removal: ['monthly', 'password', 'wap', 'mobsi', 'com', 'use', 'wap', 'phone', 'pc']\n",
      "After stemming with porters algorithm: ['monthli', 'password', 'wap', 'mobsi', 'com', 'us', 'wap', 'phone']\n",
      "Tokenized sentence: ['where', 'r', 'e', 'meeting', 'tmr']\n",
      "After stop words removal: ['r', 'e', 'meeting', 'tmr']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'tmr']\n",
      "Tokenized sentence: ['its', 'normally', 'hot', 'mail', 'com', 'you', 'see']\n",
      "After stop words removal: ['normally', 'hot', 'mail', 'com', 'see']\n",
      "After stemming with porters algorithm: ['normal', 'hot', 'mail', 'com', 'see']\n",
      "Tokenized sentence: ['hi', 'its', 'in', 'durban', 'are', 'you', 'still', 'on', 'this', 'number']\n",
      "After stop words removal: ['hi', 'durban', 'still', 'number']\n",
      "After stemming with porters algorithm: ['durban', 'still', 'number']\n",
      "Tokenized sentence: ['then', 'dun', 'wear', 'jeans', 'lor']\n",
      "After stop words removal: ['dun', 'wear', 'jeans', 'lor']\n",
      "After stemming with porters algorithm: ['dun', 'wear', 'jean', 'lor']\n",
      "Tokenized sentence: ['how', 'much', 'you', 'got', 'for', 'cleaning']\n",
      "After stop words removal: ['much', 'got', 'cleaning']\n",
      "clean\n",
      "After stemming with porters algorithm: ['much', 'got', 'clean']\n",
      "Tokenized sentence: ['mm', 'have', 'some', 'kanji', 'dont', 'eat', 'anything', 'heavy', 'ok']\n",
      "After stop words removal: ['mm', 'kanji', 'dont', 'eat', 'anything', 'heavy', 'ok']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['kanji', 'dont', 'eat', 'anyt', 'heavi']\n",
      "Tokenized sentence: ['the', 'word', 'checkmate', 'in', 'chess', 'comes', 'from', 'the', 'persian', 'phrase', 'shah', 'maat', 'which', 'means', 'the', 'king', 'is', 'dead', 'goodmorning', 'have', 'a', 'good', 'day']\n",
      "After stop words removal: ['word', 'checkmate', 'chess', 'comes', 'persian', 'phrase', 'shah', 'maat', 'means', 'king', 'dead', 'goodmorning', 'good', 'day']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['word', 'checkmat', 'chess', 'come', 'persian', 'phrase', 'shah', 'maat', 'mean', 'king', 'dead', 'goodmor', 'good', 'dai']\n",
      "Tokenized sentence: ['i', 'doubt', 'you', 'could', 'handle', 'times', 'per', 'night', 'in', 'any', 'case']\n",
      "After stop words removal: ['doubt', 'could', 'handle', 'times', 'per', 'night', 'case']\n",
      "After stemming with porters algorithm: ['doubt', 'could', 'handl', 'time', 'per', 'night', 'case']\n",
      "Tokenized sentence: ['as', 'usual', 'iam', 'fine', 'happy', 'amp', 'doing', 'well']\n",
      "After stop words removal: ['usual', 'iam', 'fine', 'happy', 'amp', 'well']\n",
      "After stemming with porters algorithm: ['usual', 'iam', 'fine', 'happi', 'amp', 'well']\n",
      "Tokenized sentence: ['so', 'u', 'gonna', 'get', 'deus', 'ex']\n",
      "After stop words removal: ['u', 'gonna', 'get', 'deus', 'ex']\n",
      "After stemming with porters algorithm: ['gonna', 'get', 'deu']\n",
      "Tokenized sentence: ['idk', 'you', 'keep', 'saying', 'that', 'you', 're', 'not', 'but', 'since', 'he', 'moved', 'we', 'keep', 'butting', 'heads', 'over', 'freedom', 'vs', 'responsibility', 'and', 'i', 'm', 'tired', 'i', 'have', 'so', 'much', 'other', 'shit', 'to', 'deal', 'with', 'that', 'i', 'm', 'barely', 'keeping', 'myself', 'together', 'once', 'this', 'gets', 'added', 'to', 'it']\n",
      "After stop words removal: ['idk', 'keep', 'saying', 'since', 'moved', 'keep', 'butting', 'heads', 'freedom', 'vs', 'responsibility', 'tired', 'much', 'shit', 'deal', 'barely', 'keeping', 'together', 'gets', 'added']\n",
      "say\n",
      "butt\n",
      "keep\n",
      "After stemming with porters algorithm: ['idk', 'keep', 'sai', 'sinc', 'move', 'keep', 'but', 'head', 'freedom', 'respons', 'tire', 'much', 'shit', 'deal', 'bare', 'keep', 'togeth', 'get', 'ad']\n",
      "Tokenized sentence: ['dont', 'give', 'a', 'monkeys', 'wot', 'they', 'think', 'and', 'i', 'certainly', 'don', 't', 'mind', 'any', 'friend', 'of', 'mine', 'all', 'that', 'just', 'don', 't', 'sleep', 'wiv', 'that', 'wud', 'be', 'annoyin']\n",
      "After stop words removal: ['dont', 'give', 'monkeys', 'wot', 'think', 'certainly', 'mind', 'friend', 'mine', 'sleep', 'wiv', 'wud', 'annoyin']\n",
      "After stemming with porters algorithm: ['dont', 'give', 'monkei', 'wot', 'think', 'certainli', 'mind', 'friend', 'mine', 'sleep', 'wiv', 'wud', 'annoyin']\n",
      "Tokenized sentence: ['o', 'guess', 'they', 'both', 'got', 'screwd']\n",
      "After stop words removal: ['guess', 'got', 'screwd']\n",
      "After stemming with porters algorithm: ['guess', 'got', 'screwd']\n",
      "Tokenized sentence: ['ur', 'awarded', 'a', 'city', 'break', 'and', 'could', 'win', 'a', 'summer', 'shopping', 'spree', 'every', 'wk', 'txt', 'store', 'to', 'skilgme', 'tscs', 'winawk', 'age', 'perwksub']\n",
      "After stop words removal: ['ur', 'awarded', 'city', 'break', 'could', 'win', 'summer', 'shopping', 'spree', 'every', 'wk', 'txt', 'store', 'skilgme', 'tscs', 'winawk', 'age', 'perwksub']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['awar', 'citi', 'break', 'could', 'win', 'summer', 'shop', 'spree', 'everi', 'txt', 'store', 'skilgm', 'tsc', 'winawk', 'ag', 'perwksub']\n",
      "Tokenized sentence: ['i', 'know', 'that', 'my', 'friend', 'already', 'told', 'that']\n",
      "After stop words removal: ['know', 'friend', 'already', 'told']\n",
      "After stemming with porters algorithm: ['know', 'friend', 'alreadi', 'told']\n",
      "Tokenized sentence: ['yavnt', 'tried', 'yet', 'and', 'never', 'played', 'original', 'either']\n",
      "After stop words removal: ['yavnt', 'tried', 'yet', 'never', 'played', 'original', 'either']\n",
      "After stemming with porters algorithm: ['yavnt', 'tri', 'yet', 'never', 'plai', 'origin', 'either']\n",
      "Tokenized sentence: ['i', 'll', 'pick', 'you', 'up', 'at', 'about', 'pm', 'to', 'go', 'to', 'taunton', 'if', 'you', 'still', 'want', 'to', 'come']\n",
      "After stop words removal: ['pick', 'pm', 'go', 'taunton', 'still', 'want', 'come']\n",
      "After stemming with porters algorithm: ['pick', 'taunton', 'still', 'want', 'come']\n",
      "Tokenized sentence: ['freemsg', 'hey', 'u', 'i', 'just', 'got', 'of', 'these', 'video', 'pic', 'fones', 'reply', 'wild', 'to', 'this', 'txt', 'ill', 'send', 'u', 'my', 'pics', 'hurry', 'up', 'im', 'so', 'bored', 'at', 'work', 'xxx', 'p', 'rcvd', 'stop', 'stop']\n",
      "After stop words removal: ['freemsg', 'hey', 'u', 'got', 'video', 'pic', 'fones', 'reply', 'wild', 'txt', 'ill', 'send', 'u', 'pics', 'hurry', 'im', 'bored', 'work', 'xxx', 'p', 'rcvd', 'stop', 'stop']\n",
      "After stemming with porters algorithm: ['freemsg', 'hei', 'got', 'video', 'pic', 'fone', 'repli', 'wild', 'txt', 'ill', 'send', 'pic', 'hurri', 'bore', 'work', 'xxx', 'rcvd', 'stop', 'stop']\n",
      "Tokenized sentence: ['just', 'buy', 'a', 'pizza', 'meat', 'lovers', 'or', 'supreme', 'u', 'get', 'to', 'pick']\n",
      "After stop words removal: ['buy', 'pizza', 'meat', 'lovers', 'supreme', 'u', 'get', 'pick']\n",
      "After stemming with porters algorithm: ['bui', 'pizza', 'meat', 'lover', 'suprem', 'get', 'pick']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'class', 'will', 'holla', 'later']\n",
      "After stop words removal: ['class', 'holla', 'later']\n",
      "After stemming with porters algorithm: ['class', 'holla', 'later']\n",
      "Tokenized sentence: ['had', 'your', 'mobile', 'mths', 'update', 'to', 'the', 'latest', 'camera', 'video', 'phones', 'for', 'free', 'keep', 'ur', 'same', 'number', 'get', 'extra', 'free', 'mins', 'texts', 'text', 'yes', 'for', 'a', 'call']\n",
      "After stop words removal: ['mobile', 'mths', 'update', 'latest', 'camera', 'video', 'phones', 'free', 'keep', 'ur', 'number', 'get', 'extra', 'free', 'mins', 'texts', 'text', 'yes', 'call']\n",
      "After stemming with porters algorithm: ['mobil', 'mth', 'updat', 'latest', 'camera', 'video', 'phone', 'free', 'keep', 'number', 'get', 'extra', 'free', 'min', 'text', 'text', 'ye', 'call']\n",
      "Tokenized sentence: ['happy', 'new', 'year', 'my', 'dear', 'brother', 'i', 'really', 'do', 'miss', 'you', 'just', 'got', 'your', 'number', 'and', 'decided', 'to', 'send', 'you', 'this', 'text', 'wishing', 'you', 'only', 'happiness', 'abiola']\n",
      "After stop words removal: ['happy', 'new', 'year', 'dear', 'brother', 'really', 'miss', 'got', 'number', 'decided', 'send', 'text', 'wishing', 'happiness', 'abiola']\n",
      "wish\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year', 'dear', 'brother', 'realli', 'miss', 'got', 'number', 'decid', 'send', 'text', 'wis', 'happi', 'abiola']\n",
      "Tokenized sentence: ['wat', 'r', 'u', 'doing', 'now']\n",
      "After stop words removal: ['wat', 'r', 'u']\n",
      "After stemming with porters algorithm: ['wat']\n",
      "Tokenized sentence: ['congrats', 'nokia', 'video', 'camera', 'phone', 'is', 'your', 'call', 'calls', 'cost', 'ppm', 'ave', 'call', 'mins', 'vary', 'from', 'mobiles', 'close', 'post', 'bcm', 'ldn', 'wc', 'n', 'xx']\n",
      "After stop words removal: ['congrats', 'nokia', 'video', 'camera', 'phone', 'call', 'calls', 'cost', 'ppm', 'ave', 'call', 'mins', 'vary', 'mobiles', 'close', 'post', 'bcm', 'ldn', 'wc', 'n', 'xx']\n",
      "After stemming with porters algorithm: ['congrat', 'nokia', 'video', 'camera', 'phone', 'call', 'call', 'cost', 'ppm', 'av', 'call', 'min', 'vari', 'mobil', 'close', 'post', 'bcm', 'ldn']\n",
      "Tokenized sentence: ['huh', 'i', 'cant', 'thk', 'of', 'more', 'oredi', 'how', 'many', 'pages', 'do', 'we', 'have']\n",
      "After stop words removal: ['huh', 'cant', 'thk', 'oredi', 'many', 'pages']\n",
      "After stemming with porters algorithm: ['huh', 'cant', 'thk', 'oredi', 'mani', 'page']\n",
      "Tokenized sentence: ['free', 'top', 'polyphonic', 'tones', 'call', 'national', 'rate', 'get', 'a', 'toppoly', 'tune', 'sent', 'every', 'week', 'just', 'text', 'subpoly', 'to', 'per', 'pole', 'unsub']\n",
      "After stop words removal: ['free', 'top', 'polyphonic', 'tones', 'call', 'national', 'rate', 'get', 'toppoly', 'tune', 'sent', 'every', 'week', 'text', 'subpoly', 'per', 'pole', 'unsub']\n",
      "After stemming with porters algorithm: ['free', 'top', 'polyphon', 'tone', 'call', 'nat', 'rate', 'get', 'toppoli', 'tune', 'sent', 'everi', 'week', 'text', 'subpoli', 'per', 'pole', 'unsub']\n",
      "Tokenized sentence: ['yeah', 'so', 'basically', 'any', 'time', 'next', 'week', 'you', 'can', 'get', 'away', 'from', 'your', 'mom', 'amp', 'get', 'up', 'before']\n",
      "After stop words removal: ['yeah', 'basically', 'time', 'next', 'week', 'get', 'away', 'mom', 'amp', 'get']\n",
      "After stemming with porters algorithm: ['yeah', 'basic', 'time', 'next', 'week', 'get', 'awai', 'mom', 'amp', 'get']\n",
      "Tokenized sentence: ['haha', 'better', 'late', 'than', 'ever', 'any', 'way', 'i', 'could', 'swing', 'by']\n",
      "After stop words removal: ['haha', 'better', 'late', 'ever', 'way', 'could', 'swing']\n",
      "After stemming with porters algorithm: ['haha', 'better', 'late', 'ever', 'wai', 'could', 'swing']\n",
      "Tokenized sentence: ['oh', 'ok', 'no', 'prob']\n",
      "After stop words removal: ['oh', 'ok', 'prob']\n",
      "After stemming with porters algorithm: ['prob']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'escape', 'theatre', 'now', 'going', 'to', 'watch', 'kavalan', 'in', 'a', 'few', 'minutes']\n",
      "After stop words removal: ['escape', 'theatre', 'going', 'watch', 'kavalan', 'minutes']\n",
      "go\n",
      "After stemming with porters algorithm: ['escap', 'theatr', 'go', 'watch', 'kavalan', 'minut']\n",
      "Tokenized sentence: ['thts', 'wat', 'wright', 'brother', 'did', 'to', 'fly']\n",
      "After stop words removal: ['thts', 'wat', 'wright', 'brother', 'fly']\n",
      "After stemming with porters algorithm: ['tht', 'wat', 'wright', 'brother', 'fly']\n",
      "Tokenized sentence: ['december', 'only', 'had', 'your', 'mobile', 'mths', 'you', 'are', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'camera', 'mobile', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']\n",
      "After stop words removal: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
      "After stemming with porters algorithm: ['decemb', 'mobil', 'mth', 'entit', 'updat', 'latest', 'colour', 'camera', 'mobil', 'free', 'call', 'mobil', 'updat', 'free']\n",
      "Tokenized sentence: ['hottest', 'pics', 'straight', 'to', 'your', 'phone', 'see', 'me', 'getting', 'wet', 'and', 'wanting', 'just', 'for', 'you', 'xx', 'text', 'pics', 'to', 'now', 'txt', 'costs', 'p', 'textoperator', 'g', 'ga', 'xxx']\n",
      "After stop words removal: ['hottest', 'pics', 'straight', 'phone', 'see', 'getting', 'wet', 'wanting', 'xx', 'text', 'pics', 'txt', 'costs', 'p', 'textoperator', 'g', 'ga', 'xxx']\n",
      "gett\n",
      "want\n",
      "After stemming with porters algorithm: ['hottest', 'pic', 'straight', 'phone', 'see', 'get', 'wet', 'wan', 'text', 'pic', 'txt', 'cost', 'textoper', 'xxx']\n",
      "Tokenized sentence: ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'am', 'pm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
      "After stop words removal: ['please', 'call', 'customer', 'service', 'representative', 'pm', 'guaranteed', 'cash', 'prize']\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'custom', 'servic', 'repres', 'guaranteed', 'cash', 'priz']\n",
      "Tokenized sentence: ['hanging', 'out', 'with', 'my', 'brother', 'and', 'his', 'family']\n",
      "After stop words removal: ['hanging', 'brother', 'family']\n",
      "hang\n",
      "After stemming with porters algorithm: ['han', 'brother', 'famili']\n",
      "Tokenized sentence: ['depends', 'on', 'where', 'u', 'going', 'lor']\n",
      "After stop words removal: ['depends', 'u', 'going', 'lor']\n",
      "go\n",
      "After stemming with porters algorithm: ['depend', 'go', 'lor']\n",
      "Tokenized sentence: ['hi', 'the', 'way', 'i', 'was', 'with', 'u', 'day', 'is', 'the', 'normal', 'way', 'this', 'is', 'the', 'real', 'me', 'ur', 'unique', 'i', 'hope', 'i', 'know', 'u', 'the', 'rest', 'of', 'mylife', 'hope', 'u', 'find', 'wot', 'was', 'lost']\n",
      "After stop words removal: ['hi', 'way', 'u', 'day', 'normal', 'way', 'real', 'ur', 'unique', 'hope', 'know', 'u', 'rest', 'mylife', 'hope', 'u', 'find', 'wot', 'lost']\n",
      "After stemming with porters algorithm: ['wai', 'dai', 'normal', 'wai', 'real', 'uniqu', 'hope', 'know', 'rest', 'mylife', 'hope', 'find', 'wot', 'lost']\n",
      "Tokenized sentence: ['by', 'the', 'way', 'rencontre', 'is', 'to', 'meet', 'again', 'mountains', 'dont']\n",
      "After stop words removal: ['way', 'rencontre', 'meet', 'mountains', 'dont']\n",
      "After stemming with porters algorithm: ['wai', 'rencontr', 'meet', 'mountain', 'dont']\n",
      "Tokenized sentence: ['goldviking', 'm', 'is', 'inviting', 'you', 'to', 'be', 'his', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'him', 'www', 'sms', 'ac', 'u', 'goldviking', 'stop', 'send', 'stop', 'frnd', 'to']\n",
      "After stop words removal: ['goldviking', 'inviting', 'friend', 'reply', 'yes', 'see', 'www', 'sms', 'ac', 'u', 'goldviking', 'stop', 'send', 'stop', 'frnd']\n",
      "goldvik\n",
      "invit\n",
      "goldvik\n",
      "After stemming with porters algorithm: ['goldvik', 'invit', 'friend', 'repli', 'ye', 'see', 'www', 'sm', 'goldvik', 'stop', 'send', 'stop', 'frnd']\n",
      "Tokenized sentence: ['apart', 'from', 'the', 'one', 'i', 'told', 'you', 'about', 'yesterday']\n",
      "After stop words removal: ['apart', 'one', 'told', 'yesterday']\n",
      "After stemming with porters algorithm: ['apart', 'on', 'told', 'yesterdai']\n",
      "Tokenized sentence: ['so', 'are', 'you', 'guys', 'asking', 'that', 'i', 'get', 'that', 'slippers', 'again', 'or', 'its', 'gone', 'with', 'last', 'year']\n",
      "After stop words removal: ['guys', 'asking', 'get', 'slippers', 'gone', 'last', 'year']\n",
      "ask\n",
      "After stemming with porters algorithm: ['gui', 'as', 'get', 'slipper', 'gone', 'last', 'year']\n",
      "Tokenized sentence: ['gain', 'the', 'rights', 'of', 'a', 'wife', 'dont', 'demand', 'it', 'i', 'am', 'trying', 'as', 'husband', 'too', 'lets', 'see']\n",
      "After stop words removal: ['gain', 'rights', 'wife', 'dont', 'demand', 'trying', 'husband', 'lets', 'see']\n",
      "After stemming with porters algorithm: ['gain', 'right', 'wife', 'dont', 'demand', 'trying', 'husband', 'let', 'see']\n",
      "Tokenized sentence: ['where', 'is', 'it', 'is', 'there', 'any', 'opening', 'for', 'mca']\n",
      "After stop words removal: ['opening', 'mca']\n",
      "open\n",
      "After stemming with porters algorithm: ['open', 'mca']\n",
      "Tokenized sentence: ['then', 'ask', 'dad', 'to', 'pick', 'up', 'lar', 'wan', 'stay', 'until', 'meh']\n",
      "After stop words removal: ['ask', 'dad', 'pick', 'lar', 'wan', 'stay', 'meh']\n",
      "After stemming with porters algorithm: ['ask', 'dad', 'pick', 'lar', 'wan', 'stai', 'meh']\n",
      "Tokenized sentence: ['free', 'st', 'week', 'entry', 'textpod', 'a', 'chance', 'win', 'gb', 'ipod', 'or', 'cash', 'every', 'wk', 'txt', 'pod', 'to', 'ts', 'cs', 'www', 'textpod', 'net', 'custcare']\n",
      "After stop words removal: ['free', 'st', 'week', 'entry', 'textpod', 'chance', 'win', 'gb', 'ipod', 'cash', 'every', 'wk', 'txt', 'pod', 'ts', 'cs', 'www', 'textpod', 'net', 'custcare']\n",
      "After stemming with porters algorithm: ['free', 'week', 'entri', 'textpod', 'chanc', 'win', 'ipod', 'cash', 'everi', 'txt', 'pod', 'www', 'textpod', 'net', 'custcar']\n",
      "Tokenized sentence: ['love', 'you', 'aathi', 'love', 'u', 'lot']\n",
      "After stop words removal: ['love', 'aathi', 'love', 'u', 'lot']\n",
      "After stemming with porters algorithm: ['love', 'aathi', 'love', 'lot']\n",
      "Tokenized sentence: ['your', 'unique', 'user', 'id', 'is', 'for', 'removal', 'send', 'stop', 'to', 'customer', 'services']\n",
      "After stop words removal: ['unique', 'user', 'id', 'removal', 'send', 'stop', 'customer', 'services']\n",
      "After stemming with porters algorithm: ['uniqu', 'user', 'remov', 'send', 'stop', 'custom', 'servic']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'send', 'something', 'that', 'can', 'sell', 'fast', 'lt', 'gt', 'k', 'is', 'not', 'easy', 'money']\n",
      "After stop words removal: ['want', 'send', 'something', 'sell', 'fast', 'lt', 'gt', 'k', 'easy', 'money']\n",
      "someth\n",
      "After stemming with porters algorithm: ['want', 'send', 'somet', 'sell', 'fast', 'easi', 'monei']\n",
      "Tokenized sentence: ['urgent', 'important', 'information', 'for', 'user', 'today', 'is', 'your', 'lucky', 'day', 'find', 'out', 'why', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'there', 'is', 'a', 'fantastic', 'surprise', 'awaiting', 'you']\n",
      "After stop words removal: ['urgent', 'important', 'information', 'user', 'today', 'lucky', 'day', 'find', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'fantastic', 'surprise', 'awaiting']\n",
      "await\n",
      "After stemming with porters algorithm: ['urgent', 'import', 'inform', 'user', 'todai', 'lucki', 'dai', 'find', 'log', 'onto', 'http', 'www', 'urawinn', 'com', 'fantast', 'surpris', 'await']\n",
      "Tokenized sentence: ['great', 'to', 'hear', 'you', 'are', 'settling', 'well', 'so', 'what', 's', 'happenin', 'wit', 'ola']\n",
      "After stop words removal: ['great', 'hear', 'settling', 'well', 'happenin', 'wit', 'ola']\n",
      "settl\n",
      "After stemming with porters algorithm: ['great', 'hear', 'settl', 'well', 'happenin', 'wit', 'ola']\n",
      "Tokenized sentence: ['i', 'don', 't', 'have', 'anybody', 's', 'number', 'i', 'still', 'haven', 't', 'thought', 'up', 'a', 'tactful', 'way', 'to', 'ask', 'alex']\n",
      "After stop words removal: ['anybody', 'number', 'still', 'thought', 'tactful', 'way', 'ask', 'alex']\n",
      "After stemming with porters algorithm: ['anybodi', 'number', 'still', 'thought', 'tact', 'wai', 'ask', 'alex']\n",
      "Tokenized sentence: ['dont', 'hesitate', 'you', 'know', 'this', 'is', 'the', 'second', 'time', 'she', 'has', 'had', 'weakness', 'like', 'that', 'so', 'keep', 'i', 'notebook', 'of', 'what', 'she', 'eat', 'and', 'did', 'the', 'day', 'before', 'or', 'if', 'anything', 'changed', 'the', 'day', 'before', 'so', 'that', 'we', 'can', 'be', 'sure', 'its', 'nothing']\n",
      "After stop words removal: ['dont', 'hesitate', 'know', 'second', 'time', 'weakness', 'like', 'keep', 'notebook', 'eat', 'day', 'anything', 'changed', 'day', 'sure', 'nothing']\n",
      "anyth\n",
      "noth\n",
      "After stemming with porters algorithm: ['dont', 'hesit', 'know', 'second', 'time', 'weak', 'like', 'keep', 'notebook', 'eat', 'dai', 'anyt', 'chan', 'dai', 'sure', 'not']\n",
      "Tokenized sentence: ['thanks', 'da', 'thangam', 'i', 'feel', 'very', 'very', 'happy', 'dear', 'i', 'also', 'miss', 'you', 'da']\n",
      "After stop words removal: ['thanks', 'da', 'thangam', 'feel', 'happy', 'dear', 'also', 'miss', 'da']\n",
      "After stemming with porters algorithm: ['thank', 'thangam', 'feel', 'happi', 'dear', 'also', 'miss']\n",
      "Tokenized sentence: ['yup', 'ok']\n",
      "After stop words removal: ['yup', 'ok']\n",
      "After stemming with porters algorithm: ['yup']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'awar', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['ok', 'anyway', 'no', 'need', 'to', 'change', 'with', 'what', 'you', 'said']\n",
      "After stop words removal: ['ok', 'anyway', 'need', 'change', 'said']\n",
      "After stemming with porters algorithm: ['anywai', 'need', 'chang', 'said']\n",
      "Tokenized sentence: ['shuhui', 'has', 'bought', 'ron', 's', 'present', 'it', 's', 'a', 'swatch', 'watch']\n",
      "After stop words removal: ['shuhui', 'bought', 'ron', 'present', 'swatch', 'watch']\n",
      "After stemming with porters algorithm: ['shuhui', 'bought', 'ron', 'present', 'swatch', 'watch']\n",
      "Tokenized sentence: ['misplaced', 'your', 'number', 'and', 'was', 'sending', 'texts', 'to', 'your', 'old', 'number', 'wondering', 'why', 'i', 've', 'not', 'heard', 'from', 'you', 'this', 'year', 'all', 'the', 'best', 'in', 'your', 'mcat', 'got', 'this', 'number', 'from', 'my', 'atlanta', 'friends']\n",
      "After stop words removal: ['misplaced', 'number', 'sending', 'texts', 'old', 'number', 'wondering', 'heard', 'year', 'best', 'mcat', 'got', 'number', 'atlanta', 'friends']\n",
      "send\n",
      "wonder\n",
      "After stemming with porters algorithm: ['misplac', 'number', 'sen', 'text', 'old', 'number', 'wonder', 'heard', 'year', 'best', 'mcat', 'got', 'number', 'atlanta', 'friend']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['hello', 'good', 'week', 'fancy', 'a', 'drink', 'or', 'something', 'later']\n",
      "After stop words removal: ['hello', 'good', 'week', 'fancy', 'drink', 'something', 'later']\n",
      "someth\n",
      "After stemming with porters algorithm: ['hello', 'good', 'week', 'fanci', 'drink', 'somet', 'later']\n",
      "Tokenized sentence: ['how', 'long', 'before', 'you', 'get', 'reply', 'just', 'defer', 'admission', 'til', 'next', 'semester']\n",
      "After stop words removal: ['long', 'get', 'reply', 'defer', 'admission', 'til', 'next', 'semester']\n",
      "After stemming with porters algorithm: ['long', 'get', 'repli', 'defer', 'admiss', 'til', 'next', 'semest']\n",
      "Tokenized sentence: ['i', 'av', 'a', 'new', 'number', 'wil', 'u', 'only', 'use', 'this', 'one', 'ta']\n",
      "After stop words removal: ['av', 'new', 'number', 'wil', 'u', 'use', 'one', 'ta']\n",
      "After stemming with porters algorithm: ['new', 'number', 'wil', 'us', 'on']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['whats', 'that', 'coming', 'over', 'the', 'hill', 'is', 'it', 'a', 'monster', 'hope', 'you', 'have', 'a', 'great', 'day', 'things', 'r', 'going', 'fine', 'here', 'busy', 'though']\n",
      "After stop words removal: ['whats', 'coming', 'hill', 'monster', 'hope', 'great', 'day', 'things', 'r', 'going', 'fine', 'busy', 'though']\n",
      "com\n",
      "go\n",
      "After stemming with porters algorithm: ['what', 'come', 'hill', 'monster', 'hope', 'great', 'dai', 'thing', 'go', 'fine', 'busi', 'though']\n",
      "Tokenized sentence: ['yes', 'he', 'is', 'really', 'great', 'bhaji', 'told', 'kallis', 'best', 'cricketer', 'after', 'sachin', 'in', 'world', 'very', 'tough', 'to', 'get', 'out']\n",
      "After stop words removal: ['yes', 'really', 'great', 'bhaji', 'told', 'kallis', 'best', 'cricketer', 'sachin', 'world', 'tough', 'get']\n",
      "After stemming with porters algorithm: ['ye', 'realli', 'great', 'bhaji', 'told', 'kalli', 'best', 'cricket', 'sachin', 'world', 'tough', 'get']\n",
      "Tokenized sentence: ['i', 'am', 'taking', 'you', 'for', 'italian', 'food', 'how', 'about', 'a', 'pretty', 'dress', 'with', 'no', 'panties']\n",
      "After stop words removal: ['taking', 'italian', 'food', 'pretty', 'dress', 'panties']\n",
      "tak\n",
      "After stemming with porters algorithm: ['take', 'italian', 'food', 'pretti', 'dress', 'panti']\n",
      "Tokenized sentence: ['but', 'i', 'dint', 'slept', 'in', 'afternoon']\n",
      "After stop words removal: ['dint', 'slept', 'afternoon']\n",
      "After stemming with porters algorithm: ['dint', 'slept', 'afternoon']\n",
      "Tokenized sentence: ['yeh', 'i', 'am', 'def', 'up', 'something', 'sat']\n",
      "After stop words removal: ['yeh', 'def', 'something', 'sat']\n",
      "someth\n",
      "After stemming with porters algorithm: ['yeh', 'def', 'somet', 'sat']\n",
      "Tokenized sentence: ['alright', 'we', 're', 'all', 'set', 'here', 'text', 'the', 'man']\n",
      "After stop words removal: ['alright', 'set', 'text', 'man']\n",
      "After stemming with porters algorithm: ['alright', 'set', 'text', 'man']\n",
      "Tokenized sentence: ['nope', 'since', 'ayo', 'travelled', 'he', 'has', 'forgotten', 'his', 'guy']\n",
      "After stop words removal: ['nope', 'since', 'ayo', 'travelled', 'forgotten', 'guy']\n",
      "After stemming with porters algorithm: ['nope', 'sinc', 'ayo', 'travel', 'forgotten', 'gui']\n",
      "Tokenized sentence: ['did', 'u', 'got', 'that', 'persons', 'story']\n",
      "After stop words removal: ['u', 'got', 'persons', 'story']\n",
      "After stemming with porters algorithm: ['got', 'person', 'stori']\n",
      "Tokenized sentence: ['not', 'to', 'worry', 'i', 'm', 'sure', 'you', 'll', 'get', 'it']\n",
      "After stop words removal: ['worry', 'sure', 'get']\n",
      "After stemming with porters algorithm: ['worri', 'sure', 'get']\n",
      "Tokenized sentence: ['then', 'mum', 's', 'repent', 'how']\n",
      "After stop words removal: ['mum', 'repent']\n",
      "After stemming with porters algorithm: ['mum', 'repent']\n",
      "Tokenized sentence: ['came', 'to', 'look', 'at', 'the', 'flat', 'seems', 'ok', 'in', 'his', 's', 'is', 'away', 'alot', 'wiv', 'work', 'got', 'woman', 'coming', 'at', 'too']\n",
      "After stop words removal: ['came', 'look', 'flat', 'seems', 'ok', 'away', 'alot', 'wiv', 'work', 'got', 'woman', 'coming']\n",
      "com\n",
      "After stemming with porters algorithm: ['came', 'look', 'flat', 'seem', 'awai', 'alot', 'wiv', 'work', 'got', 'woman', 'come']\n",
      "Tokenized sentence: ['when', 'are', 'you', 'guys', 'leaving']\n",
      "After stop words removal: ['guys', 'leaving']\n",
      "leav\n",
      "After stemming with porters algorithm: ['gui', 'leav']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['hi', 'dude', 'hw', 'r', 'u', 'da', 'realy', 'mising', 'u', 'today']\n",
      "After stop words removal: ['hi', 'dude', 'hw', 'r', 'u', 'da', 'realy', 'mising', 'u', 'today']\n",
      "mis\n",
      "After stemming with porters algorithm: ['dude', 'reali', 'mise', 'todai']\n",
      "Tokenized sentence: ['actually', 'my', 'mobile', 'is', 'full', 'of', 'msg', 'and', 'i', 'm', 'doing', 'a', 'work', 'online', 'where', 'i', 'need', 'to', 'send', 'them', 'lt', 'gt', 'sent', 'msg', 'i', 'wil', 'explain', 'u', 'later']\n",
      "After stop words removal: ['actually', 'mobile', 'full', 'msg', 'work', 'online', 'need', 'send', 'lt', 'gt', 'sent', 'msg', 'wil', 'explain', 'u', 'later']\n",
      "After stemming with porters algorithm: ['actual', 'mobil', 'full', 'msg', 'work', 'onlin', 'need', 'send', 'sent', 'msg', 'wil', 'explain', 'later']\n",
      "Tokenized sentence: ['ugh', 'just', 'got', 'outta', 'class']\n",
      "After stop words removal: ['ugh', 'got', 'outta', 'class']\n",
      "After stemming with porters algorithm: ['ugh', 'got', 'outta', 'class']\n",
      "Tokenized sentence: ['yes', 'i', 'thought', 'so', 'thanks']\n",
      "After stop words removal: ['yes', 'thought', 'thanks']\n",
      "After stemming with porters algorithm: ['ye', 'thought', 'thank']\n",
      "Tokenized sentence: ['today', 'is', 'song', 'dedicated', 'day', 'which', 'song', 'will', 'u', 'dedicate', 'for', 'me', 'send', 'this', 'to', 'all', 'ur', 'valuable', 'frnds', 'but', 'first', 'rply', 'me']\n",
      "After stop words removal: ['today', 'song', 'dedicated', 'day', 'song', 'u', 'dedicate', 'send', 'ur', 'valuable', 'frnds', 'first', 'rply']\n",
      "dedicate\n",
      "After stemming with porters algorithm: ['todai', 'song', 'dedic', 'dai', 'song', 'dedic', 'send', 'valuab', 'frnd', 'first', 'rply']\n",
      "Tokenized sentence: ['would', 'you', 'like', 'to', 'see', 'my', 'xxx', 'pics', 'they', 'are', 'so', 'hot', 'they', 'were', 'nearly', 'banned', 'in', 'the', 'uk']\n",
      "After stop words removal: ['would', 'like', 'see', 'xxx', 'pics', 'hot', 'nearly', 'banned', 'uk']\n",
      "After stemming with porters algorithm: ['would', 'like', 'see', 'xxx', 'pic', 'hot', 'nearli', 'ban']\n",
      "Tokenized sentence: ['i', 'm', 'going', 'to', 'try', 'for', 'months', 'ha', 'ha', 'only', 'joking']\n",
      "After stop words removal: ['going', 'try', 'months', 'ha', 'ha', 'joking']\n",
      "go\n",
      "jok\n",
      "After stemming with porters algorithm: ['go', 'try', 'month', 'joke']\n",
      "Tokenized sentence: ['here', 'got', 'lots', 'of', 'hair', 'dresser', 'fr', 'china']\n",
      "After stop words removal: ['got', 'lots', 'hair', 'dresser', 'fr', 'china']\n",
      "After stemming with porters algorithm: ['got', 'lot', 'hair', 'dresser', 'china']\n",
      "Tokenized sentence: ['what', 'your', 'plan', 'for', 'pongal']\n",
      "After stop words removal: ['plan', 'pongal']\n",
      "After stemming with porters algorithm: ['plan', 'pongal']\n",
      "Tokenized sentence: ['good', 'evening', 'sir', 'al', 'salam', 'wahleykkum', 'sharing', 'a', 'happy', 'news', 'by', 'the', 'grace', 'of', 'god', 'i', 'got', 'an', 'offer', 'from', 'tayseer', 'tissco', 'and', 'i', 'joined', 'hope', 'you', 'are', 'fine', 'inshah', 'allah', 'meet', 'you', 'sometime', 'rakhesh', 'visitor', 'from', 'india']\n",
      "After stop words removal: ['good', 'evening', 'sir', 'al', 'salam', 'wahleykkum', 'sharing', 'happy', 'news', 'grace', 'god', 'got', 'offer', 'tayseer', 'tissco', 'joined', 'hope', 'fine', 'inshah', 'allah', 'meet', 'sometime', 'rakhesh', 'visitor', 'india']\n",
      "even\n",
      "shar\n",
      "After stemming with porters algorithm: ['good', 'even', 'sir', 'salam', 'wahleykkum', 'share', 'happi', 'new', 'grace', 'god', 'got', 'offer', 'tayseer', 'tissco', 'join', 'hope', 'fine', 'inshah', 'allah', 'meet', 'sometim', 'rakhesh', 'visitor', 'india']\n",
      "Tokenized sentence: ['i', 'wake', 'up', 'long', 'ago', 'already', 'dunno', 'what', 'other', 'thing']\n",
      "After stop words removal: ['wake', 'long', 'ago', 'already', 'dunno', 'thing']\n",
      "After stemming with porters algorithm: ['wake', 'long', 'ago', 'alreadi', 'dunno', 'thing']\n",
      "Tokenized sentence: ['when', 'u', 'love', 'someone', 'dont', 'make', 'them', 'to', 'love', 'u', 'as', 'much', 'as', 'u', 'do', 'but', 'love', 'them', 'so', 'much', 'that', 'they', 'dont', 'want', 'to', 'be', 'loved', 'by', 'anyone', 'except', 'you', 'gud', 'nit']\n",
      "After stop words removal: ['u', 'love', 'someone', 'dont', 'make', 'love', 'u', 'much', 'u', 'love', 'much', 'dont', 'want', 'loved', 'anyone', 'except', 'gud', 'nit']\n",
      "After stemming with porters algorithm: ['love', 'someon', 'dont', 'make', 'love', 'much', 'love', 'much', 'dont', 'want', 'love', 'anyon', 'except', 'gud', 'nit']\n",
      "Tokenized sentence: ['aight', 'should', 'i', 'just', 'plan', 'to', 'come', 'up', 'later', 'tonight']\n",
      "After stop words removal: ['aight', 'plan', 'come', 'later', 'tonight']\n",
      "After stemming with porters algorithm: ['aight', 'plan', 'come', 'later', 'tonight']\n",
      "Tokenized sentence: ['ya', 'very', 'nice', 'be', 'ready', 'on', 'thursday']\n",
      "After stop words removal: ['ya', 'nice', 'ready', 'thursday']\n",
      "After stemming with porters algorithm: ['nice', 'readi', 'thursdai']\n",
      "Tokenized sentence: ['argh', 'why', 'the', 'fuck', 'is', 'nobody', 'in', 'town']\n",
      "After stop words removal: ['argh', 'fuck', 'nobody', 'town']\n",
      "After stemming with porters algorithm: ['argh', 'fuck', 'nobodi', 'town']\n",
      "Tokenized sentence: ['hey', 'babe', 'my', 'friend', 'had', 'to', 'cancel', 'still', 'up', 'for', 'a', 'visit']\n",
      "After stop words removal: ['hey', 'babe', 'friend', 'cancel', 'still', 'visit']\n",
      "After stemming with porters algorithm: ['hei', 'babe', 'friend', 'cancel', 'still', 'visit']\n",
      "Tokenized sentence: ['i', 'have', 'lost', 'kilos', 'as', 'of', 'today']\n",
      "After stop words removal: ['lost', 'kilos', 'today']\n",
      "After stemming with porters algorithm: ['lost', 'kilo', 'todai']\n",
      "Tokenized sentence: ['still', 'got', 'lessons', 'in', 'sch']\n",
      "After stop words removal: ['still', 'got', 'lessons', 'sch']\n",
      "After stemming with porters algorithm: ['still', 'got', 'lesson', 'sch']\n",
      "Tokenized sentence: ['how', 'is', 'your', 'schedule', 'next', 'week', 'i', 'am', 'out', 'of', 'town', 'this', 'weekend']\n",
      "After stop words removal: ['schedule', 'next', 'week', 'town', 'weekend']\n",
      "After stemming with porters algorithm: ['schedul', 'next', 'week', 'town', 'weekend']\n",
      "Tokenized sentence: ['ok', 'not', 'much', 'to', 'do', 'here', 'though', 'h', 'm', 'friday', 'cant', 'wait', 'dunno', 'wot', 'the', 'hell', 'im', 'gonna', 'do', 'for', 'another', 'weeks', 'become', 'a', 'slob', 'oh', 'wait', 'already', 'done', 'that']\n",
      "After stop words removal: ['ok', 'much', 'though', 'h', 'friday', 'cant', 'wait', 'dunno', 'wot', 'hell', 'im', 'gonna', 'another', 'weeks', 'become', 'slob', 'oh', 'wait', 'already', 'done']\n",
      "After stemming with porters algorithm: ['much', 'though', 'fridai', 'cant', 'wait', 'dunno', 'wot', 'hell', 'gonna', 'anoth', 'week', 'becom', 'slob', 'wait', 'alreadi', 'done']\n",
      "Tokenized sentence: ['i', 'am', 'going', 'to', 'sleep', 'i', 'am', 'tired', 'of', 'travel']\n",
      "After stop words removal: ['going', 'sleep', 'tired', 'travel']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'sleep', 'tire', 'travel']\n",
      "Tokenized sentence: ['can', 'do', 'lor']\n",
      "After stop words removal: ['lor']\n",
      "After stemming with porters algorithm: ['lor']\n",
      "Tokenized sentence: ['i', 'don', 'wake', 'since', 'i', 'checked', 'that', 'stuff', 'and', 'saw', 'that', 'its', 'true', 'no', 'available', 'spaces', 'pls', 'call', 'the', 'embassy', 'or', 'send', 'a', 'mail', 'to', 'them']\n",
      "After stop words removal: ['wake', 'since', 'checked', 'stuff', 'saw', 'true', 'available', 'spaces', 'pls', 'call', 'embassy', 'send', 'mail']\n",
      "After stemming with porters algorithm: ['wake', 'sinc', 'chec', 'stuff', 'saw', 'true', 'avail', 'space', 'pl', 'call', 'embassi', 'send', 'mail']\n",
      "Tokenized sentence: ['mean', 'it', 's', 'confirmed', 'i', 'tot', 'they', 'juz', 'say', 'oni', 'ok', 'then']\n",
      "After stop words removal: ['mean', 'confirmed', 'tot', 'juz', 'say', 'oni', 'ok']\n",
      "After stemming with porters algorithm: ['mean', 'confir', 'tot', 'juz', 'sai', 'oni']\n",
      "Tokenized sentence: ['so', 'll', 'be', 'submitting', 'da', 'project', 'tmr', 'rite']\n",
      "After stop words removal: ['submitting', 'da', 'project', 'tmr', 'rite']\n",
      "submitt\n",
      "After stemming with porters algorithm: ['submit', 'project', 'tmr', 'rite']\n",
      "Tokenized sentence: ['ok', 'lor', 'msg', 'me', 'b', 'u', 'call']\n",
      "After stop words removal: ['ok', 'lor', 'msg', 'b', 'u', 'call']\n",
      "After stemming with porters algorithm: ['lor', 'msg', 'call']\n",
      "Tokenized sentence: ['got', 'it', 'it', 'looks', 'scrumptious', 'daddy', 'wants', 'to', 'eat', 'you', 'all', 'night', 'long']\n",
      "After stop words removal: ['got', 'looks', 'scrumptious', 'daddy', 'wants', 'eat', 'night', 'long']\n",
      "After stemming with porters algorithm: ['got', 'look', 'scrumptiou', 'daddi', 'want', 'eat', 'night', 'long']\n",
      "Tokenized sentence: ['love', 'it', 'i', 'want', 'to', 'flood', 'that', 'pretty', 'pussy', 'with', 'cum']\n",
      "After stop words removal: ['love', 'want', 'flood', 'pretty', 'pussy', 'cum']\n",
      "After stemming with porters algorithm: ['love', 'want', 'flood', 'pretti', 'pussi', 'cum']\n",
      "Tokenized sentence: ['anyway', 'i', 'don', 't', 'think', 'i', 'can', 'secure', 'anything', 'up', 'here', 'lemme', 'know', 'if', 'you', 'want', 'me', 'to', 'drive', 'down', 'south', 'and', 'chill']\n",
      "After stop words removal: ['anyway', 'think', 'secure', 'anything', 'lemme', 'know', 'want', 'drive', 'south', 'chill']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anywai', 'think', 'secur', 'anyt', 'lemm', 'know', 'want', 'drive', 'south', 'chill']\n",
      "Tokenized sentence: ['as', 'usual', 'iam', 'fine', 'happy', 'amp', 'doing', 'well']\n",
      "After stop words removal: ['usual', 'iam', 'fine', 'happy', 'amp', 'well']\n",
      "After stemming with porters algorithm: ['usual', 'iam', 'fine', 'happi', 'amp', 'well']\n",
      "Tokenized sentence: ['o', 'well', 'uv', 'causes', 'mutations', 'sunscreen', 'is', 'like', 'essential', 'thesedays']\n",
      "After stop words removal: ['well', 'uv', 'causes', 'mutations', 'sunscreen', 'like', 'essential', 'thesedays']\n",
      "After stemming with porters algorithm: ['well', 'caus', 'mutat', 'sunscreen', 'like', 'essenti', 'thesedai']\n",
      "Tokenized sentence: ['yes', 'princess', 'i', 'want', 'to', 'make', 'you', 'happy']\n",
      "After stop words removal: ['yes', 'princess', 'want', 'make', 'happy']\n",
      "After stemming with porters algorithm: ['ye', 'princess', 'want', 'make', 'happi']\n",
      "Tokenized sentence: ['hiya', 'probably', 'coming', 'home', 'weekend', 'after', 'next']\n",
      "After stop words removal: ['hiya', 'probably', 'coming', 'home', 'weekend', 'next']\n",
      "com\n",
      "After stemming with porters algorithm: ['hiya', 'probab', 'come', 'home', 'weekend', 'next']\n",
      "Tokenized sentence: ['ok', 'i', 'msg', 'u', 'b', 'i', 'leave', 'my', 'house']\n",
      "After stop words removal: ['ok', 'msg', 'u', 'b', 'leave', 'house']\n",
      "After stemming with porters algorithm: ['msg', 'leav', 'hous']\n",
      "Tokenized sentence: ['k', 'k', 'i', 'm', 'also', 'fine', 'when', 'will', 'you', 'complete', 'the', 'course']\n",
      "After stop words removal: ['k', 'k', 'also', 'fine', 'complete', 'course']\n",
      "After stemming with porters algorithm: ['also', 'fine', 'complet', 'cours']\n",
      "Tokenized sentence: ['pathaya', 'enketa', 'maraikara', 'pa']\n",
      "After stop words removal: ['pathaya', 'enketa', 'maraikara', 'pa']\n",
      "After stemming with porters algorithm: ['pathaya', 'enketa', 'maraikara']\n",
      "Tokenized sentence: ['sppok', 'up', 'ur', 'mob', 'with', 'a', 'halloween', 'collection', 'of', 'nokia', 'logo', 'pic', 'message', 'plus', 'a', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'to']\n",
      "After stop words removal: ['sppok', 'ur', 'mob', 'halloween', 'collection', 'nokia', 'logo', 'pic', 'message', 'plus', 'free', 'eerie', 'tone', 'txt', 'card', 'spook']\n",
      "After stemming with porters algorithm: ['sppok', 'mob', 'halloween', 'collect', 'nokia', 'logo', 'pic', 'messag', 'plu', 'free', 'eeri', 'tone', 'txt', 'card', 'spook']\n",
      "Tokenized sentence: ['urgent', 'this', 'is', 'our', 'nd', 'attempt', 'to', 'contact', 'u', 'your', 'prize', 'from', 'yesterday', 'is', 'still', 'awaiting', 'collection', 'to', 'claim', 'call', 'now']\n",
      "After stop words removal: ['urgent', 'nd', 'attempt', 'contact', 'u', 'prize', 'yesterday', 'still', 'awaiting', 'collection', 'claim', 'call']\n",
      "await\n",
      "After stemming with porters algorithm: ['urgent', 'attempt', 'contact', 'priz', 'yesterdai', 'still', 'await', 'collect', 'claim', 'call']\n",
      "Tokenized sentence: ['k', 'k', 'advance', 'happy', 'pongal']\n",
      "After stop words removal: ['k', 'k', 'advance', 'happy', 'pongal']\n",
      "After stemming with porters algorithm: ['advanc', 'happi', 'pongal']\n",
      "Tokenized sentence: ['when', 'you', 'are', 'big', 'god', 'will', 'bring', 'success']\n",
      "After stop words removal: ['big', 'god', 'bring', 'success']\n",
      "After stemming with porters algorithm: ['big', 'god', 'bring', 'success']\n",
      "Tokenized sentence: ['good', 'evening', 'this', 'is', 'roger', 'how', 'are', 'you']\n",
      "After stop words removal: ['good', 'evening', 'roger']\n",
      "even\n",
      "After stemming with porters algorithm: ['good', 'even', 'roger']\n",
      "Tokenized sentence: ['do', 'you', 'know', 'when', 'the', 'result']\n",
      "After stop words removal: ['know', 'result']\n",
      "After stemming with porters algorithm: ['know', 'result']\n",
      "Tokenized sentence: ['nope', 'think', 'i', 'will', 'go', 'for', 'it', 'on', 'monday', 'sorry', 'i', 'replied', 'so', 'late']\n",
      "After stop words removal: ['nope', 'think', 'go', 'monday', 'sorry', 'replied', 'late']\n",
      "After stemming with porters algorithm: ['nope', 'think', 'mondai', 'sorri', 'repli', 'late']\n",
      "Tokenized sentence: ['lol', 'that', 's', 'different', 'i', 'don', 't', 'go', 'trying', 'to', 'find', 'every', 'real', 'life', 'photo', 'you', 'ever', 'took']\n",
      "After stop words removal: ['lol', 'different', 'go', 'trying', 'find', 'every', 'real', 'life', 'photo', 'ever', 'took']\n",
      "After stemming with porters algorithm: ['lol', 'differ', 'trying', 'find', 'everi', 'real', 'life', 'photo', 'ever', 'took']\n",
      "Tokenized sentence: ['to', 'day', 'class', 'is', 'there', 'are', 'no', 'class']\n",
      "After stop words removal: ['day', 'class', 'class']\n",
      "After stemming with porters algorithm: ['dai', 'class', 'class']\n",
      "Tokenized sentence: ['are', 'you', 'staying', 'in', 'town']\n",
      "After stop words removal: ['staying', 'town']\n",
      "stay\n",
      "After stemming with porters algorithm: ['stai', 'town']\n",
      "Tokenized sentence: ['pity', 'was', 'in', 'mood', 'for', 'that', 'so', 'any', 'other', 'suggestions']\n",
      "After stop words removal: ['pity', 'mood', 'suggestions']\n",
      "After stemming with porters algorithm: ['piti', 'mood', 'suggest']\n",
      "Tokenized sentence: ['god', 'bless', 'get', 'good', 'sleep', 'my', 'dear', 'i', 'will', 'pray']\n",
      "After stop words removal: ['god', 'bless', 'get', 'good', 'sleep', 'dear', 'pray']\n",
      "After stemming with porters algorithm: ['god', 'bless', 'get', 'good', 'sleep', 'dear', 'prai']\n",
      "Tokenized sentence: ['wherre', 's', 'my', 'boytoy']\n",
      "After stop words removal: ['wherre', 'boytoy']\n",
      "After stemming with porters algorithm: ['wherr', 'boytoi']\n",
      "Tokenized sentence: ['from', 'here', 'after', 'the', 'performance', 'award', 'is', 'calculated', 'every', 'two', 'month', 'not', 'for', 'current', 'one', 'month', 'period']\n",
      "After stop words removal: ['performance', 'award', 'calculated', 'every', 'two', 'month', 'current', 'one', 'month', 'period']\n",
      "calculate\n",
      "After stemming with porters algorithm: ['perform', 'award', 'calcul', 'everi', 'two', 'month', 'current', 'on', 'month', 'period']\n",
      "Tokenized sentence: ['there', 'is', 'os', 'called', 'ubandu', 'which', 'will', 'run', 'without', 'installing', 'in', 'hard', 'disk', 'you', 'can', 'use', 'that', 'os', 'to', 'copy', 'the', 'important', 'files', 'in', 'system', 'and', 'give', 'it', 'to', 'repair', 'shop']\n",
      "After stop words removal: ['os', 'called', 'ubandu', 'run', 'without', 'installing', 'hard', 'disk', 'use', 'os', 'copy', 'important', 'files', 'system', 'give', 'repair', 'shop']\n",
      "install\n",
      "After stemming with porters algorithm: ['call', 'ubandu', 'run', 'without', 'instal', 'hard', 'disk', 'us', 'copi', 'import', 'file', 'system', 'give', 'repair', 'shop']\n",
      "Tokenized sentence: ['i', 've', 'sent', 'my', 'part']\n",
      "After stop words removal: ['sent', 'part']\n",
      "After stemming with porters algorithm: ['sent', 'part']\n",
      "Tokenized sentence: ['oh', 'my', 'god', 'i', 've', 'found', 'your', 'number', 'again', 'i', 'm', 'so', 'glad', 'text', 'me', 'back', 'xafter', 'this', 'msgs', 'cst', 'std', 'ntwk', 'chg']\n",
      "After stop words removal: ['oh', 'god', 'found', 'number', 'glad', 'text', 'back', 'xafter', 'msgs', 'cst', 'std', 'ntwk', 'chg']\n",
      "After stemming with porters algorithm: ['god', 'found', 'number', 'glad', 'text', 'back', 'xafter', 'msg', 'cst', 'std', 'ntwk', 'chg']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['but', 'am', 'going', 'to', 'college', 'pa', 'what', 'to', 'do', 'are', 'else', 'ill', 'come', 'there', 'it', 'self', 'pa']\n",
      "After stop words removal: ['going', 'college', 'pa', 'else', 'ill', 'come', 'self', 'pa']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'colleg', 'els', 'ill', 'come', 'self']\n",
      "Tokenized sentence: ['guessin', 'you', 'ain', 't', 'gonna', 'be', 'here', 'before']\n",
      "After stop words removal: ['guessin', 'gonna']\n",
      "After stemming with porters algorithm: ['guessin', 'gonna']\n",
      "Tokenized sentence: ['they', 'finally', 'came', 'to', 'fix', 'the', 'ceiling']\n",
      "After stop words removal: ['finally', 'came', 'fix', 'ceiling']\n",
      "ceil\n",
      "After stemming with porters algorithm: ['final', 'came', 'fix', 'ceil']\n",
      "Tokenized sentence: ['hey', 'what', 'are', 'you', 'doing', 'y', 'no', 'reply', 'pa']\n",
      "After stop words removal: ['hey', 'reply', 'pa']\n",
      "After stemming with porters algorithm: ['hei', 'repli']\n",
      "Tokenized sentence: ['cbe', 'is', 'really', 'good', 'nowadays', 'lot', 'of', 'shop', 'and', 'showrooms', 'city', 'is', 'shaping', 'good']\n",
      "After stop words removal: ['cbe', 'really', 'good', 'nowadays', 'lot', 'shop', 'showrooms', 'city', 'shaping', 'good']\n",
      "shap\n",
      "After stemming with porters algorithm: ['cbe', 'realli', 'good', 'nowadai', 'lot', 'shop', 'showroom', 'citi', 'shape', 'good']\n",
      "Tokenized sentence: ['i', 'will', 'send', 'them', 'to', 'your', 'email', 'do', 'you', 'mind', 'lt', 'gt', 'times', 'per', 'night']\n",
      "After stop words removal: ['send', 'email', 'mind', 'lt', 'gt', 'times', 'per', 'night']\n",
      "After stemming with porters algorithm: ['send', 'email', 'mind', 'time', 'per', 'night']\n",
      "Tokenized sentence: ['please', 'attend', 'the', 'phone']\n",
      "After stop words removal: ['please', 'attend', 'phone']\n",
      "After stemming with porters algorithm: ['pleas', 'attend', 'phone']\n",
      "Tokenized sentence: ['what', 'is', 'your', 'record', 'for', 'one', 'night']\n",
      "After stop words removal: ['record', 'one', 'night']\n",
      "After stemming with porters algorithm: ['record', 'on', 'night']\n",
      "Tokenized sentence: ['okay', 'i', 've', 'seen', 'it', 'so', 'i', 'should', 'pick', 'it', 'on', 'friday']\n",
      "After stop words removal: ['okay', 'seen', 'pick', 'friday']\n",
      "After stemming with porters algorithm: ['okai', 'seen', 'pick', 'fridai']\n",
      "Tokenized sentence: ['happy', 'or', 'sad', 'one', 'thing', 'about', 'past', 'is', 'its', 'no', 'more', 'good', 'morning']\n",
      "After stop words removal: ['happy', 'sad', 'one', 'thing', 'past', 'good', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['happi', 'sad', 'on', 'thing', 'past', 'good', 'mor']\n",
      "Tokenized sentence: ['hey', 'babe', 'i', 'saw', 'you', 'came', 'online', 'for', 'a', 'second', 'and', 'then', 'you', 'disappeared', 'what', 'happened']\n",
      "After stop words removal: ['hey', 'babe', 'saw', 'came', 'online', 'second', 'disappeared', 'happened']\n",
      "After stemming with porters algorithm: ['hei', 'babe', 'saw', 'came', 'onlin', 'second', 'disappear', 'happen']\n",
      "Tokenized sentence: ['i', 'thk', 'lor', 'but', 'dunno', 'can', 'get', 'tickets', 'a', 'not', 'wat', 'u', 'doing', 'now']\n",
      "After stop words removal: ['thk', 'lor', 'dunno', 'get', 'tickets', 'wat', 'u']\n",
      "After stemming with porters algorithm: ['thk', 'lor', 'dunno', 'get', 'ticket', 'wat']\n",
      "Tokenized sentence: ['rofl', 'its', 'true', 'to', 'its', 'name']\n",
      "After stop words removal: ['rofl', 'true', 'name']\n",
      "After stemming with porters algorithm: ['rofl', 'true', 'name']\n",
      "Tokenized sentence: ['i', 'absolutely', 'love', 'south', 'park', 'i', 'only', 'recently', 'started', 'watching', 'the', 'office']\n",
      "After stop words removal: ['absolutely', 'love', 'south', 'park', 'recently', 'started', 'watching', 'office']\n",
      "watch\n",
      "After stemming with porters algorithm: ['absolut', 'love', 'south', 'park', 'recent', 'star', 'watc', 'offic']\n",
      "Tokenized sentence: ['this', 'pain', 'couldn', 't', 'have', 'come', 'at', 'a', 'worse', 'time']\n",
      "After stop words removal: ['pain', 'come', 'worse', 'time']\n",
      "After stemming with porters algorithm: ['pain', 'come', 'wors', 'time']\n",
      "Tokenized sentence: ['you', 'have', 'new', 'voicemail', 'please', 'call']\n",
      "After stop words removal: ['new', 'voicemail', 'please', 'call']\n",
      "After stemming with porters algorithm: ['new', 'voicemail', 'pleas', 'call']\n",
      "Tokenized sentence: ['friends', 'that', 'u', 'can', 'stay', 'on', 'fb', 'chat', 'with']\n",
      "After stop words removal: ['friends', 'u', 'stay', 'fb', 'chat']\n",
      "After stemming with porters algorithm: ['friend', 'stai', 'chat']\n",
      "Tokenized sentence: ['no', 'pic', 'please', 're', 'send']\n",
      "After stop words removal: ['pic', 'please', 'send']\n",
      "After stemming with porters algorithm: ['pic', 'pleas', 'send']\n",
      "Tokenized sentence: ['whom', 'you', 'waited', 'for', 'yesterday']\n",
      "After stop words removal: ['waited', 'yesterday']\n",
      "After stemming with porters algorithm: ['wait', 'yesterdai']\n",
      "Tokenized sentence: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'of', 'd', 'day', 'my', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
      "After stop words removal: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'day', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
      "After stemming with porters algorithm: ['ummmmmaah', 'mani', 'mani', 'happi', 'return', 'dai', 'dear', 'sweet', 'heart', 'happi', 'birthdai', 'dear']\n",
      "Tokenized sentence: ['if', 'he', 'started', 'searching', 'he', 'will', 'get', 'job', 'in', 'few', 'days', 'he', 'have', 'great', 'potential', 'and', 'talent']\n",
      "After stop words removal: ['started', 'searching', 'get', 'job', 'days', 'great', 'potential', 'talent']\n",
      "search\n",
      "After stemming with porters algorithm: ['star', 'searc', 'get', 'job', 'dai', 'great', 'potenti', 'talent']\n",
      "Tokenized sentence: ['ranjith', 'cal', 'drpd', 'deeraj', 'and', 'deepak', 'min', 'hold']\n",
      "After stop words removal: ['ranjith', 'cal', 'drpd', 'deeraj', 'deepak', 'min', 'hold']\n",
      "After stemming with porters algorithm: ['ranjith', 'cal', 'drpd', 'deeraj', 'deepak', 'min', 'hold']\n",
      "Tokenized sentence: ['still', 'in', 'the', 'area', 'of', 'the', 'restaurant', 'ill', 'try', 'to', 'come', 'back', 'soon']\n",
      "After stop words removal: ['still', 'area', 'restaurant', 'ill', 'try', 'come', 'back', 'soon']\n",
      "After stemming with porters algorithm: ['still', 'area', 'restaur', 'ill', 'try', 'come', 'back', 'soon']\n",
      "Tokenized sentence: ['no', 'i', 'm', 'in', 'the', 'same', 'boat', 'still', 'here', 'at', 'my', 'moms', 'check', 'me', 'out', 'on', 'yo', 'i', 'm', 'half', 'naked']\n",
      "After stop words removal: ['boat', 'still', 'moms', 'check', 'yo', 'half', 'naked']\n",
      "After stemming with porters algorithm: ['boat', 'still', 'mom', 'check', 'half', 'nake']\n",
      "Tokenized sentence: ['i', 've', 'got', 'it', 'down', 'to', 'a', 'tea', 'not', 'sure', 'which', 'flavour']\n",
      "After stop words removal: ['got', 'tea', 'sure', 'flavour']\n",
      "After stemming with porters algorithm: ['got', 'tea', 'sure', 'flavour']\n",
      "Tokenized sentence: ['ok', 'let', 'u', 'noe', 'when', 'i', 'leave', 'my', 'house']\n",
      "After stop words removal: ['ok', 'let', 'u', 'noe', 'leave', 'house']\n",
      "After stemming with porters algorithm: ['let', 'noe', 'leav', 'hous']\n",
      "Tokenized sentence: ['ya', 'i', 'knw', 'u', 'vl', 'giv', 'its', 'ok', 'thanks', 'kano', 'anyway', 'enjoy', 'wit', 'ur', 'family', 'wit', 'st', 'salary']\n",
      "After stop words removal: ['ya', 'knw', 'u', 'vl', 'giv', 'ok', 'thanks', 'kano', 'anyway', 'enjoy', 'wit', 'ur', 'family', 'wit', 'st', 'salary']\n",
      "After stemming with porters algorithm: ['knw', 'giv', 'thank', 'kano', 'anywai', 'enjoi', 'wit', 'famili', 'wit', 'salari']\n",
      "Tokenized sentence: ['ha', 'ha', 'nan', 'yalrigu', 'heltini', 'iyo', 'kothi', 'chikku', 'u', 'shared', 'many', 'things', 'wit', 'me', 'so', 'far', 'i', 'didn', 't', 'told', 'any', 'body', 'and', 'even', 'uttered', 'a', 'word', 'abt', 'u', 'if', 'ur', 'trusting', 'me', 'so', 'much', 'how', 'can', 'i', 'tell', 'these', 'to', 'others', 'plz', 'nxt', 'time', 'dont', 'use', 'those', 'words', 'to', 'me', 'ok', 'chikku', 'b']\n",
      "After stop words removal: ['ha', 'ha', 'nan', 'yalrigu', 'heltini', 'iyo', 'kothi', 'chikku', 'u', 'shared', 'many', 'things', 'wit', 'far', 'told', 'body', 'even', 'uttered', 'word', 'abt', 'u', 'ur', 'trusting', 'much', 'tell', 'others', 'plz', 'nxt', 'time', 'dont', 'use', 'words', 'ok', 'chikku', 'b']\n",
      "trust\n",
      "After stemming with porters algorithm: ['nan', 'yalrigu', 'heltini', 'iyo', 'kothi', 'chikku', 'share', 'mani', 'thing', 'wit', 'far', 'told', 'bodi', 'even', 'utter', 'word', 'abt', 'trus', 'much', 'tell', 'other', 'plz', 'nxt', 'time', 'dont', 'us', 'word', 'chikku']\n",
      "Tokenized sentence: ['helloooo', 'wake', 'up', 'sweet', 'morning', 'welcomes', 'you', 'enjoy', 'this', 'day', 'with', 'full', 'of', 'joy', 'gud', 'mrng']\n",
      "After stop words removal: ['helloooo', 'wake', 'sweet', 'morning', 'welcomes', 'enjoy', 'day', 'full', 'joy', 'gud', 'mrng']\n",
      "morn\n",
      "After stemming with porters algorithm: ['helloooo', 'wake', 'sweet', 'mor', 'welcom', 'enjoi', 'dai', 'full', 'joi', 'gud', 'mrng']\n",
      "Tokenized sentence: ['this', 'is', 'a', 'long', 'fuckin', 'showr']\n",
      "After stop words removal: ['long', 'fuckin', 'showr']\n",
      "After stemming with porters algorithm: ['long', 'fuckin', 'showr']\n",
      "Tokenized sentence: ['time', 'n', 'smile', 'r', 'the', 'two', 'crucial', 'things', 'in', 'our', 'life', 'sometimes', 'time', 'makes', 'us', 'to', 'forget', 'smile', 'and', 'sometimes', 'someone', 's', 'smile', 'makes', 'us', 'to', 'forget', 'time', 'gud', 'noon']\n",
      "After stop words removal: ['time', 'n', 'smile', 'r', 'two', 'crucial', 'things', 'life', 'sometimes', 'time', 'makes', 'us', 'forget', 'smile', 'sometimes', 'someone', 'smile', 'makes', 'us', 'forget', 'time', 'gud', 'noon']\n",
      "After stemming with porters algorithm: ['time', 'smile', 'two', 'crucial', 'thing', 'life', 'sometim', 'time', 'make', 'forget', 'smile', 'sometim', 'someon', 'smile', 'make', 'forget', 'time', 'gud', 'noon']\n",
      "Tokenized sentence: ['she', 's', 'borderline', 'but', 'yeah', 'whatever']\n",
      "After stop words removal: ['borderline', 'yeah', 'whatever']\n",
      "After stemming with porters algorithm: ['borderlin', 'yeah', 'whatev']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 'bonus', 'points', 'to', 'claim', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'bonus', 'points', 'claim', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'unredeem', 'bonu', 'point', 'claim', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['you', 'know', 'what', 'hook', 'up', 'means', 'right']\n",
      "After stop words removal: ['know', 'hook', 'means', 'right']\n",
      "After stemming with porters algorithm: ['know', 'hook', 'mean', 'right']\n",
      "Tokenized sentence: ['u', 'can', 'win', 'of', 'music', 'gift', 'vouchers', 'every', 'week', 'starting', 'now', 'txt', 'the', 'word', 'draw', 'to', 'tscs', 'www', 'idew', 'com', 'skillgame', 'winaweek', 'age', 'ppermesssubscription']\n",
      "After stop words removal: ['u', 'win', 'music', 'gift', 'vouchers', 'every', 'week', 'starting', 'txt', 'word', 'draw', 'tscs', 'www', 'idew', 'com', 'skillgame', 'winaweek', 'age', 'ppermesssubscription']\n",
      "start\n",
      "After stemming with porters algorithm: ['win', 'music', 'gift', 'voucher', 'everi', 'week', 'star', 'txt', 'word', 'draw', 'tsc', 'www', 'idew', 'com', 'skillgam', 'winaweek', 'ag', 'ppermesssubscript']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'show', 'you', 'the', 'world', 'princess', 'how', 'about', 'europe']\n",
      "After stop words removal: ['want', 'show', 'world', 'princess', 'europe']\n",
      "After stemming with porters algorithm: ['want', 'show', 'world', 'princess', 'europ']\n",
      "Tokenized sentence: ['what', 'time', 'i', 'm', 'out', 'until', 'prob', 'or', 'so']\n",
      "After stop words removal: ['time', 'prob']\n",
      "After stemming with porters algorithm: ['time', 'prob']\n",
      "Tokenized sentence: ['awesome', 'i', 'll', 'see', 'you', 'in', 'a', 'bit']\n",
      "After stop words removal: ['awesome', 'see', 'bit']\n",
      "After stemming with porters algorithm: ['awesom', 'see', 'bit']\n",
      "Tokenized sentence: ['dad', 'says', 'hurry', 'the', 'hell', 'up']\n",
      "After stop words removal: ['dad', 'says', 'hurry', 'hell']\n",
      "After stemming with porters algorithm: ['dad', 'sai', 'hurri', 'hell']\n",
      "Tokenized sentence: ['howz', 'that', 'persons', 'story']\n",
      "After stop words removal: ['howz', 'persons', 'story']\n",
      "After stemming with porters algorithm: ['howz', 'person', 'stori']\n",
      "Tokenized sentence: ['sac', 'needs', 'to', 'carry', 'on']\n",
      "After stop words removal: ['sac', 'needs', 'carry']\n",
      "After stemming with porters algorithm: ['sac', 'need', 'carri']\n",
      "Tokenized sentence: ['s', 'this', 'will', 'increase', 'the', 'chance', 'of', 'winning']\n",
      "After stop words removal: ['increase', 'chance', 'winning']\n",
      "winn\n",
      "After stemming with porters algorithm: ['increas', 'chanc', 'win']\n",
      "Tokenized sentence: ['you', 'are', 'sweet', 'as', 'well', 'princess', 'please', 'tell', 'me', 'your', 'likes', 'and', 'dislikes', 'in', 'bed']\n",
      "After stop words removal: ['sweet', 'well', 'princess', 'please', 'tell', 'likes', 'dislikes', 'bed']\n",
      "After stemming with porters algorithm: ['sweet', 'well', 'princess', 'pleas', 'tell', 'like', 'dislik', 'bed']\n",
      "Tokenized sentence: ['dunno', 'leh', 'cant', 'remember', 'mayb', 'lor', 'so', 'wat', 'time', 'r', 'we', 'meeting', 'tmr']\n",
      "After stop words removal: ['dunno', 'leh', 'cant', 'remember', 'mayb', 'lor', 'wat', 'time', 'r', 'meeting', 'tmr']\n",
      "meet\n",
      "After stemming with porters algorithm: ['dunno', 'leh', 'cant', 'rememb', 'mayb', 'lor', 'wat', 'time', 'meet', 'tmr']\n",
      "Tokenized sentence: ['today', 's', 'offer', 'claim', 'ur', 'worth', 'of', 'discount', 'vouchers', 'text', 'yes', 'to', 'now', 'savamob', 'member', 'offers', 'mobile', 't', 'cs', 'sub', 'unsub', 'reply', 'x']\n",
      "After stop words removal: ['today', 'offer', 'claim', 'ur', 'worth', 'discount', 'vouchers', 'text', 'yes', 'savamob', 'member', 'offers', 'mobile', 'cs', 'sub', 'unsub', 'reply', 'x']\n",
      "After stemming with porters algorithm: ['todai', 'offer', 'claim', 'worth', 'discount', 'voucher', 'text', 'ye', 'savamob', 'member', 'offer', 'mobil', 'sub', 'unsub', 'repli']\n",
      "Tokenized sentence: ['a', 'lot', 'of', 'this', 'sickness', 'thing', 'going', 'round', 'take', 'it', 'easy', 'hope', 'u', 'feel', 'better', 'soon', 'lol']\n",
      "After stop words removal: ['lot', 'sickness', 'thing', 'going', 'round', 'take', 'easy', 'hope', 'u', 'feel', 'better', 'soon', 'lol']\n",
      "go\n",
      "After stemming with porters algorithm: ['lot', 'sick', 'thing', 'go', 'round', 'take', 'easi', 'hope', 'feel', 'better', 'soon', 'lol']\n",
      "Tokenized sentence: ['hi', 'mobile', 'no', 'lt', 'gt', 'has', 'added', 'you', 'in', 'their', 'contact', 'list', 'on', 'www', 'fullonsms', 'com', 'it', 's', 'a', 'great', 'place', 'to', 'send', 'free', 'sms', 'to', 'people', 'for', 'more', 'visit', 'fullonsms', 'com']\n",
      "After stop words removal: ['hi', 'mobile', 'lt', 'gt', 'added', 'contact', 'list', 'www', 'fullonsms', 'com', 'great', 'place', 'send', 'free', 'sms', 'people', 'visit', 'fullonsms', 'com']\n",
      "After stemming with porters algorithm: ['mobil', 'ad', 'contact', 'list', 'www', 'fullonsm', 'com', 'great', 'place', 'send', 'free', 'sm', 'peopl', 'visit', 'fullonsm', 'com']\n",
      "Tokenized sentence: ['life', 'is', 'more', 'strict', 'than', 'teacher', 'bcoz', 'teacher', 'teaches', 'lesson', 'amp', 'then', 'conducts', 'exam', 'but', 'life', 'first', 'conducts', 'exam', 'amp', 'then', 'teaches', 'lessons', 'happy', 'morning']\n",
      "After stop words removal: ['life', 'strict', 'teacher', 'bcoz', 'teacher', 'teaches', 'lesson', 'amp', 'conducts', 'exam', 'life', 'first', 'conducts', 'exam', 'amp', 'teaches', 'lessons', 'happy', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['life', 'strict', 'teacher', 'bcoz', 'teacher', 'teach', 'lesson', 'amp', 'conduct', 'exam', 'life', 'first', 'conduct', 'exam', 'amp', 'teach', 'lesson', 'happi', 'mor']\n",
      "Tokenized sentence: ['either', 'way', 'works', 'for', 'me', 'i', 'am', 'lt', 'gt', 'years', 'old', 'hope', 'that', 'doesnt', 'bother', 'you']\n",
      "After stop words removal: ['either', 'way', 'works', 'lt', 'gt', 'years', 'old', 'hope', 'doesnt', 'bother']\n",
      "After stemming with porters algorithm: ['either', 'wai', 'work', 'year', 'old', 'hope', 'doesnt', 'bother']\n",
      "Tokenized sentence: ['please', 'protect', 'yourself', 'from', 'e', 'threats', 'sib', 'never', 'asks', 'for', 'sensitive', 'information', 'like', 'passwords', 'atm', 'sms', 'pin', 'thru', 'email', 'never', 'share', 'your', 'password', 'with', 'anybody']\n",
      "After stop words removal: ['please', 'protect', 'e', 'threats', 'sib', 'never', 'asks', 'sensitive', 'information', 'like', 'passwords', 'atm', 'sms', 'pin', 'thru', 'email', 'never', 'share', 'password', 'anybody']\n",
      "After stemming with porters algorithm: ['pleas', 'protect', 'threat', 'sib', 'never', 'ask', 'sensit', 'inform', 'like', 'password', 'atm', 'sm', 'pin', 'thru', 'email', 'never', 'share', 'password', 'anybodi']\n",
      "Tokenized sentence: ['u', 'sick', 'still', 'can', 'go', 'shopping']\n",
      "After stop words removal: ['u', 'sick', 'still', 'go', 'shopping']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['sick', 'still', 'shop']\n",
      "Tokenized sentence: ['o', 'turns', 'out', 'i', 'had', 'stereo', 'love', 'on', 'mi', 'phone', 'under', 'the', 'unknown', 'album']\n",
      "After stop words removal: ['turns', 'stereo', 'love', 'mi', 'phone', 'unknown', 'album']\n",
      "After stemming with porters algorithm: ['turn', 'stereo', 'love', 'phone', 'unknown', 'album']\n",
      "Tokenized sentence: ['can', 'you', 'please', 'send', 'me', 'my', 'aunty', 's', 'number']\n",
      "After stop words removal: ['please', 'send', 'aunty', 'number']\n",
      "After stemming with porters algorithm: ['pleas', 'send', 'aunti', 'number']\n",
      "Tokenized sentence: ['ok', 'the', 'theory', 'test', 'when', 'are', 'going', 'to', 'book', 'i', 'think', 'it', 's', 'on', 'may', 'coz', 'thought', 'wanna', 'go', 'out', 'with', 'jiayin', 'but', 'she', 'isnt', 'free']\n",
      "After stop words removal: ['ok', 'theory', 'test', 'going', 'book', 'think', 'may', 'coz', 'thought', 'wanna', 'go', 'jiayin', 'isnt', 'free']\n",
      "go\n",
      "After stemming with porters algorithm: ['theori', 'test', 'go', 'book', 'think', 'mai', 'coz', 'thought', 'wanna', 'jiayin', 'isnt', 'free']\n",
      "Tokenized sentence: ['jade', 'its', 'paul', 'y', 'didn', 't', 'u', 'txt', 'me', 'do', 'u', 'remember', 'me', 'from', 'barmed', 'i', 'want', 'talk', 'u', 'txt', 'me']\n",
      "After stop words removal: ['jade', 'paul', 'u', 'txt', 'u', 'remember', 'barmed', 'want', 'talk', 'u', 'txt']\n",
      "After stemming with porters algorithm: ['jade', 'paul', 'txt', 'rememb', 'bar', 'want', 'talk', 'txt']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'contact', 'you', 're', 'your', 'reply', 'to', 'our', 'offer', 'of', 'mins', 'textand', 'a', 'new', 'video', 'phone', 'call', 'now', 'or', 'reply', 'for', 'free', 'delivery', 'tomorrow']\n",
      "After stop words removal: ['tried', 'contact', 'reply', 'offer', 'mins', 'textand', 'new', 'video', 'phone', 'call', 'reply', 'free', 'delivery', 'tomorrow']\n",
      "After stemming with porters algorithm: ['tri', 'contact', 'repli', 'offer', 'min', 'textand', 'new', 'video', 'phone', 'call', 'repli', 'free', 'deliveri', 'tomorrow']\n",
      "Tokenized sentence: ['s', 'min', 'to', 'go', 'for', 'lunch']\n",
      "After stop words removal: ['min', 'go', 'lunch']\n",
      "After stemming with porters algorithm: ['min', 'lunch']\n",
      "Tokenized sentence: ['perhaps', 'is', 'much', 'easy', 'give', 'your', 'account', 'identification', 'so', 'i', 'will', 'tomorrow', 'at', 'uni']\n",
      "After stop words removal: ['perhaps', 'much', 'easy', 'give', 'account', 'identification', 'tomorrow', 'uni']\n",
      "After stemming with porters algorithm: ['perhap', 'much', 'easi', 'give', 'account', 'identif', 'tomorrow', 'uni']\n",
      "Tokenized sentence: ['what', 'time', 'do', 'u', 'get', 'out']\n",
      "After stop words removal: ['time', 'u', 'get']\n",
      "After stemming with porters algorithm: ['time', 'get']\n",
      "Tokenized sentence: ['haven', 't', 'left', 'yet', 'so', 'probably', 'gonna', 'be', 'here', 'til', 'dinner']\n",
      "After stop words removal: ['left', 'yet', 'probably', 'gonna', 'til', 'dinner']\n",
      "After stemming with porters algorithm: ['left', 'yet', 'probab', 'gonna', 'til', 'dinner']\n",
      "Tokenized sentence: ['try', 'neva', 'mate']\n",
      "After stop words removal: ['try', 'neva', 'mate']\n",
      "After stemming with porters algorithm: ['try', 'neva', 'mate']\n",
      "Tokenized sentence: ['sure', 'thing', 'big', 'man', 'i', 'have', 'hockey', 'elections', 'at', 'shouldn', 't', 'go', 'on', 'longer', 'than', 'an', 'hour', 'though']\n",
      "After stop words removal: ['sure', 'thing', 'big', 'man', 'hockey', 'elections', 'go', 'longer', 'hour', 'though']\n",
      "After stemming with porters algorithm: ['sure', 'thing', 'big', 'man', 'hockei', 'elect', 'longer', 'hour', 'though']\n",
      "Tokenized sentence: ['lol', 'that', 'would', 'be', 'awesome', 'payback']\n",
      "After stop words removal: ['lol', 'would', 'awesome', 'payback']\n",
      "After stemming with porters algorithm: ['lol', 'would', 'awesom', 'payback']\n",
      "Tokenized sentence: ['good', 'afternoon', 'my', 'love', 'how', 'goes', 'that', 'day', 'i', 'hope', 'maybe', 'you', 'got', 'some', 'leads', 'on', 'a', 'job', 'i', 'think', 'of', 'you', 'boytoy', 'and', 'send', 'you', 'a', 'passionate', 'kiss', 'from', 'across', 'the', 'sea']\n",
      "After stop words removal: ['good', 'afternoon', 'love', 'goes', 'day', 'hope', 'maybe', 'got', 'leads', 'job', 'think', 'boytoy', 'send', 'passionate', 'kiss', 'across', 'sea']\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'love', 'goe', 'dai', 'hope', 'mayb', 'got', 'lead', 'job', 'think', 'boytoi', 'send', 'passion', 'kiss', 'across', 'sea']\n",
      "Tokenized sentence: ['o', 'i', 'played', 'smash', 'bros', 'lt', 'gt', 'religiously']\n",
      "After stop words removal: ['played', 'smash', 'bros', 'lt', 'gt', 'religiously']\n",
      "After stemming with porters algorithm: ['plai', 'smash', 'bro', 'religi']\n",
      "Tokenized sentence: ['neither', 'in', 'sterm', 'voice', 'i', 'm', 'studying', 'all', 'fine', 'with', 'me', 'not', 'sure', 'the', 'thing', 'will', 'be', 'resolved', 'tho', 'anyway', 'have', 'a', 'fab', 'hols']\n",
      "After stop words removal: ['neither', 'sterm', 'voice', 'studying', 'fine', 'sure', 'thing', 'resolved', 'tho', 'anyway', 'fab', 'hols']\n",
      "study\n",
      "After stemming with porters algorithm: ['neither', 'sterm', 'voic', 'stud', 'fine', 'sure', 'thing', 'resol', 'tho', 'anywai', 'fab', 'hol']\n",
      "Tokenized sentence: ['nan', 'sonathaya', 'soladha', 'why', 'boss']\n",
      "After stop words removal: ['nan', 'sonathaya', 'soladha', 'boss']\n",
      "After stemming with porters algorithm: ['nan', 'sonathaya', 'soladha', 'boss']\n",
      "Tokenized sentence: ['i', 'told', 'her', 'i', 'had', 'a', 'dr', 'appt', 'next', 'week', 'she', 'thinks', 'i', 'm', 'gonna', 'die', 'i', 'told', 'her', 'its', 'just', 'a', 'check', 'nothing', 'to', 'be', 'worried', 'about', 'but', 'she', 'didn', 't', 'listen']\n",
      "After stop words removal: ['told', 'dr', 'appt', 'next', 'week', 'thinks', 'gonna', 'die', 'told', 'check', 'nothing', 'worried', 'listen']\n",
      "noth\n",
      "After stemming with porters algorithm: ['told', 'appt', 'next', 'week', 'think', 'gonna', 'die', 'told', 'check', 'not', 'worri', 'listen']\n",
      "Tokenized sentence: ['have', 'you', 'always', 'been', 'saying', 'welp']\n",
      "After stop words removal: ['always', 'saying', 'welp']\n",
      "say\n",
      "After stemming with porters algorithm: ['alwai', 'sai', 'welp']\n",
      "Tokenized sentence: ['i', 'told', 'your', 'number', 'to', 'gautham']\n",
      "After stop words removal: ['told', 'number', 'gautham']\n",
      "After stemming with porters algorithm: ['told', 'number', 'gautham']\n",
      "Tokenized sentence: ['the', 'table', 's', 'occupied', 'i', 'm', 'waiting', 'by', 'the', 'tree']\n",
      "After stop words removal: ['table', 'occupied', 'waiting', 'tree']\n",
      "wait\n",
      "After stemming with porters algorithm: ['tabl', 'occupi', 'wait', 'tree']\n",
      "Tokenized sentence: ['this', 'is', 'my', 'number', 'by', 'vivek']\n",
      "After stop words removal: ['number', 'vivek']\n",
      "After stemming with porters algorithm: ['number', 'vivek']\n",
      "Tokenized sentence: ['wat', 's', 'my', 'dear', 'doing', 'sleeping', 'ah']\n",
      "After stop words removal: ['wat', 'dear', 'sleeping', 'ah']\n",
      "sleep\n",
      "After stemming with porters algorithm: ['wat', 'dear', 'sleep']\n",
      "Tokenized sentence: ['i', 'reach', 'home', 'safe', 'n', 'sound', 'liao']\n",
      "After stop words removal: ['reach', 'home', 'safe', 'n', 'sound', 'liao']\n",
      "After stemming with porters algorithm: ['reach', 'home', 'safe', 'sound', 'liao']\n",
      "Tokenized sentence: ['how', 'long', 'has', 'it', 'been', 'since', 'you', 'screamed', 'princess']\n",
      "After stop words removal: ['long', 'since', 'screamed', 'princess']\n",
      "After stemming with porters algorithm: ['long', 'sinc', 'scream', 'princess']\n",
      "Tokenized sentence: ['when', 'did', 'you', 'get', 'to', 'the', 'library']\n",
      "After stop words removal: ['get', 'library']\n",
      "After stemming with porters algorithm: ['get', 'librari']\n",
      "Tokenized sentence: ['lol', 'yes', 'our', 'friendship', 'is', 'hanging', 'on', 'a', 'thread', 'cause', 'u', 'won', 't', 'buy', 'stuff']\n",
      "After stop words removal: ['lol', 'yes', 'friendship', 'hanging', 'thread', 'cause', 'u', 'buy', 'stuff']\n",
      "hang\n",
      "After stemming with porters algorithm: ['lol', 'ye', 'friendship', 'han', 'thread', 'caus', 'bui', 'stuff']\n",
      "Tokenized sentence: ['juz', 'go', 'google', 'n', 'search', 'qet']\n",
      "After stop words removal: ['juz', 'go', 'google', 'n', 'search', 'qet']\n",
      "After stemming with porters algorithm: ['juz', 'googl', 'search', 'qet']\n",
      "Tokenized sentence: ['if', 'you', 'r', 'home', 'then', 'come', 'down', 'within', 'min']\n",
      "After stop words removal: ['r', 'home', 'come', 'within', 'min']\n",
      "After stemming with porters algorithm: ['home', 'come', 'within', 'min']\n",
      "Tokenized sentence: ['life', 'style', 'garments', 'account', 'no', 'please']\n",
      "After stop words removal: ['life', 'style', 'garments', 'account', 'please']\n",
      "After stemming with porters algorithm: ['life', 'style', 'garment', 'account', 'pleas']\n",
      "Tokenized sentence: ['feel', 'yourself', 'that', 'you', 'are', 'always', 'happy', 'slowly', 'it', 'becomes', 'your', 'habit', 'amp', 'finally', 'it', 'becomes', 'part', 'of', 'your', 'life', 'follow', 'it', 'happy', 'morning', 'amp', 'have', 'a', 'happy', 'day']\n",
      "After stop words removal: ['feel', 'always', 'happy', 'slowly', 'becomes', 'habit', 'amp', 'finally', 'becomes', 'part', 'life', 'follow', 'happy', 'morning', 'amp', 'happy', 'day']\n",
      "morn\n",
      "After stemming with porters algorithm: ['feel', 'alwai', 'happi', 'slowli', 'becom', 'habit', 'amp', 'final', 'becom', 'part', 'life', 'follow', 'happi', 'mor', 'amp', 'happi', 'dai']\n",
      "Tokenized sentence: ['are', 'you', 'wet', 'right', 'now']\n",
      "After stop words removal: ['wet', 'right']\n",
      "After stemming with porters algorithm: ['wet', 'right']\n",
      "Tokenized sentence: ['hmmm', 'still', 'we', 'dont', 'have', 'opener']\n",
      "After stop words removal: ['hmmm', 'still', 'dont', 'opener']\n",
      "After stemming with porters algorithm: ['hmmm', 'still', 'dont', 'open']\n",
      "Tokenized sentence: ['none', 'of', 'that', 's', 'happening', 'til', 'you', 'get', 'here', 'though']\n",
      "After stop words removal: ['none', 'happening', 'til', 'get', 'though']\n",
      "happen\n",
      "After stemming with porters algorithm: ['none', 'happen', 'til', 'get', 'though']\n",
      "Tokenized sentence: ['u', 'say', 'leh', 'of', 'course', 'nothing', 'happen', 'lar', 'not', 'say', 'v', 'romantic', 'jus', 'a', 'bit', 'only', 'lor', 'i', 'thk', 'e', 'nite', 'scenery', 'not', 'so', 'nice', 'leh']\n",
      "After stop words removal: ['u', 'say', 'leh', 'course', 'nothing', 'happen', 'lar', 'say', 'v', 'romantic', 'jus', 'bit', 'lor', 'thk', 'e', 'nite', 'scenery', 'nice', 'leh']\n",
      "noth\n",
      "After stemming with porters algorithm: ['sai', 'leh', 'cours', 'not', 'happen', 'lar', 'sai', 'romant', 'ju', 'bit', 'lor', 'thk', 'nite', 'sceneri', 'nice', 'leh']\n",
      "Tokenized sentence: ['only', 'just', 'got', 'this', 'message', 'not', 'ignoring', 'you', 'yes', 'i', 'was', 'shopping', 'that', 'is']\n",
      "After stop words removal: ['got', 'message', 'ignoring', 'yes', 'shopping']\n",
      "ignor\n",
      "shopp\n",
      "After stemming with porters algorithm: ['got', 'messag', 'ignor', 'ye', 'shop']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'awar', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr', 'ppm']\n",
      "Tokenized sentence: ['i', 'm', 'an', 'actor', 'when', 'i', 'work', 'i', 'work', 'in', 'the', 'evening', 'and', 'sleep', 'late', 'since', 'i', 'm', 'unemployed', 'at', 'the', 'moment', 'i', 'always', 'sleep', 'late', 'when', 'you', 're', 'unemployed', 'every', 'day', 'is', 'saturday']\n",
      "After stop words removal: ['actor', 'work', 'work', 'evening', 'sleep', 'late', 'since', 'unemployed', 'moment', 'always', 'sleep', 'late', 'unemployed', 'every', 'day', 'saturday']\n",
      "even\n",
      "After stemming with porters algorithm: ['actor', 'work', 'work', 'even', 'sleep', 'late', 'sinc', 'unemploi', 'moment', 'alwai', 'sleep', 'late', 'unemploi', 'everi', 'dai', 'saturdai']\n",
      "Tokenized sentence: ['i', 'will', 'reach', 'before', 'ten', 'morning']\n",
      "After stop words removal: ['reach', 'ten', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['reach', 'ten', 'mor']\n",
      "Tokenized sentence: ['camera', 'you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stop words removal: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stemming with porters algorithm: ['camera', 'awar', 'sipix', 'digit', 'camera', 'call', 'fromm', 'landlin', 'deliveri', 'within', 'dai']\n",
      "Tokenized sentence: ['babe', 'i', 'm', 'back', 'come', 'back', 'to', 'me']\n",
      "After stop words removal: ['babe', 'back', 'come', 'back']\n",
      "After stemming with porters algorithm: ['babe', 'back', 'come', 'back']\n",
      "Tokenized sentence: ['in', 'fact', 'when', 'do', 'you', 'leave', 'i', 'think', 'addie', 'goes', 'back', 'to', 'school', 'tues', 'or', 'wed']\n",
      "After stop words removal: ['fact', 'leave', 'think', 'addie', 'goes', 'back', 'school', 'tues', 'wed']\n",
      "After stemming with porters algorithm: ['fact', 'leav', 'think', 'addi', 'goe', 'back', 'school', 'tue', 'wed']\n",
      "Tokenized sentence: ['customer', 'place', 'i', 'will', 'call', 'you']\n",
      "After stop words removal: ['customer', 'place', 'call']\n",
      "After stemming with porters algorithm: ['custom', 'place', 'call']\n",
      "Tokenized sentence: ['i', 'could', 'ask', 'carlos', 'if', 'we', 'could', 'get', 'more', 'if', 'anybody', 'else', 'can', 'chip', 'in']\n",
      "After stop words removal: ['could', 'ask', 'carlos', 'could', 'get', 'anybody', 'else', 'chip']\n",
      "After stemming with porters algorithm: ['could', 'ask', 'carlo', 'could', 'get', 'anybodi', 'els', 'chip']\n",
      "Tokenized sentence: ['i', 'do', 'know', 'what', 'u', 'mean', 'is', 'the', 'king', 'of', 'not', 'havin', 'credit', 'i', 'm', 'goin', 'bed', 'now', 'night', 'night', 'sweet', 'only', 'more', 'sleep']\n",
      "After stop words removal: ['know', 'u', 'mean', 'king', 'havin', 'credit', 'goin', 'bed', 'night', 'night', 'sweet', 'sleep']\n",
      "After stemming with porters algorithm: ['know', 'mean', 'king', 'havin', 'credit', 'goin', 'bed', 'night', 'night', 'sweet', 'sleep']\n",
      "Tokenized sentence: ['i', 'am', 'literally', 'in', 'bed', 'and', 'have', 'been', 'up', 'for', 'like', 'lt', 'gt', 'hours']\n",
      "After stop words removal: ['literally', 'bed', 'like', 'lt', 'gt', 'hours']\n",
      "After stemming with porters algorithm: ['liter', 'bed', 'like', 'hour']\n",
      "Tokenized sentence: ['you', 've', 'won', 'tkts', 'to', 'the', 'euro', 'cup', 'final', 'or', 'cash', 'to', 'collect', 'call', 'b', 'pobox', 'ppm']\n",
      "After stop words removal: ['tkts', 'euro', 'cup', 'final', 'cash', 'collect', 'call', 'b', 'pobox', 'ppm']\n",
      "After stemming with porters algorithm: ['tkt', 'euro', 'cup', 'final', 'cash', 'collect', 'call', 'pobox', 'ppm']\n",
      "Tokenized sentence: ['hey', 'what', 'time', 'is', 'your', 'driving', 'on', 'fri', 'we', 'go', 'for', 'evaluation', 'on', 'fri']\n",
      "After stop words removal: ['hey', 'time', 'driving', 'fri', 'go', 'evaluation', 'fri']\n",
      "driv\n",
      "After stemming with porters algorithm: ['hei', 'time', 'drive', 'fri', 'evalu', 'fri']\n",
      "Tokenized sentence: ['esplanade', 'lor', 'where', 'else']\n",
      "After stop words removal: ['esplanade', 'lor', 'else']\n",
      "After stemming with porters algorithm: ['esplanad', 'lor', 'els']\n",
      "Tokenized sentence: ['tell', 'you', 'what', 'if', 'you', 'make', 'a', 'little', 'spreadsheet', 'and', 'track', 'whose', 'idea', 'it', 'was', 'to', 'smoke', 'to', 'determine', 'who', 'smokes', 'too', 'much', 'for', 'the', 'entire', 'month', 'of', 'february']\n",
      "After stop words removal: ['tell', 'make', 'little', 'spreadsheet', 'track', 'whose', 'idea', 'smoke', 'determine', 'smokes', 'much', 'entire', 'month', 'february']\n",
      "After stemming with porters algorithm: ['tell', 'make', 'littl', 'spreadsheet', 'track', 'whose', 'idea', 'smoke', 'determin', 'smoke', 'much', 'entir', 'month', 'februari']\n",
      "Tokenized sentence: ['u', 'guys', 'never', 'invite', 'me', 'anywhere']\n",
      "After stop words removal: ['u', 'guys', 'never', 'invite', 'anywhere']\n",
      "After stemming with porters algorithm: ['gui', 'never', 'invit', 'anywher']\n",
      "Tokenized sentence: ['p', 'alfie', 'moon', 's', 'children', 'in', 'need', 'song', 'on', 'ur', 'mob', 'tell', 'ur', 'm', 's', 'txt', 'tone', 'charity', 'to', 'for', 'nokias', 'or', 'poly', 'charity', 'for', 'polys', 'zed', 'profit', 'charity']\n",
      "After stop words removal: ['p', 'alfie', 'moon', 'children', 'need', 'song', 'ur', 'mob', 'tell', 'ur', 'txt', 'tone', 'charity', 'nokias', 'poly', 'charity', 'polys', 'zed', 'profit', 'charity']\n",
      "After stemming with porters algorithm: ['alfi', 'moon', 'children', 'need', 'song', 'mob', 'tell', 'txt', 'tone', 'chariti', 'nokia', 'poli', 'chariti', 'poli', 'zed', 'profit', 'chariti']\n",
      "Tokenized sentence: ['hiya', 'stu', 'wot', 'u', 'up', 'im', 'in', 'so', 'much', 'truble', 'at', 'home', 'at', 'moment', 'evone', 'hates', 'me', 'even', 'u', 'wot', 'the', 'hell', 'av', 'i', 'done', 'now', 'y', 'wont', 'u', 'just', 'tell', 'me', 'text', 'bck', 'please', 'luv', 'dan']\n",
      "After stop words removal: ['hiya', 'stu', 'wot', 'u', 'im', 'much', 'truble', 'home', 'moment', 'evone', 'hates', 'even', 'u', 'wot', 'hell', 'av', 'done', 'wont', 'u', 'tell', 'text', 'bck', 'please', 'luv', 'dan']\n",
      "After stemming with porters algorithm: ['hiya', 'stu', 'wot', 'much', 'trubl', 'home', 'moment', 'evon', 'hate', 'even', 'wot', 'hell', 'done', 'wont', 'tell', 'text', 'bck', 'pleas', 'luv', 'dan']\n",
      "Tokenized sentence: ['i', 'm', 'done']\n",
      "After stop words removal: ['done']\n",
      "After stemming with porters algorithm: ['done']\n",
      "Tokenized sentence: ['so', 'you', 'think', 'i', 'should', 'actually', 'talk', 'to', 'him', 'not', 'call', 'his', 'boss', 'in', 'the', 'morning', 'i', 'went', 'to', 'this', 'place', 'last', 'year', 'and', 'he', 'told', 'me', 'where', 'i', 'could', 'go', 'and', 'get', 'my', 'car', 'fixed', 'cheaper', 'he', 'kept', 'telling', 'me', 'today', 'how', 'much', 'he', 'hoped', 'i', 'would', 'come', 'back', 'in', 'how', 'he', 'always', 'regretted', 'not', 'getting', 'my', 'number', 'etc']\n",
      "After stop words removal: ['think', 'actually', 'talk', 'call', 'boss', 'morning', 'went', 'place', 'last', 'year', 'told', 'could', 'go', 'get', 'car', 'fixed', 'cheaper', 'kept', 'telling', 'today', 'much', 'hoped', 'would', 'come', 'back', 'always', 'regretted', 'getting', 'number', 'etc']\n",
      "morn\n",
      "tell\n",
      "gett\n",
      "After stemming with porters algorithm: ['think', 'actual', 'talk', 'call', 'boss', 'mor', 'went', 'place', 'last', 'year', 'told', 'could', 'get', 'car', 'fix', 'cheaper', 'kept', 'tell', 'todai', 'much', 'hope', 'would', 'come', 'back', 'alwai', 'regret', 'get', 'number', 'etc']\n",
      "Tokenized sentence: ['pls', 'stop', 'bootydelious', 'f', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'www', 'sms', 'ac', 'u', 'bootydelious', 'stop', 'send', 'stop', 'frnd', 'to']\n",
      "After stop words removal: ['pls', 'stop', 'bootydelious', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'www', 'sms', 'ac', 'u', 'bootydelious', 'stop', 'send', 'stop', 'frnd']\n",
      "invit\n",
      "After stemming with porters algorithm: ['pl', 'stop', 'bootydeli', 'invit', 'friend', 'repli', 'ye', 'see', 'www', 'sm', 'bootydeli', 'stop', 'send', 'stop', 'frnd']\n",
      "Tokenized sentence: ['at', 'what', 'time', 'should', 'i', 'come', 'tomorrow']\n",
      "After stop words removal: ['time', 'come', 'tomorrow']\n",
      "After stemming with porters algorithm: ['time', 'come', 'tomorrow']\n",
      "Tokenized sentence: ['did', 'u', 'got', 'that', 'persons', 'story']\n",
      "After stop words removal: ['u', 'got', 'persons', 'story']\n",
      "After stemming with porters algorithm: ['got', 'person', 'stori']\n",
      "Tokenized sentence: ['quite', 'ok', 'but', 'a', 'bit', 'ex', 'u', 'better', 'go', 'eat', 'smth', 'now', 'else', 'i', 'll', 'feel', 'guilty']\n",
      "After stop words removal: ['quite', 'ok', 'bit', 'ex', 'u', 'better', 'go', 'eat', 'smth', 'else', 'feel', 'guilty']\n",
      "After stemming with porters algorithm: ['quit', 'bit', 'better', 'eat', 'smth', 'els', 'feel', 'guilti']\n",
      "Tokenized sentence: ['u', 'can', 'win', 'of', 'music', 'gift', 'vouchers', 'every', 'week', 'starting', 'now', 'txt', 'the', 'word', 'draw', 'to', 'tscs', 'www', 'idew', 'com', 'skillgame', 'winaweek', 'age', 'ppermesssubscription']\n",
      "After stop words removal: ['u', 'win', 'music', 'gift', 'vouchers', 'every', 'week', 'starting', 'txt', 'word', 'draw', 'tscs', 'www', 'idew', 'com', 'skillgame', 'winaweek', 'age', 'ppermesssubscription']\n",
      "start\n",
      "After stemming with porters algorithm: ['win', 'music', 'gift', 'voucher', 'everi', 'week', 'star', 'txt', 'word', 'draw', 'tsc', 'www', 'idew', 'com', 'skillgam', 'winaweek', 'ag', 'ppermesssubscript']\n",
      "Tokenized sentence: ['but', 'i', 'juz', 'remembered', 'i', 'gotta', 'bathe', 'my', 'dog', 'today']\n",
      "After stop words removal: ['juz', 'remembered', 'gotta', 'bathe', 'dog', 'today']\n",
      "After stemming with porters algorithm: ['juz', 'rememb', 'gotta', 'bath', 'dog', 'todai']\n",
      "Tokenized sentence: ['its', 'ok', 'chikku', 'and', 'its', 'my', 'of', 'favourite', 'song']\n",
      "After stop words removal: ['ok', 'chikku', 'favourite', 'song']\n",
      "After stemming with porters algorithm: ['chikku', 'favourit', 'song']\n",
      "Tokenized sentence: ['at', 'home', 'also']\n",
      "After stop words removal: ['home', 'also']\n",
      "After stemming with porters algorithm: ['home', 'also']\n",
      "Tokenized sentence: ['i', 'wil', 'be', 'there', 'with', 'in', 'lt', 'gt', 'minutes', 'got', 'any', 'space']\n",
      "After stop words removal: ['wil', 'lt', 'gt', 'minutes', 'got', 'space']\n",
      "After stemming with porters algorithm: ['wil', 'minut', 'got', 'space']\n",
      "Tokenized sentence: ['when', 'you', 'get', 'free', 'call', 'me']\n",
      "After stop words removal: ['get', 'free', 'call']\n",
      "After stemming with porters algorithm: ['get', 'free', 'call']\n",
      "Tokenized sentence: ['i', 'm', 'gonna', 'say', 'no', 'sorry', 'i', 'would', 'but', 'as', 'normal', 'am', 'starting', 'to', 'panic', 'about', 'time', 'sorry', 'again', 'are', 'you', 'seeing', 'on', 'tuesday']\n",
      "After stop words removal: ['gonna', 'say', 'sorry', 'would', 'normal', 'starting', 'panic', 'time', 'sorry', 'seeing', 'tuesday']\n",
      "start\n",
      "see\n",
      "After stemming with porters algorithm: ['gonna', 'sai', 'sorri', 'would', 'normal', 'star', 'panic', 'time', 'sorri', 'see', 'tuesdai']\n",
      "Tokenized sentence: ['me', 'too', 'mark', 'is', 'taking', 'forever', 'to', 'pick', 'up', 'my', 'prescription', 'and', 'the', 'pain', 'is', 'coming', 'back']\n",
      "After stop words removal: ['mark', 'taking', 'forever', 'pick', 'prescription', 'pain', 'coming', 'back']\n",
      "tak\n",
      "com\n",
      "After stemming with porters algorithm: ['mark', 'take', 'forev', 'pick', 'prescript', 'pain', 'come', 'back']\n",
      "Tokenized sentence: ['alright', 'tyler', 's', 'got', 'a', 'minor', 'crisis', 'and', 'has', 'to', 'be', 'home', 'sooner', 'than', 'he', 'thought', 'so', 'be', 'here', 'asap']\n",
      "After stop words removal: ['alright', 'tyler', 'got', 'minor', 'crisis', 'home', 'sooner', 'thought', 'asap']\n",
      "After stemming with porters algorithm: ['alright', 'tyler', 'got', 'minor', 'crisi', 'home', 'sooner', 'thought', 'asap']\n",
      "Tokenized sentence: ['yup', 'ok', 'i', 'go', 'home', 'look', 'at', 'the', 'timings', 'then', 'i', 'msg', 'again', 'xuhui', 'going', 'to', 'learn', 'on', 'nd', 'may', 'too', 'but', 'her', 'lesson', 'is', 'at', 'am']\n",
      "After stop words removal: ['yup', 'ok', 'go', 'home', 'look', 'timings', 'msg', 'xuhui', 'going', 'learn', 'nd', 'may', 'lesson']\n",
      "tim\n",
      "go\n",
      "After stemming with porters algorithm: ['yup', 'home', 'look', 'time', 'msg', 'xuhui', 'go', 'learn', 'mai', 'lesson']\n",
      "Tokenized sentence: ['text', 'banneduk', 'to', 'to', 'see', 'cost', 'p', 'textoperator', 'g', 'ga', 'xxx']\n",
      "After stop words removal: ['text', 'banneduk', 'see', 'cost', 'p', 'textoperator', 'g', 'ga', 'xxx']\n",
      "After stemming with porters algorithm: ['text', 'banneduk', 'see', 'cost', 'textoper', 'xxx']\n",
      "Tokenized sentence: ['where', 'r', 'we', 'meeting']\n",
      "After stop words removal: ['r', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet']\n",
      "Tokenized sentence: ['or', 'go', 'buy', 'wif', 'him', 'then', 'i', 'meet', 'later', 'can']\n",
      "After stop words removal: ['go', 'buy', 'wif', 'meet', 'later']\n",
      "After stemming with porters algorithm: ['bui', 'wif', 'meet', 'later']\n",
      "Tokenized sentence: ['neft', 'transaction', 'with', 'reference', 'number', 'lt', 'gt', 'for', 'rs', 'lt', 'decimal', 'gt', 'has', 'been', 'credited', 'to', 'the', 'beneficiary', 'account', 'on', 'lt', 'gt', 'at', 'lt', 'time', 'gt', 'lt', 'gt']\n",
      "After stop words removal: ['neft', 'transaction', 'reference', 'number', 'lt', 'gt', 'rs', 'lt', 'decimal', 'gt', 'credited', 'beneficiary', 'account', 'lt', 'gt', 'lt', 'time', 'gt', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['neft', 'transact', 'refer', 'number', 'decim', 'credit', 'beneficiari', 'account', 'time']\n",
      "Tokenized sentence: ['msg', 'me', 'when', 'rajini', 'comes']\n",
      "After stop words removal: ['msg', 'rajini', 'comes']\n",
      "After stemming with porters algorithm: ['msg', 'rajini', 'come']\n",
      "Tokenized sentence: ['stop', 'calling', 'everyone', 'saying', 'i', 'might', 'have', 'cancer', 'my', 'throat', 'hurts', 'to', 'talk', 'i', 'can', 't', 'be', 'answering', 'everyones', 'calls', 'if', 'i', 'get', 'one', 'more', 'call', 'i', 'm', 'not', 'babysitting', 'on', 'monday']\n",
      "After stop words removal: ['stop', 'calling', 'everyone', 'saying', 'might', 'cancer', 'throat', 'hurts', 'talk', 'answering', 'everyones', 'calls', 'get', 'one', 'call', 'babysitting', 'monday']\n",
      "call\n",
      "say\n",
      "answer\n",
      "babysitt\n",
      "After stemming with porters algorithm: ['stop', 'call', 'everyon', 'sai', 'might', 'cancer', 'throat', 'hurt', 'talk', 'answer', 'everyon', 'call', 'get', 'on', 'call', 'babysit', 'mondai']\n",
      "Tokenized sentence: ['did', 'u', 'turn', 'on', 'the', 'heater', 'the', 'heater', 'was', 'on', 'and', 'set', 'to', 'lt', 'gt', 'degrees']\n",
      "After stop words removal: ['u', 'turn', 'heater', 'heater', 'set', 'lt', 'gt', 'degrees']\n",
      "After stemming with porters algorithm: ['turn', 'heater', 'heater', 'set', 'degre']\n",
      "Tokenized sentence: ['i', 'am', 'waiting', 'machan', 'call', 'me', 'once', 'you', 'free']\n",
      "After stop words removal: ['waiting', 'machan', 'call', 'free']\n",
      "wait\n",
      "After stemming with porters algorithm: ['wait', 'machan', 'call', 'free']\n",
      "Tokenized sentence: ['bloomberg', 'message', 'center', 'why', 'wait', 'apply', 'for', 'your', 'future', 'http', 'careers', 'bloomberg', 'com']\n",
      "After stop words removal: ['bloomberg', 'message', 'center', 'wait', 'apply', 'future', 'http', 'careers', 'bloomberg', 'com']\n",
      "After stemming with porters algorithm: ['bloomberg', 'messag', 'center', 'wait', 'appli', 'futur', 'http', 'career', 'bloomberg', 'com']\n",
      "Tokenized sentence: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'of', 'd', 'day', 'my', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
      "After stop words removal: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'day', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
      "After stemming with porters algorithm: ['ummmmmaah', 'mani', 'mani', 'happi', 'return', 'dai', 'dear', 'sweet', 'heart', 'happi', 'birthdai', 'dear']\n",
      "Tokenized sentence: ['in', 'cbe', 'in', 'chennai']\n",
      "After stop words removal: ['cbe', 'chennai']\n",
      "After stemming with porters algorithm: ['cbe', 'chennai']\n",
      "Tokenized sentence: ['why', 'you', 'keeping', 'me', 'away', 'like', 'this']\n",
      "After stop words removal: ['keeping', 'away', 'like']\n",
      "keep\n",
      "After stemming with porters algorithm: ['keep', 'awai', 'like']\n",
      "Tokenized sentence: ['hey', 'so', 'this', 'sat', 'are', 'we', 'going', 'for', 'the', 'intro', 'pilates', 'only', 'or', 'the', 'kickboxing', 'too']\n",
      "After stop words removal: ['hey', 'sat', 'going', 'intro', 'pilates', 'kickboxing']\n",
      "go\n",
      "kickbox\n",
      "After stemming with porters algorithm: ['hei', 'sat', 'go', 'intro', 'pilat', 'kickbox']\n",
      "Tokenized sentence: ['just', 'got', 'part', 'nottingham', 'hrs', 'miles', 'good', 'thing', 'i', 'love', 'my', 'man', 'so', 'much', 'but', 'only', 'doing', 'mph', 'hey', 'ho']\n",
      "After stop words removal: ['got', 'part', 'nottingham', 'hrs', 'miles', 'good', 'thing', 'love', 'man', 'much', 'mph', 'hey', 'ho']\n",
      "After stemming with porters algorithm: ['got', 'part', 'nottingham', 'hr', 'mile', 'good', 'thing', 'love', 'man', 'much', 'mph', 'hei']\n",
      "Tokenized sentence: ['great', 'i', 'have', 'to', 'run', 'now', 'so', 'ttyl']\n",
      "After stop words removal: ['great', 'run', 'ttyl']\n",
      "After stemming with porters algorithm: ['great', 'run', 'ttyl']\n",
      "Tokenized sentence: ['meanwhile', 'in', 'the', 'shit', 'suite', 'xavier', 'decided', 'to', 'give', 'us', 'lt', 'gt', 'seconds', 'of', 'warning', 'that', 'samantha', 'was', 'coming', 'over', 'and', 'is', 'playing', 'jay', 's', 'guitar', 'to', 'impress', 'her', 'or', 'some', 'shit', 'also', 'i', 'don', 't', 'think', 'doug', 'realizes', 'i', 'don', 't', 'live', 'here', 'anymore']\n",
      "After stop words removal: ['meanwhile', 'shit', 'suite', 'xavier', 'decided', 'give', 'us', 'lt', 'gt', 'seconds', 'warning', 'samantha', 'coming', 'playing', 'jay', 'guitar', 'impress', 'shit', 'also', 'think', 'doug', 'realizes', 'live', 'anymore']\n",
      "warn\n",
      "com\n",
      "play\n",
      "After stemming with porters algorithm: ['meanwhil', 'shit', 'suit', 'xavier', 'decid', 'give', 'second', 'war', 'samantha', 'come', 'plai', 'jai', 'guitar', 'impress', 'shit', 'also', 'think', 'doug', 'realiz', 'live', 'anymor']\n",
      "Tokenized sentence: ['i', 'm', 'very', 'happy', 'for', 'you', 'babe', 'woo', 'hoo', 'party', 'on', 'dude']\n",
      "After stop words removal: ['happy', 'babe', 'woo', 'hoo', 'party', 'dude']\n",
      "After stemming with porters algorithm: ['happi', 'babe', 'woo', 'hoo', 'parti', 'dude']\n",
      "Tokenized sentence: ['s', 'but', 'not', 'able', 'to', 'sleep']\n",
      "After stop words removal: ['able', 'sleep']\n",
      "After stemming with porters algorithm: ['abl', 'sleep']\n",
      "Tokenized sentence: ['argh', 'my', 'g', 'is', 'spotty', 'anyway', 'the', 'only', 'thing', 'i', 'remember', 'from', 'the', 'research', 'we', 'did', 'was', 'that', 'province', 'and', 'sterling', 'were', 'the', 'only', 'problem', 'free', 'places', 'we', 'looked', 'at']\n",
      "After stop words removal: ['argh', 'g', 'spotty', 'anyway', 'thing', 'remember', 'research', 'province', 'sterling', 'problem', 'free', 'places', 'looked']\n",
      "sterl\n",
      "After stemming with porters algorithm: ['argh', 'spotti', 'anywai', 'thing', 'rememb', 'research', 'provinc', 'sterl', 'problem', 'free', 'place', 'look']\n",
      "Tokenized sentence: ['todays', 'vodafone', 'numbers', 'ending', 'with', 'are', 'selected', 'to', 'a', 'receive', 'a', 'award', 'if', 'your', 'number', 'matches', 'call', 'to', 'receive', 'your', 'award']\n",
      "After stop words removal: ['todays', 'vodafone', 'numbers', 'ending', 'selected', 'receive', 'award', 'number', 'matches', 'call', 'receive', 'award']\n",
      "end\n",
      "After stemming with porters algorithm: ['todai', 'vodafon', 'number', 'en', 'selec', 'receiv', 'award', 'number', 'match', 'call', 'receiv', 'award']\n",
      "Tokenized sentence: ['anything', 'lar']\n",
      "After stop words removal: ['anything', 'lar']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'lar']\n",
      "Tokenized sentence: ['oh', 'ya', 'got', 'hip', 'hop', 'open', 'haha', 'i', 'was', 'thinking', 'can', 'go', 'for', 'jazz', 'then', 'zoom', 'to', 'cine', 'actually', 'tonight', 'i', 'm', 'free', 'leh', 'and', 'there', 's', 'a', 'kb', 'lesson', 'tonight']\n",
      "After stop words removal: ['oh', 'ya', 'got', 'hip', 'hop', 'open', 'haha', 'thinking', 'go', 'jazz', 'zoom', 'cine', 'actually', 'tonight', 'free', 'leh', 'kb', 'lesson', 'tonight']\n",
      "think\n",
      "After stemming with porters algorithm: ['got', 'hip', 'hop', 'open', 'haha', 'thin', 'jazz', 'zoom', 'cine', 'actual', 'tonight', 'free', 'leh', 'lesson', 'tonight']\n",
      "Tokenized sentence: ['its', 'ok', 'i', 'just', 'askd', 'did', 'u', 'knw', 'tht', 'no']\n",
      "After stop words removal: ['ok', 'askd', 'u', 'knw', 'tht']\n",
      "After stemming with porters algorithm: ['askd', 'knw', 'tht']\n",
      "Tokenized sentence: ['mum', 'hope', 'you', 'are', 'having', 'a', 'great', 'day', 'hoping', 'this', 'text', 'meets', 'you', 'well', 'and', 'full', 'of', 'life', 'have', 'a', 'great', 'day', 'abiola']\n",
      "After stop words removal: ['mum', 'hope', 'great', 'day', 'hoping', 'text', 'meets', 'well', 'full', 'life', 'great', 'day', 'abiola']\n",
      "hop\n",
      "After stemming with porters algorithm: ['mum', 'hope', 'great', 'dai', 'hope', 'text', 'meet', 'well', 'full', 'life', 'great', 'dai', 'abiola']\n",
      "Tokenized sentence: ['one', 'of', 'best', 'dialogue', 'in', 'cute', 'reltnship', 'wen', 'i', 'die']\n",
      "After stop words removal: ['one', 'best', 'dialogue', 'cute', 'reltnship', 'wen', 'die']\n",
      "After stemming with porters algorithm: ['on', 'best', 'dialogu', 'cute', 'reltnship', 'wen', 'die']\n",
      "Tokenized sentence: ['cos', 'i', 'want', 'it', 'to', 'be', 'your', 'thing']\n",
      "After stop words removal: ['cos', 'want', 'thing']\n",
      "After stemming with porters algorithm: ['co', 'want', 'thing']\n",
      "Tokenized sentence: ['awesome', 'plan', 'to', 'get', 'here', 'any', 'time', 'after', 'like', 'lt', 'gt', 'i', 'll', 'text', 'you', 'details', 'in', 'a', 'wee', 'bit']\n",
      "After stop words removal: ['awesome', 'plan', 'get', 'time', 'like', 'lt', 'gt', 'text', 'details', 'wee', 'bit']\n",
      "After stemming with porters algorithm: ['awesom', 'plan', 'get', 'time', 'like', 'text', 'detail', 'wee', 'bit']\n",
      "Tokenized sentence: ['u', 've', 'been', 'selected', 'to', 'stay', 'in', 'of', 'top', 'british', 'hotels', 'for', 'nothing', 'holiday', 'valued', 'at', 'dial', 'to', 'claim', 'national', 'rate', 'call', 'bx', 'sw', 'ss']\n",
      "After stop words removal: ['u', 'selected', 'stay', 'top', 'british', 'hotels', 'nothing', 'holiday', 'valued', 'dial', 'claim', 'national', 'rate', 'call', 'bx', 'sw', 'ss']\n",
      "noth\n",
      "After stemming with porters algorithm: ['selec', 'stai', 'top', 'british', 'hotel', 'not', 'holidai', 'valu', 'dial', 'claim', 'nat', 'rate', 'call']\n",
      "Tokenized sentence: ['or', 'remind', 'me', 'in', 'a', 'few', 'hrs']\n",
      "After stop words removal: ['remind', 'hrs']\n",
      "After stemming with porters algorithm: ['remind', 'hr']\n",
      "Tokenized sentence: ['yo', 'the', 'game', 'almost', 'over', 'want', 'to', 'go', 'to', 'walmart', 'soon']\n",
      "After stop words removal: ['yo', 'game', 'almost', 'want', 'go', 'walmart', 'soon']\n",
      "After stemming with porters algorithm: ['game', 'almost', 'want', 'walmart', 'soon']\n",
      "Tokenized sentence: ['yeah', 'sure', 'give', 'me', 'a', 'couple', 'minutes', 'to', 'track', 'down', 'my', 'wallet']\n",
      "After stop words removal: ['yeah', 'sure', 'give', 'couple', 'minutes', 'track', 'wallet']\n",
      "After stemming with porters algorithm: ['yeah', 'sure', 'give', 'coupl', 'minut', 'track', 'wallet']\n",
      "Tokenized sentence: ['have', 'you', 'laid', 'your', 'airtel', 'line', 'to', 'rest']\n",
      "After stop words removal: ['laid', 'airtel', 'line', 'rest']\n",
      "After stemming with porters algorithm: ['laid', 'airtel', 'line', 'rest']\n",
      "Tokenized sentence: ['good', 'luck', 'draw', 'takes', 'place', 'th', 'feb', 'good', 'luck', 'for', 'removal', 'send', 'stop', 'to', 'customer', 'services']\n",
      "After stop words removal: ['good', 'luck', 'draw', 'takes', 'place', 'th', 'feb', 'good', 'luck', 'removal', 'send', 'stop', 'customer', 'services']\n",
      "After stemming with porters algorithm: ['good', 'luck', 'draw', 'take', 'place', 'feb', 'good', 'luck', 'remov', 'send', 'stop', 'custom', 'servic']\n",
      "Tokenized sentence: ['anything']\n",
      "After stop words removal: ['anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt']\n",
      "Tokenized sentence: ['what', 'time', 'u', 'wrkin']\n",
      "After stop words removal: ['time', 'u', 'wrkin']\n",
      "After stemming with porters algorithm: ['time', 'wrkin']\n",
      "Tokenized sentence: ['do', 'u', 'want', 'meet', 'up', 'morro']\n",
      "After stop words removal: ['u', 'want', 'meet', 'morro']\n",
      "After stemming with porters algorithm: ['want', 'meet', 'morro']\n",
      "Tokenized sentence: ['sports', 'fans', 'get', 'the', 'latest', 'sports', 'news', 'str', 'ur', 'mobile', 'wk', 'free', 'plus', 'a', 'free', 'tone', 'txt', 'sport', 'on', 'to', 'www', 'getzed', 'co', 'uk', 'norm', 'txt', 'p']\n",
      "After stop words removal: ['sports', 'fans', 'get', 'latest', 'sports', 'news', 'str', 'ur', 'mobile', 'wk', 'free', 'plus', 'free', 'tone', 'txt', 'sport', 'www', 'getzed', 'co', 'uk', 'norm', 'txt', 'p']\n",
      "After stemming with porters algorithm: ['sport', 'fan', 'get', 'latest', 'sport', 'new', 'str', 'mobil', 'free', 'plu', 'free', 'tone', 'txt', 'sport', 'www', 'getz', 'norm', 'txt']\n",
      "Tokenized sentence: ['so', 'that', 'takes', 'away', 'some', 'money', 'worries']\n",
      "After stop words removal: ['takes', 'away', 'money', 'worries']\n",
      "After stemming with porters algorithm: ['take', 'awai', 'monei', 'worri']\n",
      "Tokenized sentence: ['thanks', 'for', 've', 'lovely', 'wisheds', 'you', 'rock']\n",
      "After stop words removal: ['thanks', 'lovely', 'wisheds', 'rock']\n",
      "After stemming with porters algorithm: ['thank', 'love', 'wis', 'rock']\n",
      "Tokenized sentence: ['ok', 'is', 'any', 'problem', 'to', 'u', 'frm', 'him', 'wats', 'matter']\n",
      "After stop words removal: ['ok', 'problem', 'u', 'frm', 'wats', 'matter']\n",
      "After stemming with porters algorithm: ['problem', 'frm', 'wat', 'matter']\n",
      "Tokenized sentence: ['free', 'st', 'week', 'entry', 'textpod', 'a', 'chance', 'win', 'gb', 'ipod', 'or', 'cash', 'every', 'wk', 'txt', 'vpod', 'to', 'ts', 'cs', 'www', 'textpod', 'net', 'custcare']\n",
      "After stop words removal: ['free', 'st', 'week', 'entry', 'textpod', 'chance', 'win', 'gb', 'ipod', 'cash', 'every', 'wk', 'txt', 'vpod', 'ts', 'cs', 'www', 'textpod', 'net', 'custcare']\n",
      "After stemming with porters algorithm: ['free', 'week', 'entri', 'textpod', 'chanc', 'win', 'ipod', 'cash', 'everi', 'txt', 'vpod', 'www', 'textpod', 'net', 'custcar']\n",
      "Tokenized sentence: ['dear', 'are', 'you', 'angry', 'i', 'was', 'busy', 'dear']\n",
      "After stop words removal: ['dear', 'angry', 'busy', 'dear']\n",
      "After stemming with porters algorithm: ['dear', 'angri', 'busi', 'dear']\n",
      "Tokenized sentence: ['can', 'you', 'please', 'ask', 'macho', 'what', 'his', 'price', 'range', 'is', 'does', 'he', 'want', 'something', 'new', 'or', 'used', 'plus', 'it', 'he', 'only', 'interfued', 'in', 'the', 'blackberry', 'bold', 'lt', 'gt', 'or', 'any', 'bb']\n",
      "After stop words removal: ['please', 'ask', 'macho', 'price', 'range', 'want', 'something', 'new', 'used', 'plus', 'interfued', 'blackberry', 'bold', 'lt', 'gt', 'bb']\n",
      "someth\n",
      "After stemming with porters algorithm: ['pleas', 'ask', 'macho', 'price', 'rang', 'want', 'somet', 'new', 'us', 'plu', 'interfu', 'blackberri', 'bold']\n",
      "Tokenized sentence: ['a', 'boy', 'was', 'late', 'home', 'his', 'father', 'power', 'of', 'frndship']\n",
      "After stop words removal: ['boy', 'late', 'home', 'father', 'power', 'frndship']\n",
      "After stemming with porters algorithm: ['boi', 'late', 'home', 'father', 'power', 'frndship']\n",
      "Tokenized sentence: ['i', 'don', 'know', 'account', 'details', 'i', 'will', 'ask', 'my', 'mom', 'and', 'send', 'you', 'my', 'mom', 'is', 'out', 'of', 'reach', 'now']\n",
      "After stop words removal: ['know', 'account', 'details', 'ask', 'mom', 'send', 'mom', 'reach']\n",
      "After stemming with porters algorithm: ['know', 'account', 'detail', 'ask', 'mom', 'send', 'mom', 'reach']\n",
      "Tokenized sentence: ['i', 'dont', 'know', 'why', 'she', 's', 'not', 'getting', 'your', 'messages']\n",
      "After stop words removal: ['dont', 'know', 'getting', 'messages']\n",
      "gett\n",
      "After stemming with porters algorithm: ['dont', 'know', 'get', 'messag']\n",
      "Tokenized sentence: ['urgent', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'contact', 'u', 'u', 'have', 'won', 'call', 'b', 't', 'csbcm', 'wc', 'n', 'xx', 'callcost', 'ppm', 'mobilesvary', 'max']\n",
      "After stop words removal: ['urgent', 'nd', 'attempt', 'contact', 'u', 'u', 'call', 'b', 'csbcm', 'wc', 'n', 'xx', 'callcost', 'ppm', 'mobilesvary', 'max']\n",
      "After stemming with porters algorithm: ['urgent', 'attempt', 'contact', 'call', 'csbcm', 'callcost', 'ppm', 'mobilesvari', 'max']\n",
      "Tokenized sentence: ['and', 'half', 'years', 'i', 'missed', 'your', 'friendship']\n",
      "After stop words removal: ['half', 'years', 'missed', 'friendship']\n",
      "After stemming with porters algorithm: ['half', 'year', 'miss', 'friendship']\n",
      "Tokenized sentence: ['by', 'the', 'way', 'i', 've', 'put', 'a', 'skip', 'right', 'outside', 'the', 'front', 'of', 'the', 'house', 'so', 'you', 'can', 'see', 'which', 'house', 'it', 'is', 'just', 'pull', 'up', 'before', 'it']\n",
      "After stop words removal: ['way', 'put', 'skip', 'right', 'outside', 'front', 'house', 'see', 'house', 'pull']\n",
      "After stemming with porters algorithm: ['wai', 'put', 'skip', 'right', 'outsid', 'front', 'hous', 'see', 'hous', 'pull']\n",
      "Tokenized sentence: ['er', 'yep', 'sure', 'props']\n",
      "After stop words removal: ['er', 'yep', 'sure', 'props']\n",
      "After stemming with porters algorithm: ['yep', 'sure', 'prop']\n",
      "Tokenized sentence: ['just', 'sent', 'again', 'do', 'you', 'scream', 'and', 'moan', 'in', 'bed', 'princess']\n",
      "After stop words removal: ['sent', 'scream', 'moan', 'bed', 'princess']\n",
      "After stemming with porters algorithm: ['sent', 'scream', 'moan', 'bed', 'princess']\n",
      "Tokenized sentence: ['most', 'of', 'the', 'tiime', 'when', 'i', 'don', 't', 'let', 'you', 'hug', 'me', 'it', 's', 'so', 'i', 'don', 't', 'break', 'into', 'tears']\n",
      "After stop words removal: ['tiime', 'let', 'hug', 'break', 'tears']\n",
      "After stemming with porters algorithm: ['tiim', 'let', 'hug', 'break', 'tear']\n",
      "Tokenized sentence: ['do', 'i', 'i', 'thought', 'i', 'put', 'it', 'back', 'in', 'the', 'box']\n",
      "After stop words removal: ['thought', 'put', 'back', 'box']\n",
      "After stemming with porters algorithm: ['thought', 'put', 'back', 'box']\n",
      "Tokenized sentence: ['you', 'have', 'an', 'important', 'customer', 'service', 'announcement', 'call', 'freephone', 'now']\n",
      "After stop words removal: ['important', 'customer', 'service', 'announcement', 'call', 'freephone']\n",
      "After stemming with porters algorithm: ['import', 'custom', 'servic', 'announc', 'call', 'freephon']\n",
      "Tokenized sentence: ['cool', 'so', 'how', 'come', 'you', 'havent', 'been', 'wined', 'and', 'dined', 'before']\n",
      "After stop words removal: ['cool', 'come', 'havent', 'wined', 'dined']\n",
      "After stemming with porters algorithm: ['cool', 'come', 'havent', 'wine', 'dine']\n",
      "Tokenized sentence: ['dear', 'take', 'care', 'i', 'am', 'just', 'reaching', 'home', 'love', 'u', 'a', 'lot']\n",
      "After stop words removal: ['dear', 'take', 'care', 'reaching', 'home', 'love', 'u', 'lot']\n",
      "reach\n",
      "After stemming with porters algorithm: ['dear', 'take', 'care', 'reac', 'home', 'love', 'lot']\n",
      "Tokenized sentence: ['good', 'afternoon', 'babe', 'how', 'goes', 'that', 'day', 'any', 'job', 'prospects', 'yet', 'i', 'miss', 'you', 'my', 'love', 'sighs']\n",
      "After stop words removal: ['good', 'afternoon', 'babe', 'goes', 'day', 'job', 'prospects', 'yet', 'miss', 'love', 'sighs']\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'babe', 'goe', 'dai', 'job', 'prospect', 'yet', 'miss', 'love', 'sigh']\n",
      "Tokenized sentence: ['i', 'prefer', 'my', 'free', 'days', 'tues', 'wed', 'fri', 'oso', 'can', 'ask', 'those', 'workin', 'lor']\n",
      "After stop words removal: ['prefer', 'free', 'days', 'tues', 'wed', 'fri', 'oso', 'ask', 'workin', 'lor']\n",
      "After stemming with porters algorithm: ['prefer', 'free', 'dai', 'tue', 'wed', 'fri', 'oso', 'ask', 'workin', 'lor']\n",
      "Tokenized sentence: ['all', 'e', 'best', 'ur', 'exam', 'later']\n",
      "After stop words removal: ['e', 'best', 'ur', 'exam', 'later']\n",
      "After stemming with porters algorithm: ['best', 'exam', 'later']\n",
      "Tokenized sentence: ['e', 'admin', 'building', 'there', 'i', 'might', 'b', 'slightly', 'earlier', 'i', 'll', 'call', 'u', 'when', 'i', 'm', 'reaching']\n",
      "After stop words removal: ['e', 'admin', 'building', 'might', 'b', 'slightly', 'earlier', 'call', 'u', 'reaching']\n",
      "build\n",
      "reach\n",
      "After stemming with porters algorithm: ['admin', 'buil', 'might', 'slightli', 'earlier', 'call', 'reac']\n",
      "Tokenized sentence: ['have', 'you', 'emigrated', 'or', 'something', 'ok', 'maybe', 'was', 'a', 'bit', 'hopeful']\n",
      "After stop words removal: ['emigrated', 'something', 'ok', 'maybe', 'bit', 'hopeful']\n",
      "emigrate\n",
      "someth\n",
      "After stemming with porters algorithm: ['emigr', 'somet', 'mayb', 'bit', 'hope']\n",
      "Tokenized sentence: ['now', 'press', 'conference', 'da']\n",
      "After stop words removal: ['press', 'conference', 'da']\n",
      "After stemming with porters algorithm: ['press', 'confer']\n",
      "Tokenized sentence: ['yeah', 'where', 's', 'your', 'class', 'at']\n",
      "After stop words removal: ['yeah', 'class']\n",
      "After stemming with porters algorithm: ['yeah', 'class']\n",
      "Tokenized sentence: ['u', 'coming', 'pick', 'me']\n",
      "After stop words removal: ['u', 'coming', 'pick']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'pick']\n",
      "Tokenized sentence: ['you', 'll', 'never', 'believe', 'this', 'but', 'i', 'have', 'actually', 'got', 'off', 'at', 'taunton', 'wow']\n",
      "After stop words removal: ['never', 'believe', 'actually', 'got', 'taunton', 'wow']\n",
      "After stemming with porters algorithm: ['never', 'believ', 'actual', 'got', 'taunton', 'wow']\n",
      "Tokenized sentence: ['rats', 'hey', 'did', 'u', 'ever', 'vote', 'for', 'the', 'next', 'themes']\n",
      "After stop words removal: ['rats', 'hey', 'u', 'ever', 'vote', 'next', 'themes']\n",
      "After stemming with porters algorithm: ['rat', 'hei', 'ever', 'vote', 'next', 'theme']\n",
      "Tokenized sentence: ['that', 's', 'cool', 'i', 'll', 'come', 'by', 'like', 'lt', 'gt', 'ish']\n",
      "After stop words removal: ['cool', 'come', 'like', 'lt', 'gt', 'ish']\n",
      "After stemming with porters algorithm: ['cool', 'come', 'like', 'ish']\n",
      "Tokenized sentence: ['six', 'chances', 'to', 'win', 'cash', 'from', 'to', 'pounds', 'txt', 'csh', 'and', 'send', 'to', 'cost', 'p', 'day', 'days', 'tsandcs', 'apply', 'reply', 'hl', 'info']\n",
      "After stop words removal: ['six', 'chances', 'win', 'cash', 'pounds', 'txt', 'csh', 'send', 'cost', 'p', 'day', 'days', 'tsandcs', 'apply', 'reply', 'hl', 'info']\n",
      "After stemming with porters algorithm: ['six', 'chanc', 'win', 'cash', 'pound', 'txt', 'csh', 'send', 'cost', 'dai', 'dai', 'tsandc', 'appli', 'repli', 'info']\n",
      "Tokenized sentence: ['dunno', 'he', 'jus', 'say', 'go', 'lido', 'same', 'time']\n",
      "After stop words removal: ['dunno', 'jus', 'say', 'go', 'lido', 'time']\n",
      "After stemming with porters algorithm: ['dunno', 'ju', 'sai', 'lido', 'time']\n",
      "Tokenized sentence: ['oh', 'fine', 'i', 'll', 'be', 'by', 'tonight']\n",
      "After stop words removal: ['oh', 'fine', 'tonight']\n",
      "After stemming with porters algorithm: ['fine', 'tonight']\n",
      "Tokenized sentence: ['what', 'do', 'u', 'want', 'for', 'xmas', 'how', 'about', 'free', 'text', 'messages', 'a', 'new', 'video', 'phone', 'with', 'half', 'price', 'line', 'rental', 'call', 'free', 'now', 'on', 'to', 'find', 'out', 'more']\n",
      "After stop words removal: ['u', 'want', 'xmas', 'free', 'text', 'messages', 'new', 'video', 'phone', 'half', 'price', 'line', 'rental', 'call', 'free', 'find']\n",
      "After stemming with porters algorithm: ['want', 'xma', 'free', 'text', 'messag', 'new', 'video', 'phone', 'half', 'price', 'line', 'rental', 'call', 'free', 'find']\n",
      "Tokenized sentence: ['raji', 'pls', 'do', 'me', 'a', 'favour', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimya', 'pls', 'today', 'is', 'her', 'birthday']\n",
      "After stop words removal: ['raji', 'pls', 'favour', 'pls', 'convey', 'birthday', 'wishes', 'nimya', 'pls', 'today', 'birthday']\n",
      "After stemming with porters algorithm: ['raji', 'pl', 'favour', 'pl', 'convei', 'birthdai', 'wish', 'nimya', 'pl', 'todai', 'birthdai']\n",
      "Tokenized sentence: ['ok', 'no', 'problem', 'yup', 'i', 'm', 'going', 'to', 'sch', 'at', 'if', 'i', 'rem', 'correctly']\n",
      "After stop words removal: ['ok', 'problem', 'yup', 'going', 'sch', 'rem', 'correctly']\n",
      "go\n",
      "After stemming with porters algorithm: ['problem', 'yup', 'go', 'sch', 'rem', 'correctli']\n",
      "Tokenized sentence: ['tyler', 'getting', 'an', 'th', 'has', 'to', 'leave', 'not', 'long', 'after', 'can', 'you', 'get', 'here', 'in', 'like', 'an', 'hour']\n",
      "After stop words removal: ['tyler', 'getting', 'th', 'leave', 'long', 'get', 'like', 'hour']\n",
      "gett\n",
      "After stemming with porters algorithm: ['tyler', 'get', 'leav', 'long', 'get', 'like', 'hour']\n",
      "Tokenized sentence: ['ah', 'well', 'that', 'confuses', 'things', 'doesn', 't', 'it']\n",
      "After stop words removal: ['ah', 'well', 'confuses', 'things']\n",
      "After stemming with porters algorithm: ['well', 'confus', 'thing']\n",
      "Tokenized sentence: ['nothing', 'i', 'got', 'msg', 'frm', 'tht', 'unknown', 'no']\n",
      "After stop words removal: ['nothing', 'got', 'msg', 'frm', 'tht', 'unknown']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not', 'got', 'msg', 'frm', 'tht', 'unknown']\n",
      "Tokenized sentence: ['when', 'i', 'have', 'stuff', 'to', 'sell', 'i', 'll', 'tell', 'you']\n",
      "After stop words removal: ['stuff', 'sell', 'tell']\n",
      "After stemming with porters algorithm: ['stuff', 'sell', 'tell']\n",
      "Tokenized sentence: ['go', 'where', 'n', 'buy', 'juz', 'buy', 'when', 'we', 'get', 'there', 'lar']\n",
      "After stop words removal: ['go', 'n', 'buy', 'juz', 'buy', 'get', 'lar']\n",
      "After stemming with porters algorithm: ['bui', 'juz', 'bui', 'get', 'lar']\n",
      "Tokenized sentence: ['k', 'k', 'how', 'much', 'does', 'it', 'cost']\n",
      "After stop words removal: ['k', 'k', 'much', 'cost']\n",
      "After stemming with porters algorithm: ['much', 'cost']\n",
      "Tokenized sentence: ['what', 'about', 'this', 'one', 'then']\n",
      "After stop words removal: ['one']\n",
      "After stemming with porters algorithm: ['on']\n",
      "Tokenized sentence: ['ok', 'lor']\n",
      "After stop words removal: ['ok', 'lor']\n",
      "After stemming with porters algorithm: ['lor']\n",
      "Tokenized sentence: ['are', 'you', 'plans', 'with', 'your', 'family', 'set', 'in', 'stone']\n",
      "After stop words removal: ['plans', 'family', 'set', 'stone']\n",
      "After stemming with porters algorithm: ['plan', 'famili', 'set', 'stone']\n",
      "Tokenized sentence: ['two', 'teams', 'waiting', 'for', 'some', 'players']\n",
      "After stop words removal: ['two', 'teams', 'waiting', 'players']\n",
      "wait\n",
      "After stemming with porters algorithm: ['two', 'team', 'wait', 'player']\n",
      "Tokenized sentence: ['r', 'u', 'still', 'working', 'now']\n",
      "After stop words removal: ['r', 'u', 'still', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['still', 'wor']\n",
      "Tokenized sentence: ['where', 'are', 'you', 'call', 'me']\n",
      "After stop words removal: ['call']\n",
      "After stemming with porters algorithm: ['call']\n",
      "Tokenized sentence: ['tonight', 'yeah', 'i', 'd', 'be', 'down', 'for', 'that']\n",
      "After stop words removal: ['tonight', 'yeah']\n",
      "After stemming with porters algorithm: ['tonight', 'yeah']\n",
      "Tokenized sentence: ['whatsup', 'there', 'dont', 'u', 'want', 'to', 'sleep']\n",
      "After stop words removal: ['whatsup', 'dont', 'u', 'want', 'sleep']\n",
      "After stemming with porters algorithm: ['whatsup', 'dont', 'want', 'sleep']\n",
      "Tokenized sentence: ['k', 'do', 'i', 'need', 'a', 'login', 'or', 'anything']\n",
      "After stop words removal: ['k', 'need', 'login', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['need', 'login', 'anyt']\n",
      "Tokenized sentence: ['shall', 'i', 'come', 'to', 'get', 'pickle']\n",
      "After stop words removal: ['shall', 'come', 'get', 'pickle']\n",
      "After stemming with porters algorithm: ['shall', 'come', 'get', 'pickl']\n",
      "Tokenized sentence: ['my', 'parents', 'my', 'kidz', 'my', 'friends', 'n', 'my', 'colleagues', 'all', 'screaming', 'surprise', 'and', 'i', 'was', 'waiting', 'on', 'the', 'sofa', 'naked']\n",
      "After stop words removal: ['parents', 'kidz', 'friends', 'n', 'colleagues', 'screaming', 'surprise', 'waiting', 'sofa', 'naked']\n",
      "scream\n",
      "wait\n",
      "After stemming with porters algorithm: ['parent', 'kidz', 'friend', 'colleagu', 'scream', 'surpris', 'wait', 'sofa', 'nake']\n",
      "Tokenized sentence: ['oh', 'rite', 'well', 'im', 'with', 'my', 'best', 'mate', 'pete', 'who', 'i', 'went', 'out', 'with', 'a', 'week', 'now', 'were', 'geva', 'again', 'its', 'been', 'longer', 'than', 'a', 'week']\n",
      "After stop words removal: ['oh', 'rite', 'well', 'im', 'best', 'mate', 'pete', 'went', 'week', 'geva', 'longer', 'week']\n",
      "After stemming with porters algorithm: ['rite', 'well', 'best', 'mate', 'pete', 'went', 'week', 'geva', 'longer', 'week']\n",
      "Tokenized sentence: ['someone', 'u', 'know', 'has', 'asked', 'our', 'dating', 'service', 'contact', 'you', 'cant', 'guess', 'who', 'call', 'now', 'all', 'will', 'be', 'revealed', 'po', 'box', 'm', 'wu']\n",
      "After stop words removal: ['someone', 'u', 'know', 'asked', 'dating', 'service', 'contact', 'cant', 'guess', 'call', 'revealed', 'po', 'box', 'wu']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someon', 'know', 'as', 'date', 'servic', 'contact', 'cant', 'guess', 'call', 'reveal', 'box']\n",
      "Tokenized sentence: ['don', 't', 'b', 'floppy', 'b', 'snappy', 'happy', 'only', 'gay', 'chat', 'service', 'with', 'photo', 'upload', 'call', 'p', 'min', 'stop', 'our', 'texts', 'call']\n",
      "After stop words removal: ['b', 'floppy', 'b', 'snappy', 'happy', 'gay', 'chat', 'service', 'photo', 'upload', 'call', 'p', 'min', 'stop', 'texts', 'call']\n",
      "After stemming with porters algorithm: ['floppi', 'snappi', 'happi', 'gai', 'chat', 'servic', 'photo', 'upload', 'call', 'min', 'stop', 'text', 'call']\n",
      "Tokenized sentence: ['dear', 'xxxxxxx', 'u', 've', 'been', 'invited', 'to', 'xchat', 'this', 'is', 'our', 'final', 'attempt', 'to', 'contact', 'u', 'txt', 'chat', 'to', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stop words removal: ['dear', 'xxxxxxx', 'u', 'invited', 'xchat', 'final', 'attempt', 'contact', 'u', 'txt', 'chat', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stemming with porters algorithm: ['dear', 'xxxxxxx', 'invit', 'xchat', 'final', 'attempt', 'contact', 'txt', 'chat', 'msgrcvdhg', 'suit', 'land', 'row', 'ldn', 'yr']\n",
      "Tokenized sentence: ['still', 'chance', 'there', 'if', 'you', 'search', 'hard', 'you', 'will', 'get', 'it', 'let', 'have', 'a', 'try']\n",
      "After stop words removal: ['still', 'chance', 'search', 'hard', 'get', 'let', 'try']\n",
      "After stemming with porters algorithm: ['still', 'chanc', 'search', 'hard', 'get', 'let', 'try']\n",
      "Tokenized sentence: ['i', 'will', 'vote', 'for', 'wherever', 'my', 'heart', 'guides', 'me']\n",
      "After stop words removal: ['vote', 'wherever', 'heart', 'guides']\n",
      "After stemming with porters algorithm: ['vote', 'wherev', 'heart', 'guid']\n",
      "Tokenized sentence: ['hello', 'as', 'per', 'request', 'from', 'lt', 'gt', 'rs', 'has', 'been', 'transfered', 'to', 'you']\n",
      "After stop words removal: ['hello', 'per', 'request', 'lt', 'gt', 'rs', 'transfered']\n",
      "After stemming with porters algorithm: ['hello', 'per', 'request', 'transfer']\n",
      "Tokenized sentence: ['oh', 'icic', 'k', 'lor', 'den', 'meet', 'other', 'day']\n",
      "After stop words removal: ['oh', 'icic', 'k', 'lor', 'den', 'meet', 'day']\n",
      "After stemming with porters algorithm: ['icic', 'lor', 'den', 'meet', 'dai']\n",
      "Tokenized sentence: ['hey', 'mate', 'spoke', 'to', 'the', 'mag', 'people', 'we', 're', 'on', 'the', 'is', 'deliver', 'by', 'the', 'end', 'of', 'the', 'month', 'deliver', 'on', 'the', 'th', 'sept', 'talk', 'later']\n",
      "After stop words removal: ['hey', 'mate', 'spoke', 'mag', 'people', 'deliver', 'end', 'month', 'deliver', 'th', 'sept', 'talk', 'later']\n",
      "After stemming with porters algorithm: ['hei', 'mate', 'spoke', 'mag', 'peopl', 'deliv', 'end', 'month', 'deliv', 'sept', 'talk', 'later']\n",
      "Tokenized sentence: ['ryder', 'unsold', 'now', 'gibbs']\n",
      "After stop words removal: ['ryder', 'unsold', 'gibbs']\n",
      "After stemming with porters algorithm: ['ryder', 'unsold', 'gibb']\n",
      "Tokenized sentence: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mobile', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stop words removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mobile', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stemming with porters algorithm: ['free', 'week', 'nokia', 'tone', 'mobil', 'everi', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'mate', 'www', 'getz', 'pobox', 'norm', 'tone']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'guaranteed', 'award', 'or', 'even', 'cashto', 'claim', 'ur', 'award', 'call', 'free', 'on', 'stop', 'getstop', 'on', 'php', 'rg', 'jx']\n",
      "After stop words removal: ['guaranteed', 'award', 'even', 'cashto', 'claim', 'ur', 'award', 'call', 'free', 'stop', 'getstop', 'php', 'rg', 'jx']\n",
      "After stemming with porters algorithm: ['guaranteed', 'award', 'even', 'cashto', 'claim', 'award', 'call', 'free', 'stop', 'getstop', 'php']\n",
      "Tokenized sentence: ['joy', 's', 'father', 'is', 'john', 'then', 'john', 'is', 'the', 'name', 'of', 'joy', 's', 'father', 'mandan']\n",
      "After stop words removal: ['joy', 'father', 'john', 'john', 'name', 'joy', 'father', 'mandan']\n",
      "After stemming with porters algorithm: ['joi', 'father', 'john', 'john', 'name', 'joi', 'father', 'mandan']\n",
      "Tokenized sentence: ['hello', 'beautiful', 'r', 'u', 'ok', 'i', 've', 'kinda', 'ad', 'a', 'row', 'wiv', 'and', 'he', 'walked', 'out', 'the', 'pub', 'i', 'wanted', 'a', 'night', 'wiv', 'u', 'miss', 'u']\n",
      "After stop words removal: ['hello', 'beautiful', 'r', 'u', 'ok', 'kinda', 'ad', 'row', 'wiv', 'walked', 'pub', 'wanted', 'night', 'wiv', 'u', 'miss', 'u']\n",
      "After stemming with porters algorithm: ['hello', 'beauti', 'kinda', 'row', 'wiv', 'wal', 'pub', 'wan', 'night', 'wiv', 'miss']\n",
      "Tokenized sentence: ['no', 'problem', 'we', 'will', 'be', 'spending', 'a', 'lot', 'of', 'quality', 'time', 'together']\n",
      "After stop words removal: ['problem', 'spending', 'lot', 'quality', 'time', 'together']\n",
      "spend\n",
      "After stemming with porters algorithm: ['problem', 'spen', 'lot', 'qualiti', 'time', 'togeth']\n",
      "Tokenized sentence: ['storming', 'msg', 'wen', 'u', 'lift', 'd', 'phne', 'u', 'say', 'hello', 'do', 'u', 'knw', 'wt', 'is', 'd', 'real', 'meaning', 'of', 'hello', 'it', 's', 'd', 'name', 'of', 'a', 'girl', 'yes', 'and', 'u', 'knw', 'who', 'is', 'dat', 'girl', 'margaret', 'hello', 'she', 'is', 'd', 'girlfrnd', 'f', 'grahmbell', 'who', 'invnted', 'telphone', 'moral', 'one', 'can', 'get', 'd', 'name', 'of', 'a', 'person']\n",
      "After stop words removal: ['storming', 'msg', 'wen', 'u', 'lift', 'phne', 'u', 'say', 'hello', 'u', 'knw', 'wt', 'real', 'meaning', 'hello', 'name', 'girl', 'yes', 'u', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'f', 'grahmbell', 'invnted', 'telphone', 'moral', 'one', 'get', 'name', 'person']\n",
      "storm\n",
      "mean\n",
      "After stemming with porters algorithm: ['stor', 'msg', 'wen', 'lift', 'phne', 'sai', 'hello', 'knw', 'real', 'mean', 'hello', 'name', 'girl', 'ye', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'grahmbel', 'invn', 'telphon', 'moral', 'on', 'get', 'name', 'person']\n",
      "Tokenized sentence: ['winner', 'as', 'a', 'valued', 'network', 'customer', 'you', 'hvae', 'been', 'selected', 'to', 'receive', 'a', 'reward', 'to', 'collect', 'call', 'valid', 'hours', 'only', 'acl', 'pm']\n",
      "After stop words removal: ['winner', 'valued', 'network', 'customer', 'hvae', 'selected', 'receive', 'reward', 'collect', 'call', 'valid', 'hours', 'acl', 'pm']\n",
      "After stemming with porters algorithm: ['winner', 'valu', 'network', 'custom', 'hvae', 'selec', 'receiv', 'reward', 'collect', 'call', 'valid', 'hour', 'acl']\n",
      "Tokenized sentence: ['i', 've', 'told', 'him', 'that', 'i', 've', 'returned', 'it', 'that', 'should', 'i', 're', 'order', 'it']\n",
      "After stop words removal: ['told', 'returned', 'order']\n",
      "After stemming with porters algorithm: ['told', 'retur', 'order']\n",
      "Tokenized sentence: ['ok', 'ill', 'send', 'you', 'with', 'in', 'lt', 'decimal', 'gt', 'ok']\n",
      "After stop words removal: ['ok', 'ill', 'send', 'lt', 'decimal', 'gt', 'ok']\n",
      "After stemming with porters algorithm: ['ill', 'send', 'decim']\n",
      "Tokenized sentence: ['sen', 'told', 'that', 'he', 'is', 'going', 'to', 'join', 'his', 'uncle', 'finance', 'in', 'cbe']\n",
      "After stop words removal: ['sen', 'told', 'going', 'join', 'uncle', 'finance', 'cbe']\n",
      "go\n",
      "After stemming with porters algorithm: ['sen', 'told', 'go', 'join', 'uncl', 'financ', 'cbe']\n",
      "Tokenized sentence: ['can', 'you', 'let', 'me', 'know', 'details', 'of', 'fri', 'when', 'u', 'find', 'out', 'cos', 'i', 'm', 'not', 'in', 'tom', 'or', 'fri', 'mentionned', 'chinese', 'thanks']\n",
      "After stop words removal: ['let', 'know', 'details', 'fri', 'u', 'find', 'cos', 'tom', 'fri', 'mentionned', 'chinese', 'thanks']\n",
      "After stemming with porters algorithm: ['let', 'know', 'detail', 'fri', 'find', 'co', 'tom', 'fri', 'ment', 'chines', 'thank']\n",
      "Tokenized sentence: ['yes', 'it', 'completely', 'in', 'out', 'of', 'form', 'clark', 'also', 'utter', 'waste']\n",
      "After stop words removal: ['yes', 'completely', 'form', 'clark', 'also', 'utter', 'waste']\n",
      "After stemming with porters algorithm: ['ye', 'complet', 'form', 'clark', 'also', 'utter', 'wast']\n",
      "Tokenized sentence: ['i', 'was', 'slept', 'that', 'time', 'you', 'there']\n",
      "After stop words removal: ['slept', 'time']\n",
      "After stemming with porters algorithm: ['slept', 'time']\n",
      "Tokenized sentence: ['hi', 'shanil', 'rakhesh', 'here', 'thanks', 'i', 'have', 'exchanged', 'the', 'uncut', 'diamond', 'stuff', 'leaving', 'back', 'excellent', 'service', 'by', 'dino', 'and', 'prem']\n",
      "After stop words removal: ['hi', 'shanil', 'rakhesh', 'thanks', 'exchanged', 'uncut', 'diamond', 'stuff', 'leaving', 'back', 'excellent', 'service', 'dino', 'prem']\n",
      "leav\n",
      "After stemming with porters algorithm: ['shanil', 'rakhesh', 'thank', 'exchan', 'uncut', 'diamond', 'stuff', 'leav', 'back', 'excel', 'servic', 'dino', 'prem']\n",
      "Tokenized sentence: ['yo', 'dude', 'guess', 'who', 'just', 'got', 'arrested', 'the', 'other', 'day']\n",
      "After stop words removal: ['yo', 'dude', 'guess', 'got', 'arrested', 'day']\n",
      "After stemming with porters algorithm: ['dude', 'guess', 'got', 'arres', 'dai']\n",
      "Tokenized sentence: ['was', 'it', 'something', 'u', 'ate']\n",
      "After stop words removal: ['something', 'u', 'ate']\n",
      "someth\n",
      "After stemming with porters algorithm: ['somet', 'at']\n",
      "Tokenized sentence: ['noooooooo', 'please', 'last', 'thing', 'i', 'need', 'is', 'stress', 'for', 'once', 'in', 'your', 'life', 'be', 'fair']\n",
      "After stop words removal: ['noooooooo', 'please', 'last', 'thing', 'need', 'stress', 'life', 'fair']\n",
      "After stemming with porters algorithm: ['noooooooo', 'pleas', 'last', 'thing', 'need', 'stress', 'life', 'fair']\n",
      "Tokenized sentence: ['wait', 'min', 'stand', 'at', 'bus', 'stop']\n",
      "After stop words removal: ['wait', 'min', 'stand', 'bus', 'stop']\n",
      "After stemming with porters algorithm: ['wait', 'min', 'stand', 'bu', 'stop']\n",
      "Tokenized sentence: ['he', 'said', 'that', 'he', 'had', 'a', 'right', 'giggle', 'when', 'he', 'saw', 'u', 'again', 'you', 'would', 'possibly', 'be', 'the', 'first', 'person', 'die', 'from', 'nvq', 'but', 'think', 'how', 'much', 'you', 'could', 'for']\n",
      "After stop words removal: ['said', 'right', 'giggle', 'saw', 'u', 'would', 'possibly', 'first', 'person', 'die', 'nvq', 'think', 'much', 'could']\n",
      "After stemming with porters algorithm: ['said', 'right', 'giggl', 'saw', 'would', 'possibli', 'first', 'person', 'die', 'nvq', 'think', 'much', 'could']\n",
      "Tokenized sentence: ['now', 'thats', 'going', 'to', 'ruin', 'your', 'thesis']\n",
      "After stop words removal: ['thats', 'going', 'ruin', 'thesis']\n",
      "go\n",
      "After stemming with porters algorithm: ['that', 'go', 'ruin', 'thesi']\n",
      "Tokenized sentence: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'to', 'contact', 'u', 'u', 'have', 'won', 'the', 'prize', 'claim', 'is', 'easy', 'just', 'call', 'now', 'only', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['time', 'tri', 'contact', 'priz', 'claim', 'easi', 'call', 'per', 'minut', 'nat', 'rate']\n",
      "Tokenized sentence: ['but', 'i', 'm', 'really', 'really', 'broke', 'oh', 'no', 'amount', 'is', 'too', 'small', 'even', 'lt', 'gt']\n",
      "After stop words removal: ['really', 'really', 'broke', 'oh', 'amount', 'small', 'even', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['realli', 'realli', 'broke', 'amount', 'small', 'even']\n",
      "Tokenized sentence: ['friendship', 'is', 'not', 'a', 'game', 'to', 'play', 'it', 'is', 'not', 'a', 'word', 'to', 'say', 'it', 'doesn', 't', 'start', 'on', 'march', 'and', 'ends', 'on', 'may', 'it', 'is', 'tomorrow', 'yesterday', 'today', 'and', 'e']\n",
      "After stop words removal: ['friendship', 'game', 'play', 'word', 'say', 'start', 'march', 'ends', 'may', 'tomorrow', 'yesterday', 'today', 'e']\n",
      "After stemming with porters algorithm: ['friendship', 'game', 'plai', 'word', 'sai', 'start', 'march', 'end', 'mai', 'tomorrow', 'yesterdai', 'todai']\n",
      "Tokenized sentence: ['ok', 'i', 'm', 'waliking', 'ard', 'now', 'do', 'u', 'wan', 'me', 'buy', 'anything', 'go', 'ur', 'house']\n",
      "After stop words removal: ['ok', 'waliking', 'ard', 'u', 'wan', 'buy', 'anything', 'go', 'ur', 'house']\n",
      "walik\n",
      "anyth\n",
      "After stemming with porters algorithm: ['walik', 'ard', 'wan', 'bui', 'anyt', 'hous']\n",
      "Tokenized sentence: ['my', 'mobile', 'number', 'pls', 'sms', 'ur', 'mail', 'id', 'convey', 'regards', 'to', 'achan', 'amma', 'rakhesh', 'qatar']\n",
      "After stop words removal: ['mobile', 'number', 'pls', 'sms', 'ur', 'mail', 'id', 'convey', 'regards', 'achan', 'amma', 'rakhesh', 'qatar']\n",
      "After stemming with porters algorithm: ['mobil', 'number', 'pl', 'sm', 'mail', 'convei', 'regard', 'achan', 'amma', 'rakhesh', 'qatar']\n",
      "Tokenized sentence: ['he', 'didn', 't', 'see', 'his', 'shadow', 'we', 'get', 'an', 'early', 'spring', 'yay']\n",
      "After stop words removal: ['see', 'shadow', 'get', 'early', 'spring', 'yay']\n",
      "After stemming with porters algorithm: ['see', 'shadow', 'get', 'earli', 'spring', 'yai']\n",
      "Tokenized sentence: ['get', 'lions', 'england', 'tone', 'reply', 'lionm', 'mono', 'or', 'lionp', 'poly', 'more', 'go', 'www', 'ringtones', 'co', 'uk', 'the', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stop words removal: ['get', 'lions', 'england', 'tone', 'reply', 'lionm', 'mono', 'lionp', 'poly', 'go', 'www', 'ringtones', 'co', 'uk', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stemming with porters algorithm: ['get', 'lion', 'england', 'tone', 'repli', 'lionm', 'mono', 'lionp', 'poli', 'www', 'rington', 'origin', 'best', 'tone', 'gbp', 'network', 'oper', 'rate', 'appli']\n",
      "Tokenized sentence: ['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's']\n",
      "After stop words removal: ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'wkly', 'comp', 'win', 'cup', 'final', 'tkt', 'mai', 'text', 'receiv', 'entri', 'quest', 'std', 'txt', 'rate', 'appli']\n",
      "Tokenized sentence: ['we', 'have', 'sent', 'jd', 'for', 'customer', 'service', 'cum', 'accounts', 'executive', 'to', 'ur', 'mail', 'id', 'for', 'details', 'contact', 'us']\n",
      "After stop words removal: ['sent', 'jd', 'customer', 'service', 'cum', 'accounts', 'executive', 'ur', 'mail', 'id', 'details', 'contact', 'us']\n",
      "After stemming with porters algorithm: ['sent', 'custom', 'servic', 'cum', 'account', 'execut', 'mail', 'detail', 'contact']\n",
      "Tokenized sentence: ['hey', 'do', 'u', 'fancy', 'meetin', 'me', 'at', 'at', 'cha', 'hav', 'a', 'lil', 'beverage', 'on', 'me', 'if', 'not', 'txt', 'or', 'ring', 'me', 'and', 'we', 'can', 'meet', 'up', 'l', 'r', 'quite', 'tired', 'got', 'in', 'at', 'v', 'pist', 'love', 'pete', 'x', 'x', 'x']\n",
      "After stop words removal: ['hey', 'u', 'fancy', 'meetin', 'cha', 'hav', 'lil', 'beverage', 'txt', 'ring', 'meet', 'l', 'r', 'quite', 'tired', 'got', 'v', 'pist', 'love', 'pete', 'x', 'x', 'x']\n",
      "After stemming with porters algorithm: ['hei', 'fanci', 'meetin', 'cha', 'hav', 'lil', 'beverag', 'txt', 'ring', 'meet', 'quit', 'tire', 'got', 'pist', 'love', 'pete']\n",
      "Tokenized sentence: ['sorry', 'i', 'can', 't', 'help', 'you', 'on', 'this']\n",
      "After stop words removal: ['sorry', 'help']\n",
      "After stemming with porters algorithm: ['sorri', 'help']\n",
      "Tokenized sentence: ['nothing', 'smsing', 'u', 'n', 'xy', 'lor', 'sorry', 'lor', 'da', 'guys', 'neva', 'c', 'u', 'in', 'person', 'but', 'they', 'sort', 'of', 'know', 'u', 'lor', 'so', 'u', 'wan', 'meet', 'them', 'xy', 'ask', 'me', 'bring', 'u', 'along', 'our', 'next', 'meeting']\n",
      "After stop words removal: ['nothing', 'smsing', 'u', 'n', 'xy', 'lor', 'sorry', 'lor', 'da', 'guys', 'neva', 'c', 'u', 'person', 'sort', 'know', 'u', 'lor', 'u', 'wan', 'meet', 'xy', 'ask', 'bring', 'u', 'along', 'next', 'meeting']\n",
      "noth\n",
      "meet\n",
      "After stemming with porters algorithm: ['not', 'smsing', 'lor', 'sorri', 'lor', 'gui', 'neva', 'person', 'sort', 'know', 'lor', 'wan', 'meet', 'ask', 'bring', 'along', 'next', 'meet']\n",
      "Tokenized sentence: ['oh', 'shut', 'it', 'omg', 'yesterday', 'i', 'had', 'a', 'dream', 'that', 'i', 'had', 'kids', 'both', 'boys', 'i', 'was', 'so', 'pissed', 'not', 'only', 'about', 'the', 'kids', 'but', 'them', 'being', 'boys', 'i', 'even', 'told', 'mark', 'in', 'my', 'dream', 'that', 'he', 'was', 'changing', 'diapers', 'cause', 'i', 'm', 'not', 'getting', 'owed', 'in', 'the', 'face']\n",
      "After stop words removal: ['oh', 'shut', 'omg', 'yesterday', 'dream', 'kids', 'boys', 'pissed', 'kids', 'boys', 'even', 'told', 'mark', 'dream', 'changing', 'diapers', 'cause', 'getting', 'owed', 'face']\n",
      "chang\n",
      "gett\n",
      "After stemming with porters algorithm: ['shut', 'omg', 'yesterdai', 'dream', 'kid', 'boi', 'piss', 'kid', 'boi', 'even', 'told', 'mark', 'dream', 'chan', 'diaper', 'caus', 'get', 'ow', 'face']\n",
      "Tokenized sentence: ['hi', 'elaine', 'is', 'today', 's', 'meeting', 'confirmed']\n",
      "After stop words removal: ['hi', 'elaine', 'today', 'meeting', 'confirmed']\n",
      "meet\n",
      "After stemming with porters algorithm: ['elain', 'todai', 'meet', 'confir']\n",
      "Tokenized sentence: ['think', 'you', 'sent', 'the', 'text', 'to', 'the', 'home', 'phone', 'that', 'cant', 'display', 'texts', 'if', 'you', 'still', 'want', 'to', 'send', 'it', 'his', 'number', 'is']\n",
      "After stop words removal: ['think', 'sent', 'text', 'home', 'phone', 'cant', 'display', 'texts', 'still', 'want', 'send', 'number']\n",
      "After stemming with porters algorithm: ['think', 'sent', 'text', 'home', 'phone', 'cant', 'displai', 'text', 'still', 'want', 'send', 'number']\n",
      "Tokenized sentence: ['i', 'thk', 'u', 'dun', 'haf', 'hint', 'in', 'e', 'forum', 'already', 'lor', 'cos', 'i', 'told', 'ron', 'n', 'darren', 'is', 'going', 'tell', 'shuhui']\n",
      "After stop words removal: ['thk', 'u', 'dun', 'haf', 'hint', 'e', 'forum', 'already', 'lor', 'cos', 'told', 'ron', 'n', 'darren', 'going', 'tell', 'shuhui']\n",
      "go\n",
      "After stemming with porters algorithm: ['thk', 'dun', 'haf', 'hint', 'forum', 'alreadi', 'lor', 'co', 'told', 'ron', 'darren', 'go', 'tell', 'shuhui']\n",
      "Tokenized sentence: ['hmm', 'shall', 'i', 'bring', 'a', 'bottle', 'of', 'wine', 'to', 'keep', 'us', 'amused', 'just', 'joking', 'i', 'll', 'still', 'bring', 'a', 'bottle', 'red', 'or', 'white', 'see', 'you', 'tomorrow']\n",
      "After stop words removal: ['hmm', 'shall', 'bring', 'bottle', 'wine', 'keep', 'us', 'amused', 'joking', 'still', 'bring', 'bottle', 'red', 'white', 'see', 'tomorrow']\n",
      "jok\n",
      "After stemming with porters algorithm: ['hmm', 'shall', 'bring', 'bottl', 'wine', 'keep', 'amus', 'joke', 'still', 'bring', 'bottl', 'red', 'white', 'see', 'tomorrow']\n",
      "Tokenized sentence: ['u', 'goin', 'out', 'nite']\n",
      "After stop words removal: ['u', 'goin', 'nite']\n",
      "After stemming with porters algorithm: ['goin', 'nite']\n",
      "Tokenized sentence: ['glad', 'to', 'see', 'your', 'reply']\n",
      "After stop words removal: ['glad', 'see', 'reply']\n",
      "After stemming with porters algorithm: ['glad', 'see', 'repli']\n",
      "Tokenized sentence: ['no', 'need', 'for', 'the', 'drug', 'anymore']\n",
      "After stop words removal: ['need', 'drug', 'anymore']\n",
      "After stemming with porters algorithm: ['need', 'drug', 'anymor']\n",
      "Tokenized sentence: ['sounds', 'like', 'you', 'have', 'many', 'talents', 'would', 'you', 'like', 'to', 'go', 'on', 'a', 'dinner', 'date', 'next', 'week']\n",
      "After stop words removal: ['sounds', 'like', 'many', 'talents', 'would', 'like', 'go', 'dinner', 'date', 'next', 'week']\n",
      "After stemming with porters algorithm: ['sound', 'like', 'mani', 'talent', 'would', 'like', 'dinner', 'date', 'next', 'week']\n",
      "Tokenized sentence: ['it', 'doesnt', 'make', 'sense', 'to', 'take', 'it', 'there', 'unless', 'its', 'free', 'if', 'you', 'need', 'to', 'know', 'more', 'wikipedia', 'com']\n",
      "After stop words removal: ['doesnt', 'make', 'sense', 'take', 'unless', 'free', 'need', 'know', 'wikipedia', 'com']\n",
      "After stemming with porters algorithm: ['doesnt', 'make', 'sens', 'take', 'unless', 'free', 'need', 'know', 'wikipedia', 'com']\n",
      "Tokenized sentence: ['don', 't', 'forget', 'though', 'that', 'i', 'love', 'you', 'and', 'i', 'walk', 'beside', 'you', 'watching', 'over', 'you', 'and', 'keeping', 'your', 'heart', 'warm']\n",
      "After stop words removal: ['forget', 'though', 'love', 'walk', 'beside', 'watching', 'keeping', 'heart', 'warm']\n",
      "watch\n",
      "keep\n",
      "After stemming with porters algorithm: ['forget', 'though', 'love', 'walk', 'besid', 'watc', 'keep', 'heart', 'warm']\n",
      "Tokenized sentence: ['i', 'know', 'girls', 'always', 'safe', 'and', 'selfish', 'know', 'i', 'got', 'it', 'pa', 'thank', 'you', 'good', 'night']\n",
      "After stop words removal: ['know', 'girls', 'always', 'safe', 'selfish', 'know', 'got', 'pa', 'thank', 'good', 'night']\n",
      "After stemming with porters algorithm: ['know', 'girl', 'alwai', 'safe', 'selfish', 'know', 'got', 'thank', 'good', 'night']\n",
      "Tokenized sentence: ['say', 'this', 'slowly', 'god', 'i', 'love', 'you', 'amp', 'i', 'need', 'you', 'clean', 'my', 'heart', 'with', 'your', 'blood', 'send', 'this', 'to', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'do', 'it', 'pls', 'pls', 'do', 'it']\n",
      "After stop words removal: ['say', 'slowly', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'pls', 'pls']\n",
      "After stemming with porters algorithm: ['sai', 'slowli', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'peopl', 'amp', 'mirac', 'tomorrow', 'pl', 'pl']\n",
      "Tokenized sentence: ['fran', 'i', 'decided', 'go', 'n', 'e', 'way', 'im', 'completely', 'broke', 'an', 'knackered', 'i', 'got', 'up', 'bout', 'c', 'u', 'mrw', 'love', 'janx', 'p', 's', 'this', 'is', 'my', 'dads', 'fone', 'no', 'credit']\n",
      "After stop words removal: ['fran', 'decided', 'go', 'n', 'e', 'way', 'im', 'completely', 'broke', 'knackered', 'got', 'bout', 'c', 'u', 'mrw', 'love', 'janx', 'p', 'dads', 'fone', 'credit']\n",
      "After stemming with porters algorithm: ['fran', 'decid', 'wai', 'complet', 'broke', 'knacker', 'got', 'bout', 'mrw', 'love', 'janx', 'dad', 'fone', 'credit']\n",
      "Tokenized sentence: ['bring', 'tat', 'cd', 'don', 'forget']\n",
      "After stop words removal: ['bring', 'tat', 'cd', 'forget']\n",
      "After stemming with porters algorithm: ['bring', 'tat', 'forget']\n",
      "Tokenized sentence: ['right', 'it', 'wasnt', 'you', 'who', 'phoned', 'it', 'was', 'someone', 'with', 'a', 'number', 'like', 'yours']\n",
      "After stop words removal: ['right', 'wasnt', 'phoned', 'someone', 'number', 'like']\n",
      "After stemming with porters algorithm: ['right', 'wasnt', 'phone', 'someon', 'number', 'like']\n",
      "Tokenized sentence: ['good', 'morning', 'my', 'dear', 'shijutta', 'have', 'a', 'great', 'amp', 'successful', 'day']\n",
      "After stop words removal: ['good', 'morning', 'dear', 'shijutta', 'great', 'amp', 'successful', 'day']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'dear', 'shijutta', 'great', 'amp', 'success', 'dai']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['whatever', 'juliana', 'do', 'whatever', 'you', 'want']\n",
      "After stop words removal: ['whatever', 'juliana', 'whatever', 'want']\n",
      "After stemming with porters algorithm: ['whatev', 'juliana', 'whatev', 'want']\n",
      "Tokenized sentence: ['well', 'done', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tcs', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stop words removal: ['well', 'done', 'costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tcs', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stemming with porters algorithm: ['well', 'done', 'costa', 'del', 'sol', 'holidai', 'await', 'collect', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'cost', 'max', 'min']\n",
      "Tokenized sentence: ['on', 'ma', 'way', 'to', 'school', 'can', 'you', 'pls', 'send', 'me', 'ashley', 's', 'number']\n",
      "After stop words removal: ['way', 'school', 'pls', 'send', 'ashley', 'number']\n",
      "After stemming with porters algorithm: ['wai', 'school', 'pl', 'send', 'ashlei', 'number']\n",
      "Tokenized sentence: ['actually', 'fuck', 'that', 'just', 'do', 'whatever', 'do', 'find', 'an', 'excuse', 'to', 'be', 'in', 'tampa', 'at', 'some', 'point', 'before', 'january', 'though']\n",
      "After stop words removal: ['actually', 'fuck', 'whatever', 'find', 'excuse', 'tampa', 'point', 'january', 'though']\n",
      "After stemming with porters algorithm: ['actual', 'fuck', 'whatev', 'find', 'excus', 'tampa', 'point', 'januari', 'though']\n",
      "Tokenized sentence: ['ok', 'i', 'din', 'get', 'ur', 'msg']\n",
      "After stop words removal: ['ok', 'din', 'get', 'ur', 'msg']\n",
      "After stemming with porters algorithm: ['din', 'get', 'msg']\n",
      "Tokenized sentence: ['am', 'on', 'the', 'uworld', 'site', 'am', 'i', 'buying', 'the', 'qbank', 'only', 'or', 'am', 'i', 'buying', 'it', 'with', 'the', 'self', 'assessment', 'also']\n",
      "After stop words removal: ['uworld', 'site', 'buying', 'qbank', 'buying', 'self', 'assessment', 'also']\n",
      "buy\n",
      "buy\n",
      "After stemming with porters algorithm: ['uworld', 'site', 'bui', 'qbank', 'bui', 'self', 'assess', 'also']\n",
      "Tokenized sentence: ['thnx', 'dude', 'u', 'guys', 'out', 'nite']\n",
      "After stop words removal: ['thnx', 'dude', 'u', 'guys', 'nite']\n",
      "After stemming with porters algorithm: ['thnx', 'dude', 'gui', 'nite']\n",
      "Tokenized sentence: ['not', 'much', 'no', 'fights', 'it', 'was', 'a', 'good', 'nite']\n",
      "After stop words removal: ['much', 'fights', 'good', 'nite']\n",
      "After stemming with porters algorithm: ['much', 'fight', 'good', 'nite']\n",
      "Tokenized sentence: ['i', 'had', 'askd', 'u', 'a', 'question', 'some', 'hours', 'before', 'its', 'answer']\n",
      "After stop words removal: ['askd', 'u', 'question', 'hours', 'answer']\n",
      "After stemming with porters algorithm: ['askd', 'quest', 'hour', 'answer']\n",
      "Tokenized sentence: ['sitting', 'in', 'mu', 'waiting', 'for', 'everyone', 'to', 'get', 'out', 'of', 'my', 'suite', 'so', 'i', 'can', 'take', 'a', 'shower']\n",
      "After stop words removal: ['sitting', 'mu', 'waiting', 'everyone', 'get', 'suite', 'take', 'shower']\n",
      "sitt\n",
      "wait\n",
      "After stemming with porters algorithm: ['sit', 'wait', 'everyon', 'get', 'suit', 'take', 'shower']\n",
      "Tokenized sentence: ['you', 'have', 'been', 'specially', 'selected', 'to', 'receive', 'a', 'award', 'call', 'before', 'the', 'lines', 'close', 'cost', 'ppm', 't', 'cs', 'apply', 'ag', 'promo']\n",
      "After stop words removal: ['specially', 'selected', 'receive', 'award', 'call', 'lines', 'close', 'cost', 'ppm', 'cs', 'apply', 'ag', 'promo']\n",
      "After stemming with porters algorithm: ['special', 'selec', 'receiv', 'award', 'call', 'line', 'close', 'cost', 'ppm', 'appli', 'promo']\n",
      "Tokenized sentence: ['how', 's', 'it', 'going', 'got', 'any', 'exciting', 'karaoke', 'type', 'activities', 'planned', 'i', 'm', 'debating', 'whether', 'to', 'play', 'football', 'this', 'eve', 'feeling', 'lazy', 'though']\n",
      "After stop words removal: ['going', 'got', 'exciting', 'karaoke', 'type', 'activities', 'planned', 'debating', 'whether', 'play', 'football', 'eve', 'feeling', 'lazy', 'though']\n",
      "go\n",
      "excit\n",
      "debat\n",
      "debate\n",
      "feel\n",
      "After stemming with porters algorithm: ['go', 'got', 'excit', 'karaok', 'type', 'activ', 'plan', 'debat', 'whether', 'plai', 'footbal', 'ev', 'feel', 'lazi', 'though']\n",
      "Tokenized sentence: ['yup', 'ok']\n",
      "After stop words removal: ['yup', 'ok']\n",
      "After stemming with porters algorithm: ['yup']\n",
      "Tokenized sentence: ['hello', 'yeah', 'i', 've', 'just', 'got', 'out', 'of', 'the', 'bath', 'and', 'need', 'to', 'do', 'my', 'hair', 'so', 'i', 'll', 'come', 'up', 'when', 'i', 'm', 'done', 'yeah']\n",
      "After stop words removal: ['hello', 'yeah', 'got', 'bath', 'need', 'hair', 'come', 'done', 'yeah']\n",
      "After stemming with porters algorithm: ['hello', 'yeah', 'got', 'bath', 'need', 'hair', 'come', 'done', 'yeah']\n",
      "Tokenized sentence: ['aathi', 'where', 'are', 'you', 'dear']\n",
      "After stop words removal: ['aathi', 'dear']\n",
      "After stemming with porters algorithm: ['aathi', 'dear']\n",
      "Tokenized sentence: ['im', 'at', 'arestaurant', 'eating', 'squid', 'i', 'will', 'be', 'out', 'about', 'wanna', 'dosomething', 'or', 'is', 'that', 'to', 'late']\n",
      "After stop words removal: ['im', 'arestaurant', 'eating', 'squid', 'wanna', 'dosomething', 'late']\n",
      "eat\n",
      "eate\n",
      "dosometh\n",
      "After stemming with porters algorithm: ['arestaur', 'eat', 'squid', 'wanna', 'dosomet', 'late']\n",
      "Tokenized sentence: ['you', 'best', 'watch', 'what', 'you', 'say', 'cause', 'i', 'get', 'drunk', 'as', 'a', 'motherfucker']\n",
      "After stop words removal: ['best', 'watch', 'say', 'cause', 'get', 'drunk', 'motherfucker']\n",
      "After stemming with porters algorithm: ['best', 'watch', 'sai', 'caus', 'get', 'drunk', 'motherfuck']\n",
      "Tokenized sentence: ['i', 'm', 'home', 'ard', 'wat', 'time', 'will', 'u', 'reach']\n",
      "After stop words removal: ['home', 'ard', 'wat', 'time', 'u', 'reach']\n",
      "After stemming with porters algorithm: ['home', 'ard', 'wat', 'time', 'reach']\n",
      "Tokenized sentence: ['oh', 'ok', 'wait', 'me', 'there', 'my', 'lect', 'havent', 'finish']\n",
      "After stop words removal: ['oh', 'ok', 'wait', 'lect', 'havent', 'finish']\n",
      "After stemming with porters algorithm: ['wait', 'lect', 'havent', 'finish']\n",
      "Tokenized sentence: ['that', 's', 'a', 'shame', 'maybe', 'cld', 'meet', 'for', 'few', 'hrs', 'tomo']\n",
      "After stop words removal: ['shame', 'maybe', 'cld', 'meet', 'hrs', 'tomo']\n",
      "After stemming with porters algorithm: ['shame', 'mayb', 'cld', 'meet', 'hr', 'tomo']\n",
      "Tokenized sentence: ['i', 'am', 'back', 'good', 'journey', 'let', 'me', 'know', 'if', 'you', 'need', 'any', 'of', 'the', 'receipts', 'shall', 'i', 'tell', 'you', 'like', 'the', 'pendent']\n",
      "After stop words removal: ['back', 'good', 'journey', 'let', 'know', 'need', 'receipts', 'shall', 'tell', 'like', 'pendent']\n",
      "After stemming with porters algorithm: ['back', 'good', 'journei', 'let', 'know', 'need', 'receipt', 'shall', 'tell', 'like', 'pendent']\n",
      "Tokenized sentence: ['tmr', 'then', 'brin', 'lar', 'aiya', 'later', 'i', 'come', 'n', 'c', 'lar', 'mayb', 'neva', 'set', 'properly', 'got', 'da', 'help', 'sheet', 'wif']\n",
      "After stop words removal: ['tmr', 'brin', 'lar', 'aiya', 'later', 'come', 'n', 'c', 'lar', 'mayb', 'neva', 'set', 'properly', 'got', 'da', 'help', 'sheet', 'wif']\n",
      "After stemming with porters algorithm: ['tmr', 'brin', 'lar', 'aiya', 'later', 'come', 'lar', 'mayb', 'neva', 'set', 'properli', 'got', 'help', 'sheet', 'wif']\n",
      "Tokenized sentence: ['dizzamn', 'aight', 'i', 'll', 'ask', 'my', 'suitemates', 'when', 'i', 'get', 'back']\n",
      "After stop words removal: ['dizzamn', 'aight', 'ask', 'suitemates', 'get', 'back']\n",
      "After stemming with porters algorithm: ['dizzamn', 'aight', 'ask', 'suitem', 'get', 'back']\n",
      "Tokenized sentence: ['i', 've', 'reached', 'home', 'finally']\n",
      "After stop words removal: ['reached', 'home', 'finally']\n",
      "After stemming with porters algorithm: ['reac', 'home', 'final']\n",
      "Tokenized sentence: ['haha', 'okay', 'today', 'weekend', 'leh']\n",
      "After stop words removal: ['haha', 'okay', 'today', 'weekend', 'leh']\n",
      "After stemming with porters algorithm: ['haha', 'okai', 'todai', 'weekend', 'leh']\n",
      "Tokenized sentence: ['hey', 'chief', 'can', 'you', 'give', 'me', 'a', 'bell', 'when', 'you', 'get', 'this', 'need', 'to', 'talk', 'to', 'you', 'about', 'this', 'royal', 'visit', 'on', 'the', 'st', 'june']\n",
      "After stop words removal: ['hey', 'chief', 'give', 'bell', 'get', 'need', 'talk', 'royal', 'visit', 'st', 'june']\n",
      "After stemming with porters algorithm: ['hei', 'chief', 'give', 'bell', 'get', 'need', 'talk', 'royal', 'visit', 'june']\n",
      "Tokenized sentence: ['bank', 'of', 'granite', 'issues', 'strong', 'buy', 'explosive', 'pick', 'for', 'our', 'members', 'up', 'over', 'nasdaq', 'symbol', 'cdgt', 'that', 'is', 'a', 'per']\n",
      "After stop words removal: ['bank', 'granite', 'issues', 'strong', 'buy', 'explosive', 'pick', 'members', 'nasdaq', 'symbol', 'cdgt', 'per']\n",
      "After stemming with porters algorithm: ['bank', 'granit', 'issu', 'strong', 'bui', 'explos', 'pick', 'member', 'nasdaq', 'symbol', 'cdgt', 'per']\n",
      "Tokenized sentence: ['want', 'the', 'latest', 'video', 'handset', 'anytime', 'any', 'network', 'mins', 'half', 'price', 'line', 'rental', 'reply', 'or', 'call', 'for', 'delivery', 'tomorrow']\n",
      "After stop words removal: ['want', 'latest', 'video', 'handset', 'anytime', 'network', 'mins', 'half', 'price', 'line', 'rental', 'reply', 'call', 'delivery', 'tomorrow']\n",
      "After stemming with porters algorithm: ['want', 'latest', 'video', 'handset', 'anytim', 'network', 'min', 'half', 'price', 'line', 'rental', 'repli', 'call', 'deliveri', 'tomorrow']\n",
      "Tokenized sentence: ['yeah', 'whatever', 'lol']\n",
      "After stop words removal: ['yeah', 'whatever', 'lol']\n",
      "After stemming with porters algorithm: ['yeah', 'whatev', 'lol']\n",
      "Tokenized sentence: ['we', 'know', 'someone', 'who', 'you', 'know', 'that', 'fancies', 'you', 'call', 'to', 'find', 'out', 'who', 'pobox', 'ls', 'hb', 'p']\n",
      "After stop words removal: ['know', 'someone', 'know', 'fancies', 'call', 'find', 'pobox', 'ls', 'hb', 'p']\n",
      "After stemming with porters algorithm: ['know', 'someon', 'know', 'fanci', 'call', 'find', 'pobox']\n",
      "Tokenized sentence: ['lol', 'no', 'u', 'can', 'trust', 'me']\n",
      "After stop words removal: ['lol', 'u', 'trust']\n",
      "After stemming with porters algorithm: ['lol', 'trust']\n",
      "Tokenized sentence: ['dunno', 'dat', 's', 'wat', 'he', 'told', 'me', 'ok', 'lor']\n",
      "After stop words removal: ['dunno', 'dat', 'wat', 'told', 'ok', 'lor']\n",
      "After stemming with porters algorithm: ['dunno', 'dat', 'wat', 'told', 'lor']\n",
      "Tokenized sentence: ['when', 'i', 'was', 'born', 'god', 'said', 'oh', 'no', 'another', 'idiot', 'when', 'you', 'were', 'born']\n",
      "After stop words removal: ['born', 'god', 'said', 'oh', 'another', 'idiot', 'born']\n",
      "After stemming with porters algorithm: ['born', 'god', 'said', 'anoth', 'idiot', 'born']\n",
      "Tokenized sentence: ['she', 's', 'fine', 'sends', 'her', 'greetings']\n",
      "After stop words removal: ['fine', 'sends', 'greetings']\n",
      "greet\n",
      "After stemming with porters algorithm: ['fine', 'send', 'greet']\n",
      "Tokenized sentence: ['are', 'you', 'at', 'work', 'right', 'now']\n",
      "After stop words removal: ['work', 'right']\n",
      "After stemming with porters algorithm: ['work', 'right']\n",
      "Tokenized sentence: ['i', 'need', 'to', 'come', 'home', 'and', 'give', 'you', 'some', 'good', 'lovin']\n",
      "After stop words removal: ['need', 'come', 'home', 'give', 'good', 'lovin']\n",
      "After stemming with porters algorithm: ['need', 'come', 'home', 'give', 'good', 'lovin']\n",
      "Tokenized sentence: ['i', 'sent', 'you', 'lt', 'gt', 'bucks']\n",
      "After stop words removal: ['sent', 'lt', 'gt', 'bucks']\n",
      "After stemming with porters algorithm: ['sent', 'buck']\n",
      "Tokenized sentence: ['the', 'fact', 'that', 'you', 're', 'cleaning', 'shows', 'you', 'know', 'why', 'i', 'm', 'upset', 'your', 'priority', 'is', 'constantly', 'what', 'i', 'want', 'to', 'do']\n",
      "After stop words removal: ['fact', 'cleaning', 'shows', 'know', 'upset', 'priority', 'constantly', 'want']\n",
      "clean\n",
      "After stemming with porters algorithm: ['fact', 'clean', 'show', 'know', 'upset', 'prioriti', 'constantli', 'want']\n",
      "Tokenized sentence: ['is', 'fujitsu', 's', 'series', 'lifebook', 'good']\n",
      "After stop words removal: ['fujitsu', 'series', 'lifebook', 'good']\n",
      "After stemming with porters algorithm: ['fujitsu', 'seri', 'lifebook', 'good']\n",
      "Tokenized sentence: ['how', 's', 'things', 'just', 'a', 'quick', 'question']\n",
      "After stop words removal: ['things', 'quick', 'question']\n",
      "After stemming with porters algorithm: ['thing', 'quick', 'quest']\n",
      "Tokenized sentence: ['give', 'one', 'miss', 'from', 'that', 'number', 'please']\n",
      "After stop words removal: ['give', 'one', 'miss', 'number', 'please']\n",
      "After stemming with porters algorithm: ['give', 'on', 'miss', 'number', 'pleas']\n",
      "Tokenized sentence: ['so', 'i', 'm', 'doing', 'a', 'list', 'of', 'buyers']\n",
      "After stop words removal: ['list', 'buyers']\n",
      "After stemming with porters algorithm: ['list', 'buyer']\n",
      "Tokenized sentence: ['just', 'got', 'up', 'have', 'to', 'be', 'out', 'of', 'the', 'room', 'very', 'soon', 'i', 'hadn', 't', 'put', 'the', 'clocks', 'back', 'til', 'at', 'i', 'shouted', 'at', 'everyone', 'to', 'get', 'up', 'and', 'then', 'realised', 'it', 'was', 'wahay', 'another', 'hour', 'in', 'bed']\n",
      "After stop words removal: ['got', 'room', 'soon', 'put', 'clocks', 'back', 'til', 'shouted', 'everyone', 'get', 'realised', 'wahay', 'another', 'hour', 'bed']\n",
      "After stemming with porters algorithm: ['got', 'room', 'soon', 'put', 'clock', 'back', 'til', 'shout', 'everyon', 'get', 'realis', 'wahai', 'anoth', 'hour', 'bed']\n",
      "Tokenized sentence: ['maybe', 'i', 'could', 'get', 'book', 'out', 'tomo', 'then', 'return', 'it', 'immediately', 'or', 'something']\n",
      "After stop words removal: ['maybe', 'could', 'get', 'book', 'tomo', 'return', 'immediately', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['mayb', 'could', 'get', 'book', 'tomo', 'return', 'immedi', 'somet']\n",
      "Tokenized sentence: ['the', 'wine', 'is', 'flowing', 'and', 'i', 'm', 'i', 'have', 'nevering']\n",
      "After stop words removal: ['wine', 'flowing', 'nevering']\n",
      "flow\n",
      "never\n",
      "After stemming with porters algorithm: ['wine', 'flowe', 'never']\n",
      "Tokenized sentence: ['congratulations', 'thanks', 'to', 'a', 'good', 'friend', 'u', 'have', 'won', 'the', 'xmas', 'prize', 'claim', 'is', 'easy', 'just', 'call', 'now', 'only', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['congratulations', 'thanks', 'good', 'friend', 'u', 'xmas', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['congratul', 'thank', 'good', 'friend', 'xma', 'priz', 'claim', 'easi', 'call', 'per', 'minut', 'nat', 'rate']\n",
      "Tokenized sentence: ['yup']\n",
      "After stop words removal: ['yup']\n",
      "After stemming with porters algorithm: ['yup']\n",
      "Tokenized sentence: ['update', 'now', 'xmas', 'offer', 'latest', 'motorola', 'sonyericsson', 'nokia', 'free', 'bluetooth', 'double', 'mins', 'txt', 'on', 'orange', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'f', 'q']\n",
      "After stop words removal: ['update', 'xmas', 'offer', 'latest', 'motorola', 'sonyericsson', 'nokia', 'free', 'bluetooth', 'double', 'mins', 'txt', 'orange', 'call', 'mobileupd', 'call', 'optout', 'f', 'q']\n",
      "After stemming with porters algorithm: ['updat', 'xma', 'offer', 'latest', 'motorola', 'sonyericsson', 'nokia', 'free', 'bluetooth', 'doubl', 'min', 'txt', 'orang', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['am', 'on', 'a', 'train', 'back', 'from', 'northampton', 'so', 'i', 'm', 'afraid', 'not', 'i', 'm', 'staying', 'skyving', 'off', 'today', 'ho', 'ho', 'will', 'be', 'around', 'wednesday', 'though', 'do', 'you', 'fancy', 'the', 'comedy', 'club', 'this', 'week', 'by', 'the', 'way']\n",
      "After stop words removal: ['train', 'back', 'northampton', 'afraid', 'staying', 'skyving', 'today', 'ho', 'ho', 'around', 'wednesday', 'though', 'fancy', 'comedy', 'club', 'week', 'way']\n",
      "stay\n",
      "After stemming with porters algorithm: ['train', 'back', 'northampton', 'afraid', 'stai', 'skyving', 'todai', 'around', 'wednesdai', 'though', 'fanci', 'comedi', 'club', 'week', 'wai']\n",
      "Tokenized sentence: ['that', 's', 'the', 'thing', 'with', 'apes', 'u', 'can', 'fight', 'to', 'the', 'death', 'to', 'keep', 'something', 'but', 'the', 'minute', 'they', 'have', 'it', 'when', 'u', 'let', 'go', 'thats', 'it']\n",
      "After stop words removal: ['thing', 'apes', 'u', 'fight', 'death', 'keep', 'something', 'minute', 'u', 'let', 'go', 'thats']\n",
      "someth\n",
      "After stemming with porters algorithm: ['thing', 'ap', 'fight', 'death', 'keep', 'somet', 'minut', 'let', 'that']\n",
      "Tokenized sentence: ['at', 'what', 'time', 'should', 'i', 'come', 'tomorrow']\n",
      "After stop words removal: ['time', 'come', 'tomorrow']\n",
      "After stemming with porters algorithm: ['time', 'come', 'tomorrow']\n",
      "Tokenized sentence: ['i', 'll', 'probably', 'be', 'by', 'tomorrow', 'or', 'even', 'later', 'tonight', 'if', 'something', 's', 'going', 'on']\n",
      "After stop words removal: ['probably', 'tomorrow', 'even', 'later', 'tonight', 'something', 'going']\n",
      "someth\n",
      "go\n",
      "After stemming with porters algorithm: ['probab', 'tomorrow', 'even', 'later', 'tonight', 'somet', 'go']\n",
      "Tokenized sentence: ['congrats', 'that', 's', 'great', 'i', 'wanted', 'to', 'tell', 'you', 'not', 'to', 'tell', 'me', 'your', 'score', 'cos', 'it', 'might', 'make', 'me', 'relax', 'but', 'its', 'motivating', 'me', 'so', 'thanks', 'for', 'sharing']\n",
      "After stop words removal: ['congrats', 'great', 'wanted', 'tell', 'tell', 'score', 'cos', 'might', 'make', 'relax', 'motivating', 'thanks', 'sharing']\n",
      "motivat\n",
      "motivate\n",
      "shar\n",
      "After stemming with porters algorithm: ['congrat', 'great', 'wan', 'tell', 'tell', 'score', 'co', 'might', 'make', 'relax', 'motiv', 'thank', 'share']\n",
      "Tokenized sentence: ['u', 'having', 'lunch', 'alone', 'i', 'now', 'so', 'bored']\n",
      "After stop words removal: ['u', 'lunch', 'alone', 'bored']\n",
      "After stemming with porters algorithm: ['lunch', 'alon', 'bore']\n",
      "Tokenized sentence: ['so', 'why', 'didnt', 'you', 'holla']\n",
      "After stop words removal: ['didnt', 'holla']\n",
      "After stemming with porters algorithm: ['didnt', 'holla']\n",
      "Tokenized sentence: ['didn', 't', 'try', 'g', 'and', 'i', 'decided', 'not', 'to', 'head', 'out']\n",
      "After stop words removal: ['try', 'g', 'decided', 'head']\n",
      "After stemming with porters algorithm: ['try', 'decid', 'head']\n",
      "Tokenized sentence: ['ok', 'then', 'i', 'will', 'come', 'to', 'ur', 'home', 'after', 'half', 'an', 'hour']\n",
      "After stop words removal: ['ok', 'come', 'ur', 'home', 'half', 'hour']\n",
      "After stemming with porters algorithm: ['come', 'home', 'half', 'hour']\n",
      "Tokenized sentence: ['sweetheart', 'hope', 'you', 'are', 'not', 'having', 'that', 'kind', 'of', 'day', 'have', 'one', 'with', 'loads', 'of', 'reasons', 'to', 'smile', 'biola']\n",
      "After stop words removal: ['sweetheart', 'hope', 'kind', 'day', 'one', 'loads', 'reasons', 'smile', 'biola']\n",
      "After stemming with porters algorithm: ['sweetheart', 'hope', 'kind', 'dai', 'on', 'load', 'reason', 'smile', 'biola']\n",
      "Tokenized sentence: ['alrite', 'sam', 'its', 'nic', 'just', 'checkin', 'that', 'this', 'is', 'ur', 'number', 'so', 'is', 'it', 't', 'b']\n",
      "After stop words removal: ['alrite', 'sam', 'nic', 'checkin', 'ur', 'number', 'b']\n",
      "After stemming with porters algorithm: ['alrit', 'sam', 'nic', 'checkin', 'number']\n",
      "Tokenized sentence: ['yup', 'next', 'stop']\n",
      "After stop words removal: ['yup', 'next', 'stop']\n",
      "After stemming with porters algorithm: ['yup', 'next', 'stop']\n",
      "Tokenized sentence: ['you', 'have', 'new', 'voicemail', 'please', 'call']\n",
      "After stop words removal: ['new', 'voicemail', 'please', 'call']\n",
      "After stemming with porters algorithm: ['new', 'voicemail', 'pleas', 'call']\n",
      "Tokenized sentence: ['pls', 'dont', 'forget', 'to', 'study']\n",
      "After stop words removal: ['pls', 'dont', 'forget', 'study']\n",
      "After stemming with porters algorithm: ['pl', 'dont', 'forget', 'studi']\n",
      "Tokenized sentence: ['may', 'i', 'call', 'you', 'later', 'pls']\n",
      "After stop words removal: ['may', 'call', 'later', 'pls']\n",
      "After stemming with porters algorithm: ['mai', 'call', 'later', 'pl']\n",
      "Tokenized sentence: ['customer', 'place', 'i', 'will', 'call', 'you']\n",
      "After stop words removal: ['customer', 'place', 'call']\n",
      "After stemming with porters algorithm: ['custom', 'place', 'call']\n",
      "Tokenized sentence: ['i', 'thought', 'i', 'd', 'get', 'him', 'a', 'watch', 'just', 'cos', 'thats', 'the', 'kind', 'of', 'thing', 'u', 'get', 'an', 'th', 'and', 'he', 'loves', 'so', 'much']\n",
      "After stop words removal: ['thought', 'get', 'watch', 'cos', 'thats', 'kind', 'thing', 'u', 'get', 'th', 'loves', 'much']\n",
      "After stemming with porters algorithm: ['thought', 'get', 'watch', 'co', 'that', 'kind', 'thing', 'get', 'love', 'much']\n",
      "Tokenized sentence: ['txt', 'call', 'to', 'no', 'claim', 'your', 'reward', 'of', 'hours', 'talk', 'time', 'to', 'use', 'from', 'your', 'phone', 'now', 'subscribe', 'gbp', 'mnth', 'inc', 'hrs', 'stop', 'txtstop', 'www', 'gamb', 'tv']\n",
      "After stop words removal: ['txt', 'call', 'claim', 'reward', 'hours', 'talk', 'time', 'use', 'phone', 'subscribe', 'gbp', 'mnth', 'inc', 'hrs', 'stop', 'txtstop', 'www', 'gamb', 'tv']\n",
      "After stemming with porters algorithm: ['txt', 'call', 'claim', 'reward', 'hour', 'talk', 'time', 'us', 'phone', 'subscrib', 'gbp', 'mnth', 'inc', 'hr', 'stop', 'txtstop', 'www', 'gamb']\n",
      "Tokenized sentence: ['v', 'skint', 'too', 'but', 'fancied', 'few', 'bevies', 'waz', 'gona', 'go', 'meet', 'othrs', 'in', 'spoon', 'but', 'jst', 'bin', 'watchng', 'planet', 'earth', 'sofa', 'is', 'v', 'comfey', 'if', 'i', 'dont', 'make', 'it', 'hav', 'gd', 'night']\n",
      "After stop words removal: ['v', 'skint', 'fancied', 'bevies', 'waz', 'gona', 'go', 'meet', 'othrs', 'spoon', 'jst', 'bin', 'watchng', 'planet', 'earth', 'sofa', 'v', 'comfey', 'dont', 'make', 'hav', 'gd', 'night']\n",
      "After stemming with porters algorithm: ['skint', 'fanci', 'bevi', 'waz', 'gona', 'meet', 'othr', 'spoon', 'jst', 'bin', 'watchng', 'planet', 'earth', 'sofa', 'comfei', 'dont', 'make', 'hav', 'night']\n",
      "Tokenized sentence: ['jason', 'says', 'it', 's', 'cool', 'if', 'we', 'pick', 'some', 'up', 'from', 'his', 'place', 'in', 'like', 'an', 'hour']\n",
      "After stop words removal: ['jason', 'says', 'cool', 'pick', 'place', 'like', 'hour']\n",
      "After stemming with porters algorithm: ['jason', 'sai', 'cool', 'pick', 'place', 'like', 'hour']\n",
      "Tokenized sentence: ['days', 'to', 'kick', 'off', 'for', 'euro', 'u', 'will', 'be', 'kept', 'up', 'to', 'date', 'with', 'the', 'latest', 'news', 'and', 'results', 'daily', 'to', 'be', 'removed', 'send', 'get', 'txt', 'stop', 'to']\n",
      "After stop words removal: ['days', 'kick', 'euro', 'u', 'kept', 'date', 'latest', 'news', 'results', 'daily', 'removed', 'send', 'get', 'txt', 'stop']\n",
      "After stemming with porters algorithm: ['dai', 'kick', 'euro', 'kept', 'date', 'latest', 'new', 'result', 'daili', 'remov', 'send', 'get', 'txt', 'stop']\n",
      "Tokenized sentence: ['y', 'where', 'u', 'at', 'dogbreath', 'its', 'just', 'sounding', 'like', 'jan', 'c', 'that', 's', 'al']\n",
      "After stop words removal: ['u', 'dogbreath', 'sounding', 'like', 'jan', 'c', 'al']\n",
      "sound\n",
      "After stemming with porters algorithm: ['dogbreath', 'soun', 'like', 'jan']\n",
      "Tokenized sentence: ['call', 'me', 'i', 'm', 'unable', 'to', 'cal', 'lets', 'meet', 'bhaskar', 'and', 'deep']\n",
      "After stop words removal: ['call', 'unable', 'cal', 'lets', 'meet', 'bhaskar', 'deep']\n",
      "After stemming with porters algorithm: ['call', 'unab', 'cal', 'let', 'meet', 'bhaskar', 'deep']\n",
      "Tokenized sentence: ['great', 'i', 'hope', 'you', 'like', 'your', 'man', 'well', 'endowed', 'i', 'am', 'lt', 'gt', 'inches']\n",
      "After stop words removal: ['great', 'hope', 'like', 'man', 'well', 'endowed', 'lt', 'gt', 'inches']\n",
      "After stemming with porters algorithm: ['great', 'hope', 'like', 'man', 'well', 'endow', 'inch']\n",
      "Tokenized sentence: ['i', 've', 'reached', 'home', 'n', 'i', 'bathe', 'liao', 'u', 'can', 'call', 'me', 'now']\n",
      "After stop words removal: ['reached', 'home', 'n', 'bathe', 'liao', 'u', 'call']\n",
      "After stemming with porters algorithm: ['reac', 'home', 'bath', 'liao', 'call']\n",
      "Tokenized sentence: ['spook', 'up', 'your', 'mob', 'with', 'a', 'halloween', 'collection', 'of', 'a', 'logo', 'pic', 'message', 'plus', 'a', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'to', 'zed', 'p', 'per', 'logo', 'pic']\n",
      "After stop words removal: ['spook', 'mob', 'halloween', 'collection', 'logo', 'pic', 'message', 'plus', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'zed', 'p', 'per', 'logo', 'pic']\n",
      "After stemming with porters algorithm: ['spook', 'mob', 'halloween', 'collect', 'logo', 'pic', 'messag', 'plu', 'free', 'eeri', 'tone', 'txt', 'card', 'spook', 'zed', 'per', 'logo', 'pic']\n",
      "Tokenized sentence: ['no', 'rushing', 'i', 'm', 'not', 'working', 'i', 'm', 'in', 'school', 'so', 'if', 'we', 'rush', 'we', 'go', 'hungry']\n",
      "After stop words removal: ['rushing', 'working', 'school', 'rush', 'go', 'hungry']\n",
      "rush\n",
      "work\n",
      "After stemming with porters algorithm: ['rus', 'wor', 'school', 'rush', 'hungri']\n",
      "Tokenized sentence: ['so', 'how', 'many', 'days', 'since', 'then']\n",
      "After stop words removal: ['many', 'days', 'since']\n",
      "After stemming with porters algorithm: ['mani', 'dai', 'sinc']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'work', 'please', 'call']\n",
      "After stop words removal: ['work', 'please', 'call']\n",
      "After stemming with porters algorithm: ['work', 'pleas', 'call']\n",
      "Tokenized sentence: ['i', 'calls', 'you', 'later', 'afternoon', 'onwords', 'mtnl', 'service', 'get', 'problem', 'in', 'south', 'mumbai', 'i', 'can', 'hear', 'you', 'but', 'you', 'cann', 't', 'listen', 'me']\n",
      "After stop words removal: ['calls', 'later', 'afternoon', 'onwords', 'mtnl', 'service', 'get', 'problem', 'south', 'mumbai', 'hear', 'cann', 'listen']\n",
      "After stemming with porters algorithm: ['call', 'later', 'afternoon', 'onword', 'mtnl', 'servic', 'get', 'problem', 'south', 'mumbai', 'hear', 'cann', 'listen']\n",
      "Tokenized sentence: ['any', 'chance', 'you', 'might', 'have', 'had', 'with', 'me', 'evaporated', 'as', 'soon', 'as', 'you', 'violated', 'my', 'privacy', 'by', 'stealing', 'my', 'phone', 'number', 'from', 'your', 'employer', 's', 'paperwork', 'not', 'cool', 'at', 'all', 'please', 'do', 'not', 'contact', 'me', 'again', 'or', 'i', 'will', 'report', 'you', 'to', 'your', 'supervisor']\n",
      "After stop words removal: ['chance', 'might', 'evaporated', 'soon', 'violated', 'privacy', 'stealing', 'phone', 'number', 'employer', 'paperwork', 'cool', 'please', 'contact', 'report', 'supervisor']\n",
      "evaporate\n",
      "violate\n",
      "steal\n",
      "After stemming with porters algorithm: ['chanc', 'might', 'evapor', 'soon', 'violat', 'privaci', 'steal', 'phone', 'number', 'employ', 'paperwork', 'cool', 'pleas', 'contact', 'report', 'supervisor']\n",
      "Tokenized sentence: ['ok', 'every', 'night', 'take', 'a', 'warm', 'bath', 'drink', 'a', 'cup', 'of', 'milk', 'and', 'you', 'll', 'see', 'a', 'work', 'of', 'magic', 'you', 'still', 'need', 'to', 'loose', 'weight', 'just', 'so', 'that', 'you', 'know']\n",
      "After stop words removal: ['ok', 'every', 'night', 'take', 'warm', 'bath', 'drink', 'cup', 'milk', 'see', 'work', 'magic', 'still', 'need', 'loose', 'weight', 'know']\n",
      "After stemming with porters algorithm: ['everi', 'night', 'take', 'warm', 'bath', 'drink', 'cup', 'milk', 'see', 'work', 'magic', 'still', 'need', 'loos', 'weight', 'know']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'you', 'last', 'weekends', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hours', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'last', 'weekends', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hours']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'last', 'weekend', 'draw', 'show', 'priz', 'guaranteed', 'call', 'claim', 'code', 'valid', 'hour']\n",
      "Tokenized sentence: ['well', 'balls', 'time', 'to', 'make', 'calls']\n",
      "After stop words removal: ['well', 'balls', 'time', 'make', 'calls']\n",
      "After stemming with porters algorithm: ['well', 'ball', 'time', 'make', 'call']\n",
      "Tokenized sentence: ['change', 'windows', 'logoff', 'sound']\n",
      "After stop words removal: ['change', 'windows', 'logoff', 'sound']\n",
      "After stemming with porters algorithm: ['chang', 'window', 'logoff', 'sound']\n",
      "Tokenized sentence: ['i', 'met', 'you', 'as', 'a', 'stranger', 'and', 'choose', 'you', 'as', 'my', 'friend', 'as', 'long', 'as', 'the', 'world', 'stands', 'our', 'friendship', 'never', 'ends', 'lets', 'be', 'friends', 'forever', 'gud', 'nitz']\n",
      "After stop words removal: ['met', 'stranger', 'choose', 'friend', 'long', 'world', 'stands', 'friendship', 'never', 'ends', 'lets', 'friends', 'forever', 'gud', 'nitz']\n",
      "After stemming with porters algorithm: ['met', 'stranger', 'choos', 'friend', 'long', 'world', 'stand', 'friendship', 'never', 'end', 'let', 'friend', 'forev', 'gud', 'nitz']\n",
      "Tokenized sentence: ['best', 'line', 'said', 'in', 'love', 'i', 'will', 'wait', 'till', 'the', 'day', 'i', 'can', 'forget', 'u', 'or', 'the', 'day', 'u', 'realize', 'that', 'u', 'cannot', 'forget', 'me', 'gn']\n",
      "After stop words removal: ['best', 'line', 'said', 'love', 'wait', 'till', 'day', 'forget', 'u', 'day', 'u', 'realize', 'u', 'cannot', 'forget', 'gn']\n",
      "After stemming with porters algorithm: ['best', 'line', 'said', 'love', 'wait', 'till', 'dai', 'forget', 'dai', 'realiz', 'cannot', 'forget']\n",
      "Tokenized sentence: ['dating', 'service', 'cal', 'l', 'box', 'sk', 'ch']\n",
      "After stop words removal: ['dating', 'service', 'cal', 'l', 'box', 'sk', 'ch']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['date', 'servic', 'cal', 'box']\n",
      "Tokenized sentence: ['ok', 'now', 'i', 'am', 'in', 'bus', 'if', 'i', 'come', 'soon', 'i', 'will', 'come', 'otherwise', 'tomorrow']\n",
      "After stop words removal: ['ok', 'bus', 'come', 'soon', 'come', 'otherwise', 'tomorrow']\n",
      "After stemming with porters algorithm: ['bu', 'come', 'soon', 'come', 'otherwis', 'tomorrow']\n",
      "Tokenized sentence: ['s', 'i', 'will', 'take', 'mokka', 'players', 'only']\n",
      "After stop words removal: ['take', 'mokka', 'players']\n",
      "After stemming with porters algorithm: ['take', 'mokka', 'player']\n",
      "Tokenized sentence: ['wow', 'v', 'v', 'impressed', 'have', 'funs', 'shopping']\n",
      "After stop words removal: ['wow', 'v', 'v', 'impressed', 'funs', 'shopping']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['wow', 'impress', 'fun', 'shop']\n",
      "Tokenized sentence: ['ya', 'they', 'are', 'well', 'and', 'fine', 'bbd', 'pooja', 'full', 'pimples', 'even', 'she', 'become', 'quite', 'black', 'and', 'ur', 'rite', 'here', 'its', 'too', 'cold', 'wearing', 'sweatter']\n",
      "After stop words removal: ['ya', 'well', 'fine', 'bbd', 'pooja', 'full', 'pimples', 'even', 'become', 'quite', 'black', 'ur', 'rite', 'cold', 'wearing', 'sweatter']\n",
      "wear\n",
      "After stemming with porters algorithm: ['well', 'fine', 'bbd', 'pooja', 'full', 'pimpl', 'even', 'becom', 'quit', 'black', 'rite', 'cold', 'wear', 'sweatter']\n",
      "Tokenized sentence: ['do', 'you', 'work', 'all', 'this', 'week']\n",
      "After stop words removal: ['work', 'week']\n",
      "After stemming with porters algorithm: ['work', 'week']\n",
      "Tokenized sentence: ['you', 'are', 'always', 'putting', 'your', 'business', 'out', 'there', 'you', 'put', 'pictures', 'of', 'your', 'ass', 'on', 'facebook', 'you', 'are', 'one', 'of', 'the', 'most', 'open', 'people', 'i', 've', 'ever', 'met', 'why', 'would', 'i', 'think', 'a', 'picture', 'of', 'your', 'room', 'would', 'hurt', 'you', 'make', 'you', 'feel', 'violated']\n",
      "After stop words removal: ['always', 'putting', 'business', 'put', 'pictures', 'ass', 'facebook', 'one', 'open', 'people', 'ever', 'met', 'would', 'think', 'picture', 'room', 'would', 'hurt', 'make', 'feel', 'violated']\n",
      "putt\n",
      "violate\n",
      "After stemming with porters algorithm: ['alwai', 'put', 'busi', 'put', 'pictur', 'ass', 'facebook', 'on', 'open', 'peopl', 'ever', 'met', 'would', 'think', 'pictur', 'room', 'would', 'hurt', 'make', 'feel', 'violat']\n",
      "Tokenized sentence: ['bill', 'as', 'in', 'are', 'there', 'any', 'letters', 'for', 'me', 'i', 'm', 'expecting', 'one', 'from', 'orange', 'that', 'isn', 't', 'a', 'bill', 'but', 'may', 'still', 'say', 'orange', 'on', 'it']\n",
      "After stop words removal: ['bill', 'letters', 'expecting', 'one', 'orange', 'bill', 'may', 'still', 'say', 'orange']\n",
      "expect\n",
      "After stemming with porters algorithm: ['bill', 'letter', 'expec', 'on', 'orang', 'bill', 'mai', 'still', 'sai', 'orang']\n",
      "Tokenized sentence: ['god', 's', 'love', 'has', 'no', 'limit', 'god', 's', 'grace', 'has', 'no', 'measure', 'god', 's', 'power', 'has', 'no', 'boundaries', 'may', 'u', 'have', 'god', 's', 'endless', 'blessings', 'always', 'in', 'ur', 'life', 'gud', 'ni']\n",
      "After stop words removal: ['god', 'love', 'limit', 'god', 'grace', 'measure', 'god', 'power', 'boundaries', 'may', 'u', 'god', 'endless', 'blessings', 'always', 'ur', 'life', 'gud', 'ni']\n",
      "bless\n",
      "After stemming with porters algorithm: ['god', 'love', 'limit', 'god', 'grace', 'measur', 'god', 'power', 'boundari', 'mai', 'god', 'endless', 'bless', 'alwai', 'life', 'gud']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'winner', 'or', 'guaranteed', 'caller', 'prize', 'this', 'is', 'our', 'final', 'attempt', 'to', 'contact', 'you', 'to', 'claim', 'call', 'now', 'ppmpobox', 'bhamb', 'xe']\n",
      "After stop words removal: ['winner', 'guaranteed', 'caller', 'prize', 'final', 'attempt', 'contact', 'claim', 'call', 'ppmpobox', 'bhamb', 'xe']\n",
      "After stemming with porters algorithm: ['winner', 'guaranteed', 'caller', 'priz', 'final', 'attempt', 'contact', 'claim', 'call', 'ppmpobox', 'bhamb']\n",
      "Tokenized sentence: ['ok', 'then', 'r', 'we', 'meeting', 'later']\n",
      "After stop words removal: ['ok', 'r', 'meeting', 'later']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'later']\n",
      "Tokenized sentence: ['to', 'review', 'and', 'keep', 'the', 'fantastic', 'nokia', 'n', 'gage', 'game', 'deck', 'with', 'club', 'nokia', 'go', 'www', 'cnupdates', 'com', 'newsletter', 'unsubscribe', 'from', 'alerts', 'reply', 'with', 'the', 'word', 'out']\n",
      "After stop words removal: ['review', 'keep', 'fantastic', 'nokia', 'n', 'gage', 'game', 'deck', 'club', 'nokia', 'go', 'www', 'cnupdates', 'com', 'newsletter', 'unsubscribe', 'alerts', 'reply', 'word']\n",
      "After stemming with porters algorithm: ['review', 'keep', 'fantast', 'nokia', 'gage', 'game', 'deck', 'club', 'nokia', 'www', 'cnupdat', 'com', 'newslett', 'unsubscrib', 'alert', 'repli', 'word']\n",
      "Tokenized sentence: ['good', 'morning', 'my', 'dear', 'have', 'a', 'great', 'amp', 'successful', 'day']\n",
      "After stop words removal: ['good', 'morning', 'dear', 'great', 'amp', 'successful', 'day']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'dear', 'great', 'amp', 'success', 'dai']\n",
      "Tokenized sentence: ['oh', 'k', 'why', 'you', 'got', 'job', 'then', 'whats', 'up']\n",
      "After stop words removal: ['oh', 'k', 'got', 'job', 'whats']\n",
      "After stemming with porters algorithm: ['got', 'job', 'what']\n",
      "Tokenized sentence: ['i', 'don', 't', 'run', 'away', 'frm', 'u', 'i', 'walk', 'slowly', 'amp', 'it', 'kills', 'me', 'that', 'u', 'don', 't', 'care', 'enough', 'to', 'stop', 'me']\n",
      "After stop words removal: ['run', 'away', 'frm', 'u', 'walk', 'slowly', 'amp', 'kills', 'u', 'care', 'enough', 'stop']\n",
      "After stemming with porters algorithm: ['run', 'awai', 'frm', 'walk', 'slowli', 'amp', 'kill', 'care', 'enough', 'stop']\n",
      "Tokenized sentence: ['today', 'is', 'sorry', 'day', 'if', 'ever', 'i', 'was', 'angry', 'with', 'you', 'if', 'ever', 'i', 'misbehaved', 'or', 'hurt', 'you', 'plz', 'plz', 'just', 'slap', 'urself', 'bcoz', 'its', 'ur', 'fault', 'i', 'm', 'basically', 'good']\n",
      "After stop words removal: ['today', 'sorry', 'day', 'ever', 'angry', 'ever', 'misbehaved', 'hurt', 'plz', 'plz', 'slap', 'urself', 'bcoz', 'ur', 'fault', 'basically', 'good']\n",
      "After stemming with porters algorithm: ['todai', 'sorri', 'dai', 'ever', 'angri', 'ever', 'misbehav', 'hurt', 'plz', 'plz', 'slap', 'urself', 'bcoz', 'fault', 'basic', 'good']\n",
      "Tokenized sentence: ['yo', 'chad', 'which', 'gymnastics', 'class', 'do', 'you', 'wanna', 'take', 'the', 'site', 'says', 'christians', 'class', 'is', 'full']\n",
      "After stop words removal: ['yo', 'chad', 'gymnastics', 'class', 'wanna', 'take', 'site', 'says', 'christians', 'class', 'full']\n",
      "After stemming with porters algorithm: ['chad', 'gymnastic', 'class', 'wanna', 'take', 'site', 'sai', 'christian', 'class', 'full']\n",
      "Tokenized sentence: ['he', 'neva', 'grumble', 'but', 'i', 'sad', 'lor', 'hee', 'buy', 'tmr', 'lor', 'aft', 'lunch', 'but', 'we', 'still', 'meetin', 'lunch', 'tmr', 'a', 'not', 'neva', 'hear', 'fr', 'them', 'lei', 'got', 'a', 'lot', 'of', 'work', 'ar']\n",
      "After stop words removal: ['neva', 'grumble', 'sad', 'lor', 'hee', 'buy', 'tmr', 'lor', 'aft', 'lunch', 'still', 'meetin', 'lunch', 'tmr', 'neva', 'hear', 'fr', 'lei', 'got', 'lot', 'work', 'ar']\n",
      "After stemming with porters algorithm: ['neva', 'grumbl', 'sad', 'lor', 'hee', 'bui', 'tmr', 'lor', 'aft', 'lunch', 'still', 'meetin', 'lunch', 'tmr', 'neva', 'hear', 'lei', 'got', 'lot', 'work']\n",
      "Tokenized sentence: ['white', 'fudge', 'oreos', 'are', 'in', 'stores']\n",
      "After stop words removal: ['white', 'fudge', 'oreos', 'stores']\n",
      "After stemming with porters algorithm: ['white', 'fudg', 'oreo', 'store']\n",
      "Tokenized sentence: ['yes', 'i', 'come', 'to', 'nyc', 'for', 'audiitions', 'and', 'am', 'trying', 'to', 'relocate']\n",
      "After stop words removal: ['yes', 'come', 'nyc', 'audiitions', 'trying', 'relocate']\n",
      "After stemming with porters algorithm: ['ye', 'come', 'nyc', 'audiit', 'trying', 'reloc']\n",
      "Tokenized sentence: ['for', 'your', 'chance', 'to', 'win', 'a', 'free', 'bluetooth', 'headset', 'then', 'simply', 'reply', 'back', 'with', 'adp']\n",
      "After stop words removal: ['chance', 'win', 'free', 'bluetooth', 'headset', 'simply', 'reply', 'back', 'adp']\n",
      "After stemming with porters algorithm: ['chanc', 'win', 'free', 'bluetooth', 'headset', 'simpli', 'repli', 'back', 'adp']\n",
      "Tokenized sentence: ['hey', 'i', 'missed', 'you', 'tm', 'of', 'last', 'night', 'as', 'my', 'phone', 'was', 'on', 'the', 'charge', 'smiles', 'i', 'am', 'meeting', 'a', 'friend', 'shortly']\n",
      "After stop words removal: ['hey', 'missed', 'tm', 'last', 'night', 'phone', 'charge', 'smiles', 'meeting', 'friend', 'shortly']\n",
      "meet\n",
      "After stemming with porters algorithm: ['hei', 'miss', 'last', 'night', 'phone', 'charg', 'smile', 'meet', 'friend', 'shortli']\n",
      "Tokenized sentence: ['i', 'm', 'gonna', 'be', 'home', 'soon', 'and', 'i', 'don', 't', 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', 'k', 'i', 've', 'cried', 'enough', 'today']\n",
      "After stop words removal: ['gonna', 'home', 'soon', 'want', 'talk', 'stuff', 'anymore', 'tonight', 'k', 'cried', 'enough', 'today']\n",
      "After stemming with porters algorithm: ['gonna', 'home', 'soon', 'want', 'talk', 'stuff', 'anymor', 'tonight', 'cri', 'enough', 'todai']\n",
      "Tokenized sentence: ['i', 'can', 'make', 'lasagna', 'for', 'you', 'vodka']\n",
      "After stop words removal: ['make', 'lasagna', 'vodka']\n",
      "After stemming with porters algorithm: ['make', 'lasagna', 'vodka']\n",
      "Tokenized sentence: ['i', 'm', 'tired', 'of', 'arguing', 'with', 'you', 'about', 'this', 'week', 'after', 'week', 'do', 'what', 'you', 'want', 'and', 'from', 'now', 'on', 'i', 'll', 'do', 'the', 'same']\n",
      "After stop words removal: ['tired', 'arguing', 'week', 'week', 'want']\n",
      "argu\n",
      "After stemming with porters algorithm: ['tire', 'argu', 'week', 'week', 'want']\n",
      "Tokenized sentence: ['hi', 'babe', 'im', 'at', 'home', 'now', 'wanna', 'do', 'something', 'xx']\n",
      "After stop words removal: ['hi', 'babe', 'im', 'home', 'wanna', 'something', 'xx']\n",
      "someth\n",
      "After stemming with porters algorithm: ['babe', 'home', 'wanna', 'somet']\n",
      "Tokenized sentence: ['sweet', 'heart', 'how', 'are', 'you']\n",
      "After stop words removal: ['sweet', 'heart']\n",
      "After stemming with porters algorithm: ['sweet', 'heart']\n",
      "Tokenized sentence: ['i', 'm', 'ok', 'wif', 'it', 'cos', 'i', 'like', 'try', 'new', 'things', 'but', 'i', 'scared', 'u', 'dun', 'like', 'mah', 'cos', 'u', 'said', 'not', 'too', 'loud']\n",
      "After stop words removal: ['ok', 'wif', 'cos', 'like', 'try', 'new', 'things', 'scared', 'u', 'dun', 'like', 'mah', 'cos', 'u', 'said', 'loud']\n",
      "After stemming with porters algorithm: ['wif', 'co', 'like', 'try', 'new', 'thing', 'scare', 'dun', 'like', 'mah', 'co', 'said', 'loud']\n",
      "Tokenized sentence: ['hey', 'sweet', 'i', 'was', 'wondering', 'when', 'you', 'had', 'a', 'moment', 'if', 'you', 'might', 'come', 'to', 'me', 'i', 'want', 'to', 'send', 'a', 'file', 'to', 'someone', 'but', 'it', 'won', 't', 'go', 'over', 'yahoo', 'for', 'them', 'because', 'their', 'connection', 'sucks', 'remember', 'when', 'you', 'set', 'up', 'that', 'page', 'for', 'me', 'to', 'go', 'to', 'and', 'download', 'the', 'format', 'disc', 'could', 'you', 'tell', 'me', 'how', 'to', 'do', 'that', 'or', 'do', 'you', 'know', 'some', 'other', 'way', 'to', 'download', 'big', 'files', 'because', 'they', 'can', 'download', 'stuff', 'directly', 'from', 'the', 'internet', 'any', 'help', 'would', 'be', 'great', 'my', 'prey', 'teasing', 'kiss']\n",
      "After stop words removal: ['hey', 'sweet', 'wondering', 'moment', 'might', 'come', 'want', 'send', 'file', 'someone', 'go', 'yahoo', 'connection', 'sucks', 'remember', 'set', 'page', 'go', 'download', 'format', 'disc', 'could', 'tell', 'know', 'way', 'download', 'big', 'files', 'download', 'stuff', 'directly', 'internet', 'help', 'would', 'great', 'prey', 'teasing', 'kiss']\n",
      "wonder\n",
      "teas\n",
      "After stemming with porters algorithm: ['hei', 'sweet', 'wonder', 'moment', 'might', 'come', 'want', 'send', 'file', 'someon', 'yahoo', 'connect', 'suck', 'rememb', 'set', 'page', 'download', 'format', 'disc', 'could', 'tell', 'know', 'wai', 'download', 'big', 'file', 'download', 'stuff', 'directli', 'internet', 'help', 'would', 'great', 'prei', 'teas', 'kiss']\n",
      "Tokenized sentence: ['enjoy', 'ur', 'life', 'good', 'night']\n",
      "After stop words removal: ['enjoy', 'ur', 'life', 'good', 'night']\n",
      "After stemming with porters algorithm: ['enjoi', 'life', 'good', 'night']\n",
      "Tokenized sentence: ['also', 'north', 'carolina', 'and', 'texas', 'atm', 'you', 'would', 'just', 'go', 'to', 'the', 'gre', 'site', 'and', 'pay', 'for', 'the', 'test', 'results', 'to', 'be', 'sent']\n",
      "After stop words removal: ['also', 'north', 'carolina', 'texas', 'atm', 'would', 'go', 'gre', 'site', 'pay', 'test', 'results', 'sent']\n",
      "After stemming with porters algorithm: ['also', 'north', 'carolina', 'texa', 'atm', 'would', 'gre', 'site', 'pai', 'test', 'result', 'sent']\n",
      "Tokenized sentence: ['babes', 'i', 'think', 'i', 'got', 'ur', 'brolly', 'i', 'left', 'it', 'in', 'english', 'wil', 'bring', 'it', 'in', 'mrw', 'u', 'luv', 'franxx']\n",
      "After stop words removal: ['babes', 'think', 'got', 'ur', 'brolly', 'left', 'english', 'wil', 'bring', 'mrw', 'u', 'luv', 'franxx']\n",
      "After stemming with porters algorithm: ['babe', 'think', 'got', 'brolli', 'left', 'english', 'wil', 'bring', 'mrw', 'luv', 'franxx']\n",
      "Tokenized sentence: ['did', 'u', 'find', 'a', 'sitter', 'for', 'kaitlyn', 'i', 'was', 'sick', 'and', 'slept', 'all', 'day', 'yesterday']\n",
      "After stop words removal: ['u', 'find', 'sitter', 'kaitlyn', 'sick', 'slept', 'day', 'yesterday']\n",
      "After stemming with porters algorithm: ['find', 'sitter', 'kaitlyn', 'sick', 'slept', 'dai', 'yesterdai']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'y', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['sun', 'cant', 'come', 'to', 'earth', 'but', 'send', 'luv', 'as', 'rays', 'cloud', 'cant', 'come', 'to', 'river', 'but', 'send', 'luv', 'as', 'rain', 'i', 'cant', 'come', 'to', 'meet', 'u', 'but', 'can', 'send', 'my', 'care', 'as', 'msg', 'to', 'u', 'gud', 'evng']\n",
      "After stop words removal: ['sun', 'cant', 'come', 'earth', 'send', 'luv', 'rays', 'cloud', 'cant', 'come', 'river', 'send', 'luv', 'rain', 'cant', 'come', 'meet', 'u', 'send', 'care', 'msg', 'u', 'gud', 'evng']\n",
      "After stemming with porters algorithm: ['sun', 'cant', 'come', 'earth', 'send', 'luv', 'rai', 'cloud', 'cant', 'come', 'river', 'send', 'luv', 'rain', 'cant', 'come', 'meet', 'send', 'care', 'msg', 'gud', 'evng']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'home', 'please', 'call']\n",
      "After stop words removal: ['home', 'please', 'call']\n",
      "After stemming with porters algorithm: ['home', 'pleas', 'call']\n",
      "Tokenized sentence: ['s', 'of', 'girls', 'many', 'local', 'u', 'who', 'r', 'virgins', 'this', 'r', 'ready', 'fil', 'ur', 'every', 'sexual', 'need', 'can', 'u', 'fil', 'theirs', 'text', 'cute', 'to', 'p', 'm']\n",
      "After stop words removal: ['girls', 'many', 'local', 'u', 'r', 'virgins', 'r', 'ready', 'fil', 'ur', 'every', 'sexual', 'need', 'u', 'fil', 'text', 'cute', 'p']\n",
      "After stemming with porters algorithm: ['girl', 'mani', 'local', 'virgin', 'readi', 'fil', 'everi', 'sexual', 'need', 'fil', 'text', 'cute']\n",
      "Tokenized sentence: ['well', 'i', 'have', 'to', 'leave', 'for', 'my', 'class', 'babe', 'you', 'never', 'came', 'back', 'to', 'me', 'hope', 'you', 'have', 'a', 'nice', 'sleep', 'my', 'love']\n",
      "After stop words removal: ['well', 'leave', 'class', 'babe', 'never', 'came', 'back', 'hope', 'nice', 'sleep', 'love']\n",
      "After stemming with porters algorithm: ['well', 'leav', 'class', 'babe', 'never', 'came', 'back', 'hope', 'nice', 'sleep', 'love']\n",
      "Tokenized sentence: ['email', 'alertfrom', 'jeri', 'stewartsize', 'kbsubject', 'low', 'cost', 'prescripiton', 'drvgsto', 'listen', 'to', 'email', 'call']\n",
      "After stop words removal: ['email', 'alertfrom', 'jeri', 'stewartsize', 'kbsubject', 'low', 'cost', 'prescripiton', 'drvgsto', 'listen', 'email', 'call']\n",
      "After stemming with porters algorithm: ['email', 'alertfrom', 'jeri', 'stewarts', 'kbsubject', 'low', 'cost', 'prescripiton', 'drvgsto', 'listen', 'email', 'call']\n",
      "Tokenized sentence: ['hey', 'there', 'glad', 'u', 'r', 'better', 'now', 'i', 'hear', 'u', 'treated', 'urself', 'to', 'a', 'digi', 'cam', 'is', 'it', 'good', 'we', 'r', 'off', 'at', 'pm', 'have', 'a', 'fab', 'new', 'year', 'c', 'u', 'in', 'coupla', 'wks']\n",
      "After stop words removal: ['hey', 'glad', 'u', 'r', 'better', 'hear', 'u', 'treated', 'urself', 'digi', 'cam', 'good', 'r', 'pm', 'fab', 'new', 'year', 'c', 'u', 'coupla', 'wks']\n",
      "treate\n",
      "After stemming with porters algorithm: ['hei', 'glad', 'better', 'hear', 'treat', 'urself', 'digi', 'cam', 'good', 'fab', 'new', 'year', 'coupla', 'wk']\n",
      "Tokenized sentence: ['thk', 'of', 'wat', 'to', 'eat', 'tonight']\n",
      "After stop words removal: ['thk', 'wat', 'eat', 'tonight']\n",
      "After stemming with porters algorithm: ['thk', 'wat', 'eat', 'tonight']\n",
      "Tokenized sentence: ['have', 'you', 'got', 'xmas', 'radio', 'times', 'if', 'not', 'i', 'will', 'get', 'it', 'now']\n",
      "After stop words removal: ['got', 'xmas', 'radio', 'times', 'get']\n",
      "After stemming with porters algorithm: ['got', 'xma', 'radio', 'time', 'get']\n",
      "Tokenized sentence: ['lara', 'said', 'she', 'can', 'loan', 'me', 'lt', 'gt']\n",
      "After stop words removal: ['lara', 'said', 'loan', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['lara', 'said', 'loan']\n",
      "Tokenized sentence: ['mum', 'i', 've', 'sent', 'you', 'many', 'many', 'messages', 'since', 'i', 'got', 'here', 'i', 'just', 'want', 'to', 'know', 'that', 'you', 'are', 'actually', 'getting', 'them', 'do', 'enjoy', 'the', 'rest', 'of', 'your', 'day']\n",
      "After stop words removal: ['mum', 'sent', 'many', 'many', 'messages', 'since', 'got', 'want', 'know', 'actually', 'getting', 'enjoy', 'rest', 'day']\n",
      "gett\n",
      "After stemming with porters algorithm: ['mum', 'sent', 'mani', 'mani', 'messag', 'sinc', 'got', 'want', 'know', 'actual', 'get', 'enjoi', 'rest', 'dai']\n",
      "Tokenized sentence: ['i', 'went', 'to', 'project', 'centre']\n",
      "After stop words removal: ['went', 'project', 'centre']\n",
      "After stemming with porters algorithm: ['went', 'project', 'centr']\n",
      "Tokenized sentence: ['dude', 'avatar', 'd', 'was', 'imp', 'at', 'one', 'point', 'i', 'thought', 'there', 'were', 'actually', 'flies', 'in', 'the', 'room', 'and', 'almost', 'tried', 'hittng', 'one', 'as', 'a', 'reflex']\n",
      "After stop words removal: ['dude', 'avatar', 'imp', 'one', 'point', 'thought', 'actually', 'flies', 'room', 'almost', 'tried', 'hittng', 'one', 'reflex']\n",
      "After stemming with porters algorithm: ['dude', 'avatar', 'imp', 'on', 'point', 'thought', 'actual', 'fli', 'room', 'almost', 'tri', 'hittng', 'on', 'reflex']\n",
      "Tokenized sentence: ['i', 'love', 'u', 'my', 'little', 'pocy', 'bell', 'i', 'am', 'sorry', 'but', 'i', 'love', 'u']\n",
      "After stop words removal: ['love', 'u', 'little', 'pocy', 'bell', 'sorry', 'love', 'u']\n",
      "After stemming with porters algorithm: ['love', 'littl', 'poci', 'bell', 'sorri', 'love']\n",
      "Tokenized sentence: ['hope', 'you', 'are', 'having', 'a', 'great', 'new', 'semester', 'do', 'wish', 'you', 'the', 'very', 'best', 'you', 'are', 'made', 'for', 'greatness']\n",
      "After stop words removal: ['hope', 'great', 'new', 'semester', 'wish', 'best', 'made', 'greatness']\n",
      "After stemming with porters algorithm: ['hope', 'great', 'new', 'semest', 'wish', 'best', 'made', 'great']\n",
      "Tokenized sentence: ['y', 'bishan', 'lei', 'i', 'tot', 'say', 'lavender']\n",
      "After stop words removal: ['bishan', 'lei', 'tot', 'say', 'lavender']\n",
      "After stemming with porters algorithm: ['bishan', 'lei', 'tot', 'sai', 'lavend']\n",
      "Tokenized sentence: ['where', 'are', 'you', 'you', 'said', 'you', 'would', 'be', 'here', 'when', 'i', 'woke']\n",
      "After stop words removal: ['said', 'would', 'woke']\n",
      "After stemming with porters algorithm: ['said', 'would', 'woke']\n",
      "Tokenized sentence: ['chk', 'in', 'ur', 'belovd', 'ms', 'dict']\n",
      "After stop words removal: ['chk', 'ur', 'belovd', 'ms', 'dict']\n",
      "After stemming with porters algorithm: ['chk', 'belovd', 'dict']\n",
      "Tokenized sentence: ['enjoy', 'the', 'jamster', 'videosound', 'gold', 'club', 'with', 'your', 'credits', 'for', 'new', 'videosounds', 'logos', 'musicnews', 'get', 'more', 'fun', 'from', 'jamster', 'co', 'uk', 'only', 'help', 'call']\n",
      "After stop words removal: ['enjoy', 'jamster', 'videosound', 'gold', 'club', 'credits', 'new', 'videosounds', 'logos', 'musicnews', 'get', 'fun', 'jamster', 'co', 'uk', 'help', 'call']\n",
      "After stemming with porters algorithm: ['enjoi', 'jamster', 'videosound', 'gold', 'club', 'credit', 'new', 'videosound', 'logo', 'musicnew', 'get', 'fun', 'jamster', 'help', 'call']\n",
      "Tokenized sentence: ['hello', 'peach', 'my', 'cake', 'tasts', 'lush']\n",
      "After stop words removal: ['hello', 'peach', 'cake', 'tasts', 'lush']\n",
      "After stemming with porters algorithm: ['hello', 'peach', 'cake', 'tast', 'lush']\n",
      "Tokenized sentence: ['sorry', 'i', 'can', 't', 'help', 'you', 'on', 'this']\n",
      "After stop words removal: ['sorry', 'help']\n",
      "After stemming with porters algorithm: ['sorri', 'help']\n",
      "Tokenized sentence: ['are', 'you', 'still', 'getting', 'the', 'goods']\n",
      "After stop words removal: ['still', 'getting', 'goods']\n",
      "gett\n",
      "After stemming with porters algorithm: ['still', 'get', 'good']\n",
      "Tokenized sentence: ['i', 'am', 'thinking', 'of', 'going', 'down', 'to', 'reg', 'for', 'pract', 'lessons', 'flung', 'my', 'advance', 'haha', 'wat', 'time', 'u', 'going']\n",
      "After stop words removal: ['thinking', 'going', 'reg', 'pract', 'lessons', 'flung', 'advance', 'haha', 'wat', 'time', 'u', 'going']\n",
      "think\n",
      "go\n",
      "go\n",
      "After stemming with porters algorithm: ['thin', 'go', 'reg', 'pract', 'lesson', 'flung', 'advanc', 'haha', 'wat', 'time', 'go']\n",
      "Tokenized sentence: ['what', 'i', 'mean', 'was', 'i', 'left', 'too', 'early', 'to', 'check', 'cos', 'i', 'm', 'working', 'a']\n",
      "After stop words removal: ['mean', 'left', 'early', 'check', 'cos', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['mean', 'left', 'earli', 'check', 'co', 'wor']\n",
      "Tokenized sentence: ['i', 'tagged', 'my', 'friends', 'that', 'you', 'seemed', 'to', 'count', 'as', 'your', 'friends']\n",
      "After stop words removal: ['tagged', 'friends', 'seemed', 'count', 'friends']\n",
      "After stemming with porters algorithm: ['tag', 'friend', 'seem', 'count', 'friend']\n",
      "Tokenized sentence: ['shop', 'till', 'u', 'drop', 'is', 'it', 'you', 'either', 'k', 'k', 'cash', 'or', 'travel', 'voucher', 'call', 'now', 'ntt', 'po', 'box', 'cr', 'bt', 'fixedline', 'cost', 'ppm', 'mobile', 'vary']\n",
      "After stop words removal: ['shop', 'till', 'u', 'drop', 'either', 'k', 'k', 'cash', 'travel', 'voucher', 'call', 'ntt', 'po', 'box', 'cr', 'bt', 'fixedline', 'cost', 'ppm', 'mobile', 'vary']\n",
      "After stemming with porters algorithm: ['shop', 'till', 'drop', 'either', 'cash', 'travel', 'voucher', 'call', 'ntt', 'box', 'fixedlin', 'cost', 'ppm', 'mobil', 'vari']\n",
      "Tokenized sentence: ['i', 'm', 'also', 'came', 'to', 'room']\n",
      "After stop words removal: ['also', 'came', 'room']\n",
      "After stemming with porters algorithm: ['also', 'came', 'room']\n",
      "Tokenized sentence: ['hey', 'are', 'you', 'going', 'to', 'quit', 'soon', 'xuhui', 'and', 'i', 'working', 'till', 'end', 'of', 'the', 'month']\n",
      "After stop words removal: ['hey', 'going', 'quit', 'soon', 'xuhui', 'working', 'till', 'end', 'month']\n",
      "go\n",
      "work\n",
      "After stemming with porters algorithm: ['hei', 'go', 'quit', 'soon', 'xuhui', 'wor', 'till', 'end', 'month']\n",
      "Tokenized sentence: ['for', 'real', 'tho', 'this', 'sucks', 'i', 'can', 't', 'even', 'cook', 'my', 'whole', 'electricity', 'is', 'out', 'and', 'i', 'm', 'hungry']\n",
      "After stop words removal: ['real', 'tho', 'sucks', 'even', 'cook', 'whole', 'electricity', 'hungry']\n",
      "After stemming with porters algorithm: ['real', 'tho', 'suck', 'even', 'cook', 'whole', 'electr', 'hungri']\n",
      "Tokenized sentence: ['hello', 'hun', 'how', 'ru', 'its', 'here', 'by', 'the', 'way', 'im', 'good', 'been', 'on', 'dates', 'with', 'that', 'guy', 'i', 'met', 'in', 'walkabout', 'so', 'far', 'we', 'have', 'to', 'meet', 'up', 'soon', 'hows', 'everyone', 'else']\n",
      "After stop words removal: ['hello', 'hun', 'ru', 'way', 'im', 'good', 'dates', 'guy', 'met', 'walkabout', 'far', 'meet', 'soon', 'hows', 'everyone', 'else']\n",
      "After stemming with porters algorithm: ['hello', 'hun', 'wai', 'good', 'date', 'gui', 'met', 'walkabout', 'far', 'meet', 'soon', 'how', 'everyon', 'els']\n",
      "Tokenized sentence: ['thanks', 'for', 'understanding', 'i', 've', 'been', 'trying', 'to', 'tell', 'sura', 'that']\n",
      "After stop words removal: ['thanks', 'understanding', 'trying', 'tell', 'sura']\n",
      "understand\n",
      "After stemming with porters algorithm: ['thank', 'understan', 'trying', 'tell', 'sura']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'your', 'office', 'na']\n",
      "After stop words removal: ['office', 'na']\n",
      "After stemming with porters algorithm: ['offic']\n",
      "Tokenized sentence: ['new', 'mobiles', 'from', 'must', 'go', 'txt', 'nokia', 'to', 'no', 'collect', 'yours', 'today', 'from', 'only', 'www', 'tc', 'biz', 'optout', 'gbp', 'mtmsg', 'txtauction']\n",
      "After stop words removal: ['new', 'mobiles', 'must', 'go', 'txt', 'nokia', 'collect', 'today', 'www', 'tc', 'biz', 'optout', 'gbp', 'mtmsg', 'txtauction']\n",
      "After stemming with porters algorithm: ['new', 'mobil', 'must', 'txt', 'nokia', 'collect', 'todai', 'www', 'biz', 'optout', 'gbp', 'mtmsg', 'txtauct']\n",
      "Tokenized sentence: ['and', 'that', 'is', 'the', 'problem', 'you', 'walk', 'around', 'in', 'julianaland', 'oblivious', 'to', 'what', 'is', 'going', 'on', 'around', 'you', 'i', 'say', 'the', 'same', 'things', 'constantly', 'and', 'they', 'go', 'in', 'one', 'ear', 'and', 'out', 'the', 'other', 'while', 'you', 'go', 'off', 'doing', 'whatever', 'you', 'want', 'to', 'do', 'it', 's', 'not', 'that', 'you', 'don', 't', 'know', 'why', 'i', 'm', 'upset', 'it', 's', 'that', 'you', 'don', 't', 'listen', 'when', 'i', 'tell', 'you', 'what', 'is', 'going', 'to', 'upset', 'me', 'then', 'you', 'want', 'to', 'be', 'surprised', 'when', 'i', 'm', 'mad']\n",
      "After stop words removal: ['problem', 'walk', 'around', 'julianaland', 'oblivious', 'going', 'around', 'say', 'things', 'constantly', 'go', 'one', 'ear', 'go', 'whatever', 'want', 'know', 'upset', 'listen', 'tell', 'going', 'upset', 'want', 'surprised', 'mad']\n",
      "go\n",
      "go\n",
      "After stemming with porters algorithm: ['problem', 'walk', 'around', 'julianaland', 'oblivi', 'go', 'around', 'sai', 'thing', 'constantli', 'on', 'ear', 'whatev', 'want', 'know', 'upset', 'listen', 'tell', 'go', 'upset', 'want', 'surpris', 'mad']\n",
      "Tokenized sentence: ['no', 'go', 'no', 'openings', 'for', 'that', 'room', 'til', 'after', 'thanksgiving', 'without', 'an', 'upcharge']\n",
      "After stop words removal: ['go', 'openings', 'room', 'til', 'thanksgiving', 'without', 'upcharge']\n",
      "open\n",
      "thanksgiv\n",
      "After stemming with porters algorithm: ['open', 'room', 'til', 'thanksgiv', 'without', 'upcharg']\n",
      "Tokenized sentence: ['im', 'sorry', 'bout', 'last', 'nite', 'it', 'wasn', 't', 'ur', 'fault', 'it', 'was', 'me', 'spouse', 'it', 'was', 'pmt', 'or', 'sumthin', 'u', 'give', 'me', 'i', 'think', 'u', 'shldxxxx']\n",
      "After stop words removal: ['im', 'sorry', 'bout', 'last', 'nite', 'ur', 'fault', 'spouse', 'pmt', 'sumthin', 'u', 'give', 'think', 'u', 'shldxxxx']\n",
      "After stemming with porters algorithm: ['sorri', 'bout', 'last', 'nite', 'fault', 'spous', 'pmt', 'sumthin', 'give', 'think', 'shldxxxx']\n",
      "Tokenized sentence: ['for', 'real', 'when', 'u', 'getting', 'on', 'yo', 'i', 'only', 'need', 'more', 'tickets', 'and', 'one', 'more', 'jacket', 'and', 'i', 'm', 'done', 'i', 'already', 'used', 'all', 'my', 'multis']\n",
      "After stop words removal: ['real', 'u', 'getting', 'yo', 'need', 'tickets', 'one', 'jacket', 'done', 'already', 'used', 'multis']\n",
      "gett\n",
      "After stemming with porters algorithm: ['real', 'get', 'need', 'ticket', 'on', 'jacket', 'done', 'alreadi', 'us', 'multi']\n",
      "Tokenized sentence: ['i', 'jokin', 'oni', 'lar', 'busy', 'then', 'i', 'wun', 'disturb']\n",
      "After stop words removal: ['jokin', 'oni', 'lar', 'busy', 'wun', 'disturb']\n",
      "After stemming with porters algorithm: ['jokin', 'oni', 'lar', 'busi', 'wun', 'disturb']\n",
      "Tokenized sentence: ['also', 'hi', 'wesley', 'how', 've', 'you', 'been']\n",
      "After stop words removal: ['also', 'hi', 'wesley']\n",
      "After stemming with porters algorithm: ['also', 'weslei']\n",
      "Tokenized sentence: ['stupid', 'its', 'not', 'possible']\n",
      "After stop words removal: ['stupid', 'possible']\n",
      "After stemming with porters algorithm: ['stupid', 'possib']\n",
      "Tokenized sentence: ['im', 'on', 'gloucesterroad', 'what', 'are', 'uup', 'to', 'later']\n",
      "After stop words removal: ['im', 'gloucesterroad', 'uup', 'later']\n",
      "After stemming with porters algorithm: ['gloucesterroad', 'uup', 'later']\n",
      "Tokenized sentence: ['hey', 'don', 't', 'forget', 'you', 'are', 'mine', 'for', 'me', 'my', 'possession', 'my', 'property', 'mmm', 'childish', 'smile']\n",
      "After stop words removal: ['hey', 'forget', 'mine', 'possession', 'property', 'mmm', 'childish', 'smile']\n",
      "After stemming with porters algorithm: ['hei', 'forget', 'mine', 'possess', 'properti', 'mmm', 'childish', 'smile']\n",
      "Tokenized sentence: ['you', 'said', 'to', 'me', 'before', 'i', 'went', 'back', 'to', 'bed', 'that', 'you', 'can', 't', 'sleep', 'for', 'anything']\n",
      "After stop words removal: ['said', 'went', 'back', 'bed', 'sleep', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['said', 'went', 'back', 'bed', 'sleep', 'anyt']\n",
      "Tokenized sentence: ['hard', 'but', 'true', 'how', 'much', 'you', 'show', 'amp', 'express', 'your', 'love', 'to', 'someone', 'that', 'much', 'it', 'will', 'hurt', 'when', 'they', 'leave', 'you', 'or', 'you', 'get', 'seperated', 'ud', 'evening']\n",
      "After stop words removal: ['hard', 'true', 'much', 'show', 'amp', 'express', 'love', 'someone', 'much', 'hurt', 'leave', 'get', 'seperated', 'ud', 'evening']\n",
      "seperate\n",
      "even\n",
      "After stemming with porters algorithm: ['hard', 'true', 'much', 'show', 'amp', 'express', 'love', 'someon', 'much', 'hurt', 'leav', 'get', 'seper', 'even']\n",
      "Tokenized sentence: ['mila', 'age', 'blonde', 'new', 'in', 'uk', 'i', 'look', 'sex', 'with', 'uk', 'guys', 'if', 'u', 'like', 'fun', 'with', 'me', 'text', 'mtalk', 'to', 'pp', 'txt', 'st', 'free', 'increments', 'help']\n",
      "After stop words removal: ['mila', 'age', 'blonde', 'new', 'uk', 'look', 'sex', 'uk', 'guys', 'u', 'like', 'fun', 'text', 'mtalk', 'pp', 'txt', 'st', 'free', 'increments', 'help']\n",
      "After stemming with porters algorithm: ['mila', 'ag', 'blond', 'new', 'look', 'sex', 'gui', 'like', 'fun', 'text', 'mtalk', 'txt', 'free', 'increm', 'help']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'call', 'you', 're', 'your', 'reply', 'to', 'our', 'sms', 'for', 'a', 'video', 'mobile', 'mins', 'unlimited', 'text', 'free', 'camcorder', 'reply', 'or', 'call', 'now', 'del', 'thurs']\n",
      "After stop words removal: ['tried', 'call', 'reply', 'sms', 'video', 'mobile', 'mins', 'unlimited', 'text', 'free', 'camcorder', 'reply', 'call', 'del', 'thurs']\n",
      "After stemming with porters algorithm: ['tri', 'call', 'repli', 'sm', 'video', 'mobil', 'min', 'unlimit', 'text', 'free', 'camcord', 'repli', 'call', 'del', 'thur']\n",
      "Tokenized sentence: ['i', 'keep', 'ten', 'rs', 'in', 'my', 'shelf', 'buy', 'two', 'egg']\n",
      "After stop words removal: ['keep', 'ten', 'rs', 'shelf', 'buy', 'two', 'egg']\n",
      "After stemming with porters algorithm: ['keep', 'ten', 'shelf', 'bui', 'two', 'egg']\n",
      "Tokenized sentence: ['sms', 'services', 'for', 'your', 'inclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'unsubscribe', 'with', 'stop', 'no', 'extra', 'charge', 'help', 'po', 'box', 'ip', 'we']\n",
      "After stop words removal: ['sms', 'services', 'inclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'unsubscribe', 'stop', 'extra', 'charge', 'help', 'po', 'box', 'ip']\n",
      "After stemming with porters algorithm: ['sm', 'servic', 'inclus', 'text', 'credit', 'pl', 'goto', 'www', 'comuk', 'net', 'login', 'unsubscrib', 'stop', 'extra', 'charg', 'help', 'box']\n",
      "Tokenized sentence: ['ard', 'lor', 'i', 'ok', 'then', 'message', 'lor']\n",
      "After stop words removal: ['ard', 'lor', 'ok', 'message', 'lor']\n",
      "After stemming with porters algorithm: ['ard', 'lor', 'messag', 'lor']\n",
      "Tokenized sentence: ['i', 'wan', 'but', 'too', 'early', 'lei', 'me', 'outside', 'now', 'wun', 'b', 'home', 'so', 'early', 'neva', 'mind', 'then']\n",
      "After stop words removal: ['wan', 'early', 'lei', 'outside', 'wun', 'b', 'home', 'early', 'neva', 'mind']\n",
      "After stemming with porters algorithm: ['wan', 'earli', 'lei', 'outsid', 'wun', 'home', 'earli', 'neva', 'mind']\n",
      "Tokenized sentence: ['ok', 'lor', 'i', 'm', 'in', 'town', 'now', 'lei']\n",
      "After stop words removal: ['ok', 'lor', 'town', 'lei']\n",
      "After stemming with porters algorithm: ['lor', 'town', 'lei']\n",
      "Tokenized sentence: ['got', 'smaller', 'capacity', 'one', 'quite', 'ex']\n",
      "After stop words removal: ['got', 'smaller', 'capacity', 'one', 'quite', 'ex']\n",
      "After stemming with porters algorithm: ['got', 'smaller', 'capac', 'on', 'quit']\n",
      "Tokenized sentence: ['i', 'like', 'to', 'talk', 'pa', 'but', 'am', 'not', 'able', 'to', 'i', 'dont', 'know', 'y']\n",
      "After stop words removal: ['like', 'talk', 'pa', 'able', 'dont', 'know']\n",
      "After stemming with porters algorithm: ['like', 'talk', 'abl', 'dont', 'know']\n",
      "Tokenized sentence: ['ur', 'awarded', 'a', 'city', 'break', 'and', 'could', 'win', 'a', 'summer', 'shopping', 'spree', 'every', 'wk', 'txt', 'store', 'to', 'skilgme', 'tscs', 'winawk', 'age', 'perwksub']\n",
      "After stop words removal: ['ur', 'awarded', 'city', 'break', 'could', 'win', 'summer', 'shopping', 'spree', 'every', 'wk', 'txt', 'store', 'skilgme', 'tscs', 'winawk', 'age', 'perwksub']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['awar', 'citi', 'break', 'could', 'win', 'summer', 'shop', 'spree', 'everi', 'txt', 'store', 'skilgm', 'tsc', 'winawk', 'ag', 'perwksub']\n",
      "Tokenized sentence: ['raji', 'pls', 'do', 'me', 'a', 'favour', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimya', 'pls', 'today', 'is', 'her', 'birthday']\n",
      "After stop words removal: ['raji', 'pls', 'favour', 'pls', 'convey', 'birthday', 'wishes', 'nimya', 'pls', 'today', 'birthday']\n",
      "After stemming with porters algorithm: ['raji', 'pl', 'favour', 'pl', 'convei', 'birthdai', 'wish', 'nimya', 'pl', 'todai', 'birthdai']\n",
      "Tokenized sentence: ['no', 'one', 'interested', 'may', 'be', 'some', 'business', 'plan']\n",
      "After stop words removal: ['one', 'interested', 'may', 'business', 'plan']\n",
      "After stemming with porters algorithm: ['on', 'interes', 'mai', 'busi', 'plan']\n",
      "Tokenized sentence: ['hey', 'what', 's', 'up', 'charles', 'sorry', 'about', 'the', 'late', 'reply']\n",
      "After stop words removal: ['hey', 'charles', 'sorry', 'late', 'reply']\n",
      "After stemming with porters algorithm: ['hei', 'charl', 'sorri', 'late', 'repli']\n",
      "Tokenized sentence: ['yes', 'last', 'practice']\n",
      "After stop words removal: ['yes', 'last', 'practice']\n",
      "After stemming with porters algorithm: ['ye', 'last', 'practic']\n",
      "Tokenized sentence: ['wat', 'time', 'r', 'going', 'to', 'xin', 's', 'hostel']\n",
      "After stop words removal: ['wat', 'time', 'r', 'going', 'xin', 'hostel']\n",
      "go\n",
      "After stemming with porters algorithm: ['wat', 'time', 'go', 'xin', 'hostel']\n",
      "Tokenized sentence: ['i', 'm', 'stuck', 'in', 'da', 'middle', 'of', 'da', 'row', 'on', 'da', 'right', 'hand', 'side', 'of', 'da', 'lt']\n",
      "After stop words removal: ['stuck', 'da', 'middle', 'da', 'row', 'da', 'right', 'hand', 'side', 'da', 'lt']\n",
      "After stemming with porters algorithm: ['stuck', 'middl', 'row', 'right', 'hand', 'side']\n",
      "Tokenized sentence: ['so', 'wats', 'ur', 'opinion', 'abt', 'him', 'and', 'how', 'abt', 'is', 'character']\n",
      "After stop words removal: ['wats', 'ur', 'opinion', 'abt', 'abt', 'character']\n",
      "After stemming with porters algorithm: ['wat', 'opinion', 'abt', 'abt', 'charact']\n",
      "Tokenized sentence: ['i', 'had', 'a', 'good', 'time', 'too', 'its', 'nice', 'to', 'do', 'something', 'a', 'bit', 'different', 'with', 'my', 'weekends', 'for', 'a', 'change', 'see', 'ya', 'soon']\n",
      "After stop words removal: ['good', 'time', 'nice', 'something', 'bit', 'different', 'weekends', 'change', 'see', 'ya', 'soon']\n",
      "someth\n",
      "After stemming with porters algorithm: ['good', 'time', 'nice', 'somet', 'bit', 'differ', 'weekend', 'chang', 'see', 'soon']\n",
      "Tokenized sentence: ['when', 'u', 'wana', 'see', 'it', 'then']\n",
      "After stop words removal: ['u', 'wana', 'see']\n",
      "After stemming with porters algorithm: ['wana', 'see']\n",
      "Tokenized sentence: ['u', 'really', 'pig', 'leh', 'sleep', 'so', 'much', 'my', 'dad', 'wake', 'me', 'up', 'at', 'smth', 'eat', 'lunch', 'today']\n",
      "After stop words removal: ['u', 'really', 'pig', 'leh', 'sleep', 'much', 'dad', 'wake', 'smth', 'eat', 'lunch', 'today']\n",
      "After stemming with porters algorithm: ['realli', 'pig', 'leh', 'sleep', 'much', 'dad', 'wake', 'smth', 'eat', 'lunch', 'todai']\n",
      "Tokenized sentence: ['ma', 'head', 'dey', 'swell', 'oh', 'thanks', 'for', 'making', 'my', 'day']\n",
      "After stop words removal: ['head', 'dey', 'swell', 'oh', 'thanks', 'making', 'day']\n",
      "mak\n",
      "After stemming with porters algorithm: ['head', 'dei', 'swell', 'thank', 'make', 'dai']\n",
      "Tokenized sentence: ['just', 'checked', 'out', 'heading', 'out', 'to', 'drop', 'off', 'my', 'stuff', 'now']\n",
      "After stop words removal: ['checked', 'heading', 'drop', 'stuff']\n",
      "head\n",
      "After stemming with porters algorithm: ['chec', 'head', 'drop', 'stuff']\n",
      "Tokenized sentence: ['went', 'to', 'pay', 'rent', 'so', 'i', 'had', 'to', 'go', 'to', 'the', 'bank', 'to', 'authorise', 'the', 'payment']\n",
      "After stop words removal: ['went', 'pay', 'rent', 'go', 'bank', 'authorise', 'payment']\n",
      "After stemming with porters algorithm: ['went', 'pai', 'rent', 'bank', 'authoris', 'payment']\n",
      "Tokenized sentence: ['ok', 'thanx', 'take', 'care', 'then']\n",
      "After stop words removal: ['ok', 'thanx', 'take', 'care']\n",
      "After stemming with porters algorithm: ['thanx', 'take', 'care']\n",
      "Tokenized sentence: ['i', 'just', 'made', 'some', 'payments', 'so', 'dont', 'have', 'that', 'much', 'sorry', 'would', 'you', 'want', 'it', 'fedex', 'or', 'the', 'other', 'way']\n",
      "After stop words removal: ['made', 'payments', 'dont', 'much', 'sorry', 'would', 'want', 'fedex', 'way']\n",
      "After stemming with porters algorithm: ['made', 'payment', 'dont', 'much', 'sorri', 'would', 'want', 'fedex', 'wai']\n",
      "Tokenized sentence: ['gsoh', 'good', 'with', 'spam', 'the', 'ladies', 'u', 'could', 'b', 'a', 'male', 'gigolo', 'join', 'the', 'uk', 's', 'fastest', 'growing', 'mens', 'club', 'reply', 'oncall', 'mjzgroup', 'stop', 'reply', 'stop', 'msg', 'rcvd']\n",
      "After stop words removal: ['gsoh', 'good', 'spam', 'ladies', 'u', 'could', 'b', 'male', 'gigolo', 'join', 'uk', 'fastest', 'growing', 'mens', 'club', 'reply', 'oncall', 'mjzgroup', 'stop', 'reply', 'stop', 'msg', 'rcvd']\n",
      "grow\n",
      "After stemming with porters algorithm: ['gsoh', 'good', 'spam', 'ladi', 'could', 'male', 'gigolo', 'join', 'fastest', 'growe', 'men', 'club', 'repli', 'oncal', 'mjzgroup', 'stop', 'repli', 'stop', 'msg', 'rcvd']\n",
      "Tokenized sentence: ['sure', 'whenever', 'you', 'show', 'the', 'fuck', 'up', 'gt']\n",
      "After stop words removal: ['sure', 'whenever', 'show', 'fuck', 'gt']\n",
      "After stemming with porters algorithm: ['sure', 'whenev', 'show', 'fuck']\n",
      "Tokenized sentence: ['plz', 'note', 'if', 'anyone', 'calling', 'from', 'a', 'mobile', 'co', 'amp', 'asks', 'u', 'to', 'type', 'lt', 'gt', 'or', 'lt', 'gt', 'do', 'not', 'do', 'so', 'disconnect', 'the', 'call', 'coz', 'it', 'iz', 'an', 'attempt', 'of', 'terrorist', 'to', 'make', 'use', 'of', 'the', 'sim', 'card', 'no', 'itz', 'confirmd', 'by', 'nokia', 'n', 'motorola', 'n', 'has', 'been', 'verified', 'by', 'cnn', 'ibn']\n",
      "After stop words removal: ['plz', 'note', 'anyone', 'calling', 'mobile', 'co', 'amp', 'asks', 'u', 'type', 'lt', 'gt', 'lt', 'gt', 'disconnect', 'call', 'coz', 'iz', 'attempt', 'terrorist', 'make', 'use', 'sim', 'card', 'itz', 'confirmd', 'nokia', 'n', 'motorola', 'n', 'verified', 'cnn', 'ibn']\n",
      "call\n",
      "After stemming with porters algorithm: ['plz', 'note', 'anyon', 'call', 'mobil', 'amp', 'ask', 'type', 'disconnect', 'call', 'coz', 'attempt', 'terrorist', 'make', 'us', 'sim', 'card', 'itz', 'confirmd', 'nokia', 'motorola', 'verifi', 'cnn', 'ibn']\n",
      "Tokenized sentence: ['get', 'a', 'brand', 'new', 'mobile', 'phone', 'by', 'being', 'an', 'agent', 'of', 'the', 'mob', 'plus', 'loads', 'more', 'goodies', 'for', 'more', 'info', 'just', 'text', 'mat', 'to']\n",
      "After stop words removal: ['get', 'brand', 'new', 'mobile', 'phone', 'agent', 'mob', 'plus', 'loads', 'goodies', 'info', 'text', 'mat']\n",
      "After stemming with porters algorithm: ['get', 'brand', 'new', 'mobil', 'phone', 'agent', 'mob', 'plu', 'load', 'goodi', 'info', 'text', 'mat']\n",
      "Tokenized sentence: ['it', 'means', 'u', 'could', 'not', 'keep', 'ur', 'words']\n",
      "After stop words removal: ['means', 'u', 'could', 'keep', 'ur', 'words']\n",
      "After stemming with porters algorithm: ['mean', 'could', 'keep', 'word']\n",
      "Tokenized sentence: ['a', 'pure', 'hearted', 'person', 'can', 'have', 'a', 'wonderful', 'smile', 'that', 'makes', 'even', 'his', 'her', 'enemies', 'to', 'feel', 'guilty', 'for', 'being', 'an', 'enemy', 'so', 'catch', 'the', 'world', 'with', 'your', 'smile', 'goodmorning', 'amp', 'have', 'a', 'smiley', 'sunday']\n",
      "After stop words removal: ['pure', 'hearted', 'person', 'wonderful', 'smile', 'makes', 'even', 'enemies', 'feel', 'guilty', 'enemy', 'catch', 'world', 'smile', 'goodmorning', 'amp', 'smiley', 'sunday']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['pure', 'hear', 'person', 'wonder', 'smile', 'make', 'even', 'enemi', 'feel', 'guilti', 'enemi', 'catch', 'world', 'smile', 'goodmor', 'amp', 'smilei', 'sundai']\n",
      "Tokenized sentence: ['my', 'exam', 'is', 'for', 'february', 'wish', 'you', 'a', 'great', 'day']\n",
      "After stop words removal: ['exam', 'february', 'wish', 'great', 'day']\n",
      "After stemming with porters algorithm: ['exam', 'februari', 'wish', 'great', 'dai']\n",
      "Tokenized sentence: ['if', 'you', 'are', 'not', 'coughing', 'then', 'its', 'nothing']\n",
      "After stop words removal: ['coughing', 'nothing']\n",
      "cough\n",
      "noth\n",
      "After stemming with porters algorithm: ['coug', 'not']\n",
      "Tokenized sentence: ['i', 'got', 'your', 'back', 'do', 'you', 'have', 'any', 'dislikes', 'in', 'bed']\n",
      "After stop words removal: ['got', 'back', 'dislikes', 'bed']\n",
      "After stemming with porters algorithm: ['got', 'back', 'dislik', 'bed']\n",
      "Tokenized sentence: ['hi', 'finally', 'i', 'completed', 'the', 'course']\n",
      "After stop words removal: ['hi', 'finally', 'completed', 'course']\n",
      "After stemming with porters algorithm: ['final', 'complet', 'cours']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'you', 'last', 'weekends', 'draw', 'shows', 'u', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'p', 'pm']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'last', 'weekends', 'draw', 'shows', 'u', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'p', 'pm']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'last', 'weekend', 'draw', 'show', 'priz', 'guaranteed', 'call', 'claim', 'code', 'valid', 'hr']\n",
      "Tokenized sentence: ['make', 'that', 'fucks', 'sake', 'x']\n",
      "After stop words removal: ['make', 'fucks', 'sake', 'x']\n",
      "After stemming with porters algorithm: ['make', 'fuck', 'sake']\n",
      "Tokenized sentence: ['these', 'won', 't', 'do', 'have', 'to', 'move', 'on', 'to', 'morphine']\n",
      "After stop words removal: ['move', 'morphine']\n",
      "After stemming with porters algorithm: ['move', 'morphin']\n",
      "Tokenized sentence: ['oh', 'ya', 'ya', 'i', 'remember', 'da']\n",
      "After stop words removal: ['oh', 'ya', 'ya', 'remember', 'da']\n",
      "After stemming with porters algorithm: ['rememb']\n",
      "Tokenized sentence: ['aiyah', 'sorry', 'lor', 'i', 'watch', 'tv', 'watch', 'until', 'i', 'forgot', 'check', 'my', 'phone']\n",
      "After stop words removal: ['aiyah', 'sorry', 'lor', 'watch', 'tv', 'watch', 'forgot', 'check', 'phone']\n",
      "After stemming with porters algorithm: ['aiyah', 'sorri', 'lor', 'watch', 'watch', 'forgot', 'check', 'phone']\n",
      "Tokenized sentence: ['did', 'you', 'try', 'making', 'another', 'butt']\n",
      "After stop words removal: ['try', 'making', 'another', 'butt']\n",
      "mak\n",
      "After stemming with porters algorithm: ['try', 'make', 'anoth', 'butt']\n",
      "Tokenized sentence: ['quite', 'late', 'lar', 'ard', 'anyway', 'i', 'wun', 'b', 'drivin']\n",
      "After stop words removal: ['quite', 'late', 'lar', 'ard', 'anyway', 'wun', 'b', 'drivin']\n",
      "After stemming with porters algorithm: ['quit', 'late', 'lar', 'ard', 'anywai', 'wun', 'drivin']\n",
      "Tokenized sentence: ['please', 'reserve', 'ticket', 'on', 'saturday', 'eve', 'from', 'chennai', 'to', 'thirunelvali', 'and', 'again', 'from', 'tirunelvali', 'to', 'chennai', 'on', 'sunday', 'eve', 'i', 'already', 'see', 'in', 'net', 'no', 'ticket', 'available', 'i', 'want', 'to', 'book', 'ticket', 'through', 'tackle']\n",
      "After stop words removal: ['please', 'reserve', 'ticket', 'saturday', 'eve', 'chennai', 'thirunelvali', 'tirunelvali', 'chennai', 'sunday', 'eve', 'already', 'see', 'net', 'ticket', 'available', 'want', 'book', 'ticket', 'tackle']\n",
      "After stemming with porters algorithm: ['pleas', 'reserv', 'ticket', 'saturdai', 'ev', 'chennai', 'thirunelvali', 'tirunelvali', 'chennai', 'sundai', 'ev', 'alreadi', 'see', 'net', 'ticket', 'avail', 'want', 'book', 'ticket', 'tackl']\n",
      "Tokenized sentence: ['good', 'good', 'job', 'i', 'like', 'entrepreneurs']\n",
      "After stop words removal: ['good', 'good', 'job', 'like', 'entrepreneurs']\n",
      "After stemming with porters algorithm: ['good', 'good', 'job', 'like', 'entrepreneur']\n",
      "Tokenized sentence: ['thanx', 'u', 'darlin', 'im', 'cool', 'thanx', 'a', 'few', 'bday', 'drinks', 'nite', 'morrow', 'off', 'take', 'care', 'c', 'u', 'soon', 'xxx']\n",
      "After stop words removal: ['thanx', 'u', 'darlin', 'im', 'cool', 'thanx', 'bday', 'drinks', 'nite', 'morrow', 'take', 'care', 'c', 'u', 'soon', 'xxx']\n",
      "After stemming with porters algorithm: ['thanx', 'darlin', 'cool', 'thanx', 'bdai', 'drink', 'nite', 'morrow', 'take', 'care', 'soon', 'xxx']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'j', 'po', 'box', 'c', 'pm']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'j', 'po', 'box', 'c', 'pm']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'box']\n",
      "Tokenized sentence: ['i', 'notice', 'you', 'like', 'looking', 'in', 'the', 'shit', 'mirror', 'youre', 'turning', 'into', 'a', 'right', 'freak']\n",
      "After stop words removal: ['notice', 'like', 'looking', 'shit', 'mirror', 'youre', 'turning', 'right', 'freak']\n",
      "look\n",
      "turn\n",
      "After stemming with porters algorithm: ['notic', 'like', 'look', 'shit', 'mirror', 'your', 'tur', 'right', 'freak']\n",
      "Tokenized sentence: ['hi', 'the', 'sexychat', 'girls', 'are', 'waiting', 'for', 'you', 'to', 'text', 'them', 'text', 'now', 'for', 'a', 'great', 'night', 'chatting', 'send', 'stop', 'to', 'stop', 'this', 'service']\n",
      "After stop words removal: ['hi', 'sexychat', 'girls', 'waiting', 'text', 'text', 'great', 'night', 'chatting', 'send', 'stop', 'stop', 'service']\n",
      "wait\n",
      "chatt\n",
      "After stemming with porters algorithm: ['sexychat', 'girl', 'wait', 'text', 'text', 'great', 'night', 'chat', 'send', 'stop', 'stop', 'servic']\n",
      "Tokenized sentence: ['i', 'donno', 'if', 'they', 'are', 'scorable']\n",
      "After stop words removal: ['donno', 'scorable']\n",
      "After stemming with porters algorithm: ['donno', 'scorab']\n",
      "Tokenized sentence: ['me', 'sef', 'dey', 'laugh', 'you', 'meanwhile', 'how', 's', 'my', 'darling', 'anjie']\n",
      "After stop words removal: ['sef', 'dey', 'laugh', 'meanwhile', 'darling', 'anjie']\n",
      "darl\n",
      "After stemming with porters algorithm: ['sef', 'dei', 'laugh', 'meanwhil', 'darl', 'anji']\n",
      "Tokenized sentence: ['hi', 'its', 'jess', 'i', 'dont', 'know', 'if', 'you', 'are', 'at', 'work', 'but', 'call', 'me', 'when', 'u', 'can', 'im', 'at', 'home', 'all', 'eve', 'xxx']\n",
      "After stop words removal: ['hi', 'jess', 'dont', 'know', 'work', 'call', 'u', 'im', 'home', 'eve', 'xxx']\n",
      "After stemming with porters algorithm: ['jess', 'dont', 'know', 'work', 'call', 'home', 'ev', 'xxx']\n",
      "Tokenized sentence: ['the', 'battery', 'is', 'for', 'mr', 'adewale', 'my', 'uncle', 'aka', 'egbon']\n",
      "After stop words removal: ['battery', 'mr', 'adewale', 'uncle', 'aka', 'egbon']\n",
      "After stemming with porters algorithm: ['batteri', 'adewal', 'uncl', 'aka', 'egbon']\n",
      "Tokenized sentence: ['sorry', 'was', 'at', 'the', 'grocers']\n",
      "After stop words removal: ['sorry', 'grocers']\n",
      "After stemming with porters algorithm: ['sorri', 'grocer']\n",
      "Tokenized sentence: ['i', 'will', 'reach', 'office', 'around', 'lt', 'decimal', 'gt', 'amp', 'my', 'mobile', 'have', 'problem', 'you', 'cann', 't', 'get', 'my', 'voice', 'so', 'call', 'you', 'asa', 'i', 'll', 'free']\n",
      "After stop words removal: ['reach', 'office', 'around', 'lt', 'decimal', 'gt', 'amp', 'mobile', 'problem', 'cann', 'get', 'voice', 'call', 'asa', 'free']\n",
      "After stemming with porters algorithm: ['reach', 'offic', 'around', 'decim', 'amp', 'mobil', 'problem', 'cann', 'get', 'voic', 'call', 'asa', 'free']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['ic', 'there', 'are', 'a', 'lotta', 'childporn', 'cars', 'then']\n",
      "After stop words removal: ['ic', 'lotta', 'childporn', 'cars']\n",
      "After stemming with porters algorithm: ['lotta', 'childporn', 'car']\n",
      "Tokenized sentence: ['we', 'not', 'watching', 'movie', 'already', 'xy', 'wants', 'shop', 'so', 'i', 'm', 'shopping', 'w', 'her', 'now']\n",
      "After stop words removal: ['watching', 'movie', 'already', 'xy', 'wants', 'shop', 'shopping', 'w']\n",
      "watch\n",
      "shopp\n",
      "After stemming with porters algorithm: ['watc', 'movi', 'alreadi', 'want', 'shop', 'shop']\n",
      "Tokenized sentence: ['okie']\n",
      "After stop words removal: ['okie']\n",
      "After stemming with porters algorithm: ['oki']\n",
      "Tokenized sentence: ['that', 'is', 'wondar', 'full', 'flim']\n",
      "After stop words removal: ['wondar', 'full', 'flim']\n",
      "After stemming with porters algorithm: ['wondar', 'full', 'flim']\n",
      "Tokenized sentence: ['we', 'are', 'supposed', 'to', 'meet', 'to', 'discuss', 'abt', 'our', 'trip', 'thought', 'xuhui', 'told', 'you', 'in', 'the', 'afternoon', 'thought', 'we', 'can', 'go', 'for', 'lesson', 'after', 'that']\n",
      "After stop words removal: ['supposed', 'meet', 'discuss', 'abt', 'trip', 'thought', 'xuhui', 'told', 'afternoon', 'thought', 'go', 'lesson']\n",
      "After stemming with porters algorithm: ['suppos', 'meet', 'discuss', 'abt', 'trip', 'thought', 'xuhui', 'told', 'afternoon', 'thought', 'lesson']\n",
      "Tokenized sentence: ['a', 'boy', 'was', 'late', 'home', 'his', 'father', 'power', 'of', 'frndship']\n",
      "After stop words removal: ['boy', 'late', 'home', 'father', 'power', 'frndship']\n",
      "After stemming with porters algorithm: ['boi', 'late', 'home', 'father', 'power', 'frndship']\n",
      "Tokenized sentence: ['lmao', 'where', 's', 'your', 'fish', 'memory', 'when', 'i', 'need', 'it']\n",
      "After stop words removal: ['lmao', 'fish', 'memory', 'need']\n",
      "After stemming with porters algorithm: ['lmao', 'fish', 'memori', 'need']\n",
      "Tokenized sentence: ['hey', 'no', 'i', 'ad', 'a', 'crap', 'nite', 'was', 'borin', 'without', 'ya', 'boggy', 'with', 'me', 'u', 'boring', 'biatch', 'thanx', 'but', 'u', 'wait', 'til', 'nxt', 'time', 'il', 'ave', 'ya']\n",
      "After stop words removal: ['hey', 'ad', 'crap', 'nite', 'borin', 'without', 'ya', 'boggy', 'u', 'boring', 'biatch', 'thanx', 'u', 'wait', 'til', 'nxt', 'time', 'il', 'ave', 'ya']\n",
      "bor\n",
      "After stemming with porters algorithm: ['hei', 'crap', 'nite', 'borin', 'without', 'boggi', 'bore', 'biatch', 'thanx', 'wait', 'til', 'nxt', 'time', 'av']\n",
      "Tokenized sentence: ['sorry', 'battery', 'died', 'yeah', 'i', 'm', 'here']\n",
      "After stop words removal: ['sorry', 'battery', 'died', 'yeah']\n",
      "After stemming with porters algorithm: ['sorri', 'batteri', 'di', 'yeah']\n",
      "Tokenized sentence: ['ranjith', 'cal', 'drpd', 'deeraj', 'and', 'deepak', 'min', 'hold']\n",
      "After stop words removal: ['ranjith', 'cal', 'drpd', 'deeraj', 'deepak', 'min', 'hold']\n",
      "After stemming with porters algorithm: ['ranjith', 'cal', 'drpd', 'deeraj', 'deepak', 'min', 'hold']\n",
      "Tokenized sentence: ['don', 'know', 'he', 'is', 'watching', 'film', 'in', 'computer']\n",
      "After stop words removal: ['know', 'watching', 'film', 'computer']\n",
      "watch\n",
      "After stemming with porters algorithm: ['know', 'watc', 'film', 'comput']\n",
      "Tokenized sentence: ['i', 'got', 'it', 'before', 'the', 'new', 'year', 'cos', 'yetunde', 'said', 'she', 'wanted', 'to', 'surprise', 'you', 'with', 'it', 'but', 'when', 'i', 'didnt', 'see', 'money', 'i', 'returned', 'it', 'mid', 'january', 'before', 'the', 'lt', 'gt', 'day', 'return', 'period', 'ended']\n",
      "After stop words removal: ['got', 'new', 'year', 'cos', 'yetunde', 'said', 'wanted', 'surprise', 'didnt', 'see', 'money', 'returned', 'mid', 'january', 'lt', 'gt', 'day', 'return', 'period', 'ended']\n",
      "After stemming with porters algorithm: ['got', 'new', 'year', 'co', 'yetund', 'said', 'wan', 'surpris', 'didnt', 'see', 'monei', 'retur', 'mid', 'januari', 'dai', 'return', 'period', 'en']\n",
      "Tokenized sentence: ['yo', 'we', 'are', 'watching', 'a', 'movie', 'on', 'netflix']\n",
      "After stop words removal: ['yo', 'watching', 'movie', 'netflix']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'movi', 'netflix']\n",
      "Tokenized sentence: ['hello', 'lover', 'how', 'goes', 'that', 'new', 'job', 'are', 'you', 'there', 'now', 'are', 'you', 'happy', 'do', 'you', 'think', 'of', 'me', 'i', 'wake', 'my', 'slave', 'and', 'send', 'you', 'a', 'teasing', 'kiss', 'from', 'across', 'the', 'sea']\n",
      "After stop words removal: ['hello', 'lover', 'goes', 'new', 'job', 'happy', 'think', 'wake', 'slave', 'send', 'teasing', 'kiss', 'across', 'sea']\n",
      "teas\n",
      "After stemming with porters algorithm: ['hello', 'lover', 'goe', 'new', 'job', 'happi', 'think', 'wake', 'slave', 'send', 'teas', 'kiss', 'across', 'sea']\n",
      "Tokenized sentence: ['hey', 'a', 'guy', 'i', 'know', 'is', 'breathing', 'down', 'my', 'neck', 'to', 'get', 'him', 'some', 'bud', 'anyway', 'you', 'd', 'be', 'able', 'to', 'get', 'a', 'half', 'track', 'to', 'usf', 'tonight']\n",
      "After stop words removal: ['hey', 'guy', 'know', 'breathing', 'neck', 'get', 'bud', 'anyway', 'able', 'get', 'half', 'track', 'usf', 'tonight']\n",
      "breath\n",
      "After stemming with porters algorithm: ['hei', 'gui', 'know', 'breat', 'neck', 'get', 'bud', 'anywai', 'abl', 'get', 'half', 'track', 'usf', 'tonight']\n",
      "Tokenized sentence: ['tell', 'my', 'bad', 'character', 'which', 'u', 'dnt', 'lik', 'in', 'me', 'i', 'll', 'try', 'to', 'change', 'in', 'lt', 'gt', 'i', 'll', 'add', 'tat', 'my', 'new', 'year', 'resolution', 'waiting', 'for', 'ur', 'reply', 'be', 'frank', 'good', 'morning']\n",
      "After stop words removal: ['tell', 'bad', 'character', 'u', 'dnt', 'lik', 'try', 'change', 'lt', 'gt', 'add', 'tat', 'new', 'year', 'resolution', 'waiting', 'ur', 'reply', 'frank', 'good', 'morning']\n",
      "wait\n",
      "morn\n",
      "After stemming with porters algorithm: ['tell', 'bad', 'charact', 'dnt', 'lik', 'try', 'chang', 'add', 'tat', 'new', 'year', 'resolut', 'wait', 'repli', 'frank', 'good', 'mor']\n",
      "Tokenized sentence: ['if', 'you', 'r', 'home', 'then', 'come', 'down', 'within', 'min']\n",
      "After stop words removal: ['r', 'home', 'come', 'within', 'min']\n",
      "After stemming with porters algorithm: ['home', 'come', 'within', 'min']\n",
      "Tokenized sentence: ['send', 'his', 'number', 'and', 'give', 'reply', 'tomorrow', 'morning', 'for', 'why', 'you', 'said', 'that', 'to', 'him', 'like', 'that', 'ok']\n",
      "After stop words removal: ['send', 'number', 'give', 'reply', 'tomorrow', 'morning', 'said', 'like', 'ok']\n",
      "morn\n",
      "After stemming with porters algorithm: ['send', 'number', 'give', 'repli', 'tomorrow', 'mor', 'said', 'like']\n",
      "Tokenized sentence: ['sounds', 'gd', 'haha', 'can', 'wah', 'u', 'yan', 'jiu', 'so', 'fast', 'liao']\n",
      "After stop words removal: ['sounds', 'gd', 'haha', 'wah', 'u', 'yan', 'jiu', 'fast', 'liao']\n",
      "After stemming with porters algorithm: ['sound', 'haha', 'wah', 'yan', 'jiu', 'fast', 'liao']\n",
      "Tokenized sentence: ['all', 'these', 'nice', 'new', 'shirts', 'and', 'the', 'only', 'thing', 'i', 'can', 'wear', 'them', 'to', 'is', 'nudist', 'themed', 'you', 'in', 'mu']\n",
      "After stop words removal: ['nice', 'new', 'shirts', 'thing', 'wear', 'nudist', 'themed', 'mu']\n",
      "After stemming with porters algorithm: ['nice', 'new', 'shirt', 'thing', 'wear', 'nudist', 'theme']\n",
      "Tokenized sentence: ['someone', 'u', 'know', 'has', 'asked', 'our', 'dating', 'service', 'contact', 'you', 'cant', 'guess', 'who', 'call', 'now', 'all', 'will', 'be', 'revealed', 'pobox', 's', 'xy', 'p']\n",
      "After stop words removal: ['someone', 'u', 'know', 'asked', 'dating', 'service', 'contact', 'cant', 'guess', 'call', 'revealed', 'pobox', 'xy', 'p']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someon', 'know', 'as', 'date', 'servic', 'contact', 'cant', 'guess', 'call', 'reveal', 'pobox']\n",
      "Tokenized sentence: ['ok', 'give', 'me', 'minutes', 'i', 'think', 'i', 'see', 'her', 'btw', 'you', 're', 'my', 'alibi', 'you', 'were', 'cutting', 'my', 'hair', 'the', 'whole', 'time']\n",
      "After stop words removal: ['ok', 'give', 'minutes', 'think', 'see', 'btw', 'alibi', 'cutting', 'hair', 'whole', 'time']\n",
      "cutt\n",
      "After stemming with porters algorithm: ['give', 'minut', 'think', 'see', 'btw', 'alibi', 'cut', 'hair', 'whole', 'time']\n",
      "Tokenized sentence: ['so', 'what', 'u', 'doing', 'today']\n",
      "After stop words removal: ['u', 'today']\n",
      "After stemming with porters algorithm: ['todai']\n",
      "Tokenized sentence: ['omg', 'if', 'its', 'not', 'one', 'thing', 'its', 'another', 'my', 'cat', 'has', 'worms', 'when', 'does', 'this', 'bad', 'day', 'end']\n",
      "After stop words removal: ['omg', 'one', 'thing', 'another', 'cat', 'worms', 'bad', 'day', 'end']\n",
      "After stemming with porters algorithm: ['omg', 'on', 'thing', 'anoth', 'cat', 'worm', 'bad', 'dai', 'end']\n",
      "Tokenized sentence: ['is', 'there', 'a', 'reason', 'we', 've', 'not', 'spoken', 'this', 'year', 'anyways', 'have', 'a', 'great', 'week', 'and', 'all', 'the', 'best', 'in', 'your', 'exam']\n",
      "After stop words removal: ['reason', 'spoken', 'year', 'anyways', 'great', 'week', 'best', 'exam']\n",
      "After stemming with porters algorithm: ['reason', 'spoken', 'year', 'anywai', 'great', 'week', 'best', 'exam']\n",
      "Tokenized sentence: ['cancel', 'cheyyamo', 'and', 'get', 'some', 'money', 'back']\n",
      "After stop words removal: ['cancel', 'cheyyamo', 'get', 'money', 'back']\n",
      "After stemming with porters algorithm: ['cancel', 'cheyyamo', 'get', 'monei', 'back']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'ringtone', 'order', 'ref', 'number', 'r', 'your', 'mobile', 'will', 'be', 'charged', 'should', 'your', 'tone', 'not', 'arrive', 'please', 'call', 'customer', 'services', 'on']\n",
      "After stop words removal: ['thanks', 'ringtone', 'order', 'ref', 'number', 'r', 'mobile', 'charged', 'tone', 'arrive', 'please', 'call', 'customer', 'services']\n",
      "After stemming with porters algorithm: ['thank', 'rington', 'order', 'ref', 'number', 'mobil', 'char', 'tone', 'arriv', 'pleas', 'call', 'custom', 'servic']\n",
      "Tokenized sentence: ['hope', 'ur', 'head', 'doesn', 't', 'hurt', 'much', 'am', 'ploughing', 'my', 'way', 'through', 'a', 'pile', 'of', 'ironing', 'staying', 'in', 'with', 'a', 'chinky', 'tonight', 'come', 'round', 'if', 'you', 'like']\n",
      "After stop words removal: ['hope', 'ur', 'head', 'hurt', 'much', 'ploughing', 'way', 'pile', 'ironing', 'staying', 'chinky', 'tonight', 'come', 'round', 'like']\n",
      "plough\n",
      "iron\n",
      "stay\n",
      "After stemming with porters algorithm: ['hope', 'head', 'hurt', 'much', 'ploug', 'wai', 'pile', 'iron', 'stai', 'chinki', 'tonight', 'come', 'round', 'like']\n",
      "Tokenized sentence: ['freemsg', 'txt', 'call', 'to', 'no', 'claim', 'your', 'reward', 'of', 'hours', 'talk', 'time', 'to', 'use', 'from', 'your', 'phone', 'now', 'subscribe', 'gbp', 'mnth', 'inc', 'hrs', 'stop', 'txtstop']\n",
      "After stop words removal: ['freemsg', 'txt', 'call', 'claim', 'reward', 'hours', 'talk', 'time', 'use', 'phone', 'subscribe', 'gbp', 'mnth', 'inc', 'hrs', 'stop', 'txtstop']\n",
      "After stemming with porters algorithm: ['freemsg', 'txt', 'call', 'claim', 'reward', 'hour', 'talk', 'time', 'us', 'phone', 'subscrib', 'gbp', 'mnth', 'inc', 'hr', 'stop', 'txtstop']\n",
      "Tokenized sentence: ['whatever', 'im', 'pretty', 'pissed', 'off']\n",
      "After stop words removal: ['whatever', 'im', 'pretty', 'pissed']\n",
      "After stemming with porters algorithm: ['whatev', 'pretti', 'piss']\n",
      "Tokenized sentence: ['dnt', 'worry', 'use', 'ice', 'pieces', 'in', 'a', 'cloth', 'pack', 'also', 'take', 'tablets']\n",
      "After stop words removal: ['dnt', 'worry', 'use', 'ice', 'pieces', 'cloth', 'pack', 'also', 'take', 'tablets']\n",
      "After stemming with porters algorithm: ['dnt', 'worri', 'us', 'ic', 'piec', 'cloth', 'pack', 'also', 'take', 'tablet']\n",
      "Tokenized sentence: ['no', 'shoot', 'me', 'i', 'm', 'in', 'the', 'docs', 'waiting', 'room']\n",
      "After stop words removal: ['shoot', 'docs', 'waiting', 'room']\n",
      "wait\n",
      "After stemming with porters algorithm: ['shoot', 'doc', 'wait', 'room']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'doing', 'hope', 'you', 've', 'settled', 'in', 'for', 'the', 'new', 'school', 'year', 'just', 'wishin', 'you', 'a', 'gr', 'day']\n",
      "After stop words removal: ['hope', 'settled', 'new', 'school', 'year', 'wishin', 'gr', 'day']\n",
      "After stemming with porters algorithm: ['hope', 'settl', 'new', 'school', 'year', 'wishin', 'dai']\n",
      "Tokenized sentence: ['i', 'was', 'just', 'callin', 'to', 'say', 'hi', 'take', 'care', 'bruv']\n",
      "After stop words removal: ['callin', 'say', 'hi', 'take', 'care', 'bruv']\n",
      "After stemming with porters algorithm: ['callin', 'sai', 'take', 'care', 'bruv']\n",
      "Tokenized sentence: ['yun', 'ah', 'now', 'wkg', 'where', 'btw', 'if', 'go', 'nus', 'sc', 'wana', 'specialise', 'in', 'wad']\n",
      "After stop words removal: ['yun', 'ah', 'wkg', 'btw', 'go', 'nus', 'sc', 'wana', 'specialise', 'wad']\n",
      "After stemming with porters algorithm: ['yun', 'wkg', 'btw', 'nu', 'wana', 'specialis', 'wad']\n",
      "Tokenized sentence: ['a', 'xmas', 'reward', 'is', 'waiting', 'for', 'you', 'our', 'computer', 'has', 'randomly', 'picked', 'you', 'from', 'our', 'loyal', 'mobile', 'customers', 'to', 'receive', 'a', 'reward', 'just', 'call']\n",
      "After stop words removal: ['xmas', 'reward', 'waiting', 'computer', 'randomly', 'picked', 'loyal', 'mobile', 'customers', 'receive', 'reward', 'call']\n",
      "wait\n",
      "After stemming with porters algorithm: ['xma', 'reward', 'wait', 'comput', 'randomli', 'pic', 'loyal', 'mobil', 'custom', 'receiv', 'reward', 'call']\n",
      "Tokenized sentence: ['coffee', 'cake', 'i', 'guess']\n",
      "After stop words removal: ['coffee', 'cake', 'guess']\n",
      "After stemming with porters algorithm: ['coffe', 'cake', 'guess']\n",
      "Tokenized sentence: ['nothing', 'can']\n",
      "After stop words removal: ['nothing']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not']\n",
      "Tokenized sentence: ['take', 'care', 'n', 'get', 'well', 'soon']\n",
      "After stop words removal: ['take', 'care', 'n', 'get', 'well', 'soon']\n",
      "After stemming with porters algorithm: ['take', 'care', 'get', 'well', 'soon']\n",
      "Tokenized sentence: ['good', 'afternoon', 'my', 'love', 'how', 'goes', 'your', 'day', 'how', 'did', 'you', 'sleep', 'i', 'hope', 'your', 'well', 'my', 'boytoy', 'i', 'think', 'of', 'you']\n",
      "After stop words removal: ['good', 'afternoon', 'love', 'goes', 'day', 'sleep', 'hope', 'well', 'boytoy', 'think']\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'love', 'goe', 'dai', 'sleep', 'hope', 'well', 'boytoi', 'think']\n",
      "Tokenized sentence: ['slaaaaave', 'where', 'are', 'you', 'must', 'i', 'summon', 'you', 'to', 'me', 'all', 'the', 'time', 'now', 'don', 't', 'you', 'wish', 'to', 'come', 'to', 'me', 'on', 'your', 'own', 'anymore']\n",
      "After stop words removal: ['slaaaaave', 'must', 'summon', 'time', 'wish', 'come', 'anymore']\n",
      "After stemming with porters algorithm: ['slaaaaav', 'must', 'summon', 'time', 'wish', 'come', 'anymor']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'you', 'later', 'i', 'am', 'in', 'meeting', 'sir']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'meeting', 'sir']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'meet', 'sir']\n",
      "Tokenized sentence: ['sorry', 'i', 'can', 't', 'help', 'you', 'on', 'this']\n",
      "After stop words removal: ['sorry', 'help']\n",
      "After stemming with porters algorithm: ['sorri', 'help']\n",
      "Tokenized sentence: ['mum', 'not', 'going', 'robinson', 'already']\n",
      "After stop words removal: ['mum', 'going', 'robinson', 'already']\n",
      "go\n",
      "After stemming with porters algorithm: ['mum', 'go', 'robinson', 'alreadi']\n",
      "Tokenized sentence: ['aight', 'we', 'll', 'head', 'out', 'in', 'a', 'few']\n",
      "After stop words removal: ['aight', 'head']\n",
      "After stemming with porters algorithm: ['aight', 'head']\n",
      "Tokenized sentence: ['yup', 'n', 'her', 'fren', 'lor', 'i', 'm', 'meeting', 'my', 'fren', 'at']\n",
      "After stop words removal: ['yup', 'n', 'fren', 'lor', 'meeting', 'fren']\n",
      "meet\n",
      "After stemming with porters algorithm: ['yup', 'fren', 'lor', 'meet', 'fren']\n",
      "Tokenized sentence: ['neva', 'tell', 'me', 'how', 'i', 'noe', 'i', 'm', 'not', 'at', 'home', 'in', 'da', 'aft', 'wat']\n",
      "After stop words removal: ['neva', 'tell', 'noe', 'home', 'da', 'aft', 'wat']\n",
      "After stemming with porters algorithm: ['neva', 'tell', 'noe', 'home', 'aft', 'wat']\n",
      "Tokenized sentence: ['love', 'has', 'one', 'law', 'make', 'happy', 'the', 'person', 'you', 'love', 'in', 'the', 'same', 'way', 'friendship', 'has', 'one', 'law', 'never', 'make', 'ur', 'friend', 'feel', 'alone', 'until', 'you', 'are', 'alive', 'gud', 'night']\n",
      "After stop words removal: ['love', 'one', 'law', 'make', 'happy', 'person', 'love', 'way', 'friendship', 'one', 'law', 'never', 'make', 'ur', 'friend', 'feel', 'alone', 'alive', 'gud', 'night']\n",
      "After stemming with porters algorithm: ['love', 'on', 'law', 'make', 'happi', 'person', 'love', 'wai', 'friendship', 'on', 'law', 'never', 'make', 'friend', 'feel', 'alon', 'aliv', 'gud', 'night']\n",
      "Tokenized sentence: ['mmmm', 'i', 'cant', 'wait', 'to', 'lick', 'it']\n",
      "After stop words removal: ['mmmm', 'cant', 'wait', 'lick']\n",
      "After stemming with porters algorithm: ['mmmm', 'cant', 'wait', 'lick']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'won', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'reach', 'you', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'reach', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'bonu', 'caller', 'priz', 'attempt', 'reach', 'call', 'asap', 'box', 'ppm']\n",
      "Tokenized sentence: ['lol', 'i', 'knew', 'that', 'i', 'saw', 'him', 'in', 'the', 'dollar', 'store']\n",
      "After stop words removal: ['lol', 'knew', 'saw', 'dollar', 'store']\n",
      "After stemming with porters algorithm: ['lol', 'knew', 'saw', 'dollar', 'store']\n",
      "Tokenized sentence: ['it', 's', 'reassuring', 'in', 'this', 'crazy', 'world']\n",
      "After stop words removal: ['reassuring', 'crazy', 'world']\n",
      "reassur\n",
      "After stemming with porters algorithm: ['reassur', 'crazi', 'world']\n",
      "Tokenized sentence: ['what', 'i', 'meant', 'to', 'say', 'is', 'cant', 'wait', 'to', 'see', 'u', 'again', 'getting', 'bored', 'of', 'this', 'bridgwater', 'banter']\n",
      "After stop words removal: ['meant', 'say', 'cant', 'wait', 'see', 'u', 'getting', 'bored', 'bridgwater', 'banter']\n",
      "gett\n",
      "After stemming with porters algorithm: ['meant', 'sai', 'cant', 'wait', 'see', 'get', 'bore', 'bridgwat', 'banter']\n",
      "Tokenized sentence: ['haha', 'mayb', 'u', 're', 'rite', 'u', 'know', 'me', 'well', 'da', 'feeling', 'of', 'being', 'liked', 'by', 'someone', 'is', 'gd', 'lor', 'u', 'faster', 'go', 'find', 'one', 'then', 'all', 'gals', 'in', 'our', 'group', 'attached', 'liao']\n",
      "After stop words removal: ['haha', 'mayb', 'u', 'rite', 'u', 'know', 'well', 'da', 'feeling', 'liked', 'someone', 'gd', 'lor', 'u', 'faster', 'go', 'find', 'one', 'gals', 'group', 'attached', 'liao']\n",
      "feel\n",
      "After stemming with porters algorithm: ['haha', 'mayb', 'rite', 'know', 'well', 'feel', 'like', 'someon', 'lor', 'faster', 'find', 'on', 'gal', 'group', 'attac', 'liao']\n",
      "Tokenized sentence: ['finish', 'liao', 'u']\n",
      "After stop words removal: ['finish', 'liao', 'u']\n",
      "After stemming with porters algorithm: ['finish', 'liao']\n",
      "Tokenized sentence: ['yep', 'the', 'great', 'loxahatchee', 'xmas', 'tree', 'burning', 'of', 'lt', 'gt', 'starts', 'in', 'an', 'hour']\n",
      "After stop words removal: ['yep', 'great', 'loxahatchee', 'xmas', 'tree', 'burning', 'lt', 'gt', 'starts', 'hour']\n",
      "burn\n",
      "After stemming with porters algorithm: ['yep', 'great', 'loxahatche', 'xma', 'tree', 'bur', 'start', 'hour']\n",
      "Tokenized sentence: ['easy', 'ah', 'sen', 'got', 'selected', 'means', 'its', 'good']\n",
      "After stop words removal: ['easy', 'ah', 'sen', 'got', 'selected', 'means', 'good']\n",
      "After stemming with porters algorithm: ['easi', 'sen', 'got', 'selec', 'mean', 'good']\n",
      "Tokenized sentence: ['tell', 'your', 'friends', 'what', 'you', 'plan', 'to', 'do', 'on', 'valentines', 'day', 'lt', 'url', 'gt']\n",
      "After stop words removal: ['tell', 'friends', 'plan', 'valentines', 'day', 'lt', 'url', 'gt']\n",
      "After stemming with porters algorithm: ['tell', 'friend', 'plan', 'valentin', 'dai', 'url']\n",
      "Tokenized sentence: ['auntie', 'huai', 'juan', 'never', 'pick', 'up', 'her', 'phone']\n",
      "After stop words removal: ['auntie', 'huai', 'juan', 'never', 'pick', 'phone']\n",
      "After stemming with porters algorithm: ['aunti', 'huai', 'juan', 'never', 'pick', 'phone']\n",
      "Tokenized sentence: ['u', 'buy', 'newspapers', 'already']\n",
      "After stop words removal: ['u', 'buy', 'newspapers', 'already']\n",
      "After stemming with porters algorithm: ['bui', 'newspap', 'alreadi']\n",
      "Tokenized sentence: ['nope', 'i', 'waiting', 'in', 'sch', 'daddy']\n",
      "After stop words removal: ['nope', 'waiting', 'sch', 'daddy']\n",
      "wait\n",
      "After stemming with porters algorithm: ['nope', 'wait', 'sch', 'daddi']\n",
      "Tokenized sentence: ['i', 'haven', 't', 'forgotten', 'you', 'i', 'might', 'have', 'a', 'couple', 'bucks', 'to', 'send', 'you', 'tomorrow', 'k', 'i', 'love', 'ya', 'too']\n",
      "After stop words removal: ['forgotten', 'might', 'couple', 'bucks', 'send', 'tomorrow', 'k', 'love', 'ya']\n",
      "After stemming with porters algorithm: ['forgotten', 'might', 'coupl', 'buck', 'send', 'tomorrow', 'love']\n",
      "Tokenized sentence: ['where', 's', 'mummy', 's', 'boy', 'is', 'he', 'being', 'good', 'or', 'bad', 'is', 'he', 'being', 'positive', 'or', 'negative', 'why', 'is', 'mummy', 'being', 'made', 'to', 'wait', 'hmmmm']\n",
      "After stop words removal: ['mummy', 'boy', 'good', 'bad', 'positive', 'negative', 'mummy', 'made', 'wait', 'hmmmm']\n",
      "After stemming with porters algorithm: ['mummi', 'boi', 'good', 'bad', 'posit', 'neg', 'mummi', 'made', 'wait', 'hmmmm']\n",
      "Tokenized sentence: ['i', 'hope', 'your', 'pee', 'burns', 'tonite']\n",
      "After stop words removal: ['hope', 'pee', 'burns', 'tonite']\n",
      "After stemming with porters algorithm: ['hope', 'pee', 'burn', 'tonit']\n",
      "Tokenized sentence: ['hi', 'my', 'engagement', 'has', 'been', 'fixd', 'on', 'lt', 'gt', 'th', 'of', 'next', 'month', 'i', 'know', 'its', 'really', 'shocking', 'bt', 'hmm', 'njan', 'vilikkam', 't', 'ws', 'al', 'of', 'a', 'sudn']\n",
      "After stop words removal: ['hi', 'engagement', 'fixd', 'lt', 'gt', 'th', 'next', 'month', 'know', 'really', 'shocking', 'bt', 'hmm', 'njan', 'vilikkam', 'ws', 'al', 'sudn']\n",
      "shock\n",
      "After stemming with porters algorithm: ['engag', 'fixd', 'next', 'month', 'know', 'realli', 'shoc', 'hmm', 'njan', 'vilikkam', 'sudn']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['yes', 'it', 's', 'all', 'innocent', 'fun', 'o']\n",
      "After stop words removal: ['yes', 'innocent', 'fun']\n",
      "After stemming with porters algorithm: ['ye', 'innoc', 'fun']\n",
      "Tokenized sentence: ['did', 'u', 'fix', 'the', 'teeth', 'if', 'not', 'do', 'it', 'asap', 'ok', 'take', 'care']\n",
      "After stop words removal: ['u', 'fix', 'teeth', 'asap', 'ok', 'take', 'care']\n",
      "After stemming with porters algorithm: ['fix', 'teeth', 'asap', 'take', 'care']\n",
      "Tokenized sentence: ['prabha', 'i', 'm', 'soryda', 'realy', 'frm', 'heart', 'i', 'm', 'sory']\n",
      "After stop words removal: ['prabha', 'soryda', 'realy', 'frm', 'heart', 'sory']\n",
      "After stemming with porters algorithm: ['prabha', 'soryda', 'reali', 'frm', 'heart', 'sori']\n",
      "Tokenized sentence: ['sorry', 'u', 'can', 'not', 'unsubscribe', 'yet', 'the', 'mob', 'offer', 'package', 'has', 'a', 'min', 'term', 'of', 'weeks', 'pls', 'resubmit', 'request', 'after', 'expiry', 'reply', 'themob', 'help', 'more', 'info']\n",
      "After stop words removal: ['sorry', 'u', 'unsubscribe', 'yet', 'mob', 'offer', 'package', 'min', 'term', 'weeks', 'pls', 'resubmit', 'request', 'expiry', 'reply', 'themob', 'help', 'info']\n",
      "After stemming with porters algorithm: ['sorri', 'unsubscrib', 'yet', 'mob', 'offer', 'packag', 'min', 'term', 'week', 'pl', 'resubmit', 'request', 'expiri', 'repli', 'themob', 'help', 'info']\n",
      "Tokenized sentence: ['garbage', 'bags', 'eggs', 'jam', 'bread', 'hannaford', 'wheat', 'chex']\n",
      "After stop words removal: ['garbage', 'bags', 'eggs', 'jam', 'bread', 'hannaford', 'wheat', 'chex']\n",
      "After stemming with porters algorithm: ['garbag', 'bag', 'egg', 'jam', 'bread', 'hannaford', 'wheat', 'chex']\n",
      "Tokenized sentence: ['want', 'explicit', 'sex', 'in', 'secs', 'ring', 'now', 'costs', 'p', 'min', 'gsex', 'pobox', 'wc', 'n', 'xx']\n",
      "After stop words removal: ['want', 'explicit', 'sex', 'secs', 'ring', 'costs', 'p', 'min', 'gsex', 'pobox', 'wc', 'n', 'xx']\n",
      "After stemming with porters algorithm: ['want', 'explicit', 'sex', 'sec', 'ring', 'cost', 'min', 'gsex', 'pobox']\n",
      "Tokenized sentence: ['where', 'do', 'you', 'need', 'to', 'go', 'to', 'get', 'it']\n",
      "After stop words removal: ['need', 'go', 'get']\n",
      "After stemming with porters algorithm: ['need', 'get']\n",
      "Tokenized sentence: ['gd', 'luck', 'ur', 'exams']\n",
      "After stop words removal: ['gd', 'luck', 'ur', 'exams']\n",
      "After stemming with porters algorithm: ['luck', 'exam']\n",
      "Tokenized sentence: ['yeah', 'work', 'is', 'fine', 'started', 'last', 'week', 'all', 'the', 'same', 'stuff', 'as', 'before', 'dull', 'but', 'easy', 'and', 'guys', 'are', 'fun']\n",
      "After stop words removal: ['yeah', 'work', 'fine', 'started', 'last', 'week', 'stuff', 'dull', 'easy', 'guys', 'fun']\n",
      "After stemming with porters algorithm: ['yeah', 'work', 'fine', 'star', 'last', 'week', 'stuff', 'dull', 'easi', 'gui', 'fun']\n",
      "Tokenized sentence: ['yes', 'there', 'were', 'many', 'sweets']\n",
      "After stop words removal: ['yes', 'many', 'sweets']\n",
      "After stemming with porters algorithm: ['ye', 'mani', 'sweet']\n",
      "Tokenized sentence: ['why', 'you', 'dint', 'come', 'with', 'us']\n",
      "After stop words removal: ['dint', 'come', 'us']\n",
      "After stemming with porters algorithm: ['dint', 'come']\n",
      "Tokenized sentence: ['so', 'your', 'telling', 'me', 'i', 'coulda', 'been', 'your', 'real', 'valentine', 'and', 'i', 'wasn', 't', 'u', 'never', 'pick', 'me', 'for', 'nothing']\n",
      "After stop words removal: ['telling', 'coulda', 'real', 'valentine', 'u', 'never', 'pick', 'nothing']\n",
      "tell\n",
      "noth\n",
      "After stemming with porters algorithm: ['tell', 'coulda', 'real', 'valentin', 'never', 'pick', 'not']\n",
      "Tokenized sentence: ['no', 'i', 'was', 'trying', 'it', 'all', 'weekend', 'v']\n",
      "After stop words removal: ['trying', 'weekend', 'v']\n",
      "After stemming with porters algorithm: ['trying', 'weekend']\n",
      "Tokenized sentence: ['give', 'her', 'something', 'to', 'drink', 'if', 'she', 'takes', 'it', 'and', 'doesn', 't', 'vomit', 'then', 'you', 'her', 'temp', 'might', 'drop', 'if', 'she', 'unmits', 'however', 'let', 'me', 'know']\n",
      "After stop words removal: ['give', 'something', 'drink', 'takes', 'vomit', 'temp', 'might', 'drop', 'unmits', 'however', 'let', 'know']\n",
      "someth\n",
      "After stemming with porters algorithm: ['give', 'somet', 'drink', 'take', 'vomit', 'temp', 'might', 'drop', 'unmit', 'howev', 'let', 'know']\n",
      "Tokenized sentence: ['carlos', 'says', 'he', 'll', 'be', 'at', 'mu', 'in', 'lt', 'gt', 'minutes']\n",
      "After stop words removal: ['carlos', 'says', 'mu', 'lt', 'gt', 'minutes']\n",
      "After stemming with porters algorithm: ['carlo', 'sai', 'minut']\n",
      "Tokenized sentence: ['urgent', 'please', 'call', 'from', 'your', 'landline', 'your', 'complimentary', 'lux', 'costa', 'del', 'sol', 'holiday', 'or', 'cash', 'await', 'collection', 'ppm', 'sae', 't', 'cs', 'james', 'eh', 'rr']\n",
      "After stop words removal: ['urgent', 'please', 'call', 'landline', 'complimentary', 'lux', 'costa', 'del', 'sol', 'holiday', 'cash', 'await', 'collection', 'ppm', 'sae', 'cs', 'james', 'eh', 'rr']\n",
      "After stemming with porters algorithm: ['urgent', 'pleas', 'call', 'landlin', 'complimentari', 'lux', 'costa', 'del', 'sol', 'holidai', 'cash', 'await', 'collect', 'ppm', 'sae', 'jame']\n",
      "Tokenized sentence: ['you', 'made', 'my', 'day', 'do', 'have', 'a', 'great', 'day', 'too']\n",
      "After stop words removal: ['made', 'day', 'great', 'day']\n",
      "After stemming with porters algorithm: ['made', 'dai', 'great', 'dai']\n",
      "Tokenized sentence: ['sure', 'but', 'make', 'sure', 'he', 'knows', 'we', 'ain', 't', 'smokin', 'yet']\n",
      "After stop words removal: ['sure', 'make', 'sure', 'knows', 'smokin', 'yet']\n",
      "After stemming with porters algorithm: ['sure', 'make', 'sure', 'know', 'smokin', 'yet']\n",
      "Tokenized sentence: ['lol', 'u', 'drunkard', 'just', 'doing', 'my', 'hair', 'at', 'd', 'moment', 'yeah', 'still', 'up', 'tonight', 'wats', 'the', 'plan']\n",
      "After stop words removal: ['lol', 'u', 'drunkard', 'hair', 'moment', 'yeah', 'still', 'tonight', 'wats', 'plan']\n",
      "After stemming with porters algorithm: ['lol', 'drunkard', 'hair', 'moment', 'yeah', 'still', 'tonight', 'wat', 'plan']\n",
      "Tokenized sentence: ['are', 'you', 'up', 'for', 'the', 'challenge', 'i', 'know', 'i', 'am']\n",
      "After stop words removal: ['challenge', 'know']\n",
      "After stemming with porters algorithm: ['challeng', 'know']\n",
      "Tokenized sentence: ['are', 'your', 'freezing', 'are', 'you', 'home', 'yet', 'will', 'you', 'remember', 'to', 'kiss', 'your', 'mom', 'in', 'the', 'morning', 'do', 'you', 'love', 'me', 'do', 'you', 'think', 'of', 'me', 'are', 'you', 'missing', 'me', 'yet']\n",
      "After stop words removal: ['freezing', 'home', 'yet', 'remember', 'kiss', 'mom', 'morning', 'love', 'think', 'missing', 'yet']\n",
      "freez\n",
      "morn\n",
      "miss\n",
      "After stemming with porters algorithm: ['freez', 'home', 'yet', 'rememb', 'kiss', 'mom', 'mor', 'love', 'think', 'miss', 'yet']\n",
      "Tokenized sentence: ['hello', 'my', 'love', 'how', 'went', 'your', 'day', 'are', 'you', 'alright', 'i', 'think', 'of', 'you', 'my', 'sweet', 'and', 'send', 'a', 'jolt', 'to', 'your', 'heart', 'to', 'remind', 'you', 'i', 'love', 'you', 'can', 'you', 'hear', 'it', 'i', 'screamed', 'it', 'across', 'the', 'sea', 'for', 'all', 'the', 'world', 'to', 'hear', 'ahmad', 'al', 'hallaq', 'is', 'loved', 'and', 'owned', 'possessive', 'passionate', 'kiss']\n",
      "After stop words removal: ['hello', 'love', 'went', 'day', 'alright', 'think', 'sweet', 'send', 'jolt', 'heart', 'remind', 'love', 'hear', 'screamed', 'across', 'sea', 'world', 'hear', 'ahmad', 'al', 'hallaq', 'loved', 'owned', 'possessive', 'passionate', 'kiss']\n",
      "After stemming with porters algorithm: ['hello', 'love', 'went', 'dai', 'alright', 'think', 'sweet', 'send', 'jolt', 'heart', 'remind', 'love', 'hear', 'scream', 'across', 'sea', 'world', 'hear', 'ahmad', 'hallaq', 'love', 'ow', 'possess', 'passion', 'kiss']\n",
      "Tokenized sentence: ['urgent', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tc', 's', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stop words removal: ['urgent', 'costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stemming with porters algorithm: ['urgent', 'costa', 'del', 'sol', 'holidai', 'await', 'collect', 'call', 'toclaim', 'sae', 'pobox', 'stockport', 'cost', 'max', 'min']\n",
      "Tokenized sentence: ['urgent', 'you', 'have', 'won', 'a', 'week', 'free', 'membership', 'in', 'our', 'prize', 'jackpot', 'txt', 'the', 'word', 'claim', 'to', 'no', 't', 'c', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw', 'a', 'rw']\n",
      "After stop words removal: ['urgent', 'week', 'free', 'membership', 'prize', 'jackpot', 'txt', 'word', 'claim', 'c', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw', 'rw']\n",
      "After stemming with porters algorithm: ['urgent', 'week', 'free', 'membership', 'priz', 'jackpot', 'txt', 'word', 'claim', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw']\n",
      "Tokenized sentence: ['hey', 'for', 'me', 'there', 'is', 'no', 'leave', 'on', 'friday', 'wait', 'i', 'will', 'ask', 'my', 'superior', 'and', 'tell', 'you']\n",
      "After stop words removal: ['hey', 'leave', 'friday', 'wait', 'ask', 'superior', 'tell']\n",
      "After stemming with porters algorithm: ['hei', 'leav', 'fridai', 'wait', 'ask', 'superior', 'tell']\n",
      "Tokenized sentence: ['although', 'i', 'told', 'u', 'dat', 'i', 'm', 'into', 'baig', 'face', 'watches', 'now', 'but', 'i', 'really', 'like', 'e', 'watch', 'u', 'gave', 'cos', 'it', 's', 'fr', 'u', 'thanx', 'everything', 'dat', 'u', 've', 'done', 'today', 'i', 'm', 'touched']\n",
      "After stop words removal: ['although', 'told', 'u', 'dat', 'baig', 'face', 'watches', 'really', 'like', 'e', 'watch', 'u', 'gave', 'cos', 'fr', 'u', 'thanx', 'everything', 'dat', 'u', 'done', 'today', 'touched']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['although', 'told', 'dat', 'baig', 'face', 'watch', 'realli', 'like', 'watch', 'gave', 'co', 'thanx', 'everyt', 'dat', 'done', 'todai', 'touc']\n",
      "Tokenized sentence: ['we', 'took', 'hooch', 'for', 'a', 'walk', 'toaday', 'and', 'i', 'fell', 'over', 'splat', 'grazed', 'my', 'knees', 'and', 'everything', 'should', 'have', 'stayed', 'at', 'home', 'see', 'you', 'tomorrow']\n",
      "After stop words removal: ['took', 'hooch', 'walk', 'toaday', 'fell', 'splat', 'grazed', 'knees', 'everything', 'stayed', 'home', 'see', 'tomorrow']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['took', 'hooch', 'walk', 'toadai', 'fell', 'splat', 'graz', 'knee', 'everyt', 'stai', 'home', 'see', 'tomorrow']\n",
      "Tokenized sentence: ['g', 'says', 'you', 'never', 'answer', 'your', 'texts', 'confirm', 'deny']\n",
      "After stop words removal: ['g', 'says', 'never', 'answer', 'texts', 'confirm', 'deny']\n",
      "After stemming with porters algorithm: ['sai', 'never', 'answer', 'text', 'confirm', 'deni']\n",
      "Tokenized sentence: ['k', 'you', 'are', 'the', 'only', 'girl', 'waiting', 'in', 'reception', 'ah']\n",
      "After stop words removal: ['k', 'girl', 'waiting', 'reception', 'ah']\n",
      "wait\n",
      "After stemming with porters algorithm: ['girl', 'wait', 'recept']\n",
      "Tokenized sentence: ['when', 'you', 'came', 'to', 'hostel']\n",
      "After stop words removal: ['came', 'hostel']\n",
      "After stemming with porters algorithm: ['came', 'hostel']\n",
      "Tokenized sentence: ['december', 'only', 'had', 'your', 'mobile', 'mths', 'you', 'are', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'camera', 'mobile', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']\n",
      "After stop words removal: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
      "After stemming with porters algorithm: ['decemb', 'mobil', 'mth', 'entit', 'updat', 'latest', 'colour', 'camera', 'mobil', 'free', 'call', 'mobil', 'updat', 'free']\n",
      "Tokenized sentence: ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'am', 'pm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
      "After stop words removal: ['please', 'call', 'customer', 'service', 'representative', 'pm', 'guaranteed', 'cash', 'prize']\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'custom', 'servic', 'repres', 'guaranteed', 'cash', 'priz']\n",
      "Tokenized sentence: ['hey', 'happy', 'birthday']\n",
      "After stop words removal: ['hey', 'happy', 'birthday']\n",
      "After stemming with porters algorithm: ['hei', 'happi', 'birthdai']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later', 'lt', 'gt', 'mins']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'lt', 'gt', 'mins']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'min']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'a', 'prize', 'to', 'claim', 'yr', 'prize', 'call', 'our', 'customer', 'service', 'representative', 'on']\n",
      "After stop words removal: ['guaranteed', 'cash', 'prize', 'claim', 'yr', 'prize', 'call', 'customer', 'service', 'representative']\n",
      "After stemming with porters algorithm: ['guaranteed', 'cash', 'priz', 'claim', 'priz', 'call', 'custom', 'servic', 'repres']\n",
      "Tokenized sentence: ['speaking', 'of', 'does', 'he', 'have', 'any', 'cash', 'yet']\n",
      "After stop words removal: ['speaking', 'cash', 'yet']\n",
      "speak\n",
      "After stemming with porters algorithm: ['speak', 'cash', 'yet']\n",
      "Tokenized sentence: ['first', 'answer', 'my', 'question']\n",
      "After stop words removal: ['first', 'answer', 'question']\n",
      "After stemming with porters algorithm: ['first', 'answer', 'quest']\n",
      "Tokenized sentence: ['have', 'a', 'good', 'evening', 'ttyl']\n",
      "After stop words removal: ['good', 'evening', 'ttyl']\n",
      "even\n",
      "After stemming with porters algorithm: ['good', 'even', 'ttyl']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later', 'in', 'meeting']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'meet']\n",
      "Tokenized sentence: ['also', 'are', 'you', 'bringing', 'galileo', 'or', 'dobby']\n",
      "After stop words removal: ['also', 'bringing', 'galileo', 'dobby']\n",
      "bring\n",
      "After stemming with porters algorithm: ['also', 'brin', 'galileo', 'dobbi']\n",
      "Tokenized sentence: ['ladies', 'first', 'and', 'genus', 'second', 'k']\n",
      "After stop words removal: ['ladies', 'first', 'genus', 'second', 'k']\n",
      "After stemming with porters algorithm: ['ladi', 'first', 'genu', 'second']\n",
      "Tokenized sentence: ['so', 'many', 'people', 'seems', 'to', 'be', 'special', 'at', 'first', 'sight', 'but', 'only', 'very', 'few', 'will', 'remain', 'special', 'to', 'you', 'till', 'your', 'last', 'sight', 'maintain', 'them', 'till', 'life', 'ends', 'take', 'cr', 'da']\n",
      "After stop words removal: ['many', 'people', 'seems', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'ends', 'take', 'cr', 'da']\n",
      "After stemming with porters algorithm: ['mani', 'peopl', 'seem', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'end', 'take']\n",
      "Tokenized sentence: ['just', 'sent', 'it', 'so', 'what', 'type', 'of', 'food', 'do', 'you', 'like']\n",
      "After stop words removal: ['sent', 'type', 'food', 'like']\n",
      "After stemming with porters algorithm: ['sent', 'type', 'food', 'like']\n",
      "Tokenized sentence: ['ard', 'like', 'dat', 'lor']\n",
      "After stop words removal: ['ard', 'like', 'dat', 'lor']\n",
      "After stemming with porters algorithm: ['ard', 'like', 'dat', 'lor']\n",
      "Tokenized sentence: ['the', 'lt', 'gt', 'g', 'that', 'i', 'saw', 'a', 'few', 'days', 'ago', 'the', 'guy', 'wants', 'sell', 'wifi', 'only', 'for', 'lt', 'gt', 'and', 'with', 'g', 'for', 'lt', 'gt', 'that', 's', 'why', 'i', 'blanked', 'him']\n",
      "After stop words removal: ['lt', 'gt', 'g', 'saw', 'days', 'ago', 'guy', 'wants', 'sell', 'wifi', 'lt', 'gt', 'g', 'lt', 'gt', 'blanked']\n",
      "After stemming with porters algorithm: ['saw', 'dai', 'ago', 'gui', 'want', 'sell', 'wifi', 'blan']\n",
      "Tokenized sentence: ['until', 'lor', 'ya', 'can', 'go', 'dinner', 'together']\n",
      "After stop words removal: ['lor', 'ya', 'go', 'dinner', 'together']\n",
      "After stemming with porters algorithm: ['lor', 'dinner', 'togeth']\n",
      "Tokenized sentence: ['actually', 'getting', 'ready', 'to', 'leave', 'the', 'house']\n",
      "After stop words removal: ['actually', 'getting', 'ready', 'leave', 'house']\n",
      "gett\n",
      "After stemming with porters algorithm: ['actual', 'get', 'readi', 'leav', 'hous']\n",
      "Tokenized sentence: ['becoz', 'its', 'lt', 'gt', 'jan', 'whn', 'al', 'the', 'post', 'ofice', 'is', 'in', 'holiday', 'so', 'she', 'cn', 'go', 'fr', 'the', 'post', 'ofice', 'got', 'it', 'duffer']\n",
      "After stop words removal: ['becoz', 'lt', 'gt', 'jan', 'whn', 'al', 'post', 'ofice', 'holiday', 'cn', 'go', 'fr', 'post', 'ofice', 'got', 'duffer']\n",
      "After stemming with porters algorithm: ['becoz', 'jan', 'whn', 'post', 'ofic', 'holidai', 'post', 'ofic', 'got', 'duffer']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['hi', 'hope', 'you', 'had', 'a', 'good', 'day', 'have', 'a', 'better', 'night']\n",
      "After stop words removal: ['hi', 'hope', 'good', 'day', 'better', 'night']\n",
      "After stemming with porters algorithm: ['hope', 'good', 'dai', 'better', 'night']\n",
      "Tokenized sentence: ['yup', 'i', 've', 'finished', 'c', 'there']\n",
      "After stop words removal: ['yup', 'finished', 'c']\n",
      "After stemming with porters algorithm: ['yup', 'finis']\n",
      "Tokenized sentence: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'to', 'contact', 'u', 'u', 'have', 'won', 'the', 'prize', 'to', 'claim', 'just', 'call', 'b', 't', 'cs', 'stop', 'sms', 'ppm']\n",
      "After stop words removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'prize', 'claim', 'call', 'b', 'cs', 'stop', 'sms', 'ppm']\n",
      "After stemming with porters algorithm: ['time', 'tri', 'contact', 'priz', 'claim', 'call', 'stop', 'sm', 'ppm']\n",
      "Tokenized sentence: ['i', 'm', 'back', 'lemme', 'know', 'when', 'you', 're', 'ready']\n",
      "After stop words removal: ['back', 'lemme', 'know', 'ready']\n",
      "After stemming with porters algorithm: ['back', 'lemm', 'know', 'readi']\n",
      "Tokenized sentence: ['probably', 'not', 'i', 'm', 'almost', 'out', 'of', 'gas', 'and', 'i', 'get', 'some', 'cash', 'tomorrow']\n",
      "After stop words removal: ['probably', 'almost', 'gas', 'get', 'cash', 'tomorrow']\n",
      "After stemming with porters algorithm: ['probab', 'almost', 'ga', 'get', 'cash', 'tomorrow']\n",
      "Tokenized sentence: ['many', 'more', 'happy', 'returns', 'of', 'the', 'day', 'i', 'wish', 'you', 'happy', 'birthday']\n",
      "After stop words removal: ['many', 'happy', 'returns', 'day', 'wish', 'happy', 'birthday']\n",
      "After stemming with porters algorithm: ['mani', 'happi', 'return', 'dai', 'wish', 'happi', 'birthdai']\n",
      "Tokenized sentence: ['are', 'you', 'the', 'cutest', 'girl', 'in', 'the', 'world', 'or', 'what']\n",
      "After stop words removal: ['cutest', 'girl', 'world']\n",
      "After stemming with porters algorithm: ['cutest', 'girl', 'world']\n",
      "Tokenized sentence: ['unni', 'thank', 'you', 'dear', 'for', 'the', 'recharge', 'rakhesh']\n",
      "After stop words removal: ['unni', 'thank', 'dear', 'recharge', 'rakhesh']\n",
      "After stemming with porters algorithm: ['unni', 'thank', 'dear', 'recharg', 'rakhesh']\n",
      "Tokenized sentence: ['in', 'meeting', 'da', 'i', 'will', 'call', 'you']\n",
      "After stop words removal: ['meeting', 'da', 'call']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'call']\n",
      "Tokenized sentence: ['yep', 'get', 'with', 'the', 'program', 'you', 're', 'slacking']\n",
      "After stop words removal: ['yep', 'get', 'program', 'slacking']\n",
      "slack\n",
      "After stemming with porters algorithm: ['yep', 'get', 'program', 'slac']\n",
      "Tokenized sentence: ['i', 'dont', 'thnk', 'its', 'a', 'wrong', 'calling', 'between', 'us']\n",
      "After stop words removal: ['dont', 'thnk', 'wrong', 'calling', 'us']\n",
      "call\n",
      "After stemming with porters algorithm: ['dont', 'thnk', 'wrong', 'call']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'with', 'money', 'as', 'in', 'to', 'you', 'money', 'aint', 'a', 'thing', 'how', 'are', 'you', 'sha']\n",
      "After stop words removal: ['money', 'money', 'aint', 'thing', 'sha']\n",
      "After stemming with porters algorithm: ['monei', 'monei', 'aint', 'thing', 'sha']\n",
      "Tokenized sentence: ['i', 'plane', 'to', 'give', 'on', 'this', 'month', 'end']\n",
      "After stop words removal: ['plane', 'give', 'month', 'end']\n",
      "After stemming with porters algorithm: ['plane', 'give', 'month', 'end']\n",
      "Tokenized sentence: ['networking', 'job', 'is', 'there']\n",
      "After stop words removal: ['networking', 'job']\n",
      "network\n",
      "After stemming with porters algorithm: ['networ', 'job']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'wish', 'you', 'a', 'great', 'semester']\n",
      "After stop words removal: ['wish', 'great', 'semester']\n",
      "After stemming with porters algorithm: ['wish', 'great', 'semest']\n",
      "Tokenized sentence: ['what', 'is', 'your', 'account', 'number']\n",
      "After stop words removal: ['account', 'number']\n",
      "After stemming with porters algorithm: ['account', 'number']\n",
      "Tokenized sentence: ['what', 'are', 'you', 'doing', 'in', 'langport', 'sorry', 'but', 'i', 'll', 'probably', 'be', 'in', 'bed', 'by', 'pm', 'it', 'sucks', 'being', 'ill', 'at', 'xmas', 'when', 'do', 'you', 'and', 'go', 'sri', 'lanka']\n",
      "After stop words removal: ['langport', 'sorry', 'probably', 'bed', 'pm', 'sucks', 'ill', 'xmas', 'go', 'sri', 'lanka']\n",
      "After stemming with porters algorithm: ['langport', 'sorri', 'probab', 'bed', 'suck', 'ill', 'xma', 'sri', 'lanka']\n",
      "Tokenized sentence: ['s', 's', 'i', 'thinl', 'role', 'is', 'like', 'sachin', 'just', 'standing', 'others', 'have', 'to', 'hit']\n",
      "After stop words removal: ['thinl', 'role', 'like', 'sachin', 'standing', 'others', 'hit']\n",
      "stand\n",
      "After stemming with porters algorithm: ['thinl', 'role', 'like', 'sachin', 'stan', 'other', 'hit']\n",
      "Tokenized sentence: ['okay', 'i', 'booked', 'all', 'already', 'including', 'the', 'one', 'at', 'bugis']\n",
      "After stop words removal: ['okay', 'booked', 'already', 'including', 'one', 'bugis']\n",
      "includ\n",
      "After stemming with porters algorithm: ['okai', 'book', 'alreadi', 'includ', 'on', 'bugi']\n",
      "Tokenized sentence: ['badrith', 'is', 'only', 'for', 'chennai', 'i', 'will', 'surely', 'pick', 'for', 'us', 'no', 'competition', 'for', 'him']\n",
      "After stop words removal: ['badrith', 'chennai', 'surely', 'pick', 'us', 'competition']\n",
      "After stemming with porters algorithm: ['badrith', 'chennai', 'sure', 'pick', 'competit']\n",
      "Tokenized sentence: ['probably', 'gonna', 'swing', 'by', 'in', 'a', 'wee', 'bit']\n",
      "After stop words removal: ['probably', 'gonna', 'swing', 'wee', 'bit']\n",
      "After stemming with porters algorithm: ['probab', 'gonna', 'swing', 'wee', 'bit']\n",
      "Tokenized sentence: ['me', 'fine', 'absolutly', 'fine']\n",
      "After stop words removal: ['fine', 'absolutly', 'fine']\n",
      "After stemming with porters algorithm: ['fine', 'absolutli', 'fine']\n",
      "Tokenized sentence: ['jay', 'says', 'that', 'you', 're', 'a', 'double', 'faggot']\n",
      "After stop words removal: ['jay', 'says', 'double', 'faggot']\n",
      "After stemming with porters algorithm: ['jai', 'sai', 'doubl', 'faggot']\n",
      "Tokenized sentence: ['see', 'the', 'forwarding', 'message', 'for', 'proof']\n",
      "After stop words removal: ['see', 'forwarding', 'message', 'proof']\n",
      "forward\n",
      "After stemming with porters algorithm: ['see', 'forwar', 'messag', 'proof']\n",
      "Tokenized sentence: ['excellent', 'i', 'll', 'see', 'what', 'riley', 's', 'plans', 'are']\n",
      "After stop words removal: ['excellent', 'see', 'riley', 'plans']\n",
      "After stemming with porters algorithm: ['excel', 'see', 'rilei', 'plan']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'awar', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['free', 'top', 'ringtone', 'sub', 'to', 'weekly', 'ringtone', 'get', 'st', 'week', 'free', 'send', 'subpoly', 'to', 'per', 'week', 'stop', 'sms']\n",
      "After stop words removal: ['free', 'top', 'ringtone', 'sub', 'weekly', 'ringtone', 'get', 'st', 'week', 'free', 'send', 'subpoly', 'per', 'week', 'stop', 'sms']\n",
      "After stemming with porters algorithm: ['free', 'top', 'rington', 'sub', 'weekli', 'rington', 'get', 'week', 'free', 'send', 'subpoli', 'per', 'week', 'stop', 'sm']\n",
      "Tokenized sentence: ['lol', 'they', 'were', 'mad', 'at', 'first', 'but', 'then', 'they', 'woke', 'up', 'and', 'gave', 'in']\n",
      "After stop words removal: ['lol', 'mad', 'first', 'woke', 'gave']\n",
      "After stemming with porters algorithm: ['lol', 'mad', 'first', 'woke', 'gave']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'hospital', 'da', 'i', 'will', 'return', 'home', 'in', 'evening']\n",
      "After stop words removal: ['hospital', 'da', 'return', 'home', 'evening']\n",
      "even\n",
      "After stemming with porters algorithm: ['hospit', 'return', 'home', 'even']\n",
      "Tokenized sentence: ['r', 'u', 'saying', 'i', 'should', 're', 'order', 'the', 'slippers', 'cos', 'i', 'had', 'to', 'pay', 'for', 'returning', 'it']\n",
      "After stop words removal: ['r', 'u', 'saying', 'order', 'slippers', 'cos', 'pay', 'returning']\n",
      "say\n",
      "return\n",
      "After stemming with porters algorithm: ['sai', 'order', 'slipper', 'co', 'pai', 'retur']\n",
      "Tokenized sentence: ['yeah', 'that', 'd', 'pretty', 'much', 'be', 'the', 'best', 'case', 'scenario']\n",
      "After stop words removal: ['yeah', 'pretty', 'much', 'best', 'case', 'scenario']\n",
      "After stemming with porters algorithm: ['yeah', 'pretti', 'much', 'best', 'case', 'scenario']\n",
      "Tokenized sentence: ['i', 'will', 'see', 'in', 'half', 'an', 'hour']\n",
      "After stop words removal: ['see', 'half', 'hour']\n",
      "After stemming with porters algorithm: ['see', 'half', 'hour']\n",
      "Tokenized sentence: ['we', 're', 'done']\n",
      "After stop words removal: ['done']\n",
      "After stemming with porters algorithm: ['done']\n",
      "Tokenized sentence: ['an', 'excellent', 'thought', 'by', 'a', 'misundrstud', 'frnd', 'i', 'knw', 'u', 'hate', 'me', 'bt', 'the', 'day', 'wen', 'u', 'll', 'knw', 'the', 'truth', 'u', 'll', 'hate', 'urself', 'gn']\n",
      "After stop words removal: ['excellent', 'thought', 'misundrstud', 'frnd', 'knw', 'u', 'hate', 'bt', 'day', 'wen', 'u', 'knw', 'truth', 'u', 'hate', 'urself', 'gn']\n",
      "After stemming with porters algorithm: ['excel', 'thought', 'misundrstud', 'frnd', 'knw', 'hate', 'dai', 'wen', 'knw', 'truth', 'hate', 'urself']\n",
      "Tokenized sentence: ['come', 'to', 'mu', 'we', 're', 'sorting', 'out', 'our', 'narcotics', 'situation']\n",
      "After stop words removal: ['come', 'mu', 'sorting', 'narcotics', 'situation']\n",
      "sort\n",
      "After stemming with porters algorithm: ['come', 'sor', 'narcot', 'situat']\n",
      "Tokenized sentence: ['so', 'lets', 'make', 'it', 'saturday', 'or', 'monday', 'as', 'per', 'convenience']\n",
      "After stop words removal: ['lets', 'make', 'saturday', 'monday', 'per', 'convenience']\n",
      "After stemming with porters algorithm: ['let', 'make', 'saturdai', 'mondai', 'per', 'conveni']\n",
      "Tokenized sentence: ['wat', 'happened', 'to', 'the', 'cruise', 'thing']\n",
      "After stop words removal: ['wat', 'happened', 'cruise', 'thing']\n",
      "After stemming with porters algorithm: ['wat', 'happen', 'cruis', 'thing']\n",
      "Tokenized sentence: ['dear', 'i', 'have', 'reache', 'room']\n",
      "After stop words removal: ['dear', 'reache', 'room']\n",
      "After stemming with porters algorithm: ['dear', 'reach', 'room']\n",
      "Tokenized sentence: ['yeah', 'it', 's', 'jus', 'rite']\n",
      "After stop words removal: ['yeah', 'jus', 'rite']\n",
      "After stemming with porters algorithm: ['yeah', 'ju', 'rite']\n",
      "Tokenized sentence: ['get', 'lions', 'england', 'tone', 'reply', 'lionm', 'mono', 'or', 'lionp', 'poly', 'more', 'go', 'www', 'ringtones', 'co', 'uk', 'the', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stop words removal: ['get', 'lions', 'england', 'tone', 'reply', 'lionm', 'mono', 'lionp', 'poly', 'go', 'www', 'ringtones', 'co', 'uk', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stemming with porters algorithm: ['get', 'lion', 'england', 'tone', 'repli', 'lionm', 'mono', 'lionp', 'poli', 'www', 'rington', 'origin', 'best', 'tone', 'gbp', 'network', 'oper', 'rate', 'appli']\n",
      "Tokenized sentence: ['better', 'made', 'up', 'for', 'friday', 'and', 'stuffed', 'myself', 'like', 'a', 'pig', 'yesterday', 'now', 'i', 'feel', 'bleh', 'but', 'at', 'least', 'its', 'not', 'writhing', 'pain', 'kind', 'of', 'bleh']\n",
      "After stop words removal: ['better', 'made', 'friday', 'stuffed', 'like', 'pig', 'yesterday', 'feel', 'bleh', 'least', 'writhing', 'pain', 'kind', 'bleh']\n",
      "writh\n",
      "After stemming with porters algorithm: ['better', 'made', 'fridai', 'stuf', 'like', 'pig', 'yesterdai', 'feel', 'bleh', 'least', 'writ', 'pain', 'kind', 'bleh']\n",
      "Tokenized sentence: ['we', 'can', 'go', 'e', 'normal', 'pilates', 'after', 'our', 'intro']\n",
      "After stop words removal: ['go', 'e', 'normal', 'pilates', 'intro']\n",
      "After stemming with porters algorithm: ['normal', 'pilat', 'intro']\n",
      "Tokenized sentence: ['india', 'have', 'to', 'take', 'lead']\n",
      "After stop words removal: ['india', 'take', 'lead']\n",
      "After stemming with porters algorithm: ['india', 'take', 'lead']\n",
      "Tokenized sentence: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stop words removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stemming with porters algorithm: ['free', 'week', 'nokia', 'tone', 'mob', 'everi', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'mate', 'www', 'getz', 'pobox', 'norm', 'tone']\n",
      "Tokenized sentence: ['do', 'you', 'know', 'why', 'god', 'created', 'gap', 'between', 'your', 'fingers', 'so', 'that', 'one', 'who', 'is', 'made', 'for', 'you', 'comes', 'amp', 'fills', 'those', 'gaps', 'by', 'holding', 'your', 'hand', 'with', 'love']\n",
      "After stop words removal: ['know', 'god', 'created', 'gap', 'fingers', 'one', 'made', 'comes', 'amp', 'fills', 'gaps', 'holding', 'hand', 'love']\n",
      "create\n",
      "hold\n",
      "After stemming with porters algorithm: ['know', 'god', 'creat', 'gap', 'finger', 'on', 'made', 'come', 'amp', 'fill', 'gap', 'hol', 'hand', 'love']\n",
      "Tokenized sentence: ['you', 'got', 'called', 'a', 'tool']\n",
      "After stop words removal: ['got', 'called', 'tool']\n",
      "After stemming with porters algorithm: ['got', 'call', 'tool']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call']\n",
      "Tokenized sentence: ['i', 've', 'not', 'sent', 'it', 'he', 'can', 'send', 'me']\n",
      "After stop words removal: ['sent', 'send']\n",
      "After stemming with porters algorithm: ['sent', 'send']\n",
      "Tokenized sentence: ['ok', 'lor']\n",
      "After stop words removal: ['ok', 'lor']\n",
      "After stemming with porters algorithm: ['lor']\n",
      "Tokenized sentence: ['ok', 'lor', 'sony', 'ericsson', 'salesman', 'i', 'ask', 'shuhui', 'then', 'she', 'say', 'quite', 'gd', 'use', 'so', 'i', 'considering']\n",
      "After stop words removal: ['ok', 'lor', 'sony', 'ericsson', 'salesman', 'ask', 'shuhui', 'say', 'quite', 'gd', 'use', 'considering']\n",
      "consider\n",
      "After stemming with porters algorithm: ['lor', 'soni', 'ericsson', 'salesman', 'ask', 'shuhui', 'sai', 'quit', 'us', 'consid']\n",
      "Tokenized sentence: ['was', 'really', 'good', 'to', 'see', 'you', 'the', 'other', 'day', 'dudette', 'been', 'missing', 'you']\n",
      "After stop words removal: ['really', 'good', 'see', 'day', 'dudette', 'missing']\n",
      "miss\n",
      "After stemming with porters algorithm: ['realli', 'good', 'see', 'dai', 'dudett', 'miss']\n",
      "Tokenized sentence: ['only', 'send', 'me', 'the', 'contents', 'page']\n",
      "After stop words removal: ['send', 'contents', 'page']\n",
      "After stemming with porters algorithm: ['send', 'content', 'page']\n",
      "Tokenized sentence: ['oh', 'did', 'you', 'charge', 'camera']\n",
      "After stop words removal: ['oh', 'charge', 'camera']\n",
      "After stemming with porters algorithm: ['charg', 'camera']\n",
      "Tokenized sentence: ['this', 'is', 'the', 'nd', 'attempt', 'to', 'contract', 'u', 'you', 'have', 'won', 'this', 'weeks', 'top', 'prize', 'of', 'either', 'cash', 'or', 'prize', 'just', 'call']\n",
      "After stop words removal: ['nd', 'attempt', 'contract', 'u', 'weeks', 'top', 'prize', 'either', 'cash', 'prize', 'call']\n",
      "After stemming with porters algorithm: ['attempt', 'contract', 'week', 'top', 'priz', 'either', 'cash', 'priz', 'call']\n",
      "Tokenized sentence: ['see', 'you', 'then', 'we', 're', 'all', 'christmassy', 'here']\n",
      "After stop words removal: ['see', 'christmassy']\n",
      "After stemming with porters algorithm: ['see', 'christmassi']\n",
      "Tokenized sentence: ['tiwary', 'to', 'rcb', 'battle', 'between', 'bang', 'and', 'kochi']\n",
      "After stop words removal: ['tiwary', 'rcb', 'battle', 'bang', 'kochi']\n",
      "After stemming with porters algorithm: ['tiwari', 'rcb', 'battl', 'bang', 'kochi']\n",
      "Tokenized sentence: ['can', 'call', 'me', 'at', 'to', 'make', 'sure', 'dat', 'i', 've', 'woken', 'up']\n",
      "After stop words removal: ['call', 'make', 'sure', 'dat', 'woken']\n",
      "After stemming with porters algorithm: ['call', 'make', 'sure', 'dat', 'woken']\n",
      "Tokenized sentence: ['moby', 'pub', 'quiz', 'win', 'a', 'high', 'street', 'prize', 'if', 'u', 'know', 'who', 'the', 'new', 'duchess', 'of', 'cornwall', 'will', 'be', 'txt', 'her', 'first', 'name', 'to', 'unsub', 'stop', 'sp', 'arrow']\n",
      "After stop words removal: ['moby', 'pub', 'quiz', 'win', 'high', 'street', 'prize', 'u', 'know', 'new', 'duchess', 'cornwall', 'txt', 'first', 'name', 'unsub', 'stop', 'sp', 'arrow']\n",
      "After stemming with porters algorithm: ['mobi', 'pub', 'quiz', 'win', 'high', 'street', 'priz', 'know', 'new', 'duchess', 'cornwal', 'txt', 'first', 'name', 'unsub', 'stop', 'arrow']\n",
      "Tokenized sentence: ['normally', 'i', 'use', 'to', 'drink', 'more', 'water', 'daily']\n",
      "After stop words removal: ['normally', 'use', 'drink', 'water', 'daily']\n",
      "After stemming with porters algorithm: ['normal', 'us', 'drink', 'water', 'daili']\n",
      "Tokenized sentence: ['look', 'at', 'amy', 'ure', 'a', 'beautiful', 'intelligent', 'woman', 'and', 'i', 'like', 'u', 'a', 'lot', 'i', 'know', 'u', 'don', 't', 'like', 'me', 'like', 'that', 'so', 'don', 't', 'worry']\n",
      "After stop words removal: ['look', 'amy', 'ure', 'beautiful', 'intelligent', 'woman', 'like', 'u', 'lot', 'know', 'u', 'like', 'like', 'worry']\n",
      "After stemming with porters algorithm: ['look', 'ami', 'ur', 'beauti', 'intellig', 'woman', 'like', 'lot', 'know', 'like', 'like', 'worri']\n",
      "Tokenized sentence: ['it', 's', 'not', 'that', 'you', 'make', 'me', 'cry', 'it', 's', 'just', 'that', 'when', 'all', 'our', 'stuff', 'happens', 'on', 'top', 'of', 'everything', 'else', 'it', 'pushes', 'me', 'over', 'the', 'edge', 'you', 'don', 't', 'underdtand', 'how', 'often', 'i', 'cry', 'over', 'my', 'sorry', 'sorry', 'life']\n",
      "After stop words removal: ['make', 'cry', 'stuff', 'happens', 'top', 'everything', 'else', 'pushes', 'edge', 'underdtand', 'often', 'cry', 'sorry', 'sorry', 'life']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['make', 'cry', 'stuff', 'happen', 'top', 'everyt', 'els', 'push', 'edg', 'underdtand', 'often', 'cry', 'sorri', 'sorri', 'life']\n",
      "Tokenized sentence: ['alright', 'see', 'you', 'in', 'a', 'bit']\n",
      "After stop words removal: ['alright', 'see', 'bit']\n",
      "After stemming with porters algorithm: ['alright', 'see', 'bit']\n",
      "Tokenized sentence: ['s', 'i', 'think', 'he', 'is', 'waste', 'for', 'rr']\n",
      "After stop words removal: ['think', 'waste', 'rr']\n",
      "After stemming with porters algorithm: ['think', 'wast']\n",
      "Tokenized sentence: ['lol', 'i', 'know', 'hey', 'someone', 'did', 'a', 'great', 'inpersonation', 'of', 'flea', 'on', 'the', 'forums', 'i', 'love', 'it']\n",
      "After stop words removal: ['lol', 'know', 'hey', 'someone', 'great', 'inpersonation', 'flea', 'forums', 'love']\n",
      "After stemming with porters algorithm: ['lol', 'know', 'hei', 'someon', 'great', 'inperson', 'flea', 'forum', 'love']\n",
      "Tokenized sentence: ['u', 'can', 'call', 'now']\n",
      "After stop words removal: ['u', 'call']\n",
      "After stemming with porters algorithm: ['call']\n",
      "Tokenized sentence: ['i', 'love', 'to', 'give', 'massages', 'i', 'use', 'lots', 'of', 'baby', 'oil', 'what', 'is', 'your', 'fave', 'position']\n",
      "After stop words removal: ['love', 'give', 'massages', 'use', 'lots', 'baby', 'oil', 'fave', 'position']\n",
      "After stemming with porters algorithm: ['love', 'give', 'massag', 'us', 'lot', 'babi', 'oil', 'fave', 'posit']\n",
      "Tokenized sentence: ['yalru', 'lyfu', 'astne', 'chikku', 'bt', 'innu', 'mundhe', 'lyf', 'ali', 'halla', 'ke', 'bilo', 'marriage', 'program', 'edhae', 'so', 'lyf', 'is', 'nt', 'yet', 'ovr', 'chikku', 'ali', 'vargu', 'lyfu', 'meow', 'meow', 'd']\n",
      "After stop words removal: ['yalru', 'lyfu', 'astne', 'chikku', 'bt', 'innu', 'mundhe', 'lyf', 'ali', 'halla', 'ke', 'bilo', 'marriage', 'program', 'edhae', 'lyf', 'nt', 'yet', 'ovr', 'chikku', 'ali', 'vargu', 'lyfu', 'meow', 'meow']\n",
      "After stemming with porters algorithm: ['yalru', 'lyfu', 'astn', 'chikku', 'innu', 'mundh', 'lyf', 'ali', 'halla', 'bilo', 'marriag', 'program', 'edha', 'lyf', 'yet', 'ovr', 'chikku', 'ali', 'vargu', 'lyfu', 'meow', 'meow']\n",
      "Tokenized sentence: ['when', 'you', 'and', 'derek', 'done', 'with', 'class']\n",
      "After stop words removal: ['derek', 'done', 'class']\n",
      "After stemming with porters algorithm: ['derek', 'done', 'class']\n",
      "Tokenized sentence: ['the', 'lay', 'man', 'just', 'to', 'let', 'you', 'know', 'you', 'are', 'missed', 'and', 'thought', 'off', 'do', 'have', 'a', 'great', 'day', 'and', 'if', 'you', 'can', 'send', 'me', 'bimbo', 'and', 'ugo', 's', 'numbers', 'ill', 'appreciate', 'safe']\n",
      "After stop words removal: ['lay', 'man', 'let', 'know', 'missed', 'thought', 'great', 'day', 'send', 'bimbo', 'ugo', 'numbers', 'ill', 'appreciate', 'safe']\n",
      "After stemming with porters algorithm: ['lai', 'man', 'let', 'know', 'miss', 'thought', 'great', 'dai', 'send', 'bimbo', 'ugo', 'number', 'ill', 'appreci', 'safe']\n",
      "Tokenized sentence: ['i', 'm', 'reading', 'the', 'text', 'i', 'just', 'sent', 'you', 'its', 'meant', 'to', 'be', 'a', 'joke', 'so', 'read', 'it', 'in', 'that', 'light']\n",
      "After stop words removal: ['reading', 'text', 'sent', 'meant', 'joke', 'read', 'light']\n",
      "read\n",
      "After stemming with porters algorithm: ['read', 'text', 'sent', 'meant', 'joke', 'read', 'light']\n",
      "Tokenized sentence: ['well', 'i', 'meant', 'as', 'opposed', 'to', 'my', 'drunken', 'night', 'of', 'before']\n",
      "After stop words removal: ['well', 'meant', 'opposed', 'drunken', 'night']\n",
      "After stemming with porters algorithm: ['well', 'meant', 'oppos', 'drunken', 'night']\n",
      "Tokenized sentence: ['u', 'still', 'painting', 'ur', 'wall']\n",
      "After stop words removal: ['u', 'still', 'painting', 'ur', 'wall']\n",
      "paint\n",
      "After stemming with porters algorithm: ['still', 'pain', 'wall']\n",
      "Tokenized sentence: ['guess', 'what', 'somebody', 'you', 'know', 'secretly', 'fancies', 'you', 'wanna', 'find', 'out', 'who', 'it', 'is', 'give', 'us', 'a', 'call', 'on', 'from', 'landline', 'datebox', 'essexcm', 'xn', 'p', 'min']\n",
      "After stop words removal: ['guess', 'somebody', 'know', 'secretly', 'fancies', 'wanna', 'find', 'give', 'us', 'call', 'landline', 'datebox', 'essexcm', 'xn', 'p', 'min']\n",
      "After stemming with porters algorithm: ['guess', 'somebodi', 'know', 'secretli', 'fanci', 'wanna', 'find', 'give', 'call', 'landlin', 'datebox', 'essexcm', 'min']\n",
      "Tokenized sentence: ['oh', 'and', 'by', 'the', 'way', 'you', 'do', 'have', 'more', 'food', 'in', 'your', 'fridge', 'want', 'to', 'go', 'out', 'for', 'a', 'meal', 'tonight']\n",
      "After stop words removal: ['oh', 'way', 'food', 'fridge', 'want', 'go', 'meal', 'tonight']\n",
      "After stemming with porters algorithm: ['wai', 'food', 'fridg', 'want', 'meal', 'tonight']\n",
      "Tokenized sentence: ['haha', 'they', 'cant', 'what', 'at', 'the', 'most', 'tmr', 'forfeit', 'haha', 'so', 'how']\n",
      "After stop words removal: ['haha', 'cant', 'tmr', 'forfeit', 'haha']\n",
      "After stemming with porters algorithm: ['haha', 'cant', 'tmr', 'forfeit', 'haha']\n",
      "Tokenized sentence: ['got', 'ur', 'mail', 'dileep', 'thank', 'you', 'so', 'muchand', 'look', 'forward', 'to', 'lots', 'of', 'support', 'very', 'less', 'contacts', 'here', 'remember', 'one', 'venugopal', 'you', 'mentioned', 'tomorrow', 'if', 'not', 'late', 'i', 'shall', 'try', 'to', 'come', 'up', 'till', 'there', 'goodnight', 'dear']\n",
      "After stop words removal: ['got', 'ur', 'mail', 'dileep', 'thank', 'muchand', 'look', 'forward', 'lots', 'support', 'less', 'contacts', 'remember', 'one', 'venugopal', 'mentioned', 'tomorrow', 'late', 'shall', 'try', 'come', 'till', 'goodnight', 'dear']\n",
      "After stemming with porters algorithm: ['got', 'mail', 'dileep', 'thank', 'muchand', 'look', 'forward', 'lot', 'support', 'less', 'contact', 'rememb', 'on', 'venugop', 'ment', 'tomorrow', 'late', 'shall', 'try', 'come', 'till', 'goodnight', 'dear']\n",
      "Tokenized sentence: ['i', 'got', 'lousy', 'sleep', 'i', 'kept', 'waking', 'up', 'every', 'hours', 'to', 'see', 'if', 'my', 'cat', 'wanted', 'to', 'come', 'in', 'i', 'worry', 'about', 'him', 'when', 'its', 'cold']\n",
      "After stop words removal: ['got', 'lousy', 'sleep', 'kept', 'waking', 'every', 'hours', 'see', 'cat', 'wanted', 'come', 'worry', 'cold']\n",
      "wak\n",
      "After stemming with porters algorithm: ['got', 'lousi', 'sleep', 'kept', 'wake', 'everi', 'hour', 'see', 'cat', 'wan', 'come', 'worri', 'cold']\n",
      "Tokenized sentence: ['then', 'she', 'dun', 'believe', 'wat']\n",
      "After stop words removal: ['dun', 'believe', 'wat']\n",
      "After stemming with porters algorithm: ['dun', 'believ', 'wat']\n",
      "Tokenized sentence: ['crazy', 'ar', 'he', 's', 'married', 'like', 'gd', 'looking', 'guys', 'not', 'me', 'my', 'frens', 'like', 'say', 'he', 's', 'korean', 'leona', 's', 'fave', 'but', 'i', 'dun', 'thk', 'he', 'is', 'aft', 'some', 'thinking', 'mayb', 'most', 'prob', 'i', 'll', 'go']\n",
      "After stop words removal: ['crazy', 'ar', 'married', 'like', 'gd', 'looking', 'guys', 'frens', 'like', 'say', 'korean', 'leona', 'fave', 'dun', 'thk', 'aft', 'thinking', 'mayb', 'prob', 'go']\n",
      "look\n",
      "think\n",
      "After stemming with porters algorithm: ['crazi', 'marri', 'like', 'look', 'gui', 'fren', 'like', 'sai', 'korean', 'leona', 'fave', 'dun', 'thk', 'aft', 'thin', 'mayb', 'prob']\n",
      "Tokenized sentence: ['i', 'cant', 'talk', 'to', 'you', 'now', 'i', 'will', 'call', 'when', 'i', 'can', 'dont', 'keep', 'calling']\n",
      "After stop words removal: ['cant', 'talk', 'call', 'dont', 'keep', 'calling']\n",
      "call\n",
      "After stemming with porters algorithm: ['cant', 'talk', 'call', 'dont', 'keep', 'call']\n",
      "Tokenized sentence: ['i', 'couldn', 't', 'say', 'no', 'as', 'he', 'is', 'a', 'dying', 'man', 'and', 'i', 'feel', 'sad', 'for', 'him', 'so', 'i', 'will', 'go', 'and', 'i', 'just', 'wanted', 'you', 'to', 'know', 'i', 'would', 'probably', 'be', 'gone', 'late', 'into', 'your', 'night']\n",
      "After stop words removal: ['say', 'dying', 'man', 'feel', 'sad', 'go', 'wanted', 'know', 'would', 'probably', 'gone', 'late', 'night']\n",
      "After stemming with porters algorithm: ['sai', 'dying', 'man', 'feel', 'sad', 'wan', 'know', 'would', 'probab', 'gone', 'late', 'night']\n",
      "Tokenized sentence: ['there', 'the', 'size', 'of', 'elephant', 'tablets', 'u', 'shove', 'um', 'up', 'ur', 'ass']\n",
      "After stop words removal: ['size', 'elephant', 'tablets', 'u', 'shove', 'um', 'ur', 'ass']\n",
      "After stemming with porters algorithm: ['siz', 'eleph', 'tablet', 'shove', 'ass']\n",
      "Tokenized sentence: ['japanese', 'proverb', 'if', 'one', 'can', 'do', 'it', 'u', 'too', 'can', 'do', 'it', 'if', 'none', 'can', 'do', 'it', 'u', 'must', 'do', 'it', 'indian', 'version', 'if', 'one', 'can', 'do', 'it', 'let', 'him', 'do', 'it', 'if', 'none', 'can', 'do', 'it', 'leave', 'it', 'and', 'finally', 'kerala', 'version', 'if', 'one', 'can', 'do', 'it', 'stop', 'him', 'doing', 'it', 'if', 'none', 'can', 'do', 'it', 'make', 'a', 'strike', 'against', 'it']\n",
      "After stop words removal: ['japanese', 'proverb', 'one', 'u', 'none', 'u', 'must', 'indian', 'version', 'one', 'let', 'none', 'leave', 'finally', 'kerala', 'version', 'one', 'stop', 'none', 'make', 'strike']\n",
      "After stemming with porters algorithm: ['japanes', 'proverb', 'on', 'none', 'must', 'indian', 'version', 'on', 'let', 'none', 'leav', 'final', 'kerala', 'version', 'on', 'stop', 'none', 'make', 'strike']\n",
      "Tokenized sentence: ['nvm', 'it', 's', 'ok']\n",
      "After stop words removal: ['nvm', 'ok']\n",
      "After stemming with porters algorithm: ['nvm']\n",
      "Tokenized sentence: ['sorry', 'man', 'my', 'stash', 'ran', 'dry', 'last', 'night', 'and', 'i', 'can', 't', 'pick', 'up', 'more', 'until', 'sunday']\n",
      "After stop words removal: ['sorry', 'man', 'stash', 'ran', 'dry', 'last', 'night', 'pick', 'sunday']\n",
      "After stemming with porters algorithm: ['sorri', 'man', 'stash', 'ran', 'dry', 'last', 'night', 'pick', 'sundai']\n",
      "Tokenized sentence: ['his', 'frens', 'go', 'then', 'he', 'in', 'lor', 'not', 'alone', 'wif', 'my', 'mum', 'n', 'sis', 'lor']\n",
      "After stop words removal: ['frens', 'go', 'lor', 'alone', 'wif', 'mum', 'n', 'sis', 'lor']\n",
      "After stemming with porters algorithm: ['fren', 'lor', 'alon', 'wif', 'mum', 'si', 'lor']\n",
      "Tokenized sentence: ['awesome', 'lemme', 'know', 'whenever', 'you', 're', 'around']\n",
      "After stop words removal: ['awesome', 'lemme', 'know', 'whenever', 'around']\n",
      "After stemming with porters algorithm: ['awesom', 'lemm', 'know', 'whenev', 'around']\n",
      "Tokenized sentence: ['can', 'not', 'use', 'foreign', 'stamps', 'in', 'this', 'country', 'good', 'lecture']\n",
      "After stop words removal: ['use', 'foreign', 'stamps', 'country', 'good', 'lecture']\n",
      "After stemming with porters algorithm: ['us', 'foreign', 'stamp', 'countri', 'good', 'lectur']\n",
      "Tokenized sentence: ['i', 'have', 'many', 'dependents']\n",
      "After stop words removal: ['many', 'dependents']\n",
      "After stemming with porters algorithm: ['mani', 'depend']\n",
      "Tokenized sentence: ['you', 'are', 'being', 'contacted', 'by', 'our', 'dating', 'service', 'by', 'someone', 'you', 'know', 'to', 'find', 'out', 'who', 'it', 'is', 'call', 'from', 'a', 'land', 'line', 'pobox', 'w', 'tg', 'p']\n",
      "After stop words removal: ['contacted', 'dating', 'service', 'someone', 'know', 'find', 'call', 'land', 'line', 'pobox', 'w', 'tg', 'p']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['contac', 'date', 'servic', 'someon', 'know', 'find', 'call', 'land', 'line', 'pobox']\n",
      "Tokenized sentence: ['december', 'only', 'had', 'your', 'mobile', 'mths', 'you', 'are', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'camera', 'mobile', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']\n",
      "After stop words removal: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
      "After stemming with porters algorithm: ['decemb', 'mobil', 'mth', 'entit', 'updat', 'latest', 'colour', 'camera', 'mobil', 'free', 'call', 'mobil', 'updat', 'free']\n",
      "Tokenized sentence: ['hmmm', 'and', 'imagine', 'after', 'you', 've', 'come', 'home', 'from', 'that', 'having', 'to', 'rub', 'my', 'feet', 'make', 'me', 'dinner', 'and', 'help', 'me', 'get', 'ready', 'for', 'my', 'date', 'are', 'you', 'sure', 'your', 'ready', 'for', 'that', 'kind', 'of', 'life']\n",
      "After stop words removal: ['hmmm', 'imagine', 'come', 'home', 'rub', 'feet', 'make', 'dinner', 'help', 'get', 'ready', 'date', 'sure', 'ready', 'kind', 'life']\n",
      "After stemming with porters algorithm: ['hmmm', 'imagin', 'come', 'home', 'rub', 'feet', 'make', 'dinner', 'help', 'get', 'readi', 'date', 'sure', 'readi', 'kind', 'life']\n",
      "Tokenized sentence: ['a', 'bit', 'of', 'ur', 'smile', 'is', 'my', 'hppnss', 'a', 'drop', 'of', 'ur', 'tear', 'is', 'my', 'sorrow', 'a', 'part', 'of', 'ur', 'heart', 'is', 'my', 'life', 'a', 'heart', 'like', 'mine', 'wil', 'care', 'for', 'u', 'forevr', 'as', 'my', 'goodfriend']\n",
      "After stop words removal: ['bit', 'ur', 'smile', 'hppnss', 'drop', 'ur', 'tear', 'sorrow', 'part', 'ur', 'heart', 'life', 'heart', 'like', 'mine', 'wil', 'care', 'u', 'forevr', 'goodfriend']\n",
      "After stemming with porters algorithm: ['bit', 'smile', 'hppnss', 'drop', 'tear', 'sorrow', 'part', 'heart', 'life', 'heart', 'like', 'mine', 'wil', 'care', 'forevr', 'goodfriend']\n",
      "Tokenized sentence: ['u', 'still', 'going', 'to', 'the', 'mall']\n",
      "After stop words removal: ['u', 'still', 'going', 'mall']\n",
      "go\n",
      "After stemming with porters algorithm: ['still', 'go', 'mall']\n",
      "Tokenized sentence: ['ya', 'tel', 'wats', 'ur', 'problem']\n",
      "After stop words removal: ['ya', 'tel', 'wats', 'ur', 'problem']\n",
      "After stemming with porters algorithm: ['tel', 'wat', 'problem']\n",
      "Tokenized sentence: ['why', 'she', 'wants', 'to', 'talk', 'to', 'me']\n",
      "After stop words removal: ['wants', 'talk']\n",
      "After stemming with porters algorithm: ['want', 'talk']\n",
      "Tokenized sentence: ['no', 'messages', 'on', 'her', 'phone', 'i', 'm', 'holding', 'it', 'now']\n",
      "After stop words removal: ['messages', 'phone', 'holding']\n",
      "hold\n",
      "After stemming with porters algorithm: ['messag', 'phone', 'hol']\n",
      "Tokenized sentence: ['havent']\n",
      "After stop words removal: ['havent']\n",
      "After stemming with porters algorithm: ['havent']\n",
      "Tokenized sentence: ['she', 's', 'good', 'how', 'are', 'you', 'where', 'r', 'u', 'working', 'now']\n",
      "After stop words removal: ['good', 'r', 'u', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['good', 'wor']\n",
      "Tokenized sentence: ['i', 'am', 'great', 'how', 'are', 'you']\n",
      "After stop words removal: ['great']\n",
      "After stemming with porters algorithm: ['great']\n",
      "Tokenized sentence: ['ujhhhhhhh', 'computer', 'shipped', 'out', 'with', 'address', 'to', 'sandiago', 'and', 'parantella', 'lane', 'wtf', 'poop']\n",
      "After stop words removal: ['ujhhhhhhh', 'computer', 'shipped', 'address', 'sandiago', 'parantella', 'lane', 'wtf', 'poop']\n",
      "After stemming with porters algorithm: ['ujhhhhhhh', 'comput', 'ship', 'address', 'sandiago', 'parantella', 'lane', 'wtf', 'poop']\n",
      "Tokenized sentence: ['between', 'am', 'pm', 'cost', 'p']\n",
      "After stop words removal: ['pm', 'cost', 'p']\n",
      "After stemming with porters algorithm: ['cost']\n",
      "Tokenized sentence: ['but', 'i', 'haf', 'enuff', 'space', 'got', 'like', 'mb']\n",
      "After stop words removal: ['haf', 'enuff', 'space', 'got', 'like', 'mb']\n",
      "After stemming with porters algorithm: ['haf', 'enuff', 'space', 'got', 'like']\n",
      "Tokenized sentence: ['sad', 'story', 'of', 'a', 'man', 'last', 'week', 'was', 'my', 'b', 'day', 'my', 'wife', 'did', 'nt', 'wish', 'me', 'my', 'parents', 'forgot', 'n', 'so', 'did', 'my', 'kids', 'i', 'went', 'to', 'work', 'even', 'my', 'colleagues', 'did', 'not', 'wish']\n",
      "After stop words removal: ['sad', 'story', 'man', 'last', 'week', 'b', 'day', 'wife', 'nt', 'wish', 'parents', 'forgot', 'n', 'kids', 'went', 'work', 'even', 'colleagues', 'wish']\n",
      "After stemming with porters algorithm: ['sad', 'stori', 'man', 'last', 'week', 'dai', 'wife', 'wish', 'parent', 'forgot', 'kid', 'went', 'work', 'even', 'colleagu', 'wish']\n",
      "Tokenized sentence: ['and', 'how', 's', 'your', 'husband']\n",
      "After stop words removal: ['husband']\n",
      "After stemming with porters algorithm: ['husband']\n",
      "Tokenized sentence: ['i', 'can', 't', 'keep', 'going', 'through', 'this', 'it', 'was', 'never', 'my', 'intention', 'to', 'run', 'you', 'out', 'but', 'if', 'you', 'choose', 'to', 'do', 'that', 'rather', 'than', 'keep', 'the', 'room', 'clean', 'so', 'i', 'don', 't', 'have', 'to', 'say', 'no', 'to', 'visitors', 'then', 'maybe', 'that', 's', 'the', 'best', 'choice', 'yes', 'i', 'wanted', 'you', 'to', 'be', 'embarassed', 'so', 'maybe', 'you', 'd', 'feel', 'for', 'once', 'how', 'i', 'feel', 'when', 'i', 'have', 'a', 'friend', 'who', 'wants', 'to', 'drop', 'buy', 'and', 'i', 'have', 'to', 'say', 'no', 'as', 'happened', 'this', 'morning', 'i', 've', 'tried', 'everything', 'i', 'don', 't', 'know', 'what', 'else', 'to', 'do']\n",
      "After stop words removal: ['keep', 'going', 'never', 'intention', 'run', 'choose', 'rather', 'keep', 'room', 'clean', 'say', 'visitors', 'maybe', 'best', 'choice', 'yes', 'wanted', 'embarassed', 'maybe', 'feel', 'feel', 'friend', 'wants', 'drop', 'buy', 'say', 'happened', 'morning', 'tried', 'everything', 'know', 'else']\n",
      "go\n",
      "morn\n",
      "everyth\n",
      "After stemming with porters algorithm: ['keep', 'go', 'never', 'intent', 'run', 'choos', 'rather', 'keep', 'room', 'clean', 'sai', 'visitor', 'mayb', 'best', 'choic', 'ye', 'wan', 'embarass', 'mayb', 'feel', 'feel', 'friend', 'want', 'drop', 'bui', 'sai', 'happen', 'mor', 'tri', 'everyt', 'know', 'els']\n",
      "Tokenized sentence: ['where', 'did', 'u', 'go', 'my', 'phone', 'is', 'gonna', 'die', 'you', 'have', 'to', 'stay', 'in', 'here']\n",
      "After stop words removal: ['u', 'go', 'phone', 'gonna', 'die', 'stay']\n",
      "After stemming with porters algorithm: ['phone', 'gonna', 'die', 'stai']\n",
      "Tokenized sentence: ['dear', 'relieved', 'of', 'westonzoyland', 'all', 'going', 'to', 'plan', 'this', 'end', 'too']\n",
      "After stop words removal: ['dear', 'relieved', 'westonzoyland', 'going', 'plan', 'end']\n",
      "go\n",
      "After stemming with porters algorithm: ['dear', 'reliev', 'westonzoyland', 'go', 'plan', 'end']\n",
      "Tokenized sentence: ['japanese', 'proverb', 'if', 'one', 'can', 'do', 'it', 'u', 'too', 'can', 'do', 'it', 'if', 'none', 'can', 'do', 'it', 'u', 'must', 'do', 'it', 'indian', 'version', 'if', 'one', 'can', 'do', 'it', 'let', 'him', 'do', 'it', 'if', 'none', 'can', 'do', 'it', 'leave', 'it', 'and', 'finally', 'kerala', 'version', 'if', 'one', 'can', 'do', 'it', 'stop', 'him', 'doing', 'it', 'if', 'none', 'can', 'do', 'it', 'make', 'a', 'strike', 'against', 'it']\n",
      "After stop words removal: ['japanese', 'proverb', 'one', 'u', 'none', 'u', 'must', 'indian', 'version', 'one', 'let', 'none', 'leave', 'finally', 'kerala', 'version', 'one', 'stop', 'none', 'make', 'strike']\n",
      "After stemming with porters algorithm: ['japanes', 'proverb', 'on', 'none', 'must', 'indian', 'version', 'on', 'let', 'none', 'leav', 'final', 'kerala', 'version', 'on', 'stop', 'none', 'make', 'strike']\n",
      "Tokenized sentence: ['i', 'know', 'but', 'you', 'need', 'to', 'get', 'hotel', 'now', 'i', 'just', 'got', 'my', 'invitation', 'but', 'i', 'had', 'to', 'apologise', 'cali', 'is', 'to', 'sweet', 'for', 'me', 'to', 'come', 'to', 'some', 'english', 'bloke', 's', 'weddin']\n",
      "After stop words removal: ['know', 'need', 'get', 'hotel', 'got', 'invitation', 'apologise', 'cali', 'sweet', 'come', 'english', 'bloke', 'weddin']\n",
      "After stemming with porters algorithm: ['know', 'need', 'get', 'hotel', 'got', 'invit', 'apologis', 'cali', 'sweet', 'come', 'english', 'bloke', 'weddin']\n",
      "Tokenized sentence: ['found', 'it', 'enc', 'lt', 'gt', 'where', 'you', 'at']\n",
      "After stop words removal: ['found', 'enc', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['found', 'enc']\n",
      "Tokenized sentence: ['po', 'de', 'no', 'need', 'job', 'aha']\n",
      "After stop words removal: ['po', 'de', 'need', 'job', 'aha']\n",
      "After stemming with porters algorithm: ['need', 'job', 'aha']\n",
      "Tokenized sentence: ['no', 'plans', 'yet', 'what', 'are', 'you', 'doing']\n",
      "After stop words removal: ['plans', 'yet']\n",
      "After stemming with porters algorithm: ['plan', 'yet']\n",
      "Tokenized sentence: ['indians', 'r', 'poor', 'but', 'india', 'is', 'not', 'a', 'poor', 'country', 'says', 'one', 'of', 'the', 'swiss', 'bank', 'directors', 'he', 'says', 'that', 'lt', 'gt', 'lac', 'crore', 'of', 'indian', 'money', 'is', 'deposited', 'in', 'swiss', 'banks', 'which', 'can', 'be', 'used', 'for', 'taxless', 'budget', 'for', 'lt', 'gt', 'yrs', 'can', 'give', 'lt', 'gt', 'crore', 'jobs', 'to', 'all', 'indians', 'from', 'any', 'village', 'to', 'delhi', 'lane', 'roads', 'forever', 'free', 'power', 'suply', 'to', 'more', 'than', 'lt', 'gt', 'social', 'projects', 'every', 'citizen', 'can', 'get', 'monthly', 'lt', 'gt', 'for', 'lt', 'gt', 'yrs', 'no', 'need', 'of', 'world', 'bank', 'amp', 'imf', 'loan', 'think', 'how', 'our', 'money', 'is', 'blocked', 'by', 'rich', 'politicians', 'we', 'have', 'full', 'rights', 'against', 'corrupt', 'politicians', 'itna', 'forward', 'karo', 'ki', 'pura', 'india', 'padhe', 'g', 'm']\n",
      "After stop words removal: ['indians', 'r', 'poor', 'india', 'poor', 'country', 'says', 'one', 'swiss', 'bank', 'directors', 'says', 'lt', 'gt', 'lac', 'crore', 'indian', 'money', 'deposited', 'swiss', 'banks', 'used', 'taxless', 'budget', 'lt', 'gt', 'yrs', 'give', 'lt', 'gt', 'crore', 'jobs', 'indians', 'village', 'delhi', 'lane', 'roads', 'forever', 'free', 'power', 'suply', 'lt', 'gt', 'social', 'projects', 'every', 'citizen', 'get', 'monthly', 'lt', 'gt', 'lt', 'gt', 'yrs', 'need', 'world', 'bank', 'amp', 'imf', 'loan', 'think', 'money', 'blocked', 'rich', 'politicians', 'full', 'rights', 'corrupt', 'politicians', 'itna', 'forward', 'karo', 'ki', 'pura', 'india', 'padhe', 'g']\n",
      "After stemming with porters algorithm: ['indian', 'poor', 'india', 'poor', 'countri', 'sai', 'on', 'swiss', 'bank', 'director', 'sai', 'lac', 'crore', 'indian', 'monei', 'deposit', 'swiss', 'bank', 'us', 'taxless', 'budget', 'yr', 'give', 'crore', 'job', 'indian', 'villag', 'delhi', 'lane', 'road', 'forev', 'free', 'power', 'supli', 'social', 'project', 'everi', 'citizen', 'get', 'monthli', 'yr', 'need', 'world', 'bank', 'amp', 'imf', 'loan', 'think', 'monei', 'bloc', 'rich', 'politician', 'full', 'right', 'corrupt', 'politician', 'itna', 'forward', 'karo', 'pura', 'india', 'padh']\n",
      "Tokenized sentence: ['arun', 'can', 'u', 'transfr', 'me', 'd', 'amt']\n",
      "After stop words removal: ['arun', 'u', 'transfr', 'amt']\n",
      "After stemming with porters algorithm: ['arun', 'transfr', 'amt']\n",
      "Tokenized sentence: ['rct', 'thnq', 'adrian', 'for', 'u', 'text', 'rgds', 'vatian']\n",
      "After stop words removal: ['rct', 'thnq', 'adrian', 'u', 'text', 'rgds', 'vatian']\n",
      "After stemming with porters algorithm: ['rct', 'thnq', 'adrian', 'text', 'rgd', 'vatian']\n",
      "Tokenized sentence: ['i', 'got', 'arrested', 'for', 'possession', 'at', 'i', 'shit', 'you', 'not', 'lt', 'time', 'gt', 'pm']\n",
      "After stop words removal: ['got', 'arrested', 'possession', 'shit', 'lt', 'time', 'gt', 'pm']\n",
      "After stemming with porters algorithm: ['got', 'arres', 'possess', 'shit', 'time']\n",
      "Tokenized sentence: ['yes', 'we', 'were', 'outside', 'for', 'like', 'hours', 'and', 'i', 'called', 'my', 'whole', 'family', 'to', 'wake', 'them', 'up', 'cause', 'it', 'started', 'at', 'am']\n",
      "After stop words removal: ['yes', 'outside', 'like', 'hours', 'called', 'whole', 'family', 'wake', 'cause', 'started']\n",
      "After stemming with porters algorithm: ['ye', 'outsid', 'like', 'hour', 'call', 'whole', 'famili', 'wake', 'caus', 'star']\n",
      "Tokenized sentence: ['he', 'is', 'world', 'famamus']\n",
      "After stop words removal: ['world', 'famamus']\n",
      "After stemming with porters algorithm: ['world', 'famamu']\n",
      "Tokenized sentence: ['hey', 'tmr', 'meet', 'at', 'bugis']\n",
      "After stop words removal: ['hey', 'tmr', 'meet', 'bugis']\n",
      "After stemming with porters algorithm: ['hei', 'tmr', 'meet', 'bugi']\n",
      "Tokenized sentence: ['meet', 'after', 'lunch', 'la']\n",
      "After stop words removal: ['meet', 'lunch', 'la']\n",
      "After stemming with porters algorithm: ['meet', 'lunch']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['we', 're', 'finally', 'ready', 'fyi']\n",
      "After stop words removal: ['finally', 'ready', 'fyi']\n",
      "After stemming with porters algorithm: ['final', 'readi', 'fyi']\n",
      "Tokenized sentence: ['too', 'late', 'i', 'said', 'i', 'have', 'the', 'website', 'i', 'didn', 't', 'i', 'have', 'or', 'dont', 'have', 'the', 'slippers']\n",
      "After stop words removal: ['late', 'said', 'website', 'dont', 'slippers']\n",
      "After stemming with porters algorithm: ['late', 'said', 'websit', 'dont', 'slipper']\n",
      "Tokenized sentence: ['hmm', 'thinking', 'lor']\n",
      "After stop words removal: ['hmm', 'thinking', 'lor']\n",
      "think\n",
      "After stemming with porters algorithm: ['hmm', 'thin', 'lor']\n",
      "Tokenized sentence: ['no', 'he', 'didn', 't', 'spring', 'is', 'coming', 'early', 'yay']\n",
      "After stop words removal: ['spring', 'coming', 'early', 'yay']\n",
      "com\n",
      "After stemming with porters algorithm: ['spring', 'come', 'earli', 'yai']\n",
      "Tokenized sentence: ['jus', 'ans', 'me', 'lar', 'u', 'll', 'noe', 'later']\n",
      "After stop words removal: ['jus', 'ans', 'lar', 'u', 'noe', 'later']\n",
      "After stemming with porters algorithm: ['ju', 'an', 'lar', 'noe', 'later']\n",
      "Tokenized sentence: ['great', 'princess', 'i', 'love', 'giving', 'and', 'receiving', 'oral', 'doggy', 'style', 'is', 'my', 'fave', 'position', 'how', 'about', 'you', 'i', 'enjoy', 'making', 'love', 'lt', 'gt', 'times', 'per', 'night']\n",
      "After stop words removal: ['great', 'princess', 'love', 'giving', 'receiving', 'oral', 'doggy', 'style', 'fave', 'position', 'enjoy', 'making', 'love', 'lt', 'gt', 'times', 'per', 'night']\n",
      "giv\n",
      "receiv\n",
      "mak\n",
      "After stemming with porters algorithm: ['great', 'princess', 'love', 'give', 'receiv', 'oral', 'doggi', 'style', 'fave', 'posit', 'enjoi', 'make', 'love', 'time', 'per', 'night']\n",
      "Tokenized sentence: ['awesome', 'be', 'there', 'in', 'a', 'minute']\n",
      "After stop words removal: ['awesome', 'minute']\n",
      "After stemming with porters algorithm: ['awesom', 'minut']\n",
      "Tokenized sentence: ['its', 'just', 'the', 'effect', 'of', 'irritation', 'just', 'ignore', 'it']\n",
      "After stop words removal: ['effect', 'irritation', 'ignore']\n",
      "After stemming with porters algorithm: ['effect', 'irrit', 'ignor']\n",
      "Tokenized sentence: ['havent', 'mus', 'ask', 'if', 'u', 'can', 'st', 'wat', 'of', 'meet', 'lunch', 'den', 'u', 'n', 'him', 'meet', 'can', 'already', 'lor', 'or', 'u', 'wan', 'go', 'ask', 'da', 'ge', 'st', 'then', 'confirm', 'w', 'me', 'asap']\n",
      "After stop words removal: ['havent', 'mus', 'ask', 'u', 'st', 'wat', 'meet', 'lunch', 'den', 'u', 'n', 'meet', 'already', 'lor', 'u', 'wan', 'go', 'ask', 'da', 'ge', 'st', 'confirm', 'w', 'asap']\n",
      "After stemming with porters algorithm: ['havent', 'mu', 'ask', 'wat', 'meet', 'lunch', 'den', 'meet', 'alreadi', 'lor', 'wan', 'ask', 'confirm', 'asap']\n",
      "Tokenized sentence: ['i', 'went', 'to', 'project', 'centre']\n",
      "After stop words removal: ['went', 'project', 'centre']\n",
      "After stemming with porters algorithm: ['went', 'project', 'centr']\n",
      "Tokenized sentence: ['u', 'should', 'make', 'a', 'fb', 'list']\n",
      "After stop words removal: ['u', 'make', 'fb', 'list']\n",
      "After stemming with porters algorithm: ['make', 'list']\n",
      "Tokenized sentence: ['free', 'entry', 'in', 'a', 'weekly', 'comp', 'for', 'a', 'chance', 'to', 'win', 'an', 'ipod', 'txt', 'pod', 'to', 'to', 'get', 'entry', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'for', 'details']\n",
      "After stop words removal: ['free', 'entry', 'weekly', 'comp', 'chance', 'win', 'ipod', 'txt', 'pod', 'get', 'entry', 'std', 'txt', 'rate', 'c', 'apply', 'details']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'weekli', 'comp', 'chanc', 'win', 'ipod', 'txt', 'pod', 'get', 'entri', 'std', 'txt', 'rate', 'appli', 'detail']\n",
      "Tokenized sentence: ['will', 'you', 'like', 'to', 'be', 'spoiled']\n",
      "After stop words removal: ['like', 'spoiled']\n",
      "After stemming with porters algorithm: ['like', 'spoil']\n",
      "Tokenized sentence: ['bangbabes', 'ur', 'order', 'is', 'on', 'the', 'way', 'u', 'should', 'receive', 'a', 'service', 'msg', 'download', 'ur', 'content', 'if', 'u', 'do', 'not', 'goto', 'wap', 'bangb', 'tv', 'on', 'ur', 'mobile', 'internet', 'service', 'menu']\n",
      "After stop words removal: ['bangbabes', 'ur', 'order', 'way', 'u', 'receive', 'service', 'msg', 'download', 'ur', 'content', 'u', 'goto', 'wap', 'bangb', 'tv', 'ur', 'mobile', 'internet', 'service', 'menu']\n",
      "After stemming with porters algorithm: ['bangbab', 'order', 'wai', 'receiv', 'servic', 'msg', 'download', 'content', 'goto', 'wap', 'bangb', 'mobil', 'internet', 'servic', 'menu']\n",
      "Tokenized sentence: ['minimum', 'walk', 'is', 'miles', 'a', 'day']\n",
      "After stop words removal: ['minimum', 'walk', 'miles', 'day']\n",
      "After stemming with porters algorithm: ['minimum', 'walk', 'mile', 'dai']\n",
      "Tokenized sentence: ['u', 'wan', 'haf', 'lunch', 'i', 'm', 'in', 'da', 'canteen', 'now']\n",
      "After stop words removal: ['u', 'wan', 'haf', 'lunch', 'da', 'canteen']\n",
      "After stemming with porters algorithm: ['wan', 'haf', 'lunch', 'canteen']\n",
      "Tokenized sentence: ['i', 'think', 'i', 've', 'fixed', 'it', 'can', 'you', 'send', 'a', 'test', 'message']\n",
      "After stop words removal: ['think', 'fixed', 'send', 'test', 'message']\n",
      "After stemming with porters algorithm: ['think', 'fix', 'send', 'test', 'messag']\n",
      "Tokenized sentence: ['i', 'dont', 'know', 'exactly', 'could', 'you', 'ask', 'chechi']\n",
      "After stop words removal: ['dont', 'know', 'exactly', 'could', 'ask', 'chechi']\n",
      "After stemming with porters algorithm: ['dont', 'know', 'exactli', 'could', 'ask', 'chechi']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'fone', 'no', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'fone', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'fone', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['you', 'call', 'him', 'now', 'ok', 'i', 'said', 'call', 'him']\n",
      "After stop words removal: ['call', 'ok', 'said', 'call']\n",
      "After stemming with porters algorithm: ['call', 'said', 'call']\n",
      "Tokenized sentence: ['indeed', 'and', 'by', 'the', 'way', 'it', 'was', 'either', 'or', 'not', 'both']\n",
      "After stop words removal: ['indeed', 'way', 'either']\n",
      "After stemming with porters algorithm: ['inde', 'wai', 'either']\n",
      "Tokenized sentence: ['that', 's', 'one', 'of', 'the', 'issues', 'but', 'california', 'is', 'okay', 'no', 'snow', 'so', 'its', 'manageable']\n",
      "After stop words removal: ['one', 'issues', 'california', 'okay', 'snow', 'manageable']\n",
      "After stemming with porters algorithm: ['on', 'issu', 'california', 'okai', 'snow', 'manag']\n",
      "Tokenized sentence: ['sorry', 'i', 'missed', 'your', 'call', 'can', 'you', 'please', 'call', 'back']\n",
      "After stop words removal: ['sorry', 'missed', 'call', 'please', 'call', 'back']\n",
      "After stemming with porters algorithm: ['sorri', 'miss', 'call', 'pleas', 'call', 'back']\n",
      "Tokenized sentence: ['i', 'don', 't', 'think', 'he', 'has', 'spatula', 'hands']\n",
      "After stop words removal: ['think', 'spatula', 'hands']\n",
      "After stemming with porters algorithm: ['think', 'spatula', 'hand']\n",
      "Tokenized sentence: ['i', 'll', 'let', 'you', 'know', 'when', 'it', 'kicks', 'in']\n",
      "After stop words removal: ['let', 'know', 'kicks']\n",
      "After stemming with porters algorithm: ['let', 'know', 'kick']\n",
      "Tokenized sentence: ['you', 'lifted', 'my', 'hopes', 'with', 'the', 'offer', 'of', 'money', 'i', 'am', 'in', 'need', 'especially', 'when', 'the', 'end', 'of', 'the', 'month', 'approaches', 'and', 'it', 'hurts', 'my', 'studying', 'anyways', 'have', 'a', 'gr', 'weekend']\n",
      "After stop words removal: ['lifted', 'hopes', 'offer', 'money', 'need', 'especially', 'end', 'month', 'approaches', 'hurts', 'studying', 'anyways', 'gr', 'weekend']\n",
      "study\n",
      "After stemming with porters algorithm: ['lif', 'hope', 'offer', 'monei', 'need', 'especi', 'end', 'month', 'approach', 'hurt', 'stud', 'anywai', 'weekend']\n",
      "Tokenized sentence: ['a', 'bit', 'of', 'ur', 'smile', 'is', 'my', 'hppnss', 'a', 'drop', 'of', 'ur', 'tear', 'is', 'my', 'sorrow', 'a', 'part', 'of', 'ur', 'heart', 'is', 'my', 'life', 'a', 'heart', 'like', 'mine', 'wil', 'care', 'for', 'u', 'forevr', 'as', 'my', 'goodfriend']\n",
      "After stop words removal: ['bit', 'ur', 'smile', 'hppnss', 'drop', 'ur', 'tear', 'sorrow', 'part', 'ur', 'heart', 'life', 'heart', 'like', 'mine', 'wil', 'care', 'u', 'forevr', 'goodfriend']\n",
      "After stemming with porters algorithm: ['bit', 'smile', 'hppnss', 'drop', 'tear', 'sorrow', 'part', 'heart', 'life', 'heart', 'like', 'mine', 'wil', 'care', 'forevr', 'goodfriend']\n",
      "Tokenized sentence: ['predict', 'wat', 'time', 'll', 'finish', 'buying']\n",
      "After stop words removal: ['predict', 'wat', 'time', 'finish', 'buying']\n",
      "buy\n",
      "After stemming with porters algorithm: ['predict', 'wat', 'time', 'finish', 'bui']\n",
      "Tokenized sentence: ['after', 'completed', 'degree', 'there', 'is', 'no', 'use', 'in', 'joining', 'finance']\n",
      "After stop words removal: ['completed', 'degree', 'use', 'joining', 'finance']\n",
      "join\n",
      "After stemming with porters algorithm: ['complet', 'degre', 'us', 'join', 'financ']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'a', 'prize', 'to', 'claim', 'yr', 'prize', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'am', 'pm']\n",
      "After stop words removal: ['guaranteed', 'cash', 'prize', 'claim', 'yr', 'prize', 'call', 'customer', 'service', 'representative', 'pm']\n",
      "After stemming with porters algorithm: ['guaranteed', 'cash', 'priz', 'claim', 'priz', 'call', 'custom', 'servic', 'repres']\n",
      "Tokenized sentence: ['did', 'u', 'get', 'that', 'message']\n",
      "After stop words removal: ['u', 'get', 'message']\n",
      "After stemming with porters algorithm: ['get', 'messag']\n",
      "Tokenized sentence: ['the', 'whole', 'car', 'appreciated', 'the', 'last', 'two', 'dad', 'and', 'are', 'having', 'a', 'map', 'reading', 'semi', 'argument', 'but', 'apart', 'from', 'that', 'things', 'are', 'going', 'ok', 'p']\n",
      "After stop words removal: ['whole', 'car', 'appreciated', 'last', 'two', 'dad', 'map', 'reading', 'semi', 'argument', 'apart', 'things', 'going', 'ok', 'p']\n",
      "appreciate\n",
      "read\n",
      "go\n",
      "After stemming with porters algorithm: ['whole', 'car', 'appreci', 'last', 'two', 'dad', 'map', 'read', 'semi', 'argum', 'apart', 'thing', 'go']\n",
      "Tokenized sentence: ['so', 'the', 'sun', 'is', 'anti', 'sleep', 'medicine']\n",
      "After stop words removal: ['sun', 'anti', 'sleep', 'medicine']\n",
      "After stemming with porters algorithm: ['sun', 'anti', 'sleep', 'medicin']\n",
      "Tokenized sentence: ['have', 'your', 'lunch', 'and', 'come', 'quickly', 'and', 'open', 'the', 'door']\n",
      "After stop words removal: ['lunch', 'come', 'quickly', 'open', 'door']\n",
      "After stemming with porters algorithm: ['lunch', 'come', 'quickli', 'open', 'door']\n",
      "Tokenized sentence: ['our', 'brand', 'new', 'mobile', 'music', 'service', 'is', 'now', 'live', 'the', 'free', 'music', 'player', 'will', 'arrive', 'shortly', 'just', 'install', 'on', 'your', 'phone', 'to', 'browse', 'content', 'from', 'the', 'top', 'artists']\n",
      "After stop words removal: ['brand', 'new', 'mobile', 'music', 'service', 'live', 'free', 'music', 'player', 'arrive', 'shortly', 'install', 'phone', 'browse', 'content', 'top', 'artists']\n",
      "After stemming with porters algorithm: ['brand', 'new', 'mobil', 'music', 'servic', 'live', 'free', 'music', 'player', 'arriv', 'shortli', 'instal', 'phone', 'brows', 'content', 'top', 'artist']\n",
      "Tokenized sentence: ['spending', 'new', 'years', 'with', 'my', 'brother', 'and', 'his', 'family', 'lets', 'plan', 'to', 'meet', 'next', 'week', 'are', 'you', 'ready', 'to', 'be', 'spoiled']\n",
      "After stop words removal: ['spending', 'new', 'years', 'brother', 'family', 'lets', 'plan', 'meet', 'next', 'week', 'ready', 'spoiled']\n",
      "spend\n",
      "After stemming with porters algorithm: ['spen', 'new', 'year', 'brother', 'famili', 'let', 'plan', 'meet', 'next', 'week', 'readi', 'spoil']\n",
      "Tokenized sentence: ['i', 'also', 'thk', 'too', 'fast', 'xy', 'suggest', 'one', 'not', 'me', 'u', 'dun', 'wan', 'it', 's', 'ok', 'going', 'rain', 'leh', 'where', 'got', 'gd']\n",
      "After stop words removal: ['also', 'thk', 'fast', 'xy', 'suggest', 'one', 'u', 'dun', 'wan', 'ok', 'going', 'rain', 'leh', 'got', 'gd']\n",
      "go\n",
      "After stemming with porters algorithm: ['also', 'thk', 'fast', 'suggest', 'on', 'dun', 'wan', 'go', 'rain', 'leh', 'got']\n",
      "Tokenized sentence: ['wat', 'so', 'late', 'still', 'early', 'mah', 'or', 'we', 'juz', 'go', 'dinner', 'lor', 'aiya', 'i', 'dunno']\n",
      "After stop words removal: ['wat', 'late', 'still', 'early', 'mah', 'juz', 'go', 'dinner', 'lor', 'aiya', 'dunno']\n",
      "After stemming with porters algorithm: ['wat', 'late', 'still', 'earli', 'mah', 'juz', 'dinner', 'lor', 'aiya', 'dunno']\n",
      "Tokenized sentence: ['thursday', 'night', 'yeah', 'sure', 'thing', 'we', 'll', 'work', 'it', 'out', 'then']\n",
      "After stop words removal: ['thursday', 'night', 'yeah', 'sure', 'thing', 'work']\n",
      "After stemming with porters algorithm: ['thursdai', 'night', 'yeah', 'sure', 'thing', 'work']\n",
      "Tokenized sentence: ['thanks', 'your', 'continued', 'support', 'your', 'question', 'this', 'week', 'will', 'enter', 'u', 'in', 'our', 'draw', 'cash', 'name', 'the', 'new', 'us', 'president', 'txt', 'ans', 'to']\n",
      "After stop words removal: ['thanks', 'continued', 'support', 'question', 'week', 'enter', 'u', 'draw', 'cash', 'name', 'new', 'us', 'president', 'txt', 'ans']\n",
      "After stemming with porters algorithm: ['thank', 'continu', 'support', 'quest', 'week', 'enter', 'draw', 'cash', 'name', 'new', 'presid', 'txt', 'an']\n",
      "Tokenized sentence: ['hi', 'did', 'u', 'decide', 'wot', 'get', 'his', 'bday', 'if', 'not', 'ill', 'prob', 'jus', 'get', 'him', 'a', 'voucher', 'frm', 'virgin', 'or', 'sumfing']\n",
      "After stop words removal: ['hi', 'u', 'decide', 'wot', 'get', 'bday', 'ill', 'prob', 'jus', 'get', 'voucher', 'frm', 'virgin', 'sumfing']\n",
      "sumf\n",
      "After stemming with porters algorithm: ['decid', 'wot', 'get', 'bdai', 'ill', 'prob', 'ju', 'get', 'voucher', 'frm', 'virgin', 'sum']\n",
      "Tokenized sentence: ['forgot', 'it', 'takes', 'me', 'years', 'to', 'shower', 'sorry', 'where', 'you', 'at', 'your', 'phone', 'dead', 'yet']\n",
      "After stop words removal: ['forgot', 'takes', 'years', 'shower', 'sorry', 'phone', 'dead', 'yet']\n",
      "After stemming with porters algorithm: ['forgot', 'take', 'year', 'shower', 'sorri', 'phone', 'dead', 'yet']\n",
      "Tokenized sentence: ['you', 'still', 'around', 'i', 'could', 'use', 'a', 'half', 'th']\n",
      "After stop words removal: ['still', 'around', 'could', 'use', 'half', 'th']\n",
      "After stemming with porters algorithm: ['still', 'around', 'could', 'us', 'half']\n",
      "Tokenized sentence: ['lt', 'gt', 'ish', 'minutes', 'was', 'minutes', 'ago', 'wtf']\n",
      "After stop words removal: ['lt', 'gt', 'ish', 'minutes', 'minutes', 'ago', 'wtf']\n",
      "After stemming with porters algorithm: ['ish', 'minut', 'minut', 'ago', 'wtf']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['you', 'can', 'donate', 'to', 'unicef', 's', 'asian', 'tsunami', 'disaster', 'support', 'fund', 'by', 'texting', 'donate', 'to', 'will', 'be', 'added', 'to', 'your', 'next', 'bill']\n",
      "After stop words removal: ['donate', 'unicef', 'asian', 'tsunami', 'disaster', 'support', 'fund', 'texting', 'donate', 'added', 'next', 'bill']\n",
      "text\n",
      "After stemming with porters algorithm: ['donat', 'unicef', 'asian', 'tsunami', 'disast', 'support', 'fund', 'tex', 'donat', 'ad', 'next', 'bill']\n",
      "Tokenized sentence: ['ard', 'like', 'dat', 'y']\n",
      "After stop words removal: ['ard', 'like', 'dat']\n",
      "After stemming with porters algorithm: ['ard', 'like', 'dat']\n",
      "Tokenized sentence: ['final', 'chance', 'claim', 'ur', 'worth', 'of', 'discount', 'vouchers', 'today', 'text', 'yes', 'to', 'now', 'savamob', 'member', 'offers', 'mobile', 't', 'cs', 'savamob', 'pobox', 'm', 'uz', 'subs']\n",
      "After stop words removal: ['final', 'chance', 'claim', 'ur', 'worth', 'discount', 'vouchers', 'today', 'text', 'yes', 'savamob', 'member', 'offers', 'mobile', 'cs', 'savamob', 'pobox', 'uz', 'subs']\n",
      "After stemming with porters algorithm: ['final', 'chanc', 'claim', 'worth', 'discount', 'voucher', 'todai', 'text', 'ye', 'savamob', 'member', 'offer', 'mobil', 'savamob', 'pobox', 'sub']\n",
      "Tokenized sentence: ['hard', 'live', 'chat', 'just', 'p', 'min', 'choose', 'your', 'girl', 'and', 'connect', 'live', 'call', 'now', 'cheap', 'chat', 'uk', 's', 'biggest', 'live', 'service', 'vu', 'bcm', 'wc', 'n', 'xx']\n",
      "After stop words removal: ['hard', 'live', 'chat', 'p', 'min', 'choose', 'girl', 'connect', 'live', 'call', 'cheap', 'chat', 'uk', 'biggest', 'live', 'service', 'vu', 'bcm', 'wc', 'n', 'xx']\n",
      "After stemming with porters algorithm: ['hard', 'live', 'chat', 'min', 'choos', 'girl', 'connect', 'live', 'call', 'cheap', 'chat', 'biggest', 'live', 'servic', 'bcm']\n",
      "Tokenized sentence: ['she', 'went', 'to', 'attend', 'another', 'two', 'rounds', 'today', 'but', 'still', 'did', 't', 'reach', 'home']\n",
      "After stop words removal: ['went', 'attend', 'another', 'two', 'rounds', 'today', 'still', 'reach', 'home']\n",
      "After stemming with porters algorithm: ['went', 'attend', 'anoth', 'two', 'round', 'todai', 'still', 'reach', 'home']\n",
      "Tokenized sentence: ['want', 'explicit', 'sex', 'in', 'secs', 'ring', 'now', 'costs', 'p', 'min']\n",
      "After stop words removal: ['want', 'explicit', 'sex', 'secs', 'ring', 'costs', 'p', 'min']\n",
      "After stemming with porters algorithm: ['want', 'explicit', 'sex', 'sec', 'ring', 'cost', 'min']\n",
      "Tokenized sentence: ['lovely', 'smell', 'on', 'this', 'bus', 'and', 'it', 'ain', 't', 'tobacco']\n",
      "After stop words removal: ['lovely', 'smell', 'bus', 'tobacco']\n",
      "After stemming with porters algorithm: ['love', 'smell', 'bu', 'tobacco']\n",
      "Tokenized sentence: ['free', 'entry', 'in', 'a', 'weekly', 'comp', 'for', 'a', 'chance', 'to', 'win', 'an', 'ipod', 'txt', 'pod', 'to', 'to', 'get', 'entry', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'for', 'details']\n",
      "After stop words removal: ['free', 'entry', 'weekly', 'comp', 'chance', 'win', 'ipod', 'txt', 'pod', 'get', 'entry', 'std', 'txt', 'rate', 'c', 'apply', 'details']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'weekli', 'comp', 'chanc', 'win', 'ipod', 'txt', 'pod', 'get', 'entri', 'std', 'txt', 'rate', 'appli', 'detail']\n",
      "Tokenized sentence: ['lol', 'for', 'real', 'she', 'told', 'my', 'dad', 'i', 'have', 'cancer']\n",
      "After stop words removal: ['lol', 'real', 'told', 'dad', 'cancer']\n",
      "After stemming with porters algorithm: ['lol', 'real', 'told', 'dad', 'cancer']\n",
      "Tokenized sentence: ['it', 's', 'really', 'getting', 'me', 'down', 'just', 'hanging', 'around']\n",
      "After stop words removal: ['really', 'getting', 'hanging', 'around']\n",
      "gett\n",
      "hang\n",
      "After stemming with porters algorithm: ['realli', 'get', 'han', 'around']\n",
      "Tokenized sentence: ['oh', 'yeah', 'clearly', 'it', 's', 'my', 'fault']\n",
      "After stop words removal: ['oh', 'yeah', 'clearly', 'fault']\n",
      "After stemming with porters algorithm: ['yeah', 'clearli', 'fault']\n",
      "Tokenized sentence: ['send', 'me', 'your', 'id', 'and', 'password']\n",
      "After stop words removal: ['send', 'id', 'password']\n",
      "After stemming with porters algorithm: ['send', 'password']\n",
      "Tokenized sentence: ['the', 'message', 'sent', 'is', 'askin', 'for', 'lt', 'gt', 'dollars', 'shoul', 'i', 'pay', 'lt', 'gt', 'or', 'lt', 'gt']\n",
      "After stop words removal: ['message', 'sent', 'askin', 'lt', 'gt', 'dollars', 'shoul', 'pay', 'lt', 'gt', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['messag', 'sent', 'askin', 'dollar', 'shoul', 'pai']\n",
      "Tokenized sentence: ['haven', 't', 'found', 'a', 'way', 'to', 'get', 'another', 'app', 'for', 'your', 'phone', 'eh', 'will', 'you', 'go', 'to', 'the', 'net', 'cafe', 'did', 'you', 'take', 'that', 'job', 'geeee', 'i', 'need', 'you', 'babe', 'i', 'crave', 'to', 'see', 'you']\n",
      "After stop words removal: ['found', 'way', 'get', 'another', 'app', 'phone', 'eh', 'go', 'net', 'cafe', 'take', 'job', 'geeee', 'need', 'babe', 'crave', 'see']\n",
      "After stemming with porters algorithm: ['found', 'wai', 'get', 'anoth', 'app', 'phone', 'net', 'cafe', 'take', 'job', 'geeee', 'need', 'babe', 'crave', 'see']\n",
      "Tokenized sentence: ['yar', 'lor', 'wait', 'my', 'mum', 'finish', 'sch', 'then', 'have', 'lunch', 'lor', 'i', 'whole', 'morning', 'stay', 'at', 'home', 'clean', 'my', 'room', 'now', 'my', 'room', 'quite', 'clean', 'hee']\n",
      "After stop words removal: ['yar', 'lor', 'wait', 'mum', 'finish', 'sch', 'lunch', 'lor', 'whole', 'morning', 'stay', 'home', 'clean', 'room', 'room', 'quite', 'clean', 'hee']\n",
      "morn\n",
      "After stemming with porters algorithm: ['yar', 'lor', 'wait', 'mum', 'finish', 'sch', 'lunch', 'lor', 'whole', 'mor', 'stai', 'home', 'clean', 'room', 'room', 'quit', 'clean', 'hee']\n",
      "Tokenized sentence: ['i', 'didn', 't', 'get', 'the', 'second', 'half', 'of', 'that', 'message']\n",
      "After stop words removal: ['get', 'second', 'half', 'message']\n",
      "After stemming with porters algorithm: ['get', 'second', 'half', 'messag']\n",
      "Tokenized sentence: ['s', 'finish', 'meeting', 'call', 'me']\n",
      "After stop words removal: ['finish', 'meeting', 'call']\n",
      "meet\n",
      "After stemming with porters algorithm: ['finish', 'meet', 'call']\n",
      "Tokenized sentence: ['in', 'which', 'place', 'i', 'can', 'get', 'rooms', 'cheap']\n",
      "After stop words removal: ['place', 'get', 'rooms', 'cheap']\n",
      "After stemming with porters algorithm: ['place', 'get', 'room', 'cheap']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'reveal', 'who', 'thinks', 'u', 'r', 'so', 'special', 'call', 'to', 'opt', 'out', 'reply', 'reveal', 'stop', 'per', 'msg', 'recd', 'cust', 'care']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'reveal', 'thinks', 'u', 'r', 'special', 'call', 'opt', 'reply', 'reveal', 'stop', 'per', 'msg', 'recd', 'cust', 'care']\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'reveal', 'think', 'special', 'call', 'opt', 'repli', 'reveal', 'stop', 'per', 'msg', 'recd', 'cust', 'care']\n",
      "Tokenized sentence: ['we', 'are', 'hoping', 'to', 'get', 'away', 'by', 'from', 'langport', 'you', 'still', 'up', 'for', 'town', 'tonight']\n",
      "After stop words removal: ['hoping', 'get', 'away', 'langport', 'still', 'town', 'tonight']\n",
      "hop\n",
      "After stemming with porters algorithm: ['hope', 'get', 'awai', 'langport', 'still', 'town', 'tonight']\n",
      "Tokenized sentence: ['oic', 'then', 'better', 'quickly', 'go', 'bathe', 'n', 'settle', 'down']\n",
      "After stop words removal: ['oic', 'better', 'quickly', 'go', 'bathe', 'n', 'settle']\n",
      "After stemming with porters algorithm: ['oic', 'better', 'quickli', 'bath', 'settl']\n",
      "Tokenized sentence: ['shall', 'i', 'send', 'that', 'exe', 'to', 'your', 'mail', 'id']\n",
      "After stop words removal: ['shall', 'send', 'exe', 'mail', 'id']\n",
      "After stemming with porters algorithm: ['shall', 'send', 'ex', 'mail']\n",
      "Tokenized sentence: ['oh', 'ho', 'is', 'this', 'the', 'first', 'time', 'u', 'use', 'these', 'type', 'of', 'words']\n",
      "After stop words removal: ['oh', 'ho', 'first', 'time', 'u', 'use', 'type', 'words']\n",
      "After stemming with porters algorithm: ['first', 'time', 'us', 'type', 'word']\n",
      "Tokenized sentence: ['u', 'come', 'n', 'search', 'tat', 'vid', 'not', 'finishd']\n",
      "After stop words removal: ['u', 'come', 'n', 'search', 'tat', 'vid', 'finishd']\n",
      "After stemming with porters algorithm: ['come', 'search', 'tat', 'vid', 'finishd']\n",
      "Tokenized sentence: ['haha', 'just', 'what', 'i', 'was', 'thinkin']\n",
      "After stop words removal: ['haha', 'thinkin']\n",
      "After stemming with porters algorithm: ['haha', 'thinkin']\n",
      "Tokenized sentence: ['o', 'shore', 'are', 'you', 'takin', 'the', 'bus']\n",
      "After stop words removal: ['shore', 'takin', 'bus']\n",
      "After stemming with porters algorithm: ['shore', 'takin', 'bu']\n",
      "Tokenized sentence: ['hey', 'we', 'can', 'go', 'jazz', 'power', 'yoga', 'hip', 'hop', 'kb', 'and', 'yogasana']\n",
      "After stop words removal: ['hey', 'go', 'jazz', 'power', 'yoga', 'hip', 'hop', 'kb', 'yogasana']\n",
      "After stemming with porters algorithm: ['hei', 'jazz', 'power', 'yoga', 'hip', 'hop', 'yogasana']\n",
      "Tokenized sentence: ['if', 'e', 'timing', 'can', 'then', 'i', 'go', 'w', 'u', 'lor']\n",
      "After stop words removal: ['e', 'timing', 'go', 'w', 'u', 'lor']\n",
      "tim\n",
      "After stemming with porters algorithm: ['time', 'lor']\n",
      "Tokenized sentence: ['oh', 'mr', 'sheffield', 'you', 'wanna', 'play', 'that', 'game', 'okay', 'you', 're', 'the', 'boss', 'and', 'i', 'm', 'the', 'nanny', 'you', 'give', 'me', 'a', 'raise', 'and', 'i', 'll', 'give', 'you', 'one']\n",
      "After stop words removal: ['oh', 'mr', 'sheffield', 'wanna', 'play', 'game', 'okay', 'boss', 'nanny', 'give', 'raise', 'give', 'one']\n",
      "After stemming with porters algorithm: ['sheffield', 'wanna', 'plai', 'game', 'okai', 'boss', 'nanni', 'give', 'rais', 'give', 'on']\n",
      "Tokenized sentence: ['wen', 'u', 'miss', 'someone']\n",
      "After stop words removal: ['wen', 'u', 'miss', 'someone']\n",
      "After stemming with porters algorithm: ['wen', 'miss', 'someon']\n",
      "Tokenized sentence: ['if', 'you', 'don', 't', 'your', 'prize', 'will', 'go', 'to', 'another', 'customer', 't', 'c', 'at', 'www', 't', 'c', 'biz', 'p', 'min', 'polo', 'ltd', 'suite', 'london', 'w', 'j', 'hl', 'please', 'call', 'back', 'if', 'busy']\n",
      "After stop words removal: ['prize', 'go', 'another', 'customer', 'c', 'www', 'c', 'biz', 'p', 'min', 'polo', 'ltd', 'suite', 'london', 'w', 'j', 'hl', 'please', 'call', 'back', 'busy']\n",
      "After stemming with porters algorithm: ['priz', 'anoth', 'custom', 'www', 'biz', 'min', 'polo', 'ltd', 'suit', 'london', 'pleas', 'call', 'back', 'busi']\n",
      "Tokenized sentence: ['and', 'stop', 'wondering', 'wow', 'is', 'she', 'ever', 'going', 'to', 'stop', 'tm', 'ing', 'me', 'because', 'i', 'will', 'tm', 'you', 'whenever', 'i', 'want', 'because', 'you', 'are', 'mine', 'laughs']\n",
      "After stop words removal: ['stop', 'wondering', 'wow', 'ever', 'going', 'stop', 'tm', 'ing', 'tm', 'whenever', 'want', 'mine', 'laughs']\n",
      "wonder\n",
      "go\n",
      "After stemming with porters algorithm: ['stop', 'wonder', 'wow', 'ever', 'go', 'stop', 'ing', 'whenev', 'want', 'mine', 'laugh']\n",
      "Tokenized sentence: ['no', 'no', 'this', 'is', 'kallis', 'home', 'ground', 'amla', 'home', 'town', 'is', 'durban']\n",
      "After stop words removal: ['kallis', 'home', 'ground', 'amla', 'home', 'town', 'durban']\n",
      "After stemming with porters algorithm: ['kalli', 'home', 'ground', 'amla', 'home', 'town', 'durban']\n",
      "Tokenized sentence: ['nah', 'i', 'don', 't', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though']\n",
      "After stop words removal: ['nah', 'think', 'goes', 'usf', 'lives', 'around', 'though']\n",
      "After stemming with porters algorithm: ['nah', 'think', 'goe', 'usf', 'live', 'around', 'though']\n",
      "Tokenized sentence: ['east', 'coast']\n",
      "After stop words removal: ['east', 'coast']\n",
      "After stemming with porters algorithm: ['east', 'coast']\n",
      "Tokenized sentence: ['see', 'i', 'thought', 'it', 'all', 'through']\n",
      "After stop words removal: ['see', 'thought']\n",
      "After stemming with porters algorithm: ['see', 'thought']\n",
      "Tokenized sentence: ['now', 'get', 'step', 'outta', 'the', 'way', 'congrats', 'again']\n",
      "After stop words removal: ['get', 'step', 'outta', 'way', 'congrats']\n",
      "After stemming with porters algorithm: ['get', 'step', 'outta', 'wai', 'congrat']\n",
      "Tokenized sentence: ['no', 'calls', 'messages', 'missed', 'calls']\n",
      "After stop words removal: ['calls', 'messages', 'missed', 'calls']\n",
      "After stemming with porters algorithm: ['call', 'messag', 'miss', 'call']\n",
      "Tokenized sentence: ['for', 'ur', 'chance', 'to', 'win', 'a', 'cash', 'every', 'wk', 'txt', 'action', 'to', 't', 's', 'c', 's', 'www', 'movietrivia', 'tv', 'custcare', 'x', 'p', 'wk']\n",
      "After stop words removal: ['ur', 'chance', 'win', 'cash', 'every', 'wk', 'txt', 'action', 'c', 'www', 'movietrivia', 'tv', 'custcare', 'x', 'p', 'wk']\n",
      "After stemming with porters algorithm: ['chanc', 'win', 'cash', 'everi', 'txt', 'act', 'www', 'movietrivia', 'custcar']\n",
      "Tokenized sentence: ['its', 'posible', 'dnt', 'live', 'in', 'lt', 'gt', 'century', 'cm', 'frwd', 'n', 'thnk', 'different']\n",
      "After stop words removal: ['posible', 'dnt', 'live', 'lt', 'gt', 'century', 'cm', 'frwd', 'n', 'thnk', 'different']\n",
      "After stemming with porters algorithm: ['posib', 'dnt', 'live', 'centuri', 'frwd', 'thnk', 'differ']\n",
      "Tokenized sentence: ['aww', 'that', 's', 'the', 'first', 'time', 'u', 'said', 'u', 'missed', 'me', 'without', 'asking', 'if', 'i', 'missed', 'u', 'first', 'you', 'do', 'love', 'me']\n",
      "After stop words removal: ['aww', 'first', 'time', 'u', 'said', 'u', 'missed', 'without', 'asking', 'missed', 'u', 'first', 'love']\n",
      "ask\n",
      "After stemming with porters algorithm: ['aww', 'first', 'time', 'said', 'miss', 'without', 'as', 'miss', 'first', 'love']\n",
      "Tokenized sentence: ['you', 'are', 'gorgeous', 'keep', 'those', 'pix', 'cumming', 'thank', 'you']\n",
      "After stop words removal: ['gorgeous', 'keep', 'pix', 'cumming', 'thank']\n",
      "cumm\n",
      "After stemming with porters algorithm: ['gorgeou', 'keep', 'pix', 'cum', 'thank']\n",
      "Tokenized sentence: ['you', 'might', 'want', 'to', 'pull', 'out', 'more', 'just', 'in', 'case', 'and', 'just', 'plan', 'on', 'not', 'spending', 'it', 'if', 'you', 'can', 'i', 'don', 't', 'have', 'much', 'confidence', 'in', 'derek', 'and', 'taylor', 's', 'money', 'management']\n",
      "After stop words removal: ['might', 'want', 'pull', 'case', 'plan', 'spending', 'much', 'confidence', 'derek', 'taylor', 'money', 'management']\n",
      "spend\n",
      "After stemming with porters algorithm: ['might', 'want', 'pull', 'case', 'plan', 'spen', 'much', 'confid', 'derek', 'taylor', 'monei', 'manag']\n",
      "Tokenized sentence: ['watching', 'tv', 'now', 'i', 'got', 'new', 'job']\n",
      "After stop words removal: ['watching', 'tv', 'got', 'new', 'job']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'got', 'new', 'job']\n",
      "Tokenized sentence: ['babe', 'i', 'love', 'you', 'covers', 'your', 'face', 'in', 'kisses']\n",
      "After stop words removal: ['babe', 'love', 'covers', 'face', 'kisses']\n",
      "After stemming with porters algorithm: ['babe', 'love', 'cover', 'face', 'kiss']\n",
      "Tokenized sentence: ['lol', 'no', 'i', 'just', 'need', 'to', 'cash', 'in', 'my', 'nitros', 'hurry', 'come', 'on', 'before', 'i', 'crash', 'out']\n",
      "After stop words removal: ['lol', 'need', 'cash', 'nitros', 'hurry', 'come', 'crash']\n",
      "After stemming with porters algorithm: ['lol', 'need', 'cash', 'nitro', 'hurri', 'come', 'crash']\n",
      "Tokenized sentence: ['ill', 'call', 'u', 'mrw', 'at', 'ninish', 'with', 'my', 'address', 'that', 'icky', 'american', 'freek', 'wont', 'stop', 'callin', 'me', 'bad', 'jen', 'k', 'eh']\n",
      "After stop words removal: ['ill', 'call', 'u', 'mrw', 'ninish', 'address', 'icky', 'american', 'freek', 'wont', 'stop', 'callin', 'bad', 'jen', 'k', 'eh']\n",
      "After stemming with porters algorithm: ['ill', 'call', 'mrw', 'ninish', 'address', 'icki', 'american', 'freek', 'wont', 'stop', 'callin', 'bad', 'jen']\n",
      "Tokenized sentence: ['ok', 'lor']\n",
      "After stop words removal: ['ok', 'lor']\n",
      "After stemming with porters algorithm: ['lor']\n",
      "Tokenized sentence: ['no', 'home', 'work', 'to', 'do', 'meh']\n",
      "After stop words removal: ['home', 'work', 'meh']\n",
      "After stemming with porters algorithm: ['home', 'work', 'meh']\n",
      "Tokenized sentence: ['do', 'u', 'konw', 'waht', 'is', 'rael', 'friendship', 'im', 'gving', 'yuo', 'an', 'exmpel', 'jsut', 'ese', 'tihs', 'msg', 'evrey', 'splleing', 'of', 'tihs', 'msg', 'is', 'wrnog', 'bt', 'sitll', 'yuo', 'can', 'raed', 'it', 'wihtuot', 'ayn', 'mitsake', 'goodnight', 'amp', 'have', 'a', 'nice', 'sleep', 'sweet', 'dreams']\n",
      "After stop words removal: ['u', 'konw', 'waht', 'rael', 'friendship', 'im', 'gving', 'yuo', 'exmpel', 'jsut', 'ese', 'tihs', 'msg', 'evrey', 'splleing', 'tihs', 'msg', 'wrnog', 'bt', 'sitll', 'yuo', 'raed', 'wihtuot', 'ayn', 'mitsake', 'goodnight', 'amp', 'nice', 'sleep', 'sweet', 'dreams']\n",
      "splle\n",
      "After stemming with porters algorithm: ['konw', 'waht', 'rael', 'friendship', 'gving', 'yuo', 'exmpel', 'jsut', 'es', 'tih', 'msg', 'evrei', 'splle', 'tih', 'msg', 'wrnog', 'sitll', 'yuo', 'ra', 'wihtuot', 'ayn', 'mitsak', 'goodnight', 'amp', 'nice', 'sleep', 'sweet', 'dream']\n",
      "Tokenized sentence: ['i', 'love', 'you', 'you', 'know', 'can', 'you', 'feel', 'it', 'does', 'it', 'make', 'your', 'belly', 'warm', 'i', 'wish', 'it', 'does', 'my', 'love', 'i', 'shall', 'meet', 'you', 'in', 'your', 'dreams', 'ahmad', 'adoring', 'kiss']\n",
      "After stop words removal: ['love', 'know', 'feel', 'make', 'belly', 'warm', 'wish', 'love', 'shall', 'meet', 'dreams', 'ahmad', 'adoring', 'kiss']\n",
      "ador\n",
      "After stemming with porters algorithm: ['love', 'know', 'feel', 'make', 'belli', 'warm', 'wish', 'love', 'shall', 'meet', 'dream', 'ahmad', 'ador', 'kiss']\n",
      "Tokenized sentence: ['haiyoh', 'maybe', 'your', 'hamster', 'was', 'jealous', 'of', 'million']\n",
      "After stop words removal: ['haiyoh', 'maybe', 'hamster', 'jealous', 'million']\n",
      "After stemming with porters algorithm: ['haiyoh', 'mayb', 'hamster', 'jealou', 'million']\n",
      "Tokenized sentence: ['sad', 'story', 'of', 'a', 'man', 'last', 'week', 'was', 'my', 'b', 'day', 'my', 'wife', 'did', 'nt', 'wish', 'me', 'my', 'parents', 'forgot', 'n', 'so', 'did', 'my', 'kids', 'i', 'went', 'to', 'work', 'even', 'my', 'colleagues', 'did', 'not', 'wish', 'as', 'i', 'entered', 'my', 'cabin', 'my', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'i', 'felt', 'special', 'she', 'askd', 'me', 'lunch', 'after', 'lunch', 'she', 'invited', 'me', 'to', 'her', 'apartment', 'we', 'went', 'there', 'she', 'said', 'do', 'u', 'mind', 'if', 'i', 'go', 'into', 'the', 'bedroom', 'for', 'a', 'minute', 'ok', 'i', 'sed', 'in', 'a', 'sexy', 'mood', 'she', 'came', 'out', 'minuts', 'latr', 'wid', 'a', 'cake', 'n', 'my', 'wife', 'my', 'parents', 'my', 'kidz', 'my', 'friends', 'n', 'my', 'colleagues', 'all', 'screaming', 'surprise', 'and', 'i', 'was', 'waiting', 'on', 'the', 'sofa', 'naked']\n",
      "After stop words removal: ['sad', 'story', 'man', 'last', 'week', 'b', 'day', 'wife', 'nt', 'wish', 'parents', 'forgot', 'n', 'kids', 'went', 'work', 'even', 'colleagues', 'wish', 'entered', 'cabin', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invited', 'apartment', 'went', 'said', 'u', 'mind', 'go', 'bedroom', 'minute', 'ok', 'sed', 'sexy', 'mood', 'came', 'minuts', 'latr', 'wid', 'cake', 'n', 'wife', 'parents', 'kidz', 'friends', 'n', 'colleagues', 'screaming', 'surprise', 'waiting', 'sofa', 'naked']\n",
      "scream\n",
      "wait\n",
      "After stemming with porters algorithm: ['sad', 'stori', 'man', 'last', 'week', 'dai', 'wife', 'wish', 'parent', 'forgot', 'kid', 'went', 'work', 'even', 'colleagu', 'wish', 'enter', 'cabin', 'said', 'happi', 'dai', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invit', 'apart', 'went', 'said', 'mind', 'bedroom', 'minut', 'sed', 'sexi', 'mood', 'came', 'minut', 'latr', 'wid', 'cake', 'wife', 'parent', 'kidz', 'friend', 'colleagu', 'scream', 'surpris', 'wait', 'sofa', 'nake']\n",
      "Tokenized sentence: ['cos', 'darren', 'say', 'considering', 'mah', 'so', 'i', 'ask']\n",
      "After stop words removal: ['cos', 'darren', 'say', 'considering', 'mah', 'ask']\n",
      "consider\n",
      "After stemming with porters algorithm: ['co', 'darren', 'sai', 'consid', 'mah', 'ask']\n",
      "Tokenized sentence: ['aight', 'i', 'll', 'ask', 'a', 'few', 'of', 'my', 'roommates']\n",
      "After stop words removal: ['aight', 'ask', 'roommates']\n",
      "After stemming with porters algorithm: ['aight', 'ask', 'roommat']\n",
      "Tokenized sentence: ['free', 'tones', 'hope', 'you', 'enjoyed', 'your', 'new', 'content', 'text', 'stop', 'to', 'to', 'unsubscribe', 'help', 'p', 'provided', 'by', 'tones', 'you', 'co', 'uk']\n",
      "After stop words removal: ['free', 'tones', 'hope', 'enjoyed', 'new', 'content', 'text', 'stop', 'unsubscribe', 'help', 'p', 'provided', 'tones', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['free', 'tone', 'hope', 'enjoi', 'new', 'content', 'text', 'stop', 'unsubscrib', 'help', 'provid', 'tone']\n",
      "Tokenized sentence: ['good', 'afternoon', 'my', 'love', 'any', 'job', 'prospects', 'are', 'you', 'missing', 'me', 'what', 'do', 'you', 'do', 'are', 'you', 'being', 'lazy', 'and', 'bleak', 'hmmm', 'or', 'happy', 'and', 'filled', 'with', 'my', 'love']\n",
      "After stop words removal: ['good', 'afternoon', 'love', 'job', 'prospects', 'missing', 'lazy', 'bleak', 'hmmm', 'happy', 'filled', 'love']\n",
      "miss\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'love', 'job', 'prospect', 'miss', 'lazi', 'bleak', 'hmmm', 'happi', 'fill', 'love']\n",
      "Tokenized sentence: ['ha', 'i', 'wouldn', 't', 'say', 'that', 'i', 'just', 'didn', 't', 'read', 'anything', 'into', 'way', 'u', 'seemed', 'i', 'don', 't', 'like', 'be', 'judgemental', 'i', 'save', 'that', 'for', 'fridays', 'in', 'the', 'pub']\n",
      "After stop words removal: ['ha', 'say', 'read', 'anything', 'way', 'u', 'seemed', 'like', 'judgemental', 'save', 'fridays', 'pub']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['sai', 'read', 'anyt', 'wai', 'seem', 'like', 'judgem', 'save', 'fridai', 'pub']\n",
      "Tokenized sentence: ['solve', 'd', 'case', 'a', 'man', 'was', 'found', 'murdered', 'on', 'lt', 'decimal', 'gt', 'lt', 'gt', 'afternoon', 'his', 'wife', 'called', 'police', 'police', 'questioned', 'everyone', 'wife', 'sir', 'i', 'was', 'sleeping', 'when', 'the', 'murder', 'took', 'place', 'cook', 'i', 'was', 'cooking', 'gardener', 'i', 'was', 'picking', 'vegetables', 'house', 'maid', 'i', 'went', 'd', 'post', 'office', 'children', 'we', 'went', 'play', 'neighbour', 'we', 'went', 'a', 'marriage', 'police', 'arrested', 'd', 'murderer', 'immediately', 'who', 's', 'it', 'reply', 'with', 'reason', 'if', 'u', 'r', 'brilliant']\n",
      "After stop words removal: ['solve', 'case', 'man', 'found', 'murdered', 'lt', 'decimal', 'gt', 'lt', 'gt', 'afternoon', 'wife', 'called', 'police', 'police', 'questioned', 'everyone', 'wife', 'sir', 'sleeping', 'murder', 'took', 'place', 'cook', 'cooking', 'gardener', 'picking', 'vegetables', 'house', 'maid', 'went', 'post', 'office', 'children', 'went', 'play', 'neighbour', 'went', 'marriage', 'police', 'arrested', 'murderer', 'immediately', 'reply', 'reason', 'u', 'r', 'brilliant']\n",
      "sleep\n",
      "cook\n",
      "pick\n",
      "After stemming with porters algorithm: ['solv', 'case', 'man', 'found', 'murder', 'decim', 'afternoon', 'wife', 'call', 'polic', 'polic', 'quest', 'everyon', 'wife', 'sir', 'sleep', 'murder', 'took', 'place', 'cook', 'cook', 'garden', 'pic', 'veget', 'hous', 'maid', 'went', 'post', 'offic', 'children', 'went', 'plai', 'neighbour', 'went', 'marriag', 'polic', 'arres', 'murder', 'immedi', 'repli', 'reason', 'brilliant']\n",
      "Tokenized sentence: ['my', 'no', 'in', 'luton', 'ring', 'me', 'if', 'ur', 'around', 'h']\n",
      "After stop words removal: ['luton', 'ring', 'ur', 'around', 'h']\n",
      "After stemming with porters algorithm: ['luton', 'ring', 'around']\n",
      "Tokenized sentence: ['i', 'will', 'be', 'gentle', 'princess', 'we', 'will', 'make', 'sweet', 'gentle', 'love']\n",
      "After stop words removal: ['gentle', 'princess', 'make', 'sweet', 'gentle', 'love']\n",
      "After stemming with porters algorithm: ['gentl', 'princess', 'make', 'sweet', 'gentl', 'love']\n",
      "Tokenized sentence: ['no', 'message', 'no', 'responce', 'what', 'happend']\n",
      "After stop words removal: ['message', 'responce', 'happend']\n",
      "After stemming with porters algorithm: ['messag', 'responc', 'happend']\n",
      "Tokenized sentence: ['my', 'supervisor', 'find', 'me', 'one', 'lor', 'i', 'thk', 'his', 'students', 'i', 'havent', 'ask', 'her', 'yet', 'tell', 'u', 'aft', 'i', 'ask', 'her']\n",
      "After stop words removal: ['supervisor', 'find', 'one', 'lor', 'thk', 'students', 'havent', 'ask', 'yet', 'tell', 'u', 'aft', 'ask']\n",
      "After stemming with porters algorithm: ['supervisor', 'find', 'on', 'lor', 'thk', 'student', 'havent', 'ask', 'yet', 'tell', 'aft', 'ask']\n",
      "Tokenized sentence: ['is', 'that', 'on', 'the', 'telly', 'no', 'its', 'brdget', 'jones']\n",
      "After stop words removal: ['telly', 'brdget', 'jones']\n",
      "After stemming with porters algorithm: ['telli', 'brdget', 'jone']\n",
      "Tokenized sentence: ['i', 'need', 'you', 'to', 'be', 'in', 'my', 'strong', 'arms']\n",
      "After stop words removal: ['need', 'strong', 'arms']\n",
      "After stemming with porters algorithm: ['need', 'strong', 'arm']\n",
      "Tokenized sentence: ['did', 'u', 'receive', 'my', 'msg']\n",
      "After stop words removal: ['u', 'receive', 'msg']\n",
      "After stemming with porters algorithm: ['receiv', 'msg']\n",
      "Tokenized sentence: ['my', 'painful', 'personal', 'thought', 'i', 'always', 'try', 'to', 'keep', 'everybody', 'happy', 'all', 'the', 'time', 'but', 'nobody', 'recognises', 'me', 'when', 'i', 'am', 'alone']\n",
      "After stop words removal: ['painful', 'personal', 'thought', 'always', 'try', 'keep', 'everybody', 'happy', 'time', 'nobody', 'recognises', 'alone']\n",
      "After stemming with porters algorithm: ['pain', 'person', 'thought', 'alwai', 'try', 'keep', 'everybodi', 'happi', 'time', 'nobodi', 'recognis', 'alon']\n",
      "Tokenized sentence: ['what', 'time', 'you', 'think', 'you', 'll', 'have', 'it', 'need', 'to', 'know', 'when', 'i', 'should', 'be', 'near', 'campus']\n",
      "After stop words removal: ['time', 'think', 'need', 'know', 'near', 'campus']\n",
      "After stemming with porters algorithm: ['time', 'think', 'need', 'know', 'near', 'campu']\n",
      "Tokenized sentence: ['that', 's', 'my', 'honeymoon', 'outfit']\n",
      "After stop words removal: ['honeymoon', 'outfit']\n",
      "After stemming with porters algorithm: ['honeymoon', 'outfit']\n",
      "Tokenized sentence: ['urgent', 'ur', 'guaranteed', 'award', 'is', 'still', 'unclaimed', 'call', 'now', 'closingdate', 'claimcode', 'm', 'm', 'pmmorefrommobile', 'bremoved', 'mobypobox', 'ls', 'yf']\n",
      "After stop words removal: ['urgent', 'ur', 'guaranteed', 'award', 'still', 'unclaimed', 'call', 'closingdate', 'claimcode', 'pmmorefrommobile', 'bremoved', 'mobypobox', 'ls', 'yf']\n",
      "After stemming with porters algorithm: ['urgent', 'guaranteed', 'award', 'still', 'unclaim', 'call', 'closingd', 'claimcod', 'pmmorefrommobil', 'bremov', 'mobypobox']\n",
      "Tokenized sentence: ['had', 'your', 'mobile', 'months', 'or', 'more', 'u', 'r', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'mobiles', 'with', 'camera', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']\n",
      "After stop words removal: ['mobile', 'months', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobiles', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
      "After stemming with porters algorithm: ['mobil', 'month', 'entit', 'updat', 'latest', 'colour', 'mobil', 'camera', 'free', 'call', 'mobil', 'updat', 'free']\n",
      "Tokenized sentence: ['friendship', 'is', 'not', 'a', 'game', 'to', 'play', 'it', 'is', 'not', 'a', 'word', 'to', 'say', 'it', 'doesn', 't', 'start', 'on', 'march', 'and', 'ends', 'on', 'may', 'it', 'is', 'tomorrow', 'yesterday', 'today', 'and', 'e']\n",
      "After stop words removal: ['friendship', 'game', 'play', 'word', 'say', 'start', 'march', 'ends', 'may', 'tomorrow', 'yesterday', 'today', 'e']\n",
      "After stemming with porters algorithm: ['friendship', 'game', 'plai', 'word', 'sai', 'start', 'march', 'end', 'mai', 'tomorrow', 'yesterdai', 'todai']\n",
      "Tokenized sentence: ['k', 'u', 'also', 'dont', 'msg', 'or', 'reply', 'to', 'his', 'msg']\n",
      "After stop words removal: ['k', 'u', 'also', 'dont', 'msg', 'reply', 'msg']\n",
      "After stemming with porters algorithm: ['also', 'dont', 'msg', 'repli', 'msg']\n",
      "Tokenized sentence: ['i', 'ask', 'if', 'u', 'meeting', 'da', 'ge', 'tmr', 'nite']\n",
      "After stop words removal: ['ask', 'u', 'meeting', 'da', 'ge', 'tmr', 'nite']\n",
      "meet\n",
      "After stemming with porters algorithm: ['ask', 'meet', 'tmr', 'nite']\n",
      "Tokenized sentence: ['great', 'comedy', 'cant', 'stop', 'laughing', 'da']\n",
      "After stop words removal: ['great', 'comedy', 'cant', 'stop', 'laughing', 'da']\n",
      "laugh\n",
      "After stemming with porters algorithm: ['great', 'comedi', 'cant', 'stop', 'laug']\n",
      "Tokenized sentence: ['honey', 'sweetheart', 'darling', 'sexy', 'buns', 'sugar', 'plum', 'loverboy', 'i', 'miss', 'you', 'boytoy', 'smacks', 'your', 'ass', 'did', 'you', 'go', 'to', 'the', 'gym', 'too']\n",
      "After stop words removal: ['honey', 'sweetheart', 'darling', 'sexy', 'buns', 'sugar', 'plum', 'loverboy', 'miss', 'boytoy', 'smacks', 'ass', 'go', 'gym']\n",
      "darl\n",
      "After stemming with porters algorithm: ['honei', 'sweetheart', 'darl', 'sexi', 'bun', 'sugar', 'plum', 'loverboi', 'miss', 'boytoi', 'smack', 'ass', 'gym']\n",
      "Tokenized sentence: ['i', 'am', 'late', 'i', 'will', 'be', 'there', 'at']\n",
      "After stop words removal: ['late']\n",
      "After stemming with porters algorithm: ['late']\n",
      "Tokenized sentence: ['the', 'guy', 'did', 'some', 'bitching', 'but', 'i', 'acted', 'like', 'i', 'd', 'be', 'interested', 'in', 'buying', 'something', 'else', 'next', 'week', 'and', 'he', 'gave', 'it', 'to', 'us', 'for', 'free']\n",
      "After stop words removal: ['guy', 'bitching', 'acted', 'like', 'interested', 'buying', 'something', 'else', 'next', 'week', 'gave', 'us', 'free']\n",
      "bitch\n",
      "buy\n",
      "someth\n",
      "After stemming with porters algorithm: ['gui', 'bitc', 'ac', 'like', 'interes', 'bui', 'somet', 'els', 'next', 'week', 'gave', 'free']\n",
      "Tokenized sentence: ['i', 'dont', 'thnk', 'its', 'a', 'wrong', 'calling', 'between', 'us']\n",
      "After stop words removal: ['dont', 'thnk', 'wrong', 'calling', 'us']\n",
      "call\n",
      "After stemming with porters algorithm: ['dont', 'thnk', 'wrong', 'call']\n",
      "Tokenized sentence: ['pls', 'tell', 'nelson', 'that', 'the', 'bb', 's', 'are', 'no', 'longer', 'comin', 'the', 'money', 'i', 'was', 'expecting', 'aint', 'coming']\n",
      "After stop words removal: ['pls', 'tell', 'nelson', 'bb', 'longer', 'comin', 'money', 'expecting', 'aint', 'coming']\n",
      "expect\n",
      "com\n",
      "After stemming with porters algorithm: ['pl', 'tell', 'nelson', 'longer', 'comin', 'monei', 'expec', 'aint', 'come']\n",
      "Tokenized sentence: ['abeg', 'make', 'profit', 'but', 'its', 'a', 'start', 'are', 'you', 'using', 'it', 'to', 'get', 'sponsors', 'for', 'the', 'next', 'event']\n",
      "After stop words removal: ['abeg', 'make', 'profit', 'start', 'using', 'get', 'sponsors', 'next', 'event']\n",
      "us\n",
      "After stemming with porters algorithm: ['abeg', 'make', 'profit', 'start', 'us', 'get', 'sponsor', 'next', 'event']\n",
      "Tokenized sentence: ['house', 'maid', 'is', 'the', 'murderer', 'coz', 'the', 'man', 'was', 'murdered', 'on', 'lt', 'gt', 'th', 'january', 'as', 'public', 'holiday', 'all', 'govt', 'instituitions', 'are', 'closed', 'including', 'post', 'office', 'understand']\n",
      "After stop words removal: ['house', 'maid', 'murderer', 'coz', 'man', 'murdered', 'lt', 'gt', 'th', 'january', 'public', 'holiday', 'govt', 'instituitions', 'closed', 'including', 'post', 'office', 'understand']\n",
      "includ\n",
      "After stemming with porters algorithm: ['hous', 'maid', 'murder', 'coz', 'man', 'murder', 'januari', 'public', 'holidai', 'govt', 'instituit', 'close', 'includ', 'post', 'offic', 'understand']\n",
      "Tokenized sentence: ['say', 'this', 'slowly', 'god', 'i', 'love', 'you', 'amp', 'i', 'need', 'you', 'clean', 'my', 'heart', 'with', 'your', 'blood', 'send', 'this', 'to', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'do', 'it', 'pls', 'pls', 'do', 'it']\n",
      "After stop words removal: ['say', 'slowly', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'pls', 'pls']\n",
      "After stemming with porters algorithm: ['sai', 'slowli', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'peopl', 'amp', 'mirac', 'tomorrow', 'pl', 'pl']\n",
      "Tokenized sentence: ['please', 'call', 'immediately', 'as', 'there', 'is', 'an', 'urgent', 'message', 'waiting', 'for', 'you']\n",
      "After stop words removal: ['please', 'call', 'immediately', 'urgent', 'message', 'waiting']\n",
      "wait\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'immedi', 'urgent', 'messag', 'wait']\n",
      "Tokenized sentence: ['kay', 'since', 'we', 'are', 'out', 'already']\n",
      "After stop words removal: ['kay', 'since', 'already']\n",
      "After stemming with porters algorithm: ['kai', 'sinc', 'alreadi']\n",
      "Tokenized sentence: ['taka', 'lor', 'wat', 'time', 'u', 'wan', 'come', 'n', 'look', 'us']\n",
      "After stop words removal: ['taka', 'lor', 'wat', 'time', 'u', 'wan', 'come', 'n', 'look', 'us']\n",
      "After stemming with porters algorithm: ['taka', 'lor', 'wat', 'time', 'wan', 'come', 'look']\n",
      "Tokenized sentence: ['no', 'break', 'time', 'one', 'how', 'i', 'come', 'out', 'n', 'get', 'my', 'stuff', 'fr']\n",
      "After stop words removal: ['break', 'time', 'one', 'come', 'n', 'get', 'stuff', 'fr']\n",
      "After stemming with porters algorithm: ['break', 'time', 'on', 'come', 'get', 'stuff']\n",
      "Tokenized sentence: ['k', 'my', 'roommate', 'also', 'wants', 'a', 'dubsack', 'and', 'another', 'friend', 'may', 'also', 'want', 'some', 'so', 'plan', 'on', 'bringing', 'extra', 'i', 'll', 'tell', 'you', 'when', 'they', 'know', 'for', 'sure']\n",
      "After stop words removal: ['k', 'roommate', 'also', 'wants', 'dubsack', 'another', 'friend', 'may', 'also', 'want', 'plan', 'bringing', 'extra', 'tell', 'know', 'sure']\n",
      "bring\n",
      "After stemming with porters algorithm: ['roommat', 'also', 'want', 'dubsack', 'anoth', 'friend', 'mai', 'also', 'want', 'plan', 'brin', 'extra', 'tell', 'know', 'sure']\n",
      "Tokenized sentence: ['hi', 'darlin', 'im', 'missin', 'u', 'hope', 'you', 'are', 'having', 'a', 'good', 'time', 'when', 'are', 'u', 'back', 'and', 'what', 'time', 'if', 'u', 'can', 'give', 'me', 'a', 'call', 'at', 'home', 'jess', 'xx']\n",
      "After stop words removal: ['hi', 'darlin', 'im', 'missin', 'u', 'hope', 'good', 'time', 'u', 'back', 'time', 'u', 'give', 'call', 'home', 'jess', 'xx']\n",
      "After stemming with porters algorithm: ['darlin', 'missin', 'hope', 'good', 'time', 'back', 'time', 'give', 'call', 'home', 'jess']\n",
      "Tokenized sentence: ['just', 'glad', 'to', 'be', 'talking', 'to', 'you']\n",
      "After stop words removal: ['glad', 'talking']\n",
      "talk\n",
      "After stemming with porters algorithm: ['glad', 'tal']\n",
      "Tokenized sentence: ['hi', 'dear', 'call', 'me', 'its', 'urgnt', 'i', 'don', 't', 'know', 'whats', 'your', 'problem', 'you', 'don', 't', 'want', 'to', 'work', 'or', 'if', 'you', 'have', 'any', 'other', 'problem', 'at', 'least', 'tell', 'me', 'wating', 'for', 'your', 'reply']\n",
      "After stop words removal: ['hi', 'dear', 'call', 'urgnt', 'know', 'whats', 'problem', 'want', 'work', 'problem', 'least', 'tell', 'wating', 'reply']\n",
      "wat\n",
      "wate\n",
      "After stemming with porters algorithm: ['dear', 'call', 'urgnt', 'know', 'what', 'problem', 'want', 'work', 'problem', 'least', 'tell', 'wate', 'repli']\n",
      "Tokenized sentence: ['ta', 'daaaaa', 'i', 'am', 'home', 'babe', 'are', 'you', 'still', 'up']\n",
      "After stop words removal: ['ta', 'daaaaa', 'home', 'babe', 'still']\n",
      "After stemming with porters algorithm: ['daaaaa', 'home', 'babe', 'still']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'tel', 'u', 'one', 'thing', 'u', 'should', 'not', 'mistake', 'me', 'k', 'this', 'is', 'the', 'message', 'that', 'you', 'sent']\n",
      "After stop words removal: ['want', 'tel', 'u', 'one', 'thing', 'u', 'mistake', 'k', 'message', 'sent']\n",
      "After stemming with porters algorithm: ['want', 'tel', 'on', 'thing', 'mistak', 'messag', 'sent']\n",
      "Tokenized sentence: ['hi', 'i', 'm', 'always', 'online', 'on', 'yahoo', 'and', 'would', 'like', 'to', 'chat', 'with', 'you', 'someday']\n",
      "After stop words removal: ['hi', 'always', 'online', 'yahoo', 'would', 'like', 'chat', 'someday']\n",
      "After stemming with porters algorithm: ['alwai', 'onlin', 'yahoo', 'would', 'like', 'chat', 'somedai']\n",
      "Tokenized sentence: ['todays', 'voda', 'numbers', 'ending', 'are', 'selected', 'to', 'receive', 'a', 'award', 'if', 'you', 'hava', 'a', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "After stop words removal: ['todays', 'voda', 'numbers', 'ending', 'selected', 'receive', 'award', 'hava', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "end\n",
      "quot\n",
      "After stemming with porters algorithm: ['todai', 'voda', 'number', 'en', 'selec', 'receiv', 'award', 'hava', 'match', 'pleas', 'call', 'quot', 'claim', 'code', 'standard', 'rate', 'app']\n",
      "Tokenized sentence: ['will', 'do', 'was', 'exhausted', 'on', 'train', 'this', 'morning', 'too', 'much', 'wine', 'and', 'pie', 'you', 'sleep', 'well', 'too']\n",
      "After stop words removal: ['exhausted', 'train', 'morning', 'much', 'wine', 'pie', 'sleep', 'well']\n",
      "morn\n",
      "After stemming with porters algorithm: ['exhaus', 'train', 'mor', 'much', 'wine', 'pie', 'sleep', 'well']\n",
      "Tokenized sentence: ['free', 'msg', 'ringtone', 'from', 'http', 'tms', 'widelive', 'com', 'index', 'wml', 'id', 'b', 'a', 'ecef', 'ff', 'first', 'true', 'jul']\n",
      "After stop words removal: ['free', 'msg', 'ringtone', 'http', 'tms', 'widelive', 'com', 'index', 'wml', 'id', 'b', 'ecef', 'ff', 'first', 'true', 'jul']\n",
      "After stemming with porters algorithm: ['free', 'msg', 'rington', 'http', 'tm', 'widel', 'com', 'index', 'wml', 'ecef', 'first', 'true', 'jul']\n",
      "Tokenized sentence: ['oh', 'ok', 'i', 'didnt', 'know', 'what', 'you', 'meant', 'yep', 'i', 'am', 'baby', 'jontin']\n",
      "After stop words removal: ['oh', 'ok', 'didnt', 'know', 'meant', 'yep', 'baby', 'jontin']\n",
      "After stemming with porters algorithm: ['didnt', 'know', 'meant', 'yep', 'babi', 'jontin']\n",
      "Tokenized sentence: ['yes', 'da', 'any', 'plm', 'at', 'ur', 'office']\n",
      "After stop words removal: ['yes', 'da', 'plm', 'ur', 'office']\n",
      "After stemming with porters algorithm: ['ye', 'plm', 'offic']\n",
      "Tokenized sentence: ['i', 'dun', 'believe', 'u', 'i', 'thk', 'u', 'told', 'him']\n",
      "After stop words removal: ['dun', 'believe', 'u', 'thk', 'u', 'told']\n",
      "After stemming with porters algorithm: ['dun', 'believ', 'thk', 'told']\n",
      "Tokenized sentence: ['what', 'i', 'mean', 'is', 'do', 'they', 'come', 'chase', 'you', 'out', 'when', 'its', 'over', 'or', 'is', 'it', 'stated', 'you', 'can', 'watch', 'as', 'many', 'movies', 'as', 'you', 'want']\n",
      "After stop words removal: ['mean', 'come', 'chase', 'stated', 'watch', 'many', 'movies', 'want']\n",
      "state\n",
      "After stemming with porters algorithm: ['mean', 'come', 'chase', 'state', 'watch', 'mani', 'movi', 'want']\n",
      "Tokenized sentence: ['hi', 'my', 'email', 'address', 'has', 'changed', 'now', 'it', 'is']\n",
      "After stop words removal: ['hi', 'email', 'address', 'changed']\n",
      "After stemming with porters algorithm: ['email', 'address', 'chan']\n",
      "Tokenized sentence: ['haha', 'get', 'used', 'to', 'driving', 'to', 'usf', 'man', 'i', 'know', 'a', 'lot', 'of', 'stoners']\n",
      "After stop words removal: ['haha', 'get', 'used', 'driving', 'usf', 'man', 'know', 'lot', 'stoners']\n",
      "driv\n",
      "After stemming with porters algorithm: ['haha', 'get', 'us', 'drive', 'usf', 'man', 'know', 'lot', 'stoner']\n",
      "Tokenized sentence: ['do', 'all', 'wan', 'meet', 'up', 'n', 'combine', 'all', 'the', 'parts', 'how', 's', 'da', 'rest', 'of', 'da', 'project', 'going']\n",
      "After stop words removal: ['wan', 'meet', 'n', 'combine', 'parts', 'da', 'rest', 'da', 'project', 'going']\n",
      "go\n",
      "After stemming with porters algorithm: ['wan', 'meet', 'combin', 'part', 'rest', 'project', 'go']\n",
      "Tokenized sentence: ['i', 'will', 'once', 'i', 'get', 'home']\n",
      "After stop words removal: ['get', 'home']\n",
      "After stemming with porters algorithm: ['get', 'home']\n",
      "Tokenized sentence: ['thank', 'you', 'for', 'calling', 'forgot', 'to', 'say', 'happy', 'onam', 'to', 'you', 'sirji', 'i', 'am', 'fine', 'here', 'and', 'remembered', 'you', 'when', 'i', 'met', 'an', 'insurance', 'person', 'meet', 'you', 'in', 'qatar', 'insha', 'allah', 'rakhesh', 'ex', 'tata', 'aig', 'who', 'joined', 'tissco', 'tayseer']\n",
      "After stop words removal: ['thank', 'calling', 'forgot', 'say', 'happy', 'onam', 'sirji', 'fine', 'remembered', 'met', 'insurance', 'person', 'meet', 'qatar', 'insha', 'allah', 'rakhesh', 'ex', 'tata', 'aig', 'joined', 'tissco', 'tayseer']\n",
      "call\n",
      "After stemming with porters algorithm: ['thank', 'call', 'forgot', 'sai', 'happi', 'onam', 'sirji', 'fine', 'rememb', 'met', 'insur', 'person', 'meet', 'qatar', 'insha', 'allah', 'rakhesh', 'tata', 'aig', 'join', 'tissco', 'tayseer']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['you', 'call', 'times', 'job', 'today', 'ok', 'umma', 'and', 'ask', 'them', 'to', 'speed', 'up']\n",
      "After stop words removal: ['call', 'times', 'job', 'today', 'ok', 'umma', 'ask', 'speed']\n",
      "After stemming with porters algorithm: ['call', 'time', 'job', 'todai', 'umma', 'ask', 'speed']\n",
      "Tokenized sentence: ['so', 'what', 'did', 'the', 'bank', 'say', 'about', 'the', 'money']\n",
      "After stop words removal: ['bank', 'say', 'money']\n",
      "After stemming with porters algorithm: ['bank', 'sai', 'monei']\n",
      "Tokenized sentence: ['yesterday', 'its', 'with', 'me', 'only', 'now', 'am', 'going', 'home']\n",
      "After stop words removal: ['yesterday', 'going', 'home']\n",
      "go\n",
      "After stemming with porters algorithm: ['yesterdai', 'go', 'home']\n",
      "Tokenized sentence: ['aiyah', 'e', 'rain', 'like', 'quite', 'big', 'leh', 'if', 'drizzling', 'i', 'can', 'at', 'least', 'run', 'home']\n",
      "After stop words removal: ['aiyah', 'e', 'rain', 'like', 'quite', 'big', 'leh', 'drizzling', 'least', 'run', 'home']\n",
      "drizzl\n",
      "After stemming with porters algorithm: ['aiyah', 'rain', 'like', 'quit', 'big', 'leh', 'drizzl', 'least', 'run', 'home']\n",
      "Tokenized sentence: ['thanx', 'a', 'lot']\n",
      "After stop words removal: ['thanx', 'lot']\n",
      "After stemming with porters algorithm: ['thanx', 'lot']\n",
      "Tokenized sentence: ['lol', 'they', 'don', 't', 'know', 'about', 'my', 'awesome', 'phone', 'i', 'could', 'click', 'delete', 'right', 'now', 'if', 'i', 'want']\n",
      "After stop words removal: ['lol', 'know', 'awesome', 'phone', 'could', 'click', 'delete', 'right', 'want']\n",
      "After stemming with porters algorithm: ['lol', 'know', 'awesom', 'phone', 'could', 'click', 'delet', 'right', 'want']\n",
      "Tokenized sentence: ['i', 'want', 'lt', 'gt', 'rs', 'da', 'do', 'you', 'have', 'it']\n",
      "After stop words removal: ['want', 'lt', 'gt', 'rs', 'da']\n",
      "After stemming with porters algorithm: ['want']\n",
      "Tokenized sentence: ['hi', 'here', 'have', 'birth', 'at', 'on', 'the', 'to', 'at', 'lb', 'oz', 'mother', 'and', 'baby', 'doing', 'brilliantly']\n",
      "After stop words removal: ['hi', 'birth', 'lb', 'oz', 'mother', 'baby', 'brilliantly']\n",
      "After stemming with porters algorithm: ['birth', 'mother', 'babi', 'brilliantli']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['buzzzz', 'grins', 'did', 'i', 'buzz', 'your', 'ass', 'buzz', 'your', 'chest', 'buzz', 'your', 'cock', 'where', 'do', 'you', 'keep', 'your', 'phone', 'is', 'the', 'vibrator', 'on', 'did', 'you', 'feel', 'it', 'shake']\n",
      "After stop words removal: ['buzzzz', 'grins', 'buzz', 'ass', 'buzz', 'chest', 'buzz', 'cock', 'keep', 'phone', 'vibrator', 'feel', 'shake']\n",
      "After stemming with porters algorithm: ['buzzzz', 'grin', 'buzz', 'ass', 'buzz', 'chest', 'buzz', 'cock', 'keep', 'phone', 'vibrat', 'feel', 'shake']\n",
      "Tokenized sentence: ['i', 'know', 'grumpy', 'old', 'people', 'my', 'mom', 'was', 'like', 'you', 'better', 'not', 'be', 'lying', 'then', 'again', 'i', 'am', 'always', 'the', 'one', 'to', 'play', 'jokes']\n",
      "After stop words removal: ['know', 'grumpy', 'old', 'people', 'mom', 'like', 'better', 'lying', 'always', 'one', 'play', 'jokes']\n",
      "After stemming with porters algorithm: ['know', 'grumpi', 'old', 'peopl', 'mom', 'like', 'better', 'lying', 'alwai', 'on', 'plai', 'joke']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'xx', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'xx', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['s', 'flirting', 'now', 'txt', 'girl', 'or', 'bloke', 'ur', 'name', 'age', 'eg', 'girl', 'zoe', 'to', 'to', 'join', 'and', 'get', 'chatting']\n",
      "After stop words removal: ['flirting', 'txt', 'girl', 'bloke', 'ur', 'name', 'age', 'eg', 'girl', 'zoe', 'join', 'get', 'chatting']\n",
      "flirt\n",
      "chatt\n",
      "After stemming with porters algorithm: ['flir', 'txt', 'girl', 'bloke', 'name', 'ag', 'girl', 'zoe', 'join', 'get', 'chat']\n",
      "Tokenized sentence: ['yes', 'see', 'ya', 'not', 'on', 'the', 'dot']\n",
      "After stop words removal: ['yes', 'see', 'ya', 'dot']\n",
      "After stemming with porters algorithm: ['ye', 'see', 'dot']\n",
      "Tokenized sentence: ['really', 'i', 'crashed', 'out', 'cuddled', 'on', 'my', 'sofa']\n",
      "After stop words removal: ['really', 'crashed', 'cuddled', 'sofa']\n",
      "After stemming with porters algorithm: ['realli', 'cras', 'cuddl', 'sofa']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['me', 'n', 'him', 'so', 'funny']\n",
      "After stop words removal: ['n', 'funny']\n",
      "After stemming with porters algorithm: ['funni']\n",
      "Tokenized sentence: ['guai', 'shd', 'haf', 'seen', 'him', 'when', 'he', 's', 'naughty', 'so', 'free', 'today', 'can', 'go', 'jogging']\n",
      "After stop words removal: ['guai', 'shd', 'haf', 'seen', 'naughty', 'free', 'today', 'go', 'jogging']\n",
      "jogg\n",
      "After stemming with porters algorithm: ['guai', 'shd', 'haf', 'seen', 'naughti', 'free', 'todai', 'jog']\n",
      "Tokenized sentence: ['new', 'mobiles', 'from', 'must', 'go', 'txt', 'nokia', 'to', 'no', 'collect', 'yours', 'today', 'from', 'only', 'www', 'tc', 'biz', 'optout', 'gbp', 'mtmsg', 'txtauction']\n",
      "After stop words removal: ['new', 'mobiles', 'must', 'go', 'txt', 'nokia', 'collect', 'today', 'www', 'tc', 'biz', 'optout', 'gbp', 'mtmsg', 'txtauction']\n",
      "After stemming with porters algorithm: ['new', 'mobil', 'must', 'txt', 'nokia', 'collect', 'todai', 'www', 'biz', 'optout', 'gbp', 'mtmsg', 'txtauct']\n",
      "Tokenized sentence: ['lol', 'oh', 'no', 'babe', 'i', 'wont', 'be', 'sliding', 'into', 'your', 'place', 'after', 'midnight', 'but', 'thanks', 'for', 'the', 'invite']\n",
      "After stop words removal: ['lol', 'oh', 'babe', 'wont', 'sliding', 'place', 'midnight', 'thanks', 'invite']\n",
      "slid\n",
      "After stemming with porters algorithm: ['lol', 'babe', 'wont', 'slide', 'place', 'midnight', 'thank', 'invit']\n",
      "Tokenized sentence: ['k', 'k', 'sms', 'chat', 'with', 'me']\n",
      "After stop words removal: ['k', 'k', 'sms', 'chat']\n",
      "After stemming with porters algorithm: ['sm', 'chat']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['and', 'miss', 'vday', 'the', 'parachute', 'and', 'double', 'coins', 'u', 'must', 'not', 'know', 'me', 'very', 'well']\n",
      "After stop words removal: ['miss', 'vday', 'parachute', 'double', 'coins', 'u', 'must', 'know', 'well']\n",
      "After stemming with porters algorithm: ['miss', 'vdai', 'parachut', 'doubl', 'coin', 'must', 'know', 'well']\n",
      "Tokenized sentence: ['well', 'i', 'm', 'gonna', 'finish', 'my', 'bath', 'now', 'have', 'a', 'good', 'fine', 'night']\n",
      "After stop words removal: ['well', 'gonna', 'finish', 'bath', 'good', 'fine', 'night']\n",
      "After stemming with porters algorithm: ['well', 'gonna', 'finish', 'bath', 'good', 'fine', 'night']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'winner', 'u', 'have', 'been', 'specially', 'selected', 'receive', 'cash', 'or', 'a', 'holiday', 'flights', 'inc', 'speak', 'to', 'a', 'live', 'operator', 'claim']\n",
      "After stop words removal: ['winner', 'u', 'specially', 'selected', 'receive', 'cash', 'holiday', 'flights', 'inc', 'speak', 'live', 'operator', 'claim']\n",
      "After stemming with porters algorithm: ['winner', 'special', 'selec', 'receiv', 'cash', 'holidai', 'flight', 'inc', 'speak', 'live', 'oper', 'claim']\n",
      "Tokenized sentence: ['tunji', 'how', 's', 'the', 'queen', 'how', 'are', 'you', 'doing', 'this', 'is', 'just', 'wishing', 'you', 'a', 'great', 'day', 'abiola']\n",
      "After stop words removal: ['tunji', 'queen', 'wishing', 'great', 'day', 'abiola']\n",
      "wish\n",
      "After stemming with porters algorithm: ['tunji', 'queen', 'wis', 'great', 'dai', 'abiola']\n",
      "Tokenized sentence: ['i', 'am', 'on', 'the', 'way', 'to', 'ur', 'home']\n",
      "After stop words removal: ['way', 'ur', 'home']\n",
      "After stemming with porters algorithm: ['wai', 'home']\n",
      "Tokenized sentence: ['sometimes', 'heart', 'remembrs', 'someone', 'very', 'much', 'forgets', 'someone', 'soon', 'bcoz', 'heart', 'will', 'not', 'like', 'everyone', 'but', 'liked', 'ones', 'will', 'be', 'remembered', 'everytime', 'bslvyl']\n",
      "After stop words removal: ['sometimes', 'heart', 'remembrs', 'someone', 'much', 'forgets', 'someone', 'soon', 'bcoz', 'heart', 'like', 'everyone', 'liked', 'ones', 'remembered', 'everytime', 'bslvyl']\n",
      "After stemming with porters algorithm: ['sometim', 'heart', 'remembr', 'someon', 'much', 'forget', 'someon', 'soon', 'bcoz', 'heart', 'like', 'everyon', 'like', 'on', 'rememb', 'everytim', 'bslvyl']\n",
      "Tokenized sentence: ['i', 'not', 'at', 'home', 'now', 'lei']\n",
      "After stop words removal: ['home', 'lei']\n",
      "After stemming with porters algorithm: ['home', 'lei']\n",
      "Tokenized sentence: ['me', 'also', 'da', 'i', 'feel', 'yesterday', 'night', 'wait', 'til', 'day', 'night', 'dear']\n",
      "After stop words removal: ['also', 'da', 'feel', 'yesterday', 'night', 'wait', 'til', 'day', 'night', 'dear']\n",
      "After stemming with porters algorithm: ['also', 'feel', 'yesterdai', 'night', 'wait', 'til', 'dai', 'night', 'dear']\n",
      "Tokenized sentence: ['we', 're', 'all', 'getting', 'worried', 'over', 'here', 'derek', 'and', 'taylor', 'have', 'already', 'assumed', 'the', 'worst']\n",
      "After stop words removal: ['getting', 'worried', 'derek', 'taylor', 'already', 'assumed', 'worst']\n",
      "gett\n",
      "After stemming with porters algorithm: ['get', 'worri', 'derek', 'taylor', 'alreadi', 'assum', 'worst']\n",
      "Tokenized sentence: ['are', 'you', 'this', 'much', 'buzy']\n",
      "After stop words removal: ['much', 'buzy']\n",
      "After stemming with porters algorithm: ['much', 'buzi']\n",
      "Tokenized sentence: ['then', 'anything', 'special']\n",
      "After stop words removal: ['anything', 'special']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'special']\n",
      "Tokenized sentence: ['oh', 'shit', 'i', 'thought', 'that', 'was', 'your', 'trip', 'loooooool', 'that', 'just', 'makes', 'so', 'much', 'more', 'sense', 'now', 'grins', 'and', 'the', 'sofa', 'reference', 'was', 'the', 'sleep', 'on', 'a', 'couch', 'link', 'you', 'sent', 'me', 'wasn', 't', 'that', 'how', 'you', 'went', 'on', 'your', 'trip', 'oh', 'and', 'didn', 't', 'your', 'babe', 'go', 'with', 'you', 'for', 'that', 'celebration', 'with', 'your', 'rents']\n",
      "After stop words removal: ['oh', 'shit', 'thought', 'trip', 'loooooool', 'makes', 'much', 'sense', 'grins', 'sofa', 'reference', 'sleep', 'couch', 'link', 'sent', 'went', 'trip', 'oh', 'babe', 'go', 'celebration', 'rents']\n",
      "After stemming with porters algorithm: ['shit', 'thought', 'trip', 'loooooool', 'make', 'much', 'sens', 'grin', 'sofa', 'refer', 'sleep', 'couch', 'link', 'sent', 'went', 'trip', 'babe', 'celebr', 'rent']\n",
      "Tokenized sentence: ['even', 'if', 'he', 'my', 'friend', 'he', 'is', 'a', 'priest', 'call', 'him', 'now']\n",
      "After stop words removal: ['even', 'friend', 'priest', 'call']\n",
      "After stemming with porters algorithm: ['even', 'friend', 'priest', 'call']\n",
      "Tokenized sentence: ['i', 'wish', 'i', 'don', 't', 'think', 'its', 'gonna', 'snow', 'that', 'much', 'but', 'it', 'will', 'be', 'more', 'than', 'those', 'flurries', 'we', 'usually', 'get', 'that', 'melt', 'before', 'they', 'hit', 'the', 'ground', 'eek', 'we', 'haven', 't', 'had', 'snow', 'since', 'lt', 'gt', 'before', 'i', 'was', 'even', 'born']\n",
      "After stop words removal: ['wish', 'think', 'gonna', 'snow', 'much', 'flurries', 'usually', 'get', 'melt', 'hit', 'ground', 'eek', 'snow', 'since', 'lt', 'gt', 'even', 'born']\n",
      "After stemming with porters algorithm: ['wish', 'think', 'gonna', 'snow', 'much', 'flurri', 'usual', 'get', 'melt', 'hit', 'ground', 'eek', 'snow', 'sinc', 'even', 'born']\n",
      "Tokenized sentence: ['ok', 'she', 'll', 'be', 'ok', 'i', 'guess']\n",
      "After stop words removal: ['ok', 'ok', 'guess']\n",
      "After stemming with porters algorithm: ['guess']\n",
      "Tokenized sentence: ['dear', 'xxxxxxx', 'u', 've', 'been', 'invited', 'to', 'xchat', 'this', 'is', 'our', 'final', 'attempt', 'to', 'contact', 'u', 'txt', 'chat', 'to', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stop words removal: ['dear', 'xxxxxxx', 'u', 'invited', 'xchat', 'final', 'attempt', 'contact', 'u', 'txt', 'chat', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stemming with porters algorithm: ['dear', 'xxxxxxx', 'invit', 'xchat', 'final', 'attempt', 'contact', 'txt', 'chat', 'msgrcvdhg', 'suit', 'land', 'row', 'ldn', 'yr']\n",
      "Tokenized sentence: ['it', 'has', 'everything', 'to', 'do', 'with', 'the', 'weather', 'keep', 'extra', 'warm', 'its', 'a', 'cold', 'but', 'nothing', 'serious', 'pls', 'lots', 'of', 'vitamin', 'c']\n",
      "After stop words removal: ['everything', 'weather', 'keep', 'extra', 'warm', 'cold', 'nothing', 'serious', 'pls', 'lots', 'vitamin', 'c']\n",
      "everyth\n",
      "noth\n",
      "After stemming with porters algorithm: ['everyt', 'weather', 'keep', 'extra', 'warm', 'cold', 'not', 'seriou', 'pl', 'lot', 'vitamin']\n",
      "Tokenized sentence: ['congratulations', 'ur', 'awarded', 'either', 'of', 'cd', 'gift', 'vouchers', 'free', 'entry', 'our', 'weekly', 'draw', 'txt', 'music', 'to', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stop words removal: ['congratulations', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'weekly', 'draw', 'txt', 'music', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stemming with porters algorithm: ['congratul', 'awar', 'either', 'gift', 'voucher', 'free', 'entri', 'weekli', 'draw', 'txt', 'music', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'ag']\n",
      "Tokenized sentence: ['how', 'about', 'clothes', 'jewelry', 'and', 'trips']\n",
      "After stop words removal: ['clothes', 'jewelry', 'trips']\n",
      "After stemming with porters algorithm: ['cloth', 'jewelri', 'trip']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['wat', 'time', 'finish']\n",
      "After stop words removal: ['wat', 'time', 'finish']\n",
      "After stemming with porters algorithm: ['wat', 'time', 'finish']\n",
      "Tokenized sentence: ['i', 'm', 'home', 'doc', 'gave', 'me', 'pain', 'meds', 'says', 'everything', 'is', 'fine']\n",
      "After stop words removal: ['home', 'doc', 'gave', 'pain', 'meds', 'says', 'everything', 'fine']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['home', 'doc', 'gave', 'pain', 'med', 'sai', 'everyt', 'fine']\n",
      "Tokenized sentence: ['on', 'hen', 'night', 'going', 'with', 'a', 'swing']\n",
      "After stop words removal: ['hen', 'night', 'going', 'swing']\n",
      "go\n",
      "After stemming with porters algorithm: ['hen', 'night', 'go', 'swing']\n",
      "Tokenized sentence: ['aiyah', 'ok', 'wat', 'as', 'long', 'as', 'got', 'improve', 'can', 'already', 'wat']\n",
      "After stop words removal: ['aiyah', 'ok', 'wat', 'long', 'got', 'improve', 'already', 'wat']\n",
      "After stemming with porters algorithm: ['aiyah', 'wat', 'long', 'got', 'improv', 'alreadi', 'wat']\n",
      "Tokenized sentence: ['your', 'account', 'has', 'been', 'credited', 'with', 'free', 'text', 'messages', 'to', 'activate', 'just', 'txt', 'the', 'word', 'credit', 'to', 'no', 't', 'cs', 'www', 'biz']\n",
      "After stop words removal: ['account', 'credited', 'free', 'text', 'messages', 'activate', 'txt', 'word', 'credit', 'cs', 'www', 'biz']\n",
      "After stemming with porters algorithm: ['account', 'credit', 'free', 'text', 'messag', 'activ', 'txt', 'word', 'credit', 'www', 'biz']\n",
      "Tokenized sentence: ['ur', 'hmv', 'quiz', 'cash', 'balance', 'is', 'currently', 'to', 'maximize', 'ur', 'cash', 'in', 'now', 'send', 'hmv', 'to', 'only', 'p', 'msg']\n",
      "After stop words removal: ['ur', 'hmv', 'quiz', 'cash', 'balance', 'currently', 'maximize', 'ur', 'cash', 'send', 'hmv', 'p', 'msg']\n",
      "After stemming with porters algorithm: ['hmv', 'quiz', 'cash', 'balanc', 'current', 'maxim', 'cash', 'send', 'hmv', 'msg']\n",
      "Tokenized sentence: ['babe', 'i', 'lost', 'you']\n",
      "After stop words removal: ['babe', 'lost']\n",
      "After stemming with porters algorithm: ['babe', 'lost']\n",
      "Tokenized sentence: ['better', 'than', 'bb', 'if', 'he', 'wont', 'use', 'it', 'his', 'wife', 'will', 'or', 'them', 'doctor']\n",
      "After stop words removal: ['better', 'bb', 'wont', 'use', 'wife', 'doctor']\n",
      "After stemming with porters algorithm: ['better', 'wont', 'us', 'wife', 'doctor']\n",
      "Tokenized sentence: ['yes', 'gauti', 'and', 'sehwag', 'out', 'of', 'odi', 'series']\n",
      "After stop words removal: ['yes', 'gauti', 'sehwag', 'odi', 'series']\n",
      "After stemming with porters algorithm: ['ye', 'gauti', 'sehwag', 'odi', 'seri']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['pls', 'pls', 'find', 'out', 'from', 'aunt', 'nike']\n",
      "After stop words removal: ['pls', 'pls', 'find', 'aunt', 'nike']\n",
      "After stemming with porters algorithm: ['pl', 'pl', 'find', 'aunt', 'nike']\n",
      "Tokenized sentence: ['didn', 't', 'you', 'get', 'hep', 'b', 'immunisation', 'in', 'nigeria']\n",
      "After stop words removal: ['get', 'hep', 'b', 'immunisation', 'nigeria']\n",
      "After stemming with porters algorithm: ['get', 'hep', 'immunis', 'nigeria']\n",
      "Tokenized sentence: ['romantic', 'paris', 'nights', 'flights', 'from', 'book', 'now', 'next', 'year', 'call', 'ts', 'cs', 'apply']\n",
      "After stop words removal: ['romantic', 'paris', 'nights', 'flights', 'book', 'next', 'year', 'call', 'ts', 'cs', 'apply']\n",
      "After stemming with porters algorithm: ['romant', 'pari', 'night', 'flight', 'book', 'next', 'year', 'call', 'appli']\n",
      "Tokenized sentence: ['i', 'm', 'sorry', 'i', 've', 'joined', 'the', 'league', 'of', 'people', 'that', 'dont', 'keep', 'in', 'touch', 'you', 'mean', 'a', 'great', 'deal', 'to', 'me', 'you', 'have', 'been', 'a', 'friend', 'at', 'all', 'times', 'even', 'at', 'great', 'personal', 'cost', 'do', 'have', 'a', 'great', 'week']\n",
      "After stop words removal: ['sorry', 'joined', 'league', 'people', 'dont', 'keep', 'touch', 'mean', 'great', 'deal', 'friend', 'times', 'even', 'great', 'personal', 'cost', 'great', 'week']\n",
      "After stemming with porters algorithm: ['sorri', 'join', 'leagu', 'peopl', 'dont', 'keep', 'touch', 'mean', 'great', 'deal', 'friend', 'time', 'even', 'great', 'person', 'cost', 'great', 'week']\n",
      "Tokenized sentence: ['lol', 'grins', 'i', 'm', 'not', 'babe', 'but', 'thanks', 'for', 'thinking', 'of', 'me']\n",
      "After stop words removal: ['lol', 'grins', 'babe', 'thanks', 'thinking']\n",
      "think\n",
      "After stemming with porters algorithm: ['lol', 'grin', 'babe', 'thank', 'thin']\n",
      "Tokenized sentence: ['remember', 'to', 'ask', 'alex', 'about', 'his', 'pizza']\n",
      "After stop words removal: ['remember', 'ask', 'alex', 'pizza']\n",
      "After stemming with porters algorithm: ['rememb', 'ask', 'alex', 'pizza']\n",
      "Tokenized sentence: ['yoyyooo', 'u', 'know', 'how', 'to', 'change', 'permissions', 'for', 'a', 'drive', 'in', 'mac', 'my', 'usb', 'flash', 'drive']\n",
      "After stop words removal: ['yoyyooo', 'u', 'know', 'change', 'permissions', 'drive', 'mac', 'usb', 'flash', 'drive']\n",
      "After stemming with porters algorithm: ['yoyyooo', 'know', 'chang', 'permiss', 'drive', 'mac', 'usb', 'flash', 'drive']\n",
      "Tokenized sentence: ['i', 'ain', 't', 'answerin', 'no', 'phone', 'at', 'what', 'is', 'actually', 'a', 'pretty', 'reasonable', 'hour', 'but', 'i', 'm', 'sleepy']\n",
      "After stop words removal: ['answerin', 'phone', 'actually', 'pretty', 'reasonable', 'hour', 'sleepy']\n",
      "After stemming with porters algorithm: ['answerin', 'phone', 'actual', 'pretti', 'reason', 'hour', 'sleepi']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'awar', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr', 'ppm']\n",
      "Tokenized sentence: ['aah', 'bless', 'how', 's', 'your', 'arm']\n",
      "After stop words removal: ['aah', 'bless', 'arm']\n",
      "After stemming with porters algorithm: ['aah', 'bless', 'arm']\n",
      "Tokenized sentence: ['lol', 'well', 'quality', 'aint', 'bad', 'at', 'all', 'so', 'i', 'aint', 'complaining']\n",
      "After stop words removal: ['lol', 'well', 'quality', 'aint', 'bad', 'aint', 'complaining']\n",
      "complain\n",
      "After stemming with porters algorithm: ['lol', 'well', 'qualiti', 'aint', 'bad', 'aint', 'complain']\n",
      "Tokenized sentence: ['your', 'board', 'is', 'working', 'fine', 'the', 'issue', 'of', 'overheating', 'is', 'also', 'reslove', 'but', 'still', 'software', 'inst', 'is', 'pending', 'i', 'will', 'come', 'around', 'o', 'clock']\n",
      "After stop words removal: ['board', 'working', 'fine', 'issue', 'overheating', 'also', 'reslove', 'still', 'software', 'inst', 'pending', 'come', 'around', 'clock']\n",
      "work\n",
      "overheat\n",
      "overheate\n",
      "pend\n",
      "After stemming with porters algorithm: ['board', 'wor', 'fine', 'issu', 'overh', 'also', 'reslov', 'still', 'softwar', 'inst', 'pen', 'come', 'around', 'clock']\n",
      "Tokenized sentence: ['k', 'i', 'm', 'ready', 'lt', 'gt']\n",
      "After stop words removal: ['k', 'ready', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['readi']\n",
      "Tokenized sentence: ['at', 'the', 'funeral', 'home', 'with', 'audrey', 'and', 'dad']\n",
      "After stop words removal: ['funeral', 'home', 'audrey', 'dad']\n",
      "After stemming with porters algorithm: ['funer', 'home', 'audrei', 'dad']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'home', 'please', 'call']\n",
      "After stop words removal: ['home', 'please', 'call']\n",
      "After stemming with porters algorithm: ['home', 'pleas', 'call']\n",
      "Tokenized sentence: ['haf', 'u', 'eaten', 'wat', 'time', 'u', 'wan', 'me', 'come']\n",
      "After stop words removal: ['haf', 'u', 'eaten', 'wat', 'time', 'u', 'wan', 'come']\n",
      "After stemming with porters algorithm: ['haf', 'eaten', 'wat', 'time', 'wan', 'come']\n",
      "Tokenized sentence: ['jus', 'came', 'back', 'fr', 'lunch', 'wif', 'my', 'sis', 'only', 'u', 'leh']\n",
      "After stop words removal: ['jus', 'came', 'back', 'fr', 'lunch', 'wif', 'sis', 'u', 'leh']\n",
      "After stemming with porters algorithm: ['ju', 'came', 'back', 'lunch', 'wif', 'si', 'leh']\n",
      "Tokenized sentence: ['great', 'so', 'what', 'attracts', 'you', 'to', 'the', 'brothas']\n",
      "After stop words removal: ['great', 'attracts', 'brothas']\n",
      "After stemming with porters algorithm: ['great', 'attract', 'brotha']\n",
      "Tokenized sentence: ['laptop', 'i', 'noe', 'infra', 'but', 'too', 'slow', 'lar', 'i', 'wan', 'fast', 'one']\n",
      "After stop words removal: ['laptop', 'noe', 'infra', 'slow', 'lar', 'wan', 'fast', 'one']\n",
      "After stemming with porters algorithm: ['laptop', 'noe', 'infra', 'slow', 'lar', 'wan', 'fast', 'on']\n",
      "Tokenized sentence: ['okies', 'i', 'll', 'go', 'yan', 'jiu', 'too', 'we', 'can', 'skip', 'ard', 'oso', 'go', 'cine', 'den', 'go', 'mrt', 'one', 'blah', 'blah', 'blah']\n",
      "After stop words removal: ['okies', 'go', 'yan', 'jiu', 'skip', 'ard', 'oso', 'go', 'cine', 'den', 'go', 'mrt', 'one', 'blah', 'blah', 'blah']\n",
      "After stemming with porters algorithm: ['oki', 'yan', 'jiu', 'skip', 'ard', 'oso', 'cine', 'den', 'mrt', 'on', 'blah', 'blah', 'blah']\n",
      "Tokenized sentence: ['fr', 'ndship', 'is', 'like', 'a', 'needle', 'of', 'a', 'clock', 'though', 'v', 'r', 'in', 'd', 'same', 'clock', 'v', 'r', 'nt', 'able', 'met', 'evn', 'if', 'v', 'meet', 'itz', 'only', 'few', 'seconds', 'bt', 'v', 'alwys', 'stay', 'conected', 'gud', 't']\n",
      "After stop words removal: ['fr', 'ndship', 'like', 'needle', 'clock', 'though', 'v', 'r', 'clock', 'v', 'r', 'nt', 'able', 'met', 'evn', 'v', 'meet', 'itz', 'seconds', 'bt', 'v', 'alwys', 'stay', 'conected', 'gud']\n",
      "After stemming with porters algorithm: ['ndship', 'like', 'needl', 'clock', 'though', 'clock', 'abl', 'met', 'evn', 'meet', 'itz', 'second', 'alwi', 'stai', 'conec', 'gud']\n",
      "Tokenized sentence: ['i', 'am', 'on', 'the', 'way', 'to', 'tirupur']\n",
      "After stop words removal: ['way', 'tirupur']\n",
      "After stemming with porters algorithm: ['wai', 'tirupur']\n",
      "Tokenized sentence: ['dont', 'flatter', 'yourself', 'tell', 'that', 'man', 'of', 'mine', 'two', 'pints', 'of', 'carlin', 'in', 'ten', 'minutes', 'please']\n",
      "After stop words removal: ['dont', 'flatter', 'tell', 'man', 'mine', 'two', 'pints', 'carlin', 'ten', 'minutes', 'please']\n",
      "After stemming with porters algorithm: ['dont', 'flatter', 'tell', 'man', 'mine', 'two', 'pint', 'carlin', 'ten', 'minut', 'pleas']\n",
      "Tokenized sentence: ['yeah', 'sure', 'thing', 'mate', 'haunt', 'got', 'all', 'my', 'stuff', 'sorted', 'but', 'im', 'going', 'sound', 'anyway', 'promoting', 'hex', 'for', 'by', 'the', 'way', 'who', 'is', 'this', 'dont', 'know', 'number', 'joke']\n",
      "After stop words removal: ['yeah', 'sure', 'thing', 'mate', 'haunt', 'got', 'stuff', 'sorted', 'im', 'going', 'sound', 'anyway', 'promoting', 'hex', 'way', 'dont', 'know', 'number', 'joke']\n",
      "go\n",
      "promot\n",
      "After stemming with porters algorithm: ['yeah', 'sure', 'thing', 'mate', 'haunt', 'got', 'stuff', 'sor', 'go', 'sound', 'anywai', 'promot', 'hex', 'wai', 'dont', 'know', 'number', 'joke']\n",
      "Tokenized sentence: ['your', 'gonna', 'be', 'the', 'death', 'if', 'me', 'i', 'm', 'gonna', 'leave', 'a', 'note', 'that', 'says', 'its', 'all', 'robs', 'fault', 'avenge', 'me']\n",
      "After stop words removal: ['gonna', 'death', 'gonna', 'leave', 'note', 'says', 'robs', 'fault', 'avenge']\n",
      "After stemming with porters algorithm: ['gonna', 'death', 'gonna', 'leav', 'note', 'sai', 'rob', 'fault', 'aveng']\n",
      "Tokenized sentence: ['yup', 'hey', 'then', 'one', 'day', 'on', 'fri', 'we', 'can', 'ask', 'miwa', 'and', 'jiayin', 'take', 'leave', 'go', 'karaoke']\n",
      "After stop words removal: ['yup', 'hey', 'one', 'day', 'fri', 'ask', 'miwa', 'jiayin', 'take', 'leave', 'go', 'karaoke']\n",
      "After stemming with porters algorithm: ['yup', 'hei', 'on', 'dai', 'fri', 'ask', 'miwa', 'jiayin', 'take', 'leav', 'karaok']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'winner', 'u', 'have', 'been', 'specially', 'selected', 'receive', 'or', 'a', 'holiday', 'flights', 'inc', 'speak', 'to', 'a', 'live', 'operator', 'claim', 'p', 'min']\n",
      "After stop words removal: ['winner', 'u', 'specially', 'selected', 'receive', 'holiday', 'flights', 'inc', 'speak', 'live', 'operator', 'claim', 'p', 'min']\n",
      "After stemming with porters algorithm: ['winner', 'special', 'selec', 'receiv', 'holidai', 'flight', 'inc', 'speak', 'live', 'oper', 'claim', 'min']\n",
      "Tokenized sentence: ['when', 'where', 'do', 'i', 'pick', 'you', 'up']\n",
      "After stop words removal: ['pick']\n",
      "After stemming with porters algorithm: ['pick']\n",
      "Tokenized sentence: ['good', 'morning', 'plz', 'call', 'me', 'sir']\n",
      "After stop words removal: ['good', 'morning', 'plz', 'call', 'sir']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'plz', 'call', 'sir']\n",
      "Tokenized sentence: ['its', 'ok', 'come', 'to', 'my', 'home', 'it', 'vl', 'nice', 'to', 'meet', 'and', 'v', 'can', 'chat']\n",
      "After stop words removal: ['ok', 'come', 'home', 'vl', 'nice', 'meet', 'v', 'chat']\n",
      "After stemming with porters algorithm: ['come', 'home', 'nice', 'meet', 'chat']\n",
      "Tokenized sentence: ['but', 'i', 'm', 'surprised', 'she', 'still', 'can', 'guess', 'right', 'lor']\n",
      "After stop words removal: ['surprised', 'still', 'guess', 'right', 'lor']\n",
      "After stemming with porters algorithm: ['surpris', 'still', 'guess', 'right', 'lor']\n",
      "Tokenized sentence: ['or', 'i', 'go', 'home', 'first', 'lar', 'wait', 'me', 'lor', 'i', 'put', 'down', 'my', 'stuff', 'first']\n",
      "After stop words removal: ['go', 'home', 'first', 'lar', 'wait', 'lor', 'put', 'stuff', 'first']\n",
      "After stemming with porters algorithm: ['home', 'first', 'lar', 'wait', 'lor', 'put', 'stuff', 'first']\n",
      "Tokenized sentence: ['how', 'come', 'i', 'din', 'c', 'yup', 'i', 'cut', 'my', 'hair']\n",
      "After stop words removal: ['come', 'din', 'c', 'yup', 'cut', 'hair']\n",
      "After stemming with porters algorithm: ['come', 'din', 'yup', 'cut', 'hair']\n",
      "Tokenized sentence: ['please', 'give', 'it', 'or', 'i', 'will', 'pick', 'it', 'up', 'on', 'tuesday', 'evening', 'about', 'if', 'that', 'is', 'ok']\n",
      "After stop words removal: ['please', 'give', 'pick', 'tuesday', 'evening', 'ok']\n",
      "even\n",
      "After stemming with porters algorithm: ['pleas', 'give', 'pick', 'tuesdai', 'even']\n",
      "Tokenized sentence: ['havent', 'shopping', 'now', 'lor', 'i', 'juz', 'arrive', 'only']\n",
      "After stop words removal: ['havent', 'shopping', 'lor', 'juz', 'arrive']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['havent', 'shop', 'lor', 'juz', 'arriv']\n",
      "Tokenized sentence: ['what', 'is', 'important', 'is', 'that', 'you', 'prevent', 'dehydration', 'by', 'giving', 'her', 'enough', 'fluids']\n",
      "After stop words removal: ['important', 'prevent', 'dehydration', 'giving', 'enough', 'fluids']\n",
      "giv\n",
      "After stemming with porters algorithm: ['import', 'prevent', 'dehydrat', 'give', 'enough', 'fluid']\n",
      "Tokenized sentence: ['okay', 'same', 'with', 'me', 'well', 'thanks', 'for', 'the', 'clarification']\n",
      "After stop words removal: ['okay', 'well', 'thanks', 'clarification']\n",
      "After stemming with porters algorithm: ['okai', 'well', 'thank', 'clarif']\n",
      "Tokenized sentence: ['they', 'don', 't', 'put', 'that', 'stuff', 'on', 'the', 'roads', 'to', 'keep', 'it', 'from', 'getting', 'slippery', 'over', 'there']\n",
      "After stop words removal: ['put', 'stuff', 'roads', 'keep', 'getting', 'slippery']\n",
      "gett\n",
      "After stemming with porters algorithm: ['put', 'stuff', 'road', 'keep', 'get', 'slipperi']\n",
      "Tokenized sentence: ['hey', 'babe', 'how', 's', 'it', 'going', 'did', 'you', 'ever', 'figure', 'out', 'where', 'your', 'going', 'for', 'new', 'years']\n",
      "After stop words removal: ['hey', 'babe', 'going', 'ever', 'figure', 'going', 'new', 'years']\n",
      "go\n",
      "go\n",
      "After stemming with porters algorithm: ['hei', 'babe', 'go', 'ever', 'figur', 'go', 'new', 'year']\n",
      "Tokenized sentence: ['feb', 'lt', 'gt', 'is', 'i', 'love', 'u', 'day', 'send', 'dis', 'to', 'all', 'ur', 'valued', 'frnds', 'evn', 'me', 'if', 'comes', 'back', 'u', 'll', 'gt', 'married', 'd', 'person', 'u', 'luv', 'if', 'u', 'ignore', 'dis', 'u', 'will', 'lose', 'ur', 'luv', 'evr']\n",
      "After stop words removal: ['feb', 'lt', 'gt', 'love', 'u', 'day', 'send', 'dis', 'ur', 'valued', 'frnds', 'evn', 'comes', 'back', 'u', 'gt', 'married', 'person', 'u', 'luv', 'u', 'ignore', 'dis', 'u', 'lose', 'ur', 'luv', 'evr']\n",
      "After stemming with porters algorithm: ['feb', 'love', 'dai', 'send', 'di', 'valu', 'frnd', 'evn', 'come', 'back', 'marri', 'person', 'luv', 'ignor', 'di', 'lose', 'luv', 'evr']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['oh', 'ic', 'i', 'thought', 'you', 'meant', 'mary', 'jane']\n",
      "After stop words removal: ['oh', 'ic', 'thought', 'meant', 'mary', 'jane']\n",
      "After stemming with porters algorithm: ['thought', 'meant', 'mari', 'jane']\n",
      "Tokenized sentence: ['it', 'so', 'happens', 'that', 'there', 'r', 'waxsto', 'do', 'wat', 'you', 'want', 'she', 'can', 'come', 'and', 'ill', 'get', 'her', 'medical', 'insurance', 'and', 'she', 'll', 'be', 'able', 'to', 'deliver', 'and', 'have', 'basic', 'care', 'i', 'm', 'currently', 'shopping', 'for', 'the', 'right', 'medical', 'insurance', 'for', 'her', 'so', 'just', 'give', 'me', 'til', 'friday', 'morning', 'thats', 'when', 'i', 'll', 'see', 'the', 'major', 'person', 'that', 'can', 'guide', 'me', 'to', 'the', 'right', 'insurance']\n",
      "After stop words removal: ['happens', 'r', 'waxsto', 'wat', 'want', 'come', 'ill', 'get', 'medical', 'insurance', 'able', 'deliver', 'basic', 'care', 'currently', 'shopping', 'right', 'medical', 'insurance', 'give', 'til', 'friday', 'morning', 'thats', 'see', 'major', 'person', 'guide', 'right', 'insurance']\n",
      "shopp\n",
      "morn\n",
      "After stemming with porters algorithm: ['happen', 'waxsto', 'wat', 'want', 'come', 'ill', 'get', 'medic', 'insur', 'abl', 'deliv', 'basic', 'care', 'current', 'shop', 'right', 'medic', 'insur', 'give', 'til', 'fridai', 'mor', 'that', 'see', 'major', 'person', 'guid', 'right', 'insur']\n",
      "Tokenized sentence: ['not', 'heard', 'from', 'u', 'a', 'while', 'call', 'rude', 'chat', 'private', 'line', 'to', 'cum', 'wan', 'c', 'pics', 'of', 'me', 'gettin', 'shagged', 'then', 'text', 'pix', 'to', 'end', 'send', 'stop', 'sam', 'xxx']\n",
      "After stop words removal: ['heard', 'u', 'call', 'rude', 'chat', 'private', 'line', 'cum', 'wan', 'c', 'pics', 'gettin', 'shagged', 'text', 'pix', 'end', 'send', 'stop', 'sam', 'xxx']\n",
      "After stemming with porters algorithm: ['heard', 'call', 'rude', 'chat', 'privat', 'line', 'cum', 'wan', 'pic', 'gettin', 'shag', 'text', 'pix', 'end', 'send', 'stop', 'sam', 'xxx']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later', 'in', 'meeting']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'meet']\n",
      "Tokenized sentence: ['santa', 'calling', 'would', 'your', 'little', 'ones', 'like', 'a', 'call', 'from', 'santa', 'xmas', 'eve', 'call', 'to', 'book', 'you', 'time', 'calls', 'ppm', 'last', 'mins', 's', 't', 'c', 'www', 'santacalling', 'com']\n",
      "After stop words removal: ['santa', 'calling', 'would', 'little', 'ones', 'like', 'call', 'santa', 'xmas', 'eve', 'call', 'book', 'time', 'calls', 'ppm', 'last', 'mins', 'c', 'www', 'santacalling', 'com']\n",
      "call\n",
      "santacall\n",
      "After stemming with porters algorithm: ['santa', 'call', 'would', 'littl', 'on', 'like', 'call', 'santa', 'xma', 'ev', 'call', 'book', 'time', 'call', 'ppm', 'last', 'min', 'www', 'santacal', 'com']\n",
      "Tokenized sentence: ['aiyo', 'u', 'so', 'poor', 'thing', 'then', 'u', 'dun', 'wan', 'eat', 'u', 'bathe', 'already']\n",
      "After stop words removal: ['aiyo', 'u', 'poor', 'thing', 'u', 'dun', 'wan', 'eat', 'u', 'bathe', 'already']\n",
      "After stemming with porters algorithm: ['aiyo', 'poor', 'thing', 'dun', 'wan', 'eat', 'bath', 'alreadi']\n",
      "Tokenized sentence: ['babe', 'you', 'said', 'hours', 'and', 'it', 's', 'been', 'almost', 'is', 'your', 'internet', 'down']\n",
      "After stop words removal: ['babe', 'said', 'hours', 'almost', 'internet']\n",
      "After stemming with porters algorithm: ['babe', 'said', 'hour', 'almost', 'internet']\n",
      "Tokenized sentence: ['i', 'm', 'watching', 'lotr', 'w', 'my', 'sis', 'dis', 'aft', 'so', 'u', 'wan', 'meet', 'me', 'dinner', 'at', 'nite', 'a', 'not']\n",
      "After stop words removal: ['watching', 'lotr', 'w', 'sis', 'dis', 'aft', 'u', 'wan', 'meet', 'dinner', 'nite']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'lotr', 'si', 'di', 'aft', 'wan', 'meet', 'dinner', 'nite']\n",
      "Tokenized sentence: ['hi', 'ibh', 'customer', 'loyalty', 'offer', 'the', 'new', 'nokia', 'mobile', 'from', 'only', 'at', 'txtauction', 'txt', 'word', 'start', 'to', 'no', 'get', 'yours', 'now', 't']\n",
      "After stop words removal: ['hi', 'ibh', 'customer', 'loyalty', 'offer', 'new', 'nokia', 'mobile', 'txtauction', 'txt', 'word', 'start', 'get']\n",
      "After stemming with porters algorithm: ['ibh', 'custom', 'loyalti', 'offer', 'new', 'nokia', 'mobil', 'txtauct', 'txt', 'word', 'start', 'get']\n",
      "Tokenized sentence: ['comin', 'to', 'fetch', 'us', 'oredi']\n",
      "After stop words removal: ['comin', 'fetch', 'us', 'oredi']\n",
      "After stemming with porters algorithm: ['comin', 'fetch', 'oredi']\n",
      "Tokenized sentence: ['say', 'this', 'slowly', 'god', 'i', 'love', 'you', 'amp', 'i', 'need', 'you', 'clean', 'my', 'heart', 'with', 'your', 'blood', 'send', 'this', 'to', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'do', 'it', 'pls', 'pls', 'do', 'it']\n",
      "After stop words removal: ['say', 'slowly', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'people', 'amp', 'u', 'c', 'miracle', 'tomorrow', 'pls', 'pls']\n",
      "After stemming with porters algorithm: ['sai', 'slowli', 'god', 'love', 'amp', 'need', 'clean', 'heart', 'blood', 'send', 'ten', 'special', 'peopl', 'amp', 'mirac', 'tomorrow', 'pl', 'pl']\n",
      "Tokenized sentence: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stop words removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stemming with porters algorithm: ['free', 'week', 'nokia', 'tone', 'mob', 'everi', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'mate', 'www', 'getz', 'pobox', 'norm', 'tone']\n",
      "Tokenized sentence: ['tunde', 'how', 'are', 'you', 'doing', 'this', 'is', 'just', 'wishing', 'you', 'a', 'great', 'day', 'abiola']\n",
      "After stop words removal: ['tunde', 'wishing', 'great', 'day', 'abiola']\n",
      "wish\n",
      "After stemming with porters algorithm: ['tund', 'wis', 'great', 'dai', 'abiola']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'a', 'movie', 'call', 'me', 'wat']\n",
      "After stop words removal: ['movie', 'call', 'wat']\n",
      "After stemming with porters algorithm: ['movi', 'call', 'wat']\n",
      "Tokenized sentence: ['am', 'i', 'that', 'much', 'bad', 'to', 'avoid', 'like', 'this']\n",
      "After stop words removal: ['much', 'bad', 'avoid', 'like']\n",
      "After stemming with porters algorithm: ['much', 'bad', 'avoid', 'like']\n",
      "Tokenized sentence: ['i', 'll', 'hand', 'her', 'my', 'phone', 'to', 'chat', 'wit', 'u']\n",
      "After stop words removal: ['hand', 'phone', 'chat', 'wit', 'u']\n",
      "After stemming with porters algorithm: ['hand', 'phone', 'chat', 'wit']\n",
      "Tokenized sentence: ['not', 'sure', 'i', 'have', 'the', 'stomach', 'for', 'it']\n",
      "After stop words removal: ['sure', 'stomach']\n",
      "After stemming with porters algorithm: ['sure', 'stomach']\n",
      "Tokenized sentence: ['jane', 'babes', 'not', 'goin', 'wrk', 'feel', 'ill', 'after', 'lst', 'nite', 'foned', 'in', 'already', 'cover', 'me', 'chuck']\n",
      "After stop words removal: ['jane', 'babes', 'goin', 'wrk', 'feel', 'ill', 'lst', 'nite', 'foned', 'already', 'cover', 'chuck']\n",
      "After stemming with porters algorithm: ['jane', 'babe', 'goin', 'wrk', 'feel', 'ill', 'lst', 'nite', 'fone', 'alreadi', 'cover', 'chuck']\n",
      "Tokenized sentence: ['your', 'free', 'ringtone', 'is', 'waiting', 'to', 'be', 'collected', 'simply', 'text', 'the', 'password', 'mix', 'to', 'to', 'verify', 'get', 'usher', 'and', 'britney', 'fml']\n",
      "After stop words removal: ['free', 'ringtone', 'waiting', 'collected', 'simply', 'text', 'password', 'mix', 'verify', 'get', 'usher', 'britney', 'fml']\n",
      "wait\n",
      "After stemming with porters algorithm: ['free', 'rington', 'wait', 'collec', 'simpli', 'text', 'password', 'mix', 'verifi', 'get', 'usher', 'britnei', 'fml']\n",
      "Tokenized sentence: ['ya', 'ok', 'vikky', 'vl', 'c', 'witin', 'lt', 'gt', 'mins', 'and', 'il', 'reply', 'u']\n",
      "After stop words removal: ['ya', 'ok', 'vikky', 'vl', 'c', 'witin', 'lt', 'gt', 'mins', 'il', 'reply', 'u']\n",
      "After stemming with porters algorithm: ['vikki', 'witin', 'min', 'repli']\n",
      "Tokenized sentence: ['its', 'lt', 'gt', 'k', 'here', 'oh', 'should', 'i', 'send', 'home', 'for', 'sale']\n",
      "After stop words removal: ['lt', 'gt', 'k', 'oh', 'send', 'home', 'sale']\n",
      "After stemming with porters algorithm: ['send', 'home', 'sale']\n",
      "Tokenized sentence: ['xxxmobilemovieclub', 'to', 'use', 'your', 'credit', 'click', 'the', 'wap', 'link', 'in', 'the', 'next', 'txt', 'message', 'or', 'click', 'here', 'http', 'wap', 'xxxmobilemovieclub', 'com', 'n', 'qjkgighjjgcbl']\n",
      "After stop words removal: ['xxxmobilemovieclub', 'use', 'credit', 'click', 'wap', 'link', 'next', 'txt', 'message', 'click', 'http', 'wap', 'xxxmobilemovieclub', 'com', 'n', 'qjkgighjjgcbl']\n",
      "After stemming with porters algorithm: ['xxxmobilemovieclub', 'us', 'credit', 'click', 'wap', 'link', 'next', 'txt', 'messag', 'click', 'http', 'wap', 'xxxmobilemovieclub', 'com', 'qjkgighjjgcbl']\n",
      "Tokenized sentence: ['free', 'ring', 'tone', 'just', 'text', 'polys', 'to', 'then', 'every', 'week', 'get', 'a', 'new', 'tone', 'yrs', 'only', 'wk']\n",
      "After stop words removal: ['free', 'ring', 'tone', 'text', 'polys', 'every', 'week', 'get', 'new', 'tone', 'yrs', 'wk']\n",
      "After stemming with porters algorithm: ['free', 'ring', 'tone', 'text', 'poli', 'everi', 'week', 'get', 'new', 'tone', 'yr']\n",
      "Tokenized sentence: ['a', 'little', 'meds', 'say', 'take', 'once', 'every', 'hours', 'it', 's', 'only', 'been', 'but', 'pain', 'is', 'back', 'so', 'i', 'took', 'another', 'hope', 'i', 'don', 't', 'die']\n",
      "After stop words removal: ['little', 'meds', 'say', 'take', 'every', 'hours', 'pain', 'back', 'took', 'another', 'hope', 'die']\n",
      "After stemming with porters algorithm: ['littl', 'med', 'sai', 'take', 'everi', 'hour', 'pain', 'back', 'took', 'anoth', 'hope', 'die']\n",
      "Tokenized sentence: ['wow', 'didn', 't', 'think', 'it', 'was', 'that', 'common', 'i', 'take', 'it', 'all', 'back', 'ur', 'not', 'a', 'freak', 'unless', 'u', 'chop', 'it', 'off']\n",
      "After stop words removal: ['wow', 'think', 'common', 'take', 'back', 'ur', 'freak', 'unless', 'u', 'chop']\n",
      "After stemming with porters algorithm: ['wow', 'think', 'common', 'take', 'back', 'freak', 'unless', 'chop']\n",
      "Tokenized sentence: ['response', 'is', 'one', 'of', 'd', 'powerful', 'weapon', 'occupy', 'a', 'place', 'in', 'others', 'heart', 'so']\n",
      "After stop words removal: ['response', 'one', 'powerful', 'weapon', 'occupy', 'place', 'others', 'heart']\n",
      "After stemming with porters algorithm: ['respons', 'on', 'power', 'weapon', 'occupi', 'place', 'other', 'heart']\n",
      "Tokenized sentence: ['aight', 'i', 'm', 'chillin', 'in', 'a', 'friend', 's', 'room', 'so', 'text', 'me', 'when', 'you', 're', 'on', 'the', 'way']\n",
      "After stop words removal: ['aight', 'chillin', 'friend', 'room', 'text', 'way']\n",
      "After stemming with porters algorithm: ['aight', 'chillin', 'friend', 'room', 'text', 'wai']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'as', 'a', 'valued', 'vodafone', 'customer', 'our', 'computer', 'has', 'picked', 'you', 'to', 'win', 'a', 'prize', 'to', 'collect', 'is', 'easy', 'just', 'call']\n",
      "After stop words removal: ['valued', 'vodafone', 'customer', 'computer', 'picked', 'win', 'prize', 'collect', 'easy', 'call']\n",
      "After stemming with porters algorithm: ['valu', 'vodafon', 'custom', 'comput', 'pic', 'win', 'priz', 'collect', 'easi', 'call']\n",
      "Tokenized sentence: ['jus', 'finish', 'my', 'lunch', 'on', 'my', 'way', 'home', 'lor', 'i', 'tot', 'u', 'dun', 'wan', 'stay', 'in', 'sch', 'today']\n",
      "After stop words removal: ['jus', 'finish', 'lunch', 'way', 'home', 'lor', 'tot', 'u', 'dun', 'wan', 'stay', 'sch', 'today']\n",
      "After stemming with porters algorithm: ['ju', 'finish', 'lunch', 'wai', 'home', 'lor', 'tot', 'dun', 'wan', 'stai', 'sch', 'todai']\n",
      "Tokenized sentence: ['auction', 'round', 'the', 'highest', 'bid', 'is', 'now', 'next', 'maximum', 'bid', 'is', 'to', 'bid', 'send', 'bids', 'e', 'g', 'to', 'bid', 'to', 'good', 'luck']\n",
      "After stop words removal: ['auction', 'round', 'highest', 'bid', 'next', 'maximum', 'bid', 'bid', 'send', 'bids', 'e', 'g', 'bid', 'good', 'luck']\n",
      "After stemming with porters algorithm: ['auct', 'round', 'highest', 'bid', 'next', 'maximum', 'bid', 'bid', 'send', 'bid', 'bid', 'good', 'luck']\n",
      "Tokenized sentence: ['what', 'number', 'do', 'u', 'live', 'at', 'is', 'it']\n",
      "After stop words removal: ['number', 'u', 'live']\n",
      "After stemming with porters algorithm: ['number', 'live']\n",
      "Tokenized sentence: ['then', 'why', 'you', 'not', 'responding']\n",
      "After stop words removal: ['responding']\n",
      "respond\n",
      "After stemming with porters algorithm: ['respon']\n",
      "Tokenized sentence: ['i', 'am', 'not', 'sure', 'about', 'night', 'menu', 'i', 'know', 'only', 'about', 'noon', 'menu']\n",
      "After stop words removal: ['sure', 'night', 'menu', 'know', 'noon', 'menu']\n",
      "After stemming with porters algorithm: ['sure', 'night', 'menu', 'know', 'noon', 'menu']\n",
      "Tokenized sentence: ['todays', 'voda', 'numbers', 'ending', 'are', 'selected', 'to', 'receive', 'a', 'award', 'if', 'you', 'have', 'a', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "After stop words removal: ['todays', 'voda', 'numbers', 'ending', 'selected', 'receive', 'award', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "end\n",
      "quot\n",
      "After stemming with porters algorithm: ['todai', 'voda', 'number', 'en', 'selec', 'receiv', 'award', 'match', 'pleas', 'call', 'quot', 'claim', 'code', 'standard', 'rate', 'app']\n",
      "Tokenized sentence: ['oh', 'great', 'i', 'll', 'disturb', 'him', 'more', 'so', 'that', 'we', 'can', 'talk']\n",
      "After stop words removal: ['oh', 'great', 'disturb', 'talk']\n",
      "After stemming with porters algorithm: ['great', 'disturb', 'talk']\n",
      "Tokenized sentence: ['r', 'u', 'in', 'this', 'continent']\n",
      "After stop words removal: ['r', 'u', 'continent']\n",
      "After stemming with porters algorithm: ['contin']\n",
      "Tokenized sentence: ['but', 'i', 'have', 'to', 'i', 'like', 'to', 'have', 'love', 'and', 'arrange']\n",
      "After stop words removal: ['like', 'love', 'arrange']\n",
      "After stemming with porters algorithm: ['like', 'love', 'arrang']\n",
      "Tokenized sentence: ['k', 'come', 'to', 'nordstrom', 'when', 'you', 're', 'done']\n",
      "After stop words removal: ['k', 'come', 'nordstrom', 'done']\n",
      "After stemming with porters algorithm: ['come', 'nordstrom', 'done']\n",
      "Tokenized sentence: ['guy', 'no', 'flash', 'me', 'now', 'if', 'you', 'go', 'call', 'me', 'call', 'me', 'how', 'madam', 'take', 'care', 'oh']\n",
      "After stop words removal: ['guy', 'flash', 'go', 'call', 'call', 'madam', 'take', 'care', 'oh']\n",
      "After stemming with porters algorithm: ['gui', 'flash', 'call', 'call', 'madam', 'take', 'care']\n",
      "Tokenized sentence: ['anytime', 'lor']\n",
      "After stop words removal: ['anytime', 'lor']\n",
      "After stemming with porters algorithm: ['anytim', 'lor']\n",
      "Tokenized sentence: ['winner', 'as', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'prize', 'reward', 'to', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hours', 'only']\n",
      "After stop words removal: ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hours']\n",
      "After stemming with porters algorithm: ['winner', 'valu', 'network', 'custom', 'selec', 'receivea', 'priz', 'reward', 'claim', 'call', 'claim', 'code', 'valid', 'hour']\n",
      "Tokenized sentence: ['dun', 'need', 'to', 'use', 'dial', 'up', 'juz', 'open', 'da', 'browser', 'n', 'surf']\n",
      "After stop words removal: ['dun', 'need', 'use', 'dial', 'juz', 'open', 'da', 'browser', 'n', 'surf']\n",
      "After stemming with porters algorithm: ['dun', 'need', 'us', 'dial', 'juz', 'open', 'browser', 'surf']\n",
      "Tokenized sentence: ['i', 'accidentally', 'brought', 'em', 'home', 'in', 'the', 'box']\n",
      "After stop words removal: ['accidentally', 'brought', 'em', 'home', 'box']\n",
      "After stemming with porters algorithm: ['accid', 'brought', 'home', 'box']\n",
      "Tokenized sentence: ['ya', 'when', 'are', 'taking', 'ure', 'practical', 'lessons', 'i', 'start', 'in', 'june']\n",
      "After stop words removal: ['ya', 'taking', 'ure', 'practical', 'lessons', 'start', 'june']\n",
      "tak\n",
      "After stemming with porters algorithm: ['take', 'ur', 'practic', 'lesson', 'start', 'june']\n",
      "Tokenized sentence: ['was', 'gr', 'to', 'see', 'that', 'message', 'so', 'when', 'r', 'u', 'leaving', 'congrats', 'dear', 'what', 'school', 'and', 'wat', 'r', 'ur', 'plans']\n",
      "After stop words removal: ['gr', 'see', 'message', 'r', 'u', 'leaving', 'congrats', 'dear', 'school', 'wat', 'r', 'ur', 'plans']\n",
      "leav\n",
      "After stemming with porters algorithm: ['see', 'messag', 'leav', 'congrat', 'dear', 'school', 'wat', 'plan']\n",
      "Tokenized sentence: ['ugh', 'i', 'don', 't', 'wanna', 'get', 'out', 'of', 'bed', 'it', 's', 'so', 'warm']\n",
      "After stop words removal: ['ugh', 'wanna', 'get', 'bed', 'warm']\n",
      "After stemming with porters algorithm: ['ugh', 'wanna', 'get', 'bed', 'warm']\n",
      "Tokenized sentence: ['sms', 'services', 'for', 'your', 'inclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'with', 'stop', 'no', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stop words removal: ['sms', 'services', 'inclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'stop', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stemming with porters algorithm: ['sm', 'servic', 'inclus', 'text', 'credit', 'pl', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscrib', 'stop', 'extra', 'charg', 'help', 'comuk']\n",
      "Tokenized sentence: ['k', 'then', 'marrow', 'are', 'you', 'coming', 'to', 'class']\n",
      "After stop words removal: ['k', 'marrow', 'coming', 'class']\n",
      "com\n",
      "After stemming with porters algorithm: ['marrow', 'come', 'class']\n",
      "Tokenized sentence: ['hi', 'its', 'kate', 'can', 'u', 'give', 'me', 'a', 'ring', 'asap', 'xxx']\n",
      "After stop words removal: ['hi', 'kate', 'u', 'give', 'ring', 'asap', 'xxx']\n",
      "After stemming with porters algorithm: ['kate', 'give', 'ring', 'asap', 'xxx']\n",
      "Tokenized sentence: ['then', 'what', 'about', 'further', 'plan']\n",
      "After stop words removal: ['plan']\n",
      "After stemming with porters algorithm: ['plan']\n",
      "Tokenized sentence: ['mm', 'you', 'ask', 'him', 'to', 'come', 'its', 'enough']\n",
      "After stop words removal: ['mm', 'ask', 'come', 'enough']\n",
      "After stemming with porters algorithm: ['ask', 'come', 'enough']\n",
      "Tokenized sentence: ['wat', 'makes', 'some', 'people', 'dearer', 'is', 'not', 'just', 'de', 'happiness', 'dat', 'u', 'feel', 'when', 'u', 'meet', 'them', 'but', 'de', 'pain', 'u', 'feel', 'when', 'u', 'miss', 'dem']\n",
      "After stop words removal: ['wat', 'makes', 'people', 'dearer', 'de', 'happiness', 'dat', 'u', 'feel', 'u', 'meet', 'de', 'pain', 'u', 'feel', 'u', 'miss', 'dem']\n",
      "After stemming with porters algorithm: ['wat', 'make', 'peopl', 'dearer', 'happi', 'dat', 'feel', 'meet', 'pain', 'feel', 'miss', 'dem']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'a', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['we', 'spend', 'our', 'days', 'waiting', 'for', 'the', 'ideal', 'path', 'to', 'appear', 'in', 'front', 'of', 'us', 'but', 'what', 'we', 'forget', 'is', 'paths', 'are', 'made', 'by', 'walking', 'not', 'by', 'waiting', 'goodnight']\n",
      "After stop words removal: ['spend', 'days', 'waiting', 'ideal', 'path', 'appear', 'front', 'us', 'forget', 'paths', 'made', 'walking', 'waiting', 'goodnight']\n",
      "wait\n",
      "walk\n",
      "wait\n",
      "After stemming with porters algorithm: ['spend', 'dai', 'wait', 'ideal', 'path', 'appear', 'front', 'forget', 'path', 'made', 'wal', 'wait', 'goodnight']\n",
      "Tokenized sentence: ['nokia', 'phone', 'is', 'lovly']\n",
      "After stop words removal: ['nokia', 'phone', 'lovly']\n",
      "After stemming with porters algorithm: ['nokia', 'phone', 'lovli']\n",
      "Tokenized sentence: ['any', 'way', 'where', 'are', 'you', 'and', 'what', 'doing']\n",
      "After stop words removal: ['way']\n",
      "After stemming with porters algorithm: ['wai']\n",
      "Tokenized sentence: ['call', 'me', 'when', 'you', 'carlos', 'is', 'are', 'here', 'my', 'phone', 's', 'vibrate', 'is', 'acting', 'up', 'and', 'i', 'might', 'not', 'hear', 'texts']\n",
      "After stop words removal: ['call', 'carlos', 'phone', 'vibrate', 'acting', 'might', 'hear', 'texts']\n",
      "act\n",
      "After stemming with porters algorithm: ['call', 'carlo', 'phone', 'vibrat', 'ac', 'might', 'hear', 'text']\n",
      "Tokenized sentence: ['sent', 'me', 'ur', 'email', 'id', 'soon']\n",
      "After stop words removal: ['sent', 'ur', 'email', 'id', 'soon']\n",
      "After stemming with porters algorithm: ['sent', 'email', 'soon']\n",
      "Tokenized sentence: ['ha', 'you', 'don', 't', 'know', 'either', 'i', 'did', 'a', 'a', 'clever', 'but', 'simple', 'thing', 'with', 'pears', 'the', 'other', 'day', 'perfect', 'for', 'christmas']\n",
      "After stop words removal: ['ha', 'know', 'either', 'clever', 'simple', 'thing', 'pears', 'day', 'perfect', 'christmas']\n",
      "After stemming with porters algorithm: ['know', 'either', 'clever', 'simpl', 'thing', 'pear', 'dai', 'perfect', 'christma']\n",
      "Tokenized sentence: ['they', 'did', 't', 'play', 'one', 'day', 'last', 'year', 'know', 'even', 'though', 'they', 'have', 'very', 'good', 'team', 'like', 'india']\n",
      "After stop words removal: ['play', 'one', 'day', 'last', 'year', 'know', 'even', 'though', 'good', 'team', 'like', 'india']\n",
      "After stemming with porters algorithm: ['plai', 'on', 'dai', 'last', 'year', 'know', 'even', 'though', 'good', 'team', 'like', 'india']\n",
      "Tokenized sentence: ['hi', 'you', 'just', 'spoke', 'to', 'maneesha', 'v', 'we', 'd', 'like', 'to', 'know', 'if', 'you', 'were', 'satisfied', 'with', 'the', 'experience', 'reply', 'toll', 'free', 'with', 'yes', 'or', 'no']\n",
      "After stop words removal: ['hi', 'spoke', 'maneesha', 'v', 'like', 'know', 'satisfied', 'experience', 'reply', 'toll', 'free', 'yes']\n",
      "After stemming with porters algorithm: ['spoke', 'maneesha', 'like', 'know', 'satisfi', 'experi', 'repli', 'toll', 'free', 'ye']\n",
      "Tokenized sentence: ['probably', 'gonna', 'be', 'here', 'for', 'a', 'while', 'see', 'you', 'later', 'tonight', 'lt']\n",
      "After stop words removal: ['probably', 'gonna', 'see', 'later', 'tonight', 'lt']\n",
      "After stemming with porters algorithm: ['probab', 'gonna', 'see', 'later', 'tonight']\n",
      "Tokenized sentence: ['am', 'on', 'a', 'train', 'back', 'from', 'northampton', 'so', 'i', 'm', 'afraid', 'not']\n",
      "After stop words removal: ['train', 'back', 'northampton', 'afraid']\n",
      "After stemming with porters algorithm: ['train', 'back', 'northampton', 'afraid']\n",
      "Tokenized sentence: ['do', 'u', 'noe', 'how', 'send', 'files', 'between', 'computers']\n",
      "After stop words removal: ['u', 'noe', 'send', 'files', 'computers']\n",
      "After stemming with porters algorithm: ['noe', 'send', 'file', 'comput']\n",
      "Tokenized sentence: ['night', 'has', 'ended', 'for', 'another', 'day', 'morning', 'has', 'come', 'in', 'a', 'special', 'way', 'may', 'you', 'smile', 'like', 'the', 'sunny', 'rays', 'and', 'leaves', 'your', 'worries', 'at', 'the', 'blue', 'blue', 'bay']\n",
      "After stop words removal: ['night', 'ended', 'another', 'day', 'morning', 'come', 'special', 'way', 'may', 'smile', 'like', 'sunny', 'rays', 'leaves', 'worries', 'blue', 'blue', 'bay']\n",
      "morn\n",
      "After stemming with porters algorithm: ['night', 'en', 'anoth', 'dai', 'mor', 'come', 'special', 'wai', 'mai', 'smile', 'like', 'sunni', 'rai', 'leav', 'worri', 'blue', 'blue', 'bai']\n",
      "Tokenized sentence: ['life', 'spend', 'with', 'someone', 'for', 'a', 'lifetime', 'may', 'be', 'meaningless', 'but', 'a', 'few', 'moments', 'spent', 'with', 'someone', 'who', 'really', 'love', 'you', 'means', 'more', 'than', 'life', 'itself']\n",
      "After stop words removal: ['life', 'spend', 'someone', 'lifetime', 'may', 'meaningless', 'moments', 'spent', 'someone', 'really', 'love', 'means', 'life']\n",
      "After stemming with porters algorithm: ['life', 'spend', 'someon', 'lifetim', 'mai', 'meaningless', 'moment', 'spent', 'someon', 'realli', 'love', 'mean', 'life']\n",
      "Tokenized sentence: ['babe', 'i', 'lost', 'you', 'will', 'you', 'try', 'rebooting']\n",
      "After stop words removal: ['babe', 'lost', 'try', 'rebooting']\n",
      "reboot\n",
      "After stemming with porters algorithm: ['babe', 'lost', 'try', 'reboot']\n",
      "Tokenized sentence: ['or', 'i', 'guess', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['guess', 'lt', 'gt', 'min']\n",
      "After stemming with porters algorithm: ['guess', 'min']\n",
      "Tokenized sentence: ['ok', 'lor']\n",
      "After stop words removal: ['ok', 'lor']\n",
      "After stemming with porters algorithm: ['lor']\n",
      "Tokenized sentence: ['you', 'sure', 'your', 'neighbors', 'didnt', 'pick', 'it', 'up']\n",
      "After stop words removal: ['sure', 'neighbors', 'didnt', 'pick']\n",
      "After stemming with porters algorithm: ['sure', 'neighbor', 'didnt', 'pick']\n",
      "Tokenized sentence: ['yo', 'my', 'trip', 'got', 'postponed', 'you', 'still', 'stocked', 'up']\n",
      "After stop words removal: ['yo', 'trip', 'got', 'postponed', 'still', 'stocked']\n",
      "After stemming with porters algorithm: ['trip', 'got', 'postpon', 'still', 'stoc']\n",
      "Tokenized sentence: ['soon', 'you', 'will', 'have', 'the', 'real', 'thing', 'princess', 'do', 'i', 'make', 'you', 'wet']\n",
      "After stop words removal: ['soon', 'real', 'thing', 'princess', 'make', 'wet']\n",
      "After stemming with porters algorithm: ['soon', 'real', 'thing', 'princess', 'make', 'wet']\n",
      "Tokenized sentence: ['r', 'u', 'meeting', 'da', 'ge', 'at', 'nite', 'tmr']\n",
      "After stop words removal: ['r', 'u', 'meeting', 'da', 'ge', 'nite', 'tmr']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'nite', 'tmr']\n",
      "Tokenized sentence: ['hmm', 'yeah', 'if', 'your', 'not', 'too', 'grooved', 'out', 'and', 'im', 'looking', 'forward', 'to', 'my', 'pound', 'special']\n",
      "After stop words removal: ['hmm', 'yeah', 'grooved', 'im', 'looking', 'forward', 'pound', 'special']\n",
      "look\n",
      "After stemming with porters algorithm: ['hmm', 'yeah', 'groov', 'look', 'forward', 'pound', 'special']\n",
      "Tokenized sentence: ['nothing', 'just', 'getting', 'msgs', 'by', 'dis', 'name', 'wit', 'different', 'no', 's']\n",
      "After stop words removal: ['nothing', 'getting', 'msgs', 'dis', 'name', 'wit', 'different']\n",
      "noth\n",
      "gett\n",
      "After stemming with porters algorithm: ['not', 'get', 'msg', 'di', 'name', 'wit', 'differ']\n",
      "Tokenized sentence: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'a', 'top', 'sony', 'dvd', 'player', 'if', 'u', 'know', 'which', 'country', 'liverpool', 'played', 'in', 'mid', 'week', 'txt', 'ansr', 'to', 'sp', 'tyrone']\n",
      "After stop words removal: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'top', 'sony', 'dvd', 'player', 'u', 'know', 'country', 'liverpool', 'played', 'mid', 'week', 'txt', 'ansr', 'sp', 'tyrone']\n",
      "After stemming with porters algorithm: ['sunshin', 'quiz', 'wkly', 'win', 'top', 'soni', 'dvd', 'player', 'know', 'countri', 'liverpool', 'plai', 'mid', 'week', 'txt', 'ansr', 'tyrone']\n",
      "Tokenized sentence: ['yes', 'my', 'reg', 'is', 'ciao']\n",
      "After stop words removal: ['yes', 'reg', 'ciao']\n",
      "After stemming with porters algorithm: ['ye', 'reg', 'ciao']\n",
      "Tokenized sentence: ['bored', 'housewives', 'chat', 'n', 'date', 'now', 'bt', 'national', 'rate', 'p', 'min', 'only', 'from', 'landlines']\n",
      "After stop words removal: ['bored', 'housewives', 'chat', 'n', 'date', 'bt', 'national', 'rate', 'p', 'min', 'landlines']\n",
      "After stemming with porters algorithm: ['bore', 'housew', 'chat', 'date', 'nat', 'rate', 'min', 'landlin']\n",
      "Tokenized sentence: ['you', 'have', 'new', 'voicemail', 'please', 'call']\n",
      "After stop words removal: ['new', 'voicemail', 'please', 'call']\n",
      "After stemming with porters algorithm: ['new', 'voicemail', 'pleas', 'call']\n",
      "Tokenized sentence: ['dont', 'search', 'love', 'let', 'love', 'find', 'u', 'thats', 'why', 'its', 'called', 'falling', 'in', 'love', 'bcoz', 'u', 'dont', 'force', 'yourself', 'u', 'just', 'fall', 'and', 'u', 'know', 'there', 'is', 'smeone', 'to', 'hold', 'u', 'bslvyl']\n",
      "After stop words removal: ['dont', 'search', 'love', 'let', 'love', 'find', 'u', 'thats', 'called', 'falling', 'love', 'bcoz', 'u', 'dont', 'force', 'u', 'fall', 'u', 'know', 'smeone', 'hold', 'u', 'bslvyl']\n",
      "fall\n",
      "After stemming with porters algorithm: ['dont', 'search', 'love', 'let', 'love', 'find', 'that', 'call', 'fall', 'love', 'bcoz', 'dont', 'forc', 'fall', 'know', 'smeon', 'hold', 'bslvyl']\n",
      "Tokenized sentence: ['hi', 'babe', 'its', 'chloe', 'how', 'r', 'u', 'i', 'was', 'smashed', 'on', 'saturday', 'night', 'it', 'was', 'great', 'how', 'was', 'your', 'weekend', 'u', 'been', 'missing', 'me', 'sp', 'visionsms', 'com', 'text', 'stop', 'to', 'stop', 'p', 'text']\n",
      "After stop words removal: ['hi', 'babe', 'chloe', 'r', 'u', 'smashed', 'saturday', 'night', 'great', 'weekend', 'u', 'missing', 'sp', 'visionsms', 'com', 'text', 'stop', 'stop', 'p', 'text']\n",
      "miss\n",
      "After stemming with porters algorithm: ['babe', 'chloe', 'smas', 'saturdai', 'night', 'great', 'weekend', 'miss', 'visionsm', 'com', 'text', 'stop', 'stop', 'text']\n",
      "Tokenized sentence: ['what', 's', 'the', 'significance']\n",
      "After stop words removal: ['significance']\n",
      "After stemming with porters algorithm: ['signif']\n",
      "Tokenized sentence: ['hope', 'you', 're', 'not', 'having', 'too', 'much', 'fun', 'without', 'me', 'see', 'u', 'tomorrow', 'love', 'jess', 'x']\n",
      "After stop words removal: ['hope', 'much', 'fun', 'without', 'see', 'u', 'tomorrow', 'love', 'jess', 'x']\n",
      "After stemming with porters algorithm: ['hope', 'much', 'fun', 'without', 'see', 'tomorrow', 'love', 'jess']\n",
      "Tokenized sentence: ['huh', 'but', 'i', 'cant', 'go', 'ur', 'house', 'empty', 'handed', 'right']\n",
      "After stop words removal: ['huh', 'cant', 'go', 'ur', 'house', 'empty', 'handed', 'right']\n",
      "After stemming with porters algorithm: ['huh', 'cant', 'hous', 'empti', 'han', 'right']\n",
      "Tokenized sentence: ['tell', 'me', 'they', 're', 'female', 'v', 'how', 're', 'you', 'throwing', 'in', 'we', 're', 'deciding', 'what', 'all', 'to', 'get', 'now']\n",
      "After stop words removal: ['tell', 'female', 'v', 'throwing', 'deciding', 'get']\n",
      "throw\n",
      "decid\n",
      "After stemming with porters algorithm: ['tell', 'femal', 'throwe', 'decid', 'get']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'price', 'to', 'claim', 'call']\n",
      "After stop words removal: ['price', 'claim', 'call']\n",
      "After stemming with porters algorithm: ['price', 'claim', 'call']\n",
      "Tokenized sentence: ['ahhhh', 'just', 'woken', 'up', 'had', 'a', 'bad', 'dream', 'about', 'u', 'tho', 'so', 'i', 'dont', 'like', 'u', 'right', 'now', 'i', 'didnt', 'know', 'anything', 'about', 'comedy', 'night', 'but', 'i', 'guess', 'im', 'up', 'for', 'it']\n",
      "After stop words removal: ['ahhhh', 'woken', 'bad', 'dream', 'u', 'tho', 'dont', 'like', 'u', 'right', 'didnt', 'know', 'anything', 'comedy', 'night', 'guess', 'im']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['ahhhh', 'woken', 'bad', 'dream', 'tho', 'dont', 'like', 'right', 'didnt', 'know', 'anyt', 'comedi', 'night', 'guess']\n",
      "Tokenized sentence: ['hi', 'petey', 'noi', 'm', 'ok', 'just', 'wanted', 'chat', 'coz', 'avent', 'spoken', 'u', 'a', 'long', 'time', 'hope', 'ur', 'doin', 'alrite', 'have', 'good', 'nit', 'at', 'js', 'love', 'ya', 'am', 'x']\n",
      "After stop words removal: ['hi', 'petey', 'noi', 'ok', 'wanted', 'chat', 'coz', 'avent', 'spoken', 'u', 'long', 'time', 'hope', 'ur', 'doin', 'alrite', 'good', 'nit', 'js', 'love', 'ya', 'x']\n",
      "After stemming with porters algorithm: ['petei', 'noi', 'wan', 'chat', 'coz', 'avent', 'spoken', 'long', 'time', 'hope', 'doin', 'alrit', 'good', 'nit', 'love']\n",
      "Tokenized sentence: ['sms', 'auction', 'a', 'brand', 'new', 'nokia', 'is', 'up', 'auction', 'today', 'auction', 'is', 'free', 'join', 'take', 'part', 'txt', 'nokia', 'to', 'now']\n",
      "After stop words removal: ['sms', 'auction', 'brand', 'new', 'nokia', 'auction', 'today', 'auction', 'free', 'join', 'take', 'part', 'txt', 'nokia']\n",
      "After stemming with porters algorithm: ['sm', 'auct', 'brand', 'new', 'nokia', 'auct', 'todai', 'auct', 'free', 'join', 'take', 'part', 'txt', 'nokia']\n",
      "Tokenized sentence: ['and', 'now', 'electricity', 'just', 'went', 'out', 'fml']\n",
      "After stop words removal: ['electricity', 'went', 'fml']\n",
      "After stemming with porters algorithm: ['electr', 'went', 'fml']\n",
      "Tokenized sentence: ['thanks', 'for', 'looking', 'out', 'for', 'me', 'i', 'really', 'appreciate']\n",
      "After stop words removal: ['thanks', 'looking', 'really', 'appreciate']\n",
      "look\n",
      "After stemming with porters algorithm: ['thank', 'look', 'realli', 'appreci']\n",
      "Tokenized sentence: ['saw', 'guys', 'and', 'dolls', 'last', 'night', 'with', 'patrick', 'swayze', 'it', 'was', 'great']\n",
      "After stop words removal: ['saw', 'guys', 'dolls', 'last', 'night', 'patrick', 'swayze', 'great']\n",
      "After stemming with porters algorithm: ['saw', 'gui', 'doll', 'last', 'night', 'patrick', 'swayz', 'great']\n",
      "Tokenized sentence: ['how', 'izzit', 'still', 'raining']\n",
      "After stop words removal: ['izzit', 'still', 'raining']\n",
      "rain\n",
      "After stemming with porters algorithm: ['izzit', 'still', 'rain']\n",
      "Tokenized sentence: ['joy', 's', 'father', 'is', 'john', 'then', 'john', 'is', 'the', 'of', 'joy', 's', 'father', 'if', 'u', 'ans', 'ths', 'you', 'hav', 'lt', 'gt', 'iq', 'tis', 's', 'ias', 'question', 'try', 'to', 'answer']\n",
      "After stop words removal: ['joy', 'father', 'john', 'john', 'joy', 'father', 'u', 'ans', 'ths', 'hav', 'lt', 'gt', 'iq', 'tis', 'ias', 'question', 'try', 'answer']\n",
      "After stemming with porters algorithm: ['joi', 'father', 'john', 'john', 'joi', 'father', 'an', 'th', 'hav', 'ti', 'ia', 'quest', 'try', 'answer']\n",
      "Tokenized sentence: ['holy', 'living', 'christ', 'what', 'is', 'taking', 'you', 'so', 'long']\n",
      "After stop words removal: ['holy', 'living', 'christ', 'taking', 'long']\n",
      "liv\n",
      "tak\n",
      "After stemming with porters algorithm: ['holi', 'live', 'christ', 'take', 'long']\n",
      "Tokenized sentence: ['i', 'have', 'a', 'date', 'on', 'sunday', 'with', 'will']\n",
      "After stop words removal: ['date', 'sunday']\n",
      "After stemming with porters algorithm: ['date', 'sundai']\n",
      "Tokenized sentence: ['babe', 'i', 'fucking', 'love', 'you', 'too', 'you', 'know', 'fuck', 'it', 'was', 'so', 'good', 'to', 'hear', 'your', 'voice', 'i', 'so', 'need', 'that', 'i', 'crave', 'it', 'i', 'can', 't', 'get', 'enough', 'i', 'adore', 'you', 'ahmad', 'kisses']\n",
      "After stop words removal: ['babe', 'fucking', 'love', 'know', 'fuck', 'good', 'hear', 'voice', 'need', 'crave', 'get', 'enough', 'adore', 'ahmad', 'kisses']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['babe', 'fuc', 'love', 'know', 'fuck', 'good', 'hear', 'voic', 'need', 'crave', 'get', 'enough', 'ador', 'ahmad', 'kiss']\n",
      "Tokenized sentence: ['sounds', 'great', 'are', 'you', 'home', 'now']\n",
      "After stop words removal: ['sounds', 'great', 'home']\n",
      "After stemming with porters algorithm: ['sound', 'great', 'home']\n",
      "Tokenized sentence: ['send', 'a', 'logo', 'ur', 'lover', 'names', 'joined', 'by', 'a', 'heart', 'txt', 'love', 'name', 'name', 'mobno', 'eg', 'love', 'adam', 'eve', 'to', 'yahoo', 'pobox', 'w', 'wq', 'txtno', 'no', 'ads', 'p']\n",
      "After stop words removal: ['send', 'logo', 'ur', 'lover', 'names', 'joined', 'heart', 'txt', 'love', 'name', 'name', 'mobno', 'eg', 'love', 'adam', 'eve', 'yahoo', 'pobox', 'w', 'wq', 'txtno', 'ads', 'p']\n",
      "After stemming with porters algorithm: ['send', 'logo', 'lover', 'name', 'join', 'heart', 'txt', 'love', 'name', 'name', 'mobno', 'love', 'adam', 'ev', 'yahoo', 'pobox', 'txtno', 'ad']\n",
      "Tokenized sentence: ['did', 'i', 'forget', 'to', 'tell', 'you', 'i', 'want', 'you', 'i', 'need', 'you', 'i', 'crave', 'you', 'but', 'most', 'of', 'all', 'i', 'love', 'you', 'my', 'sweet', 'arabian', 'steed', 'mmmmmm', 'yummy']\n",
      "After stop words removal: ['forget', 'tell', 'want', 'need', 'crave', 'love', 'sweet', 'arabian', 'steed', 'mmmmmm', 'yummy']\n",
      "After stemming with porters algorithm: ['forget', 'tell', 'want', 'need', 'crave', 'love', 'sweet', 'arabian', 'steed', 'mmmmmm', 'yummi']\n",
      "Tokenized sentence: ['k', 'can', 'that', 'happen', 'tonight']\n",
      "After stop words removal: ['k', 'happen', 'tonight']\n",
      "After stemming with porters algorithm: ['happen', 'tonight']\n",
      "Tokenized sentence: ['cause', 'i', 'm', 'not', 'freaky', 'lol']\n",
      "After stop words removal: ['cause', 'freaky', 'lol']\n",
      "After stemming with porters algorithm: ['caus', 'freaki', 'lol']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call']\n",
      "Tokenized sentence: ['text', 'her', 'if', 'she', 'doesnt', 'reply', 'let', 'me', 'know', 'so', 'i', 'can', 'have', 'her', 'log', 'in']\n",
      "After stop words removal: ['text', 'doesnt', 'reply', 'let', 'know', 'log']\n",
      "After stemming with porters algorithm: ['text', 'doesnt', 'repli', 'let', 'know', 'log']\n",
      "Tokenized sentence: ['hello', 'drivby', 'quit', 'edrunk', 'sorry', 'iff', 'pthis', 'makes', 'no', 'senrd', 'dnot', 'no', 'how', 'dancce', 'drum', 'n', 'basq', 'ihave', 'fun', 'nhite', 'x', 'ros', 'xxxxxxx']\n",
      "After stop words removal: ['hello', 'drivby', 'quit', 'edrunk', 'sorry', 'iff', 'pthis', 'makes', 'senrd', 'dnot', 'dancce', 'drum', 'n', 'basq', 'ihave', 'fun', 'nhite', 'x', 'ros', 'xxxxxxx']\n",
      "After stemming with porters algorithm: ['hello', 'drivbi', 'quit', 'edrunk', 'sorri', 'iff', 'pthi', 'make', 'senrd', 'dnot', 'dancc', 'drum', 'basq', 'ihav', 'fun', 'nhite', 'ro', 'xxxxxxx']\n",
      "Tokenized sentence: ['yun', 'ah', 'the', 'ubi', 'one', 'say', 'if', 'wan', 'call', 'by', 'tomorrow', 'call', 'look', 'for', 'irene', 'ere', 'only', 'got', 'bus', 'ubi', 'cres', 'ubi', 'tech', 'park', 'ph', 'for', 'st', 'wkg', 'days', 'n']\n",
      "After stop words removal: ['yun', 'ah', 'ubi', 'one', 'say', 'wan', 'call', 'tomorrow', 'call', 'look', 'irene', 'ere', 'got', 'bus', 'ubi', 'cres', 'ubi', 'tech', 'park', 'ph', 'st', 'wkg', 'days', 'n']\n",
      "After stemming with porters algorithm: ['yun', 'ubi', 'on', 'sai', 'wan', 'call', 'tomorrow', 'call', 'look', 'iren', 'er', 'got', 'bu', 'ubi', 'cre', 'ubi', 'tech', 'park', 'wkg', 'dai']\n",
      "Tokenized sentence: ['i', 'll', 'be', 'late']\n",
      "After stop words removal: ['late']\n",
      "After stemming with porters algorithm: ['late']\n",
      "Tokenized sentence: ['as', 'a', 'sim', 'subscriber', 'you', 'are', 'selected', 'to', 'receive', 'a', 'bonus', 'get', 'it', 'delivered', 'to', 'your', 'door', 'txt', 'the', 'word', 'ok', 'to', 'no', 'to', 'claim', 'p', 'msg', 'exp', 'apr']\n",
      "After stop words removal: ['sim', 'subscriber', 'selected', 'receive', 'bonus', 'get', 'delivered', 'door', 'txt', 'word', 'ok', 'claim', 'p', 'msg', 'exp', 'apr']\n",
      "After stemming with porters algorithm: ['sim', 'subscrib', 'selec', 'receiv', 'bonu', 'get', 'deliv', 'door', 'txt', 'word', 'claim', 'msg', 'exp', 'apr']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call']\n",
      "Tokenized sentence: ['urgent', 'call', 'from', 'your', 'landline', 'your', 'complimentary', 'ibiza', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
      "After stop words removal: ['urgent', 'call', 'landline', 'complimentary', 'ibiza', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'call', 'landlin', 'complimentari', 'ibiza', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['happy', 'new', 'year', 'to', 'u', 'and', 'ur', 'family', 'may', 'this', 'new', 'year', 'bring', 'happiness', 'stability', 'and', 'tranquility', 'to', 'ur', 'vibrant', 'colourful', 'life']\n",
      "After stop words removal: ['happy', 'new', 'year', 'u', 'ur', 'family', 'may', 'new', 'year', 'bring', 'happiness', 'stability', 'tranquility', 'ur', 'vibrant', 'colourful', 'life']\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year', 'famili', 'mai', 'new', 'year', 'bring', 'happi', 'stabil', 'tranquil', 'vibrant', 'colour', 'life']\n",
      "Tokenized sentence: ['no', 'idea', 'i', 'guess', 'we', 'll', 'work', 'that', 'out', 'an', 'hour', 'after', 'we', 're', 'supposed', 'to', 'leave', 'since', 'as', 'usual', 'nobody', 'has', 'any', 'interest', 'in', 'figuring', 'shit', 'out', 'before', 'the', 'last', 'second']\n",
      "After stop words removal: ['idea', 'guess', 'work', 'hour', 'supposed', 'leave', 'since', 'usual', 'nobody', 'interest', 'figuring', 'shit', 'last', 'second']\n",
      "figur\n",
      "After stemming with porters algorithm: ['idea', 'guess', 'work', 'hour', 'suppos', 'leav', 'sinc', 'usual', 'nobodi', 'interest', 'figur', 'shit', 'last', 'second']\n",
      "Tokenized sentence: ['haven', 't', 'eaten', 'all', 'day', 'i', 'm', 'sitting', 'here', 'staring', 'at', 'this', 'juicy', 'pizza', 'and', 'i', 'can', 't', 'eat', 'it', 'these', 'meds', 'are', 'ruining', 'my', 'life']\n",
      "After stop words removal: ['eaten', 'day', 'sitting', 'staring', 'juicy', 'pizza', 'eat', 'meds', 'ruining', 'life']\n",
      "sitt\n",
      "star\n",
      "ruin\n",
      "After stemming with porters algorithm: ['eaten', 'dai', 'sit', 'stare', 'juici', 'pizza', 'eat', 'med', 'ruin', 'life']\n",
      "Tokenized sentence: ['you', 'aren', 't', 'coming', 'home', 'between', 'class', 'right', 'i', 'need', 'to', 'work', 'out', 'and', 'shower']\n",
      "After stop words removal: ['coming', 'home', 'class', 'right', 'need', 'work', 'shower']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'home', 'class', 'right', 'need', 'work', 'shower']\n",
      "Tokenized sentence: ['yeah', 'why', 'not', 'is', 'the', 'gang', 'all', 'ready']\n",
      "After stop words removal: ['yeah', 'gang', 'ready']\n",
      "After stemming with porters algorithm: ['yeah', 'gang', 'readi']\n",
      "Tokenized sentence: ['congratulations', 'thanks', 'to', 'a', 'good', 'friend', 'u', 'have', 'won', 'the', 'xmas', 'prize', 'claim', 'is', 'easy', 'just', 'call', 'now', 'only', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['congratulations', 'thanks', 'good', 'friend', 'u', 'xmas', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['congratul', 'thank', 'good', 'friend', 'xma', 'priz', 'claim', 'easi', 'call', 'per', 'minut', 'nat', 'rate']\n",
      "Tokenized sentence: ['no', 'da', 'if', 'you', 'run', 'that', 'it', 'activate', 'the', 'full', 'version', 'da']\n",
      "After stop words removal: ['da', 'run', 'activate', 'full', 'version', 'da']\n",
      "After stemming with porters algorithm: ['run', 'activ', 'full', 'version']\n",
      "Tokenized sentence: ['lul', 'im', 'gettin', 'some', 'juicy', 'gossip', 'at', 'the', 'hospital', 'two', 'nurses', 'are', 'talking', 'about', 'how', 'fat', 'they', 'are', 'gettin', 'and', 'one', 'thinks', 'shes', 'obese', 'oyea']\n",
      "After stop words removal: ['lul', 'im', 'gettin', 'juicy', 'gossip', 'hospital', 'two', 'nurses', 'talking', 'fat', 'gettin', 'one', 'thinks', 'shes', 'obese', 'oyea']\n",
      "talk\n",
      "After stemming with porters algorithm: ['lul', 'gettin', 'juici', 'gossip', 'hospit', 'two', 'nurs', 'tal', 'fat', 'gettin', 'on', 'think', 'she', 'obes', 'oyea']\n",
      "Tokenized sentence: ['ok', 'that', 'would', 'b', 'lovely', 'if', 'u', 'r', 'sure', 'think', 'about', 'wot', 'u', 'want', 'to', 'do', 'drinkin', 'dancin', 'eatin', 'cinema', 'in', 'out', 'about', 'up', 'to', 'u', 'wot', 'about']\n",
      "After stop words removal: ['ok', 'would', 'b', 'lovely', 'u', 'r', 'sure', 'think', 'wot', 'u', 'want', 'drinkin', 'dancin', 'eatin', 'cinema', 'u', 'wot']\n",
      "After stemming with porters algorithm: ['would', 'love', 'sure', 'think', 'wot', 'want', 'drinkin', 'dancin', 'eatin', 'cinema', 'wot']\n",
      "Tokenized sentence: ['kallis', 'wont', 'bat', 'in', 'nd', 'innings']\n",
      "After stop words removal: ['kallis', 'wont', 'bat', 'nd', 'innings']\n",
      "inn\n",
      "After stemming with porters algorithm: ['kalli', 'wont', 'bat', 'in']\n",
      "Tokenized sentence: ['i', 'dont', 'want', 'to', 'hear', 'philosophy', 'just', 'say', 'what', 'happen']\n",
      "After stop words removal: ['dont', 'want', 'hear', 'philosophy', 'say', 'happen']\n",
      "After stemming with porters algorithm: ['dont', 'want', 'hear', 'philosophi', 'sai', 'happen']\n",
      "Tokenized sentence: ['sorry', 'i', 'm', 'not', 'free']\n",
      "After stop words removal: ['sorry', 'free']\n",
      "After stemming with porters algorithm: ['sorri', 'free']\n",
      "Tokenized sentence: ['lord', 'of', 'the', 'rings', 'return', 'of', 'the', 'king', 'in', 'store', 'now', 'reply', 'lotr', 'by', 'june', 'chance', 'win', 'lotr', 'soundtrack', 'cds', 'stdtxtrate', 'reply', 'stop', 'to', 'end', 'txts']\n",
      "After stop words removal: ['lord', 'rings', 'return', 'king', 'store', 'reply', 'lotr', 'june', 'chance', 'win', 'lotr', 'soundtrack', 'cds', 'stdtxtrate', 'reply', 'stop', 'end', 'txts']\n",
      "After stemming with porters algorithm: ['lord', 'ring', 'return', 'king', 'store', 'repli', 'lotr', 'june', 'chanc', 'win', 'lotr', 'soundtrack', 'cd', 'stdtxtrate', 'repli', 'stop', 'end', 'txt']\n",
      "Tokenized sentence: ['at', 'the', 'latest', 'g', 's', 'still', 'there', 'if', 'you', 'can', 'scrounge', 'up', 'some', 'ammo', 'and', 'want', 'to', 'give', 'the', 'new', 'ak', 'a', 'try']\n",
      "After stop words removal: ['latest', 'g', 'still', 'scrounge', 'ammo', 'want', 'give', 'new', 'ak', 'try']\n",
      "After stemming with porters algorithm: ['latest', 'still', 'scroung', 'ammo', 'want', 'give', 'new', 'try']\n",
      "Tokenized sentence: ['k', 'must', 'book', 'a', 'not', 'huh', 'so', 'going', 'for', 'yoga', 'basic', 'on', 'sunday']\n",
      "After stop words removal: ['k', 'must', 'book', 'huh', 'going', 'yoga', 'basic', 'sunday']\n",
      "go\n",
      "After stemming with porters algorithm: ['must', 'book', 'huh', 'go', 'yoga', 'basic', 'sundai']\n",
      "Tokenized sentence: ['hey', 'sathya', 'till', 'now', 'we', 'dint', 'meet', 'not', 'even', 'a', 'single', 'time', 'then', 'how', 'can', 'i', 'saw', 'the', 'situation', 'sathya']\n",
      "After stop words removal: ['hey', 'sathya', 'till', 'dint', 'meet', 'even', 'single', 'time', 'saw', 'situation', 'sathya']\n",
      "After stemming with porters algorithm: ['hei', 'sathya', 'till', 'dint', 'meet', 'even', 'singl', 'time', 'saw', 'situat', 'sathya']\n",
      "Tokenized sentence: ['smile', 'in', 'pleasure', 'smile', 'in', 'pain', 'smile', 'when', 'trouble', 'pours', 'like', 'rain', 'smile', 'when', 'sum', 'hurts', 'u', 'smile', 'becoz', 'someone', 'still', 'loves', 'to', 'see', 'u', 'smiling']\n",
      "After stop words removal: ['smile', 'pleasure', 'smile', 'pain', 'smile', 'trouble', 'pours', 'like', 'rain', 'smile', 'sum', 'hurts', 'u', 'smile', 'becoz', 'someone', 'still', 'loves', 'see', 'u', 'smiling']\n",
      "smil\n",
      "After stemming with porters algorithm: ['smile', 'pleasur', 'smile', 'pain', 'smile', 'troubl', 'pour', 'like', 'rain', 'smile', 'sum', 'hurt', 'smile', 'becoz', 'someon', 'still', 'love', 'see', 'smile']\n",
      "Tokenized sentence: ['so', 'wat', 's', 'da', 'decision']\n",
      "After stop words removal: ['wat', 'da', 'decision']\n",
      "After stemming with porters algorithm: ['wat', 'decis']\n",
      "Tokenized sentence: ['urgent', 'please', 'call', 'from', 'landline', 'your', 'abta', 'complimentary', 'tenerife', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'box', 'cw', 'wx', 'ppm']\n",
      "After stop words removal: ['urgent', 'please', 'call', 'landline', 'abta', 'complimentary', 'tenerife', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'box', 'cw', 'wx', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'pleas', 'call', 'landlin', 'abta', 'complimentari', 'tenerif', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['hmmm', 'but', 'you', 'should', 'give', 'it', 'on', 'one', 'day']\n",
      "After stop words removal: ['hmmm', 'give', 'one', 'day']\n",
      "After stemming with porters algorithm: ['hmmm', 'give', 'on', 'dai']\n",
      "Tokenized sentence: ['don', 't', 'fret', 'i', 'll', 'buy', 'the', 'ovulation', 'test', 'strips', 'and', 'send', 'them', 'to', 'you', 'you', 'wont', 'get', 'them', 'til', 'like', 'march', 'can', 'you', 'send', 'me', 'your', 'postal', 'address', 'u', 'll', 'be', 'alright', 'okay']\n",
      "After stop words removal: ['fret', 'buy', 'ovulation', 'test', 'strips', 'send', 'wont', 'get', 'til', 'like', 'march', 'send', 'postal', 'address', 'u', 'alright', 'okay']\n",
      "After stemming with porters algorithm: ['fret', 'bui', 'ovul', 'test', 'strip', 'send', 'wont', 'get', 'til', 'like', 'march', 'send', 'postal', 'address', 'alright', 'okai']\n",
      "Tokenized sentence: ['how', 'come', 'it', 'takes', 'so', 'little', 'time', 'for', 'a', 'child', 'who', 'is', 'afraid', 'of', 'the', 'dark', 'to', 'become', 'a', 'teenager', 'who', 'wants', 'to', 'stay', 'out', 'all', 'night']\n",
      "After stop words removal: ['come', 'takes', 'little', 'time', 'child', 'afraid', 'dark', 'become', 'teenager', 'wants', 'stay', 'night']\n",
      "After stemming with porters algorithm: ['come', 'take', 'littl', 'time', 'child', 'afraid', 'dark', 'becom', 'teenag', 'want', 'stai', 'night']\n",
      "Tokenized sentence: ['yes', 'princess', 'are', 'you', 'going', 'to', 'make', 'me', 'moan']\n",
      "After stop words removal: ['yes', 'princess', 'going', 'make', 'moan']\n",
      "go\n",
      "After stemming with porters algorithm: ['ye', 'princess', 'go', 'make', 'moan']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on', 'stopsms', 'ppm']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call', 'stopsms', 'ppm']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call', 'stopsm', 'ppm']\n",
      "Tokenized sentence: ['i', 'think', 'just', 'yourself', 'thanks', 'and', 'see', 'you', 'tomo']\n",
      "After stop words removal: ['think', 'thanks', 'see', 'tomo']\n",
      "After stemming with porters algorithm: ['think', 'thank', 'see', 'tomo']\n",
      "Tokenized sentence: ['i', 'am', 'on', 'the', 'way', 'to', 'ur', 'home']\n",
      "After stop words removal: ['way', 'ur', 'home']\n",
      "After stemming with porters algorithm: ['wai', 'home']\n",
      "Tokenized sentence: ['fancy', 'a', 'shag', 'i', 'do', 'interested', 'sextextuk', 'com', 'txt', 'xxuk', 'suzy', 'to', 'txts', 'cost', 'per', 'msg', 'tncs', 'on', 'website', 'x']\n",
      "After stop words removal: ['fancy', 'shag', 'interested', 'sextextuk', 'com', 'txt', 'xxuk', 'suzy', 'txts', 'cost', 'per', 'msg', 'tncs', 'website', 'x']\n",
      "After stemming with porters algorithm: ['fanci', 'shag', 'interes', 'sextextuk', 'com', 'txt', 'xxuk', 'suzi', 'txt', 'cost', 'per', 'msg', 'tnc', 'websit']\n",
      "Tokenized sentence: ['sad', 'story', 'of', 'a', 'man', 'last', 'week', 'was', 'my', 'b', 'day', 'my', 'wife', 'did', 'nt', 'wish', 'me', 'my', 'parents', 'forgot', 'n', 'so', 'did', 'my', 'kids', 'i', 'went', 'to', 'work', 'even', 'my', 'colleagues', 'did', 'not', 'wish']\n",
      "After stop words removal: ['sad', 'story', 'man', 'last', 'week', 'b', 'day', 'wife', 'nt', 'wish', 'parents', 'forgot', 'n', 'kids', 'went', 'work', 'even', 'colleagues', 'wish']\n",
      "After stemming with porters algorithm: ['sad', 'stori', 'man', 'last', 'week', 'dai', 'wife', 'wish', 'parent', 'forgot', 'kid', 'went', 'work', 'even', 'colleagu', 'wish']\n",
      "Tokenized sentence: ['k', 'give', 'me', 'a', 'sec', 'breaking', 'a', 'lt', 'gt', 'at', 'cstore']\n",
      "After stop words removal: ['k', 'give', 'sec', 'breaking', 'lt', 'gt', 'cstore']\n",
      "break\n",
      "After stemming with porters algorithm: ['give', 'sec', 'break', 'cstore']\n",
      "Tokenized sentence: ['yes', 'baby', 'we', 'can', 'study', 'all', 'the', 'positions', 'of', 'the', 'kama', 'sutra']\n",
      "After stop words removal: ['yes', 'baby', 'study', 'positions', 'kama', 'sutra']\n",
      "After stemming with porters algorithm: ['ye', 'babi', 'studi', 'posit', 'kama', 'sutra']\n",
      "Tokenized sentence: ['so', 'that', 'means', 'you', 'still', 'think', 'of', 'teju']\n",
      "After stop words removal: ['means', 'still', 'think', 'teju']\n",
      "After stemming with porters algorithm: ['mean', 'still', 'think', 'teju']\n",
      "Tokenized sentence: ['you', 'know', 'there', 'is', 'i', 'shall', 'speak', 'to', 'you', 'in', 'lt', 'gt', 'minutes', 'then']\n",
      "After stop words removal: ['know', 'shall', 'speak', 'lt', 'gt', 'minutes']\n",
      "After stemming with porters algorithm: ['know', 'shall', 'speak', 'minut']\n",
      "Tokenized sentence: ['thank', 'you', 'princess', 'i', 'want', 'to', 'see', 'your', 'nice', 'juicy', 'booty']\n",
      "After stop words removal: ['thank', 'princess', 'want', 'see', 'nice', 'juicy', 'booty']\n",
      "After stemming with porters algorithm: ['thank', 'princess', 'want', 'see', 'nice', 'juici', 'booti']\n",
      "Tokenized sentence: ['six', 'chances', 'to', 'win', 'cash', 'from', 'to', 'pounds', 'txt', 'csh', 'and', 'send', 'to', 'cost', 'p', 'day', 'days', 'tsandcs', 'apply', 'reply', 'hl', 'info']\n",
      "After stop words removal: ['six', 'chances', 'win', 'cash', 'pounds', 'txt', 'csh', 'send', 'cost', 'p', 'day', 'days', 'tsandcs', 'apply', 'reply', 'hl', 'info']\n",
      "After stemming with porters algorithm: ['six', 'chanc', 'win', 'cash', 'pound', 'txt', 'csh', 'send', 'cost', 'dai', 'dai', 'tsandc', 'appli', 'repli', 'info']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'winner', 'you', 'have', 'been', 'specially', 'selected', 'to', 'receive', 'cash', 'or', 'a', 'award', 'speak', 'to', 'a', 'live', 'operator', 'to', 'claim', 'call', 'am', 'pm', 'cost', 'p']\n",
      "After stop words removal: ['winner', 'specially', 'selected', 'receive', 'cash', 'award', 'speak', 'live', 'operator', 'claim', 'call', 'pm', 'cost', 'p']\n",
      "After stemming with porters algorithm: ['winner', 'special', 'selec', 'receiv', 'cash', 'award', 'speak', 'live', 'oper', 'claim', 'call', 'cost']\n",
      "Tokenized sentence: ['well', 'i', 'might', 'not', 'come', 'then']\n",
      "After stop words removal: ['well', 'might', 'come']\n",
      "After stemming with porters algorithm: ['well', 'might', 'come']\n",
      "Tokenized sentence: ['yo', 'guess', 'what', 'i', 'just', 'dropped']\n",
      "After stop words removal: ['yo', 'guess', 'dropped']\n",
      "After stemming with porters algorithm: ['guess', 'drop']\n",
      "Tokenized sentence: ['stop', 'the', 'story', 'i', 've', 'told', 'him', 'i', 've', 'returned', 'it', 'and', 'he', 's', 'saying', 'i', 'should', 'not', 're', 'order', 'it']\n",
      "After stop words removal: ['stop', 'story', 'told', 'returned', 'saying', 'order']\n",
      "say\n",
      "After stemming with porters algorithm: ['stop', 'stori', 'told', 'retur', 'sai', 'order']\n",
      "Tokenized sentence: ['dear', 'voucher', 'holder', 'claim', 'this', 'weeks', 'offer', 'at', 'your', 'pc', 'go', 'to', 'http', 'www', 'e', 'tlp', 'co', 'uk', 'expressoffer', 'ts', 'cs', 'apply', 'stop', 'texts', 'txt', 'stop', 'to']\n",
      "After stop words removal: ['dear', 'voucher', 'holder', 'claim', 'weeks', 'offer', 'pc', 'go', 'http', 'www', 'e', 'tlp', 'co', 'uk', 'expressoffer', 'ts', 'cs', 'apply', 'stop', 'texts', 'txt', 'stop']\n",
      "After stemming with porters algorithm: ['dear', 'voucher', 'holder', 'claim', 'week', 'offer', 'http', 'www', 'tlp', 'expressoff', 'appli', 'stop', 'text', 'txt', 'stop']\n",
      "Tokenized sentence: ['you', 'didn', 't', 'have', 'to', 'tell', 'me', 'that', 'now', 'i', 'm', 'thinking', 'plus', 'he', 's', 'going', 'to', 'stop', 'all', 'your', 'runs']\n",
      "After stop words removal: ['tell', 'thinking', 'plus', 'going', 'stop', 'runs']\n",
      "think\n",
      "go\n",
      "After stemming with porters algorithm: ['tell', 'thin', 'plu', 'go', 'stop', 'run']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['finished', 'class', 'where', 'are', 'you']\n",
      "After stop words removal: ['finished', 'class']\n",
      "After stemming with porters algorithm: ['finis', 'class']\n",
      "Tokenized sentence: ['easy', 'mate', 'guess', 'the', 'quick', 'drink', 'was', 'bit', 'ambitious']\n",
      "After stop words removal: ['easy', 'mate', 'guess', 'quick', 'drink', 'bit', 'ambitious']\n",
      "After stemming with porters algorithm: ['easi', 'mate', 'guess', 'quick', 'drink', 'bit', 'ambiti']\n",
      "Tokenized sentence: ['was', 'thinking', 'about', 'chuckin', 'ur', 'red', 'green', 'n', 'black', 'trainners', 'save', 'carryin', 'them', 'bac', 'on', 'train']\n",
      "After stop words removal: ['thinking', 'chuckin', 'ur', 'red', 'green', 'n', 'black', 'trainners', 'save', 'carryin', 'bac', 'train']\n",
      "think\n",
      "After stemming with porters algorithm: ['thin', 'chuckin', 'red', 'green', 'black', 'trainner', 'save', 'carryin', 'bac', 'train']\n",
      "Tokenized sentence: ['ya', 'it', 'came', 'a', 'while', 'ago']\n",
      "After stop words removal: ['ya', 'came', 'ago']\n",
      "After stemming with porters algorithm: ['came', 'ago']\n",
      "Tokenized sentence: ['why', 'tired', 'what', 'special', 'there', 'you', 'had']\n",
      "After stop words removal: ['tired', 'special']\n",
      "After stemming with porters algorithm: ['tire', 'special']\n",
      "Tokenized sentence: ['i', 'havent', 'lei', 'next', 'mon', 'can']\n",
      "After stop words removal: ['havent', 'lei', 'next', 'mon']\n",
      "After stemming with porters algorithm: ['havent', 'lei', 'next', 'mon']\n",
      "Tokenized sentence: ['merry', 'christmas', 'to', 'u', 'too', 'annie']\n",
      "After stop words removal: ['merry', 'christmas', 'u', 'annie']\n",
      "After stemming with porters algorithm: ['merri', 'christma', 'anni']\n",
      "Tokenized sentence: ['lol', 'i', 'would', 'but', 'my', 'mom', 'would', 'have', 'a', 'fit', 'and', 'tell', 'the', 'whole', 'family', 'how', 'crazy', 'and', 'terrible', 'i', 'am']\n",
      "After stop words removal: ['lol', 'would', 'mom', 'would', 'fit', 'tell', 'whole', 'family', 'crazy', 'terrible']\n",
      "After stemming with porters algorithm: ['lol', 'would', 'mom', 'would', 'fit', 'tell', 'whole', 'famili', 'crazi', 'terrib']\n",
      "Tokenized sentence: ['have', 'you', 'heard', 'about', 'that', 'job', 'i', 'm', 'going', 'to', 'that', 'wildlife', 'talk', 'again', 'tonight', 'if', 'u', 'want', 'come', 'its', 'that', 'worzels', 'and', 'a', 'wizzle', 'or', 'whatever', 'it', 'is']\n",
      "After stop words removal: ['heard', 'job', 'going', 'wildlife', 'talk', 'tonight', 'u', 'want', 'come', 'worzels', 'wizzle', 'whatever']\n",
      "go\n",
      "After stemming with porters algorithm: ['heard', 'job', 'go', 'wildlif', 'talk', 'tonight', 'want', 'come', 'worzel', 'wizzl', 'whatev']\n",
      "Tokenized sentence: ['it', 'does', 'it', 'on', 'its', 'own', 'most', 'of', 'the', 'time', 'it', 'fixes', 'my', 'spelling', 'but', 'sometimes', 'it', 'gets', 'a', 'completely', 'diff', 'word', 'go', 'figure']\n",
      "After stop words removal: ['time', 'fixes', 'spelling', 'sometimes', 'gets', 'completely', 'diff', 'word', 'go', 'figure']\n",
      "spell\n",
      "After stemming with porters algorithm: ['time', 'fix', 'spell', 'sometim', 'get', 'complet', 'diff', 'word', 'figur']\n",
      "Tokenized sentence: ['lol', 'great', 'now', 'im', 'getting', 'hungry']\n",
      "After stop words removal: ['lol', 'great', 'im', 'getting', 'hungry']\n",
      "gett\n",
      "After stemming with porters algorithm: ['lol', 'great', 'get', 'hungri']\n",
      "Tokenized sentence: ['i', 'hope', 'you', 'that', 's', 'the', 'result', 'of', 'being', 'consistently', 'intelligent', 'and', 'kind', 'start', 'asking', 'him', 'about', 'practicum', 'links', 'and', 'keep', 'your', 'ears', 'open', 'and', 'all', 'the', 'best', 'ttyl']\n",
      "After stop words removal: ['hope', 'result', 'consistently', 'intelligent', 'kind', 'start', 'asking', 'practicum', 'links', 'keep', 'ears', 'open', 'best', 'ttyl']\n",
      "ask\n",
      "After stemming with porters algorithm: ['hope', 'result', 'consist', 'intellig', 'kind', 'start', 'as', 'practicum', 'link', 'keep', 'ear', 'open', 'best', 'ttyl']\n",
      "Tokenized sentence: ['faith', 'makes', 'things', 'possible', 'hope', 'makes', 'things', 'work', 'love', 'makes', 'things', 'beautiful', 'may', 'you', 'have', 'all', 'three', 'this', 'christmas', 'merry', 'christmas']\n",
      "After stop words removal: ['faith', 'makes', 'things', 'possible', 'hope', 'makes', 'things', 'work', 'love', 'makes', 'things', 'beautiful', 'may', 'three', 'christmas', 'merry', 'christmas']\n",
      "After stemming with porters algorithm: ['faith', 'make', 'thing', 'possib', 'hope', 'make', 'thing', 'work', 'love', 'make', 'thing', 'beauti', 'mai', 'three', 'christma', 'merri', 'christma']\n",
      "Tokenized sentence: ['i', 'had', 'been', 'hoping', 'i', 'would', 'not', 'have', 'to', 'send', 'you', 'this', 'message', 'my', 'rent', 'is', 'due', 'and', 'i', 'dont', 'have', 'enough', 'for', 'it', 'my', 'reserves', 'are', 'completely', 'gone', 'its', 'a', 'loan', 'i', 'need', 'and', 'was', 'hoping', 'you', 'could', 'her', 'the', 'balance', 'is', 'lt', 'gt', 'is', 'there', 'a', 'way', 'i', 'could', 'get', 'that', 'from', 'you', 'till', 'mid', 'march', 'when', 'i', 'hope', 'to', 'pay', 'back']\n",
      "After stop words removal: ['hoping', 'would', 'send', 'message', 'rent', 'due', 'dont', 'enough', 'reserves', 'completely', 'gone', 'loan', 'need', 'hoping', 'could', 'balance', 'lt', 'gt', 'way', 'could', 'get', 'till', 'mid', 'march', 'hope', 'pay', 'back']\n",
      "hop\n",
      "hop\n",
      "After stemming with porters algorithm: ['hope', 'would', 'send', 'messag', 'rent', 'due', 'dont', 'enough', 'reserv', 'complet', 'gone', 'loan', 'need', 'hope', 'could', 'balanc', 'wai', 'could', 'get', 'till', 'mid', 'march', 'hope', 'pai', 'back']\n",
      "Tokenized sentence: ['oops', 'i', 'was', 'in', 'the', 'shower', 'when', 'u', 'called', 'hey', 'a', 'parking', 'garage', 'collapsed', 'at', 'university', 'hospital', 'see', 'i', 'm', 'not', 'crazy', 'stuff', 'like', 'that', 'does', 'happen']\n",
      "After stop words removal: ['oops', 'shower', 'u', 'called', 'hey', 'parking', 'garage', 'collapsed', 'university', 'hospital', 'see', 'crazy', 'stuff', 'like', 'happen']\n",
      "park\n",
      "After stemming with porters algorithm: ['oop', 'shower', 'call', 'hei', 'par', 'garag', 'collaps', 'univers', 'hospit', 'see', 'crazi', 'stuff', 'like', 'happen']\n",
      "Tokenized sentence: ['wanna', 'get', 'laid', 'nite', 'want', 'real', 'dogging', 'locations', 'sent', 'direct', 'to', 'ur', 'mobile', 'join', 'the', 'uk', 's', 'largest', 'dogging', 'network', 'txt', 'park', 'to', 'now', 'nyt', 'ec', 'a', 'lp', 'msg']\n",
      "After stop words removal: ['wanna', 'get', 'laid', 'nite', 'want', 'real', 'dogging', 'locations', 'sent', 'direct', 'ur', 'mobile', 'join', 'uk', 'largest', 'dogging', 'network', 'txt', 'park', 'nyt', 'ec', 'lp', 'msg']\n",
      "dogg\n",
      "dogg\n",
      "After stemming with porters algorithm: ['wanna', 'get', 'laid', 'nite', 'want', 'real', 'dog', 'locat', 'sent', 'direct', 'mobil', 'join', 'largest', 'dog', 'network', 'txt', 'park', 'nyt', 'msg']\n",
      "Tokenized sentence: ['dun', 'b', 'sad', 'it', 's', 'over', 'dun', 'thk', 'abt', 'it', 'already', 'concentrate', 'on', 'ur', 'other', 'papers', 'k']\n",
      "After stop words removal: ['dun', 'b', 'sad', 'dun', 'thk', 'abt', 'already', 'concentrate', 'ur', 'papers', 'k']\n",
      "After stemming with porters algorithm: ['dun', 'sad', 'dun', 'thk', 'abt', 'alreadi', 'concentr', 'paper']\n",
      "Tokenized sentence: ['yeah', 'that', 's', 'what', 'i', 'was', 'thinking']\n",
      "After stop words removal: ['yeah', 'thinking']\n",
      "think\n",
      "After stemming with porters algorithm: ['yeah', 'thin']\n",
      "Tokenized sentence: ['darren', 'was', 'saying', 'dat', 'if', 'u', 'meeting', 'da', 'ge', 'den', 'we', 'dun', 'meet', 'dinner', 'cos', 'later', 'u', 'leave', 'xy', 'will', 'feel', 'awkward', 'den', 'u', 'meet', 'him', 'lunch', 'lor']\n",
      "After stop words removal: ['darren', 'saying', 'dat', 'u', 'meeting', 'da', 'ge', 'den', 'dun', 'meet', 'dinner', 'cos', 'later', 'u', 'leave', 'xy', 'feel', 'awkward', 'den', 'u', 'meet', 'lunch', 'lor']\n",
      "say\n",
      "meet\n",
      "After stemming with porters algorithm: ['darren', 'sai', 'dat', 'meet', 'den', 'dun', 'meet', 'dinner', 'co', 'later', 'leav', 'feel', 'awkward', 'den', 'meet', 'lunch', 'lor']\n",
      "Tokenized sentence: ['aight', 'fuck', 'it', 'i', 'll', 'get', 'it', 'later']\n",
      "After stop words removal: ['aight', 'fuck', 'get', 'later']\n",
      "After stemming with porters algorithm: ['aight', 'fuck', 'get', 'later']\n",
      "Tokenized sentence: ['wishing', 'you', 'a', 'wonderful', 'week']\n",
      "After stop words removal: ['wishing', 'wonderful', 'week']\n",
      "wish\n",
      "After stemming with porters algorithm: ['wis', 'wonder', 'week']\n",
      "Tokenized sentence: ['not', 'really', 'dude', 'have', 'no', 'friends', 'i', 'm', 'afraid']\n",
      "After stop words removal: ['really', 'dude', 'friends', 'afraid']\n",
      "After stemming with porters algorithm: ['realli', 'dude', 'friend', 'afraid']\n",
      "Tokenized sentence: ['i', 'wish', 'u', 'were', 'here', 'i', 'feel', 'so', 'alone']\n",
      "After stop words removal: ['wish', 'u', 'feel', 'alone']\n",
      "After stemming with porters algorithm: ['wish', 'feel', 'alon']\n",
      "Tokenized sentence: ['good', 'morning', 'princess', 'how', 'are', 'you']\n",
      "After stop words removal: ['good', 'morning', 'princess']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'princess']\n",
      "Tokenized sentence: ['mostly', 'sports', 'type', 'lyk', 'footbl', 'crckt']\n",
      "After stop words removal: ['mostly', 'sports', 'type', 'lyk', 'footbl', 'crckt']\n",
      "After stemming with porters algorithm: ['mostli', 'sport', 'type', 'lyk', 'footbl', 'crckt']\n",
      "Tokenized sentence: ['eatin', 'later', 'but', 'i', 'm', 'eatin', 'wif', 'my', 'frens', 'now', 'lei', 'going', 'home', 'first']\n",
      "After stop words removal: ['eatin', 'later', 'eatin', 'wif', 'frens', 'lei', 'going', 'home', 'first']\n",
      "go\n",
      "After stemming with porters algorithm: ['eatin', 'later', 'eatin', 'wif', 'fren', 'lei', 'go', 'home', 'first']\n",
      "Tokenized sentence: ['themob', 'yo', 'yo', 'yo', 'here', 'comes', 'a', 'new', 'selection', 'of', 'hot', 'downloads', 'for', 'our', 'members', 'to', 'get', 'for', 'free', 'just', 'click', 'open', 'the', 'next', 'link', 'sent', 'to', 'ur', 'fone']\n",
      "After stop words removal: ['themob', 'yo', 'yo', 'yo', 'comes', 'new', 'selection', 'hot', 'downloads', 'members', 'get', 'free', 'click', 'open', 'next', 'link', 'sent', 'ur', 'fone']\n",
      "After stemming with porters algorithm: ['themob', 'come', 'new', 'select', 'hot', 'download', 'member', 'get', 'free', 'click', 'open', 'next', 'link', 'sent', 'fone']\n",
      "Tokenized sentence: ['those', 'were', 'my', 'exact', 'intentions']\n",
      "After stop words removal: ['exact', 'intentions']\n",
      "After stemming with porters algorithm: ['exact', 'intent']\n",
      "Tokenized sentence: ['the', 'world', 'suffers', 'a', 'lot', 'not', 'because', 'of', 'the', 'violence', 'of', 'bad', 'people', 'but', 'because', 'of', 'the', 'silence', 'of', 'good', 'people']\n",
      "After stop words removal: ['world', 'suffers', 'lot', 'violence', 'bad', 'people', 'silence', 'good', 'people']\n",
      "After stemming with porters algorithm: ['world', 'suffer', 'lot', 'violenc', 'bad', 'peopl', 'silenc', 'good', 'peopl']\n",
      "Tokenized sentence: ['i', 'jus', 'hope', 'its', 'true', 'that', 'missin', 'me', 'cos', 'i', 'm', 'really', 'missin', 'him', 'you', 'haven', 't', 'done', 'anything', 'to', 'feel', 'guilty', 'about', 'yet']\n",
      "After stop words removal: ['jus', 'hope', 'true', 'missin', 'cos', 'really', 'missin', 'done', 'anything', 'feel', 'guilty', 'yet']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['ju', 'hope', 'true', 'missin', 'co', 'realli', 'missin', 'done', 'anyt', 'feel', 'guilti', 'yet']\n",
      "Tokenized sentence: ['tomorrow', 'i', 'am', 'not', 'going', 'to', 'theatre', 'so', 'i', 'can', 'come', 'wherever', 'u', 'call', 'me', 'tell', 'me', 'where', 'and', 'when', 'to', 'come', 'tomorrow']\n",
      "After stop words removal: ['tomorrow', 'going', 'theatre', 'come', 'wherever', 'u', 'call', 'tell', 'come', 'tomorrow']\n",
      "go\n",
      "After stemming with porters algorithm: ['tomorrow', 'go', 'theatr', 'come', 'wherev', 'call', 'tell', 'come', 'tomorrow']\n",
      "Tokenized sentence: ['how', 'will', 'i', 'creep', 'on', 'you', 'now']\n",
      "After stop words removal: ['creep']\n",
      "After stemming with porters algorithm: ['creep']\n",
      "Tokenized sentence: ['so', 'many', 'people', 'seems', 'to', 'be', 'special', 'at', 'first', 'sight', 'but', 'only', 'very', 'few', 'will', 'remain', 'special', 'to', 'you', 'till', 'your', 'last', 'sight', 'maintain', 'them', 'till', 'life', 'ends', 'sh', 'jas']\n",
      "After stop words removal: ['many', 'people', 'seems', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'ends', 'sh', 'jas']\n",
      "After stemming with porters algorithm: ['mani', 'peopl', 'seem', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'end', 'ja']\n",
      "Tokenized sentence: ['then', 'you', 'are', 'eldest', 'know']\n",
      "After stop words removal: ['eldest', 'know']\n",
      "After stemming with porters algorithm: ['eldest', 'know']\n",
      "Tokenized sentence: ['boy', 'i', 'love', 'u', 'grl', 'hogolo', 'boy', 'gold', 'chain', 'kodstini', 'grl', 'agalla', 'boy', 'necklace', 'madstini', 'grl', 'agalla', 'boy', 'hogli', 'mutai', 'eerulli', 'kodthini', 'grl', 'i', 'love', 'u', 'kano']\n",
      "After stop words removal: ['boy', 'love', 'u', 'grl', 'hogolo', 'boy', 'gold', 'chain', 'kodstini', 'grl', 'agalla', 'boy', 'necklace', 'madstini', 'grl', 'agalla', 'boy', 'hogli', 'mutai', 'eerulli', 'kodthini', 'grl', 'love', 'u', 'kano']\n",
      "After stemming with porters algorithm: ['boi', 'love', 'grl', 'hogolo', 'boi', 'gold', 'chain', 'kodstini', 'grl', 'agalla', 'boi', 'necklac', 'madstini', 'grl', 'agalla', 'boi', 'hogli', 'mutai', 'eerulli', 'kodthini', 'grl', 'love', 'kano']\n",
      "Tokenized sentence: ['its', 'worse', 'if', 'if', 'uses', 'half', 'way', 'then', 'stops', 'its', 'better', 'for', 'him', 'to', 'complete', 'it']\n",
      "After stop words removal: ['worse', 'uses', 'half', 'way', 'stops', 'better', 'complete']\n",
      "After stemming with porters algorithm: ['wors', 'us', 'half', 'wai', 'stop', 'better', 'complet']\n",
      "Tokenized sentence: ['yeah', 'like', 'if', 'it', 'goes', 'like', 'it', 'did', 'with', 'my', 'friends', 'imma', 'flip', 'my', 'shit', 'in', 'like', 'half', 'an', 'hour']\n",
      "After stop words removal: ['yeah', 'like', 'goes', 'like', 'friends', 'imma', 'flip', 'shit', 'like', 'half', 'hour']\n",
      "After stemming with porters algorithm: ['yeah', 'like', 'goe', 'like', 'friend', 'imma', 'flip', 'shit', 'like', 'half', 'hour']\n",
      "Tokenized sentence: ['miss', 'call', 'miss', 'call', 'khelate', 'kintu', 'opponenter', 'miss', 'call', 'dhorte', 'lage', 'thats', 'd', 'rule', 'one', 'with', 'great', 'phone', 'receiving', 'quality', 'wins']\n",
      "After stop words removal: ['miss', 'call', 'miss', 'call', 'khelate', 'kintu', 'opponenter', 'miss', 'call', 'dhorte', 'lage', 'thats', 'rule', 'one', 'great', 'phone', 'receiving', 'quality', 'wins']\n",
      "receiv\n",
      "After stemming with porters algorithm: ['miss', 'call', 'miss', 'call', 'khelat', 'kintu', 'oppon', 'miss', 'call', 'dhort', 'lage', 'that', 'rule', 'on', 'great', 'phone', 'receiv', 'qualiti', 'win']\n",
      "Tokenized sentence: ['haven', 't', 'heard', 'anything', 'and', 'he', 's', 'not', 'answering', 'my', 'texts', 'so', 'i', 'm', 'guessing', 'he', 'flaked', 'that', 'said', 'the', 'jb', 'is', 'fantastic']\n",
      "After stop words removal: ['heard', 'anything', 'answering', 'texts', 'guessing', 'flaked', 'said', 'jb', 'fantastic']\n",
      "anyth\n",
      "answer\n",
      "guess\n",
      "After stemming with porters algorithm: ['heard', 'anyt', 'answer', 'text', 'guess', 'flake', 'said', 'fantast']\n",
      "Tokenized sentence: ['i', 'don', 't', 'know', 'u', 'and', 'u', 'don', 't', 'know', 'me', 'send', 'chat', 'to', 'now', 'and', 'let', 's', 'find', 'each', 'other', 'only', 'p', 'msg', 'rcvd', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'years', 'or', 'over']\n",
      "After stop words removal: ['know', 'u', 'u', 'know', 'send', 'chat', 'let', 'find', 'p', 'msg', 'rcvd', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'years']\n",
      "After stemming with porters algorithm: ['know', 'know', 'send', 'chat', 'let', 'find', 'msg', 'rcvd', 'suit', 'land', 'row', 'ldn', 'year']\n",
      "Tokenized sentence: ['k', 'sure', 'am', 'in', 'my', 'relatives', 'home', 'sms', 'me', 'de', 'pls']\n",
      "After stop words removal: ['k', 'sure', 'relatives', 'home', 'sms', 'de', 'pls']\n",
      "After stemming with porters algorithm: ['sure', 'rel', 'home', 'sm', 'pl']\n",
      "Tokenized sentence: ['ha', 'ha', 'ha', 'good', 'joke', 'girls', 'are', 'situation', 'seekers']\n",
      "After stop words removal: ['ha', 'ha', 'ha', 'good', 'joke', 'girls', 'situation', 'seekers']\n",
      "After stemming with porters algorithm: ['good', 'joke', 'girl', 'situat', 'seeker']\n",
      "Tokenized sentence: ['doing', 'my', 'masters', 'when', 'will', 'you', 'buy', 'a', 'bb', 'cos', 'i', 'have', 'for', 'sale', 'and', 'how', 's', 'bf']\n",
      "After stop words removal: ['masters', 'buy', 'bb', 'cos', 'sale', 'bf']\n",
      "After stemming with porters algorithm: ['master', 'bui', 'co', 'sale']\n",
      "Tokenized sentence: ['just', 'trying', 'to', 'figure', 'out', 'when', 'i', 'm', 'suppose', 'to', 'see', 'a', 'couple', 'different', 'people', 'this', 'week', 'we', 'said', 'we', 'd', 'get', 'together', 'but', 'i', 'didn', 't', 'set', 'dates']\n",
      "After stop words removal: ['trying', 'figure', 'suppose', 'see', 'couple', 'different', 'people', 'week', 'said', 'get', 'together', 'set', 'dates']\n",
      "After stemming with porters algorithm: ['trying', 'figur', 'suppos', 'see', 'coupl', 'differ', 'peopl', 'week', 'said', 'get', 'togeth', 'set', 'date']\n",
      "Tokenized sentence: ['aaooooright', 'are', 'you', 'at', 'work']\n",
      "After stop words removal: ['aaooooright', 'work']\n",
      "After stemming with porters algorithm: ['aaooooright', 'work']\n",
      "Tokenized sentence: ['am', 'slow', 'in', 'using', 'biola', 's', 'fne']\n",
      "After stop words removal: ['slow', 'using', 'biola', 'fne']\n",
      "us\n",
      "After stemming with porters algorithm: ['slow', 'us', 'biola', 'fne']\n",
      "Tokenized sentence: ['just', 'forced', 'myself', 'to', 'eat', 'a', 'slice', 'i', 'm', 'really', 'not', 'hungry', 'tho', 'this', 'sucks', 'mark', 'is', 'getting', 'worried', 'he', 'knows', 'i', 'm', 'sick', 'when', 'i', 'turn', 'down', 'pizza', 'lol']\n",
      "After stop words removal: ['forced', 'eat', 'slice', 'really', 'hungry', 'tho', 'sucks', 'mark', 'getting', 'worried', 'knows', 'sick', 'turn', 'pizza', 'lol']\n",
      "gett\n",
      "After stemming with porters algorithm: ['for', 'eat', 'slice', 'realli', 'hungri', 'tho', 'suck', 'mark', 'get', 'worri', 'know', 'sick', 'turn', 'pizza', 'lol']\n",
      "Tokenized sentence: ['i', 'm', 'still', 'looking', 'for', 'a', 'car', 'to', 'buy', 'and', 'have', 'not', 'gone', 'the', 'driving', 'test', 'yet']\n",
      "After stop words removal: ['still', 'looking', 'car', 'buy', 'gone', 'driving', 'test', 'yet']\n",
      "look\n",
      "driv\n",
      "After stemming with porters algorithm: ['still', 'look', 'car', 'bui', 'gone', 'drive', 'test', 'yet']\n",
      "Tokenized sentence: ['nt', 'yet', 'chikku', 'simple', 'habba', 'hw', 'abt', 'u']\n",
      "After stop words removal: ['nt', 'yet', 'chikku', 'simple', 'habba', 'hw', 'abt', 'u']\n",
      "After stemming with porters algorithm: ['yet', 'chikku', 'simpl', 'habba', 'abt']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'xxxxxxxxx', 'won', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'reach', 'you', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'xxxxxxxxx', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'reach', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'xxxxxxxxx', 'bonu', 'caller', 'priz', 'attempt', 'reach', 'call', 'asap', 'box', 'ppm']\n",
      "Tokenized sentence: ['get', 'down', 'in', 'gandhipuram', 'and', 'walk', 'to', 'cross', 'cut', 'road', 'right', 'side', 'lt', 'gt', 'street', 'road', 'and', 'turn', 'at', 'first', 'right']\n",
      "After stop words removal: ['get', 'gandhipuram', 'walk', 'cross', 'cut', 'road', 'right', 'side', 'lt', 'gt', 'street', 'road', 'turn', 'first', 'right']\n",
      "After stemming with porters algorithm: ['get', 'gandhipuram', 'walk', 'cross', 'cut', 'road', 'right', 'side', 'street', 'road', 'turn', 'first', 'right']\n",
      "Tokenized sentence: ['yeah', 'probably', 'here', 'for', 'a', 'while']\n",
      "After stop words removal: ['yeah', 'probably']\n",
      "After stemming with porters algorithm: ['yeah', 'probab']\n",
      "Tokenized sentence: ['its', 'a', 'big', 'difference', 'lt', 'gt', 'versus', 'lt', 'gt', 'every', 'lt', 'gt', 'hrs']\n",
      "After stop words removal: ['big', 'difference', 'lt', 'gt', 'versus', 'lt', 'gt', 'every', 'lt', 'gt', 'hrs']\n",
      "After stemming with porters algorithm: ['big', 'differ', 'versu', 'everi', 'hr']\n",
      "Tokenized sentence: ['y', 'cant', 'u', 'try', 'new', 'invention', 'to', 'fly', 'i', 'm', 'not', 'joking']\n",
      "After stop words removal: ['cant', 'u', 'try', 'new', 'invention', 'fly', 'joking']\n",
      "jok\n",
      "After stemming with porters algorithm: ['cant', 'try', 'new', 'invent', 'fly', 'joke']\n",
      "Tokenized sentence: ['got', 'meh', 'when']\n",
      "After stop words removal: ['got', 'meh']\n",
      "After stemming with porters algorithm: ['got', 'meh']\n",
      "Tokenized sentence: ['s', 'kallis', 'wont', 'play', 'in', 'first', 'two', 'odi']\n",
      "After stop words removal: ['kallis', 'wont', 'play', 'first', 'two', 'odi']\n",
      "After stemming with porters algorithm: ['kalli', 'wont', 'plai', 'first', 'two', 'odi']\n",
      "Tokenized sentence: ['some', 'are', 'lasting', 'as', 'much', 'as', 'hours', 'you', 'might', 'get', 'lucky']\n",
      "After stop words removal: ['lasting', 'much', 'hours', 'might', 'get', 'lucky']\n",
      "last\n",
      "After stemming with porters algorithm: ['las', 'much', 'hour', 'might', 'get', 'lucki']\n",
      "Tokenized sentence: ['k', 'da', 'how', 'many', 'page', 'you', 'want']\n",
      "After stop words removal: ['k', 'da', 'many', 'page', 'want']\n",
      "After stemming with porters algorithm: ['mani', 'page', 'want']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['how', 'many', 'times', 'i', 'told', 'in', 'the', 'stage', 'all', 'use', 'to', 'laugh', 'you', 'not', 'listen', 'aha']\n",
      "After stop words removal: ['many', 'times', 'told', 'stage', 'use', 'laugh', 'listen', 'aha']\n",
      "After stemming with porters algorithm: ['mani', 'time', 'told', 'stage', 'us', 'laugh', 'listen', 'aha']\n",
      "Tokenized sentence: ['aight', 'no', 'rush', 'i', 'll', 'ask', 'jay']\n",
      "After stop words removal: ['aight', 'rush', 'ask', 'jay']\n",
      "After stemming with porters algorithm: ['aight', 'rush', 'ask', 'jai']\n",
      "Tokenized sentence: ['check', 'out', 'choose', 'your', 'babe', 'videos', 'sms', 'shsex', 'netun', 'fgkslpopw', 'fgkslpo']\n",
      "After stop words removal: ['check', 'choose', 'babe', 'videos', 'sms', 'shsex', 'netun', 'fgkslpopw', 'fgkslpo']\n",
      "After stemming with porters algorithm: ['check', 'choos', 'babe', 'video', 'sm', 'shsex', 'netun', 'fgkslpopw', 'fgkslpo']\n",
      "Tokenized sentence: ['omg', 'how', 'did', 'u', 'know', 'what', 'i', 'ate']\n",
      "After stop words removal: ['omg', 'u', 'know', 'ate']\n",
      "After stemming with porters algorithm: ['omg', 'know', 'at']\n",
      "Tokenized sentence: ['do', 'have', 'a', 'nice', 'day', 'today', 'i', 'love', 'you', 'so', 'dearly']\n",
      "After stop words removal: ['nice', 'day', 'today', 'love', 'dearly']\n",
      "After stemming with porters algorithm: ['nice', 'dai', 'todai', 'love', 'dearli']\n",
      "Tokenized sentence: ['sorry', 'got', 'a', 'late', 'start', 'we', 're', 'on', 'the', 'way']\n",
      "After stop words removal: ['sorry', 'got', 'late', 'start', 'way']\n",
      "After stemming with porters algorithm: ['sorri', 'got', 'late', 'start', 'wai']\n",
      "Tokenized sentence: ['sir', 'send', 'to', 'group', 'mail', 'check', 'it']\n",
      "After stop words removal: ['sir', 'send', 'group', 'mail', 'check']\n",
      "After stemming with porters algorithm: ['sir', 'send', 'group', 'mail', 'check']\n",
      "Tokenized sentence: ['boo', 'i', 'm', 'on', 'my', 'way', 'to', 'my', 'moms', 'she', 's', 'making', 'tortilla', 'soup', 'yummmm']\n",
      "After stop words removal: ['boo', 'way', 'moms', 'making', 'tortilla', 'soup', 'yummmm']\n",
      "mak\n",
      "After stemming with porters algorithm: ['boo', 'wai', 'mom', 'make', 'tortilla', 'soup', 'yummmm']\n",
      "Tokenized sentence: ['i', 'forgot', 'ask', 'all', 'smth', 'there', 's', 'a', 'card', 'on', 'da', 'present', 'lei', 'how', 'all', 'want', 'write', 'smth', 'or', 'sign', 'on', 'it']\n",
      "After stop words removal: ['forgot', 'ask', 'smth', 'card', 'da', 'present', 'lei', 'want', 'write', 'smth', 'sign']\n",
      "After stemming with porters algorithm: ['forgot', 'ask', 'smth', 'card', 'present', 'lei', 'want', 'write', 'smth', 'sign']\n",
      "Tokenized sentence: ['good', 'friends', 'care', 'for', 'each', 'other', 'close', 'friends', 'understand', 'each', 'other', 'and', 'true', 'friends', 'stay', 'forever', 'beyond', 'words', 'beyond', 'time', 'gud', 'ni']\n",
      "After stop words removal: ['good', 'friends', 'care', 'close', 'friends', 'understand', 'true', 'friends', 'stay', 'forever', 'beyond', 'words', 'beyond', 'time', 'gud', 'ni']\n",
      "After stemming with porters algorithm: ['good', 'friend', 'care', 'close', 'friend', 'understand', 'true', 'friend', 'stai', 'forev', 'beyond', 'word', 'beyond', 'time', 'gud']\n",
      "Tokenized sentence: ['hi', 'di', 'is', 'yijue', 'we', 're', 'meeting', 'at', 'pm', 'at', 'esaplanade', 'tonight']\n",
      "After stop words removal: ['hi', 'di', 'yijue', 'meeting', 'pm', 'esaplanade', 'tonight']\n",
      "meet\n",
      "After stemming with porters algorithm: ['yiju', 'meet', 'esaplanad', 'tonight']\n",
      "Tokenized sentence: ['this', 'message', 'is', 'free', 'welcome', 'to', 'the', 'new', 'improved', 'sex', 'dogging', 'club', 'to', 'unsubscribe', 'from', 'this', 'service', 'reply', 'stop', 'msgs', 'p', 'only']\n",
      "After stop words removal: ['message', 'free', 'welcome', 'new', 'improved', 'sex', 'dogging', 'club', 'unsubscribe', 'service', 'reply', 'stop', 'msgs', 'p']\n",
      "dogg\n",
      "After stemming with porters algorithm: ['messag', 'free', 'welcom', 'new', 'improv', 'sex', 'dog', 'club', 'unsubscrib', 'servic', 'repli', 'stop', 'msg']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'winner', 'u', 'have', 'been', 'specially', 'selected', 'receive', 'cash', 'or', 'a', 'holiday', 'flights', 'inc', 'speak', 'to', 'a', 'live', 'operator', 'claim']\n",
      "After stop words removal: ['winner', 'u', 'specially', 'selected', 'receive', 'cash', 'holiday', 'flights', 'inc', 'speak', 'live', 'operator', 'claim']\n",
      "After stemming with porters algorithm: ['winner', 'special', 'selec', 'receiv', 'cash', 'holidai', 'flight', 'inc', 'speak', 'live', 'oper', 'claim']\n",
      "Tokenized sentence: ['im', 'good', 'i', 'have', 'been', 'thinking', 'about', 'you']\n",
      "After stop words removal: ['im', 'good', 'thinking']\n",
      "think\n",
      "After stemming with porters algorithm: ['good', 'thin']\n",
      "Tokenized sentence: ['wot', 'u', 'up', 'u', 'weirdo']\n",
      "After stop words removal: ['wot', 'u', 'u', 'weirdo']\n",
      "After stemming with porters algorithm: ['wot', 'weirdo']\n",
      "Tokenized sentence: ['why', 'must', 'we', 'sit', 'around', 'and', 'wait', 'for', 'summer', 'days', 'to', 'celebrate', 'such', 'a', 'magical', 'sight', 'when', 'the', 'worlds', 'dressed', 'in', 'white', 'oooooh', 'let', 'there', 'be', 'snow']\n",
      "After stop words removal: ['must', 'sit', 'around', 'wait', 'summer', 'days', 'celebrate', 'magical', 'sight', 'worlds', 'dressed', 'white', 'oooooh', 'let', 'snow']\n",
      "After stemming with porters algorithm: ['must', 'sit', 'around', 'wait', 'summer', 'dai', 'celebr', 'magic', 'sight', 'world', 'dress', 'white', 'oooooh', 'let', 'snow']\n",
      "Tokenized sentence: ['a', 'bloo', 'bloo', 'bloo', 'i', 'll', 'miss', 'the', 'first', 'bowl']\n",
      "After stop words removal: ['bloo', 'bloo', 'bloo', 'miss', 'first', 'bowl']\n",
      "After stemming with porters algorithm: ['bloo', 'bloo', 'bloo', 'miss', 'first', 'bowl']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['am', 'on', 'my', 'way']\n",
      "After stop words removal: ['way']\n",
      "After stemming with porters algorithm: ['wai']\n",
      "Tokenized sentence: ['x', 'lt', 'gt', 'are', 'you', 'going', 'to', 'get', 'that']\n",
      "After stop words removal: ['x', 'lt', 'gt', 'going', 'get']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'get']\n",
      "Tokenized sentence: ['hi', 'darlin', 'ive', 'just', 'got', 'back', 'and', 'i', 'had', 'a', 'really', 'nice', 'night', 'and', 'thanks', 'so', 'much', 'for', 'the', 'lift', 'see', 'u', 'tomorrow', 'xxx']\n",
      "After stop words removal: ['hi', 'darlin', 'ive', 'got', 'back', 'really', 'nice', 'night', 'thanks', 'much', 'lift', 'see', 'u', 'tomorrow', 'xxx']\n",
      "After stemming with porters algorithm: ['darlin', 'iv', 'got', 'back', 'realli', 'nice', 'night', 'thank', 'much', 'lift', 'see', 'tomorrow', 'xxx']\n",
      "Tokenized sentence: ['now', 'whats', 'your', 'house', 'again', 'and', 'do', 'you', 'have', 'any', 'beer', 'there']\n",
      "After stop words removal: ['whats', 'house', 'beer']\n",
      "After stemming with porters algorithm: ['what', 'hous', 'beer']\n",
      "Tokenized sentence: ['tessy', 'pls', 'do', 'me', 'a', 'favor', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimya', 'pls', 'dnt', 'forget', 'it', 'today', 'is', 'her', 'birthday', 'shijas']\n",
      "After stop words removal: ['tessy', 'pls', 'favor', 'pls', 'convey', 'birthday', 'wishes', 'nimya', 'pls', 'dnt', 'forget', 'today', 'birthday', 'shijas']\n",
      "After stemming with porters algorithm: ['tessi', 'pl', 'favor', 'pl', 'convei', 'birthdai', 'wish', 'nimya', 'pl', 'dnt', 'forget', 'todai', 'birthdai', 'shija']\n",
      "Tokenized sentence: ['i', 'can', 'take', 'you', 'at', 'like', 'noon']\n",
      "After stop words removal: ['take', 'like', 'noon']\n",
      "After stemming with porters algorithm: ['take', 'like', 'noon']\n",
      "Tokenized sentence: ['hey', 'you', 'got', 'any', 'mail']\n",
      "After stop words removal: ['hey', 'got', 'mail']\n",
      "After stemming with porters algorithm: ['hei', 'got', 'mail']\n",
      "Tokenized sentence: ['ok', 'then', 'i', 'come', 'n', 'pick', 'u', 'at', 'engin']\n",
      "After stop words removal: ['ok', 'come', 'n', 'pick', 'u', 'engin']\n",
      "After stemming with porters algorithm: ['come', 'pick', 'engin']\n",
      "Tokenized sentence: ['forwarded', 'from', 'hi', 'this', 'is', 'your', 'mailbox', 'messaging', 'sms', 'alert', 'you', 'have', 'matches', 'please', 'call', 'back', 'on', 'to', 'retrieve', 'your', 'messages', 'and', 'matches', 'cc', 'p', 'min']\n",
      "After stop words removal: ['forwarded', 'hi', 'mailbox', 'messaging', 'sms', 'alert', 'matches', 'please', 'call', 'back', 'retrieve', 'messages', 'matches', 'cc', 'p', 'min']\n",
      "messag\n",
      "After stemming with porters algorithm: ['forwar', 'mailbox', 'messag', 'sm', 'alert', 'match', 'pleas', 'call', 'back', 'retriev', 'messag', 'match', 'min']\n",
      "Tokenized sentence: ['no', 'i', 'got', 'rumour', 'that', 'you', 'going', 'to', 'buy', 'apartment', 'in', 'chennai']\n",
      "After stop words removal: ['got', 'rumour', 'going', 'buy', 'apartment', 'chennai']\n",
      "go\n",
      "After stemming with porters algorithm: ['got', 'rumour', 'go', 'bui', 'apart', 'chennai']\n",
      "Tokenized sentence: ['dont', 'kick', 'coco', 'when', 'he', 's', 'down']\n",
      "After stop words removal: ['dont', 'kick', 'coco']\n",
      "After stemming with porters algorithm: ['dont', 'kick', 'coco']\n",
      "Tokenized sentence: ['you', 'in', 'your', 'room', 'i', 'need', 'a', 'few']\n",
      "After stop words removal: ['room', 'need']\n",
      "After stemming with porters algorithm: ['room', 'need']\n",
      "Tokenized sentence: ['will', 'be', 'out', 'of', 'class', 'in', 'a', 'few', 'hours', 'sorry']\n",
      "After stop words removal: ['class', 'hours', 'sorry']\n",
      "After stemming with porters algorithm: ['class', 'hour', 'sorri']\n",
      "Tokenized sentence: ['not', 'able', 'to', 'do', 'anything']\n",
      "After stop words removal: ['able', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['abl', 'anyt']\n",
      "Tokenized sentence: ['r', 'we', 'going', 'with', 'the', 'lt', 'gt', 'bus']\n",
      "After stop words removal: ['r', 'going', 'lt', 'gt', 'bus']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'bu']\n",
      "Tokenized sentence: ['lyricalladie', 'f', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'www', 'sms', 'ac', 'u', 'hmmross', 'stop', 'send', 'stop', 'frnd', 'to']\n",
      "After stop words removal: ['lyricalladie', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'www', 'sms', 'ac', 'u', 'hmmross', 'stop', 'send', 'stop', 'frnd']\n",
      "invit\n",
      "After stemming with porters algorithm: ['lyricalladi', 'invit', 'friend', 'repli', 'ye', 'see', 'www', 'sm', 'hmmross', 'stop', 'send', 'stop', 'frnd']\n",
      "Tokenized sentence: ['i', 'dun', 'thk', 'i', 'll', 'quit', 'yet', 'hmmm', 'can', 'go', 'jazz', 'yogasana', 'oso', 'can', 'we', 'can', 'go', 'meet', 'em', 'after', 'our', 'lessons', 'den']\n",
      "After stop words removal: ['dun', 'thk', 'quit', 'yet', 'hmmm', 'go', 'jazz', 'yogasana', 'oso', 'go', 'meet', 'em', 'lessons', 'den']\n",
      "After stemming with porters algorithm: ['dun', 'thk', 'quit', 'yet', 'hmmm', 'jazz', 'yogasana', 'oso', 'meet', 'lesson', 'den']\n",
      "Tokenized sentence: ['i', 'm', 'coming', 'back', 'on', 'thursday', 'yay', 'is', 'it', 'gonna', 'be', 'ok', 'to', 'get', 'the', 'money', 'cheers', 'oh', 'yeah', 'and', 'how', 'are', 'you', 'everything', 'alright', 'hows', 'school', 'or', 'do', 'you', 'call', 'it', 'work', 'now']\n",
      "After stop words removal: ['coming', 'back', 'thursday', 'yay', 'gonna', 'ok', 'get', 'money', 'cheers', 'oh', 'yeah', 'everything', 'alright', 'hows', 'school', 'call', 'work']\n",
      "com\n",
      "everyth\n",
      "After stemming with porters algorithm: ['come', 'back', 'thursdai', 'yai', 'gonna', 'get', 'monei', 'cheer', 'yeah', 'everyt', 'alright', 'how', 'school', 'call', 'work']\n",
      "Tokenized sentence: ['ok', 'ok', 'take', 'care', 'i', 'can', 'understand']\n",
      "After stop words removal: ['ok', 'ok', 'take', 'care', 'understand']\n",
      "After stemming with porters algorithm: ['take', 'care', 'understand']\n",
      "Tokenized sentence: ['yeah', 'right', 'i', 'll', 'bring', 'my', 'tape', 'measure', 'fri']\n",
      "After stop words removal: ['yeah', 'right', 'bring', 'tape', 'measure', 'fri']\n",
      "After stemming with porters algorithm: ['yeah', 'right', 'bring', 'tape', 'measur', 'fri']\n",
      "Tokenized sentence: ['what', 'pa', 'tell', 'me', 'i', 'went', 'to', 'bath']\n",
      "After stop words removal: ['pa', 'tell', 'went', 'bath']\n",
      "After stemming with porters algorithm: ['tell', 'went', 'bath']\n",
      "Tokenized sentence: ['urgent', 'please', 'call', 'from', 'landline', 'cash', 'or', 'a', 'luxury', 'canary', 'islands', 'holiday', 'await', 'collection', 't', 'cs', 'sae', 'po', 'box', 'm', 'xy', 'ppm']\n",
      "After stop words removal: ['urgent', 'please', 'call', 'landline', 'cash', 'luxury', 'canary', 'islands', 'holiday', 'await', 'collection', 'cs', 'sae', 'po', 'box', 'xy', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'pleas', 'call', 'landlin', 'cash', 'luxuri', 'canari', 'island', 'holidai', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['hey', 'gals', 'u', 'all', 'wanna', 'meet', 'dinner', 'at', 'n', 'te']\n",
      "After stop words removal: ['hey', 'gals', 'u', 'wanna', 'meet', 'dinner', 'n', 'te']\n",
      "After stemming with porters algorithm: ['hei', 'gal', 'wanna', 'meet', 'dinner']\n",
      "Tokenized sentence: ['lol', 'no', 'ouch', 'but', 'wish', 'i', 'd', 'stayed', 'out', 'a', 'bit', 'longer']\n",
      "After stop words removal: ['lol', 'ouch', 'wish', 'stayed', 'bit', 'longer']\n",
      "After stemming with porters algorithm: ['lol', 'ouch', 'wish', 'stai', 'bit', 'longer']\n",
      "Tokenized sentence: ['k', 'k', 'from', 'tomorrow', 'onwards', 'started', 'ah']\n",
      "After stop words removal: ['k', 'k', 'tomorrow', 'onwards', 'started', 'ah']\n",
      "After stemming with porters algorithm: ['tomorrow', 'onward', 'star']\n",
      "Tokenized sentence: ['give', 'me', 'a', 'sec', 'to', 'think', 'think', 'about', 'it']\n",
      "After stop words removal: ['give', 'sec', 'think', 'think']\n",
      "After stemming with porters algorithm: ['give', 'sec', 'think', 'think']\n",
      "Tokenized sentence: ['right', 'on', 'brah', 'see', 'you', 'later']\n",
      "After stop words removal: ['right', 'brah', 'see', 'later']\n",
      "After stemming with porters algorithm: ['right', 'brah', 'see', 'later']\n",
      "Tokenized sentence: ['ok', 'i', 'asked', 'for', 'money', 'how', 'far']\n",
      "After stop words removal: ['ok', 'asked', 'money', 'far']\n",
      "After stemming with porters algorithm: ['as', 'monei', 'far']\n",
      "Tokenized sentence: ['ok', 'sweet', 'dreams']\n",
      "After stop words removal: ['ok', 'sweet', 'dreams']\n",
      "After stemming with porters algorithm: ['sweet', 'dream']\n",
      "Tokenized sentence: ['do', 'u', 'ever', 'get', 'a', 'song', 'stuck', 'in', 'your', 'head', 'for', 'no', 'reason', 'and', 'it', 'won', 't', 'go', 'away', 'til', 'u', 'listen', 'to', 'it', 'like', 'times']\n",
      "After stop words removal: ['u', 'ever', 'get', 'song', 'stuck', 'head', 'reason', 'go', 'away', 'til', 'u', 'listen', 'like', 'times']\n",
      "After stemming with porters algorithm: ['ever', 'get', 'song', 'stuck', 'head', 'reason', 'awai', 'til', 'listen', 'like', 'time']\n",
      "Tokenized sentence: ['you', 'have', 'new', 'message', 'call']\n",
      "After stop words removal: ['new', 'message', 'call']\n",
      "After stemming with porters algorithm: ['new', 'messag', 'call']\n",
      "Tokenized sentence: ['nimbomsons', 'yep', 'phone', 'knows', 'that', 'one', 'obviously', 'cos', 'thats', 'a', 'real', 'word']\n",
      "After stop words removal: ['nimbomsons', 'yep', 'phone', 'knows', 'one', 'obviously', 'cos', 'thats', 'real', 'word']\n",
      "After stemming with porters algorithm: ['nimbomson', 'yep', 'phone', 'know', 'on', 'obvious', 'co', 'that', 'real', 'word']\n",
      "Tokenized sentence: ['aight', 'can', 'you', 'text', 'me', 'the', 'address']\n",
      "After stop words removal: ['aight', 'text', 'address']\n",
      "After stemming with porters algorithm: ['aight', 'text', 'address']\n",
      "Tokenized sentence: ['tired', 'i', 'haven', 't', 'slept', 'well', 'the', 'past', 'few', 'nights']\n",
      "After stop words removal: ['tired', 'slept', 'well', 'past', 'nights']\n",
      "After stemming with porters algorithm: ['tire', 'slept', 'well', 'past', 'night']\n",
      "Tokenized sentence: ['i', 've', 'told', 'you', 'everything', 'will', 'stop', 'just', 'dont', 'let', 'her', 'get', 'dehydrated']\n",
      "After stop words removal: ['told', 'everything', 'stop', 'dont', 'let', 'get', 'dehydrated']\n",
      "everyth\n",
      "dehydrate\n",
      "After stemming with porters algorithm: ['told', 'everyt', 'stop', 'dont', 'let', 'get', 'dehydrat']\n",
      "Tokenized sentence: ['haha', 'figures', 'well', 'i', 'found', 'the', 'piece', 'and', 'priscilla', 's', 'bowl']\n",
      "After stop words removal: ['haha', 'figures', 'well', 'found', 'piece', 'priscilla', 'bowl']\n",
      "After stemming with porters algorithm: ['haha', 'figur', 'well', 'found', 'piec', 'priscilla', 'bowl']\n",
      "Tokenized sentence: ['waaaat', 'lololo', 'ok', 'next', 'time', 'then']\n",
      "After stop words removal: ['waaaat', 'lololo', 'ok', 'next', 'time']\n",
      "After stemming with porters algorithm: ['waaaat', 'lololo', 'next', 'time']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['be', 'sure', 'to', 'check', 'your', 'yahoo', 'email', 'we', 'sent', 'photos', 'yesterday']\n",
      "After stop words removal: ['sure', 'check', 'yahoo', 'email', 'sent', 'photos', 'yesterday']\n",
      "After stemming with porters algorithm: ['sure', 'check', 'yahoo', 'email', 'sent', 'photo', 'yesterdai']\n",
      "Tokenized sentence: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'a', 'top', 'sony', 'dvd', 'player', 'if', 'u', 'know', 'which', 'country', 'the', 'algarve', 'is', 'in', 'txt', 'ansr', 'to', 'sp', 'tyrone']\n",
      "After stop words removal: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'top', 'sony', 'dvd', 'player', 'u', 'know', 'country', 'algarve', 'txt', 'ansr', 'sp', 'tyrone']\n",
      "After stemming with porters algorithm: ['sunshin', 'quiz', 'wkly', 'win', 'top', 'soni', 'dvd', 'player', 'know', 'countri', 'algarv', 'txt', 'ansr', 'tyrone']\n",
      "Tokenized sentence: ['as', 'per', 'your', 'request', 'maangalyam', 'alaipayuthe', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', 'to', 'copy', 'your', 'friends', 'callertune']\n",
      "After stop words removal: ['per', 'request', 'maangalyam', 'alaipayuthe', 'set', 'callertune', 'callers', 'press', 'copy', 'friends', 'callertune']\n",
      "After stemming with porters algorithm: ['per', 'request', 'maangalyam', 'alaipayuth', 'set', 'callertun', 'caller', 'press', 'copi', 'friend', 'callertun']\n",
      "Tokenized sentence: ['can', 'i', 'meet', 'at', 'as', 'where', 'depends', 'on', 'where', 'wan', 'in', 'lor']\n",
      "After stop words removal: ['meet', 'depends', 'wan', 'lor']\n",
      "After stemming with porters algorithm: ['meet', 'depend', 'wan', 'lor']\n",
      "Tokenized sentence: ['hi', 'da', 'how', 'is', 'the', 'todays', 'class']\n",
      "After stop words removal: ['hi', 'da', 'todays', 'class']\n",
      "After stemming with porters algorithm: ['todai', 'class']\n",
      "Tokenized sentence: ['usually', 'the', 'person', 'is', 'unconscious', 'that', 's', 'in', 'children', 'but', 'in', 'adults', 'they', 'may', 'just', 'behave', 'abnormally', 'i', 'll', 'call', 'you', 'now']\n",
      "After stop words removal: ['usually', 'person', 'unconscious', 'children', 'adults', 'may', 'behave', 'abnormally', 'call']\n",
      "After stemming with porters algorithm: ['usual', 'person', 'unconsci', 'children', 'adult', 'mai', 'behav', 'abnorm', 'call']\n",
      "Tokenized sentence: ['it', 'shall', 'be', 'fine', 'i', 'have', 'avalarr', 'now', 'will', 'hollalater']\n",
      "After stop words removal: ['shall', 'fine', 'avalarr', 'hollalater']\n",
      "After stemming with porters algorithm: ['shall', 'fine', 'avalarr', 'hollalat']\n",
      "Tokenized sentence: ['bloody', 'hell', 'cant', 'believe', 'you', 'forgot', 'my', 'surname', 'mr', 'ill', 'give', 'u', 'a', 'clue', 'its', 'spanish', 'and', 'begins', 'with', 'm']\n",
      "After stop words removal: ['bloody', 'hell', 'cant', 'believe', 'forgot', 'surname', 'mr', 'ill', 'give', 'u', 'clue', 'spanish', 'begins']\n",
      "After stemming with porters algorithm: ['bloodi', 'hell', 'cant', 'believ', 'forgot', 'surnam', 'ill', 'give', 'clue', 'spanish', 'begin']\n",
      "Tokenized sentence: ['dear', 'how', 'you', 'are', 'you', 'ok']\n",
      "After stop words removal: ['dear', 'ok']\n",
      "After stemming with porters algorithm: ['dear']\n",
      "Tokenized sentence: ['i', 'will', 'see', 'in', 'half', 'an', 'hour']\n",
      "After stop words removal: ['see', 'half', 'hour']\n",
      "After stemming with porters algorithm: ['see', 'half', 'hour']\n",
      "Tokenized sentence: ['purity', 'of', 'friendship', 'between', 'two', 'is', 'not', 'about', 'smiling', 'after', 'reading', 'the', 'forwarded', 'message', 'its', 'about', 'smiling', 'just', 'by', 'seeing', 'the', 'name', 'gud', 'evng', 'musthu']\n",
      "After stop words removal: ['purity', 'friendship', 'two', 'smiling', 'reading', 'forwarded', 'message', 'smiling', 'seeing', 'name', 'gud', 'evng', 'musthu']\n",
      "smil\n",
      "read\n",
      "smil\n",
      "see\n",
      "After stemming with porters algorithm: ['puriti', 'friendship', 'two', 'smile', 'read', 'forwar', 'messag', 'smile', 'see', 'name', 'gud', 'evng', 'musthu']\n",
      "Tokenized sentence: ['freemsg', 'month', 'unlimited', 'free', 'calls', 'activate', 'smartcall', 'txt', 'call', 'to', 'no', 'subscriptn', 'gbp', 'wk', 'unlimited', 'calls', 'help', 'stop', 'txt', 'stop', 'landlineonly']\n",
      "After stop words removal: ['freemsg', 'month', 'unlimited', 'free', 'calls', 'activate', 'smartcall', 'txt', 'call', 'subscriptn', 'gbp', 'wk', 'unlimited', 'calls', 'help', 'stop', 'txt', 'stop', 'landlineonly']\n",
      "After stemming with porters algorithm: ['freemsg', 'month', 'unlimit', 'free', 'call', 'activ', 'smartcal', 'txt', 'call', 'subscriptn', 'gbp', 'unlimit', 'call', 'help', 'stop', 'txt', 'stop', 'landlineonli']\n",
      "Tokenized sentence: ['fun', 'fact', 'although', 'you', 'would', 'think', 'armand', 'would', 'eventually', 'build', 'up', 'a', 'tolerance', 'or', 'some', 'shit', 'considering', 'how', 'much', 'he', 'smokes', 'he', 'gets', 'fucked', 'up', 'in', 'like', 'hits']\n",
      "After stop words removal: ['fun', 'fact', 'although', 'would', 'think', 'armand', 'would', 'eventually', 'build', 'tolerance', 'shit', 'considering', 'much', 'smokes', 'gets', 'fucked', 'like', 'hits']\n",
      "consider\n",
      "After stemming with porters algorithm: ['fun', 'fact', 'although', 'would', 'think', 'armand', 'would', 'eventu', 'build', 'toler', 'shit', 'consid', 'much', 'smoke', 'get', 'fuc', 'like', 'hit']\n",
      "Tokenized sentence: ['what', 'u', 'mean', 'u', 'almost', 'done', 'done', 'wif', 'sleeping', 'but', 'i', 'tot', 'u', 'going', 'to', 'take', 'a', 'nap', 'yup', 'i', 'send', 'her', 'liao', 'so', 'i', 'm', 'picking', 'her', 'up', 'at', 'ard', 'smth', 'lor']\n",
      "After stop words removal: ['u', 'mean', 'u', 'almost', 'done', 'done', 'wif', 'sleeping', 'tot', 'u', 'going', 'take', 'nap', 'yup', 'send', 'liao', 'picking', 'ard', 'smth', 'lor']\n",
      "sleep\n",
      "go\n",
      "pick\n",
      "After stemming with porters algorithm: ['mean', 'almost', 'done', 'done', 'wif', 'sleep', 'tot', 'go', 'take', 'nap', 'yup', 'send', 'liao', 'pic', 'ard', 'smth', 'lor']\n",
      "Tokenized sentence: ['squeeeeeze', 'this', 'is', 'christmas', 'hug', 'if', 'u', 'lik', 'my', 'frndshp', 'den', 'hug', 'me', 'back', 'if', 'u', 'get', 'u', 'r', 'cute', 'u', 'r', 'luvd', 'u', 'r', 'so', 'lucky', 'none', 'people', 'hate', 'u']\n",
      "After stop words removal: ['squeeeeeze', 'christmas', 'hug', 'u', 'lik', 'frndshp', 'den', 'hug', 'back', 'u', 'get', 'u', 'r', 'cute', 'u', 'r', 'luvd', 'u', 'r', 'lucky', 'none', 'people', 'hate', 'u']\n",
      "After stemming with porters algorithm: ['squeeeeez', 'christma', 'hug', 'lik', 'frndshp', 'den', 'hug', 'back', 'get', 'cute', 'luvd', 'lucki', 'none', 'peopl', 'hate']\n",
      "Tokenized sentence: ['we', 'r', 'stayin', 'here', 'an', 'extra', 'week', 'back', 'next', 'wed', 'how', 'did', 'we', 'do', 'in', 'the', 'rugby', 'this', 'weekend', 'hi', 'to', 'and', 'and', 'c', 'u', 'soon', 'ham']\n",
      "After stop words removal: ['r', 'stayin', 'extra', 'week', 'back', 'next', 'wed', 'rugby', 'weekend', 'hi', 'c', 'u', 'soon', 'ham']\n",
      "After stemming with porters algorithm: ['stayin', 'extra', 'week', 'back', 'next', 'wed', 'rugbi', 'weekend', 'soon', 'ham']\n",
      "Tokenized sentence: ['yo', 'you', 'around', 'a', 'friend', 'of', 'mine', 's', 'lookin', 'to', 'pick', 'up', 'later', 'tonight']\n",
      "After stop words removal: ['yo', 'around', 'friend', 'mine', 'lookin', 'pick', 'later', 'tonight']\n",
      "After stemming with porters algorithm: ['around', 'friend', 'mine', 'lookin', 'pick', 'later', 'tonight']\n",
      "Tokenized sentence: ['gimme', 'a', 'few', 'was', 'lt', 'gt', 'minutes', 'ago']\n",
      "After stop words removal: ['gimme', 'lt', 'gt', 'minutes', 'ago']\n",
      "After stemming with porters algorithm: ['gimm', 'minut', 'ago']\n",
      "Tokenized sentence: ['had', 'your', 'mobile', 'mths', 'update', 'for', 'free', 'to', 'oranges', 'latest', 'colour', 'camera', 'mobiles', 'unlimited', 'weekend', 'calls', 'call', 'mobile', 'upd', 'on', 'freefone', 'or', 'stoptxt']\n",
      "After stop words removal: ['mobile', 'mths', 'update', 'free', 'oranges', 'latest', 'colour', 'camera', 'mobiles', 'unlimited', 'weekend', 'calls', 'call', 'mobile', 'upd', 'freefone', 'stoptxt']\n",
      "After stemming with porters algorithm: ['mobil', 'mth', 'updat', 'free', 'orang', 'latest', 'colour', 'camera', 'mobil', 'unlimit', 'weekend', 'call', 'call', 'mobil', 'upd', 'freefon', 'stoptxt']\n",
      "Tokenized sentence: ['horrible', 'u', 'eat', 'macs', 'eat', 'until', 'u', 'forgot', 'abt', 'me', 'already', 'rite', 'u', 'take', 'so', 'long', 'reply', 'i', 'thk', 'it', 's', 'more', 'toot', 'than', 'b', 'so', 'b', 'prepared', 'now', 'wat', 'shall', 'i', 'eat']\n",
      "After stop words removal: ['horrible', 'u', 'eat', 'macs', 'eat', 'u', 'forgot', 'abt', 'already', 'rite', 'u', 'take', 'long', 'reply', 'thk', 'toot', 'b', 'b', 'prepared', 'wat', 'shall', 'eat']\n",
      "After stemming with porters algorithm: ['horrib', 'eat', 'mac', 'eat', 'forgot', 'abt', 'alreadi', 'rite', 'take', 'long', 'repli', 'thk', 'toot', 'prepar', 'wat', 'shall', 'eat']\n",
      "Tokenized sentence: ['ur', 'cash', 'balance', 'is', 'currently', 'pounds', 'to', 'maximize', 'ur', 'cash', 'in', 'now', 'send', 'go', 'to', 'only', 'p', 'msg', 'cc', 'po', 'box', 'tcr', 'w']\n",
      "After stop words removal: ['ur', 'cash', 'balance', 'currently', 'pounds', 'maximize', 'ur', 'cash', 'send', 'go', 'p', 'msg', 'cc', 'po', 'box', 'tcr', 'w']\n",
      "After stemming with porters algorithm: ['cash', 'balanc', 'current', 'pound', 'maxim', 'cash', 'send', 'msg', 'box', 'tcr']\n",
      "Tokenized sentence: ['prepare', 'to', 'be', 'pounded', 'every', 'night']\n",
      "After stop words removal: ['prepare', 'pounded', 'every', 'night']\n",
      "After stemming with porters algorithm: ['prepar', 'poun', 'everi', 'night']\n",
      "Tokenized sentence: ['well', 'i', 'was', 'about', 'to', 'give', 'up', 'cos', 'they', 'all', 'said', 'no', 'they', 'didn', 't', 'do', 'one', 'nighters', 'i', 'persevered', 'and', 'found', 'one', 'but', 'it', 'is', 'very', 'cheap', 'so', 'i', 'apologise', 'in', 'advance', 'it', 'is', 'just', 'somewhere', 'to', 'sleep', 'isnt', 'it']\n",
      "After stop words removal: ['well', 'give', 'cos', 'said', 'one', 'nighters', 'persevered', 'found', 'one', 'cheap', 'apologise', 'advance', 'somewhere', 'sleep', 'isnt']\n",
      "After stemming with porters algorithm: ['well', 'give', 'co', 'said', 'on', 'nighter', 'persev', 'found', 'on', 'cheap', 'apologis', 'advanc', 'somewher', 'sleep', 'isnt']\n",
      "Tokenized sentence: ['pls', 'accept', 'me', 'for', 'one', 'day', 'or', 'am', 'begging', 'you', 'change', 'the', 'number']\n",
      "After stop words removal: ['pls', 'accept', 'one', 'day', 'begging', 'change', 'number']\n",
      "begg\n",
      "After stemming with porters algorithm: ['pl', 'accept', 'on', 'dai', 'beg', 'chang', 'number']\n",
      "Tokenized sentence: ['hey', 'they', 'r', 'not', 'watching', 'movie', 'tonight', 'so', 'i', 'll', 'prob', 'b', 'home', 'early']\n",
      "After stop words removal: ['hey', 'r', 'watching', 'movie', 'tonight', 'prob', 'b', 'home', 'early']\n",
      "watch\n",
      "After stemming with porters algorithm: ['hei', 'watc', 'movi', 'tonight', 'prob', 'home', 'earli']\n",
      "Tokenized sentence: ['every', 'king', 'was', 'once', 'a', 'crying', 'baby', 'and', 'every', 'great', 'building', 'was', 'once', 'a', 'map', 'not', 'imprtant', 'where', 'u', 'r', 'today', 'but', 'where', 'u', 'wil', 'reach', 'tomorw', 'gud', 'ni']\n",
      "After stop words removal: ['every', 'king', 'crying', 'baby', 'every', 'great', 'building', 'map', 'imprtant', 'u', 'r', 'today', 'u', 'wil', 'reach', 'tomorw', 'gud', 'ni']\n",
      "build\n",
      "After stemming with porters algorithm: ['everi', 'king', 'crying', 'babi', 'everi', 'great', 'buil', 'map', 'imprtant', 'todai', 'wil', 'reach', 'tomorw', 'gud']\n",
      "Tokenized sentence: ['k', 'k', 'then', 'watch', 'some', 'films']\n",
      "After stop words removal: ['k', 'k', 'watch', 'films']\n",
      "After stemming with porters algorithm: ['watch', 'film']\n",
      "Tokenized sentence: ['no', 'she', 's', 'currently', 'in', 'scotland', 'for', 'that']\n",
      "After stop words removal: ['currently', 'scotland']\n",
      "After stemming with porters algorithm: ['current', 'scotland']\n",
      "Tokenized sentence: ['yeah', 'don', 't', 'go', 'to', 'bed', 'i', 'll', 'be', 'back', 'before', 'midnight']\n",
      "After stop words removal: ['yeah', 'go', 'bed', 'back', 'midnight']\n",
      "After stemming with porters algorithm: ['yeah', 'bed', 'back', 'midnight']\n",
      "Tokenized sentence: ['just', 'haven', 't', 'decided', 'where', 'yet', 'eh']\n",
      "After stop words removal: ['decided', 'yet', 'eh']\n",
      "After stemming with porters algorithm: ['decid', 'yet']\n",
      "Tokenized sentence: ['hi', 'darlin', 'its', 'kate', 'are', 'u', 'up', 'for', 'doin', 'somethin', 'tonight', 'im', 'going', 'to', 'a', 'pub', 'called', 'the', 'swan', 'or', 'something', 'with', 'my', 'parents', 'for', 'one', 'drink', 'so', 'phone', 'me', 'if', 'u', 'can']\n",
      "After stop words removal: ['hi', 'darlin', 'kate', 'u', 'doin', 'somethin', 'tonight', 'im', 'going', 'pub', 'called', 'swan', 'something', 'parents', 'one', 'drink', 'phone', 'u']\n",
      "go\n",
      "someth\n",
      "After stemming with porters algorithm: ['darlin', 'kate', 'doin', 'somethin', 'tonight', 'go', 'pub', 'call', 'swan', 'somet', 'parent', 'on', 'drink', 'phone']\n",
      "Tokenized sentence: ['ello', 'babe', 'u', 'ok']\n",
      "After stop words removal: ['ello', 'babe', 'u', 'ok']\n",
      "After stemming with porters algorithm: ['ello', 'babe']\n",
      "Tokenized sentence: ['haha', 'i', 'think', 'i', 'did', 'too']\n",
      "After stop words removal: ['haha', 'think']\n",
      "After stemming with porters algorithm: ['haha', 'think']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'nokia', 'i', 'colour', 'phone', 'deliveredtomorrow', 'with', 'free', 'minutes', 'to', 'any', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'or', 'call']\n",
      "After stop words removal: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minutes', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minut', 'mobil', 'free', 'text', 'free', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['my', 'phone']\n",
      "After stop words removal: ['phone']\n",
      "After stemming with porters algorithm: ['phone']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'anytime', 'any', 'network', 'mins', 'text', 'and', 'a', 'new', 'video', 'phone', 'for', 'only', 'five', 'pounds', 'per', 'week', 'call', 'or', 'reply', 'for', 'delivery', 'tomorrow']\n",
      "After stop words removal: ['want', 'anytime', 'network', 'mins', 'text', 'new', 'video', 'phone', 'five', 'pounds', 'per', 'week', 'call', 'reply', 'delivery', 'tomorrow']\n",
      "After stemming with porters algorithm: ['want', 'anytim', 'network', 'min', 'text', 'new', 'video', 'phone', 'five', 'pound', 'per', 'week', 'call', 'repli', 'deliveri', 'tomorrow']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'inside', 'office', 'still', 'filling', 'forms', 'don', 'know', 'when', 'they', 'leave', 'me']\n",
      "After stop words removal: ['inside', 'office', 'still', 'filling', 'forms', 'know', 'leave']\n",
      "fill\n",
      "After stemming with porters algorithm: ['insid', 'offic', 'still', 'fill', 'form', 'know', 'leav']\n",
      "Tokenized sentence: ['lol', 'have', 'you', 'made', 'plans', 'for', 'new', 'years']\n",
      "After stop words removal: ['lol', 'made', 'plans', 'new', 'years']\n",
      "After stemming with porters algorithm: ['lol', 'made', 'plan', 'new', 'year']\n",
      "Tokenized sentence: ['alright', 'i', 'm', 'out', 'have', 'a', 'good', 'night']\n",
      "After stop words removal: ['alright', 'good', 'night']\n",
      "After stemming with porters algorithm: ['alright', 'good', 'night']\n",
      "Tokenized sentence: ['summers', 'finally', 'here', 'fancy', 'a', 'chat', 'or', 'flirt', 'with', 'sexy', 'singles', 'in', 'yr', 'area', 'to', 'get', 'matched', 'up', 'just', 'reply', 'summer', 'now', 'free', 'join', 'optout', 'txt', 'stop', 'help']\n",
      "After stop words removal: ['summers', 'finally', 'fancy', 'chat', 'flirt', 'sexy', 'singles', 'yr', 'area', 'get', 'matched', 'reply', 'summer', 'free', 'join', 'optout', 'txt', 'stop', 'help']\n",
      "After stemming with porters algorithm: ['summer', 'final', 'fanci', 'chat', 'flirt', 'sexi', 'singl', 'area', 'get', 'matc', 'repli', 'summer', 'free', 'join', 'optout', 'txt', 'stop', 'help']\n",
      "Tokenized sentence: ['loosu', 'go', 'to', 'hospital', 'de', 'dont', 'let', 'it', 'careless']\n",
      "After stop words removal: ['loosu', 'go', 'hospital', 'de', 'dont', 'let', 'careless']\n",
      "After stemming with porters algorithm: ['loosu', 'hospit', 'dont', 'let', 'careless']\n",
      "Tokenized sentence: ['i', 'm', 'used', 'to', 'it', 'i', 'just', 'hope', 'my', 'agents', 'don', 't', 'drop', 'me', 'since', 'i', 've', 'only', 'booked', 'a', 'few', 'things', 'this', 'year', 'this', 'whole', 'me', 'in', 'boston', 'them', 'in', 'nyc', 'was', 'an', 'experiment']\n",
      "After stop words removal: ['used', 'hope', 'agents', 'drop', 'since', 'booked', 'things', 'year', 'whole', 'boston', 'nyc', 'experiment']\n",
      "After stemming with porters algorithm: ['us', 'hope', 'agent', 'drop', 'sinc', 'book', 'thing', 'year', 'whole', 'boston', 'nyc', 'experi']\n",
      "Tokenized sentence: ['i', 'am', 'going', 'to', 'sao', 'mu', 'today', 'will', 'be', 'done', 'only', 'at']\n",
      "After stop words removal: ['going', 'sao', 'mu', 'today', 'done']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'sao', 'todai', 'done']\n",
      "Tokenized sentence: ['no', 'management', 'puzzeles']\n",
      "After stop words removal: ['management', 'puzzeles']\n",
      "After stemming with porters algorithm: ['manag', 'puzzel']\n",
      "Tokenized sentence: ['you', 'flippin', 'your', 'shit', 'yet']\n",
      "After stop words removal: ['flippin', 'shit', 'yet']\n",
      "After stemming with porters algorithm: ['flippin', 'shit', 'yet']\n",
      "Tokenized sentence: ['panasonic', 'bluetoothhdset', 'free', 'nokia', 'free', 'motorola', 'free', 'doublemins', 'doubletxt', 'on', 'orange', 'contract', 'call', 'mobileupd', 'on', 'or', 'call', 'optout']\n",
      "After stop words removal: ['panasonic', 'bluetoothhdset', 'free', 'nokia', 'free', 'motorola', 'free', 'doublemins', 'doubletxt', 'orange', 'contract', 'call', 'mobileupd', 'call', 'optout']\n",
      "After stemming with porters algorithm: ['panason', 'bluetoothhdset', 'free', 'nokia', 'free', 'motorola', 'free', 'doublemin', 'doubletxt', 'orang', 'contract', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['i', 'always', 'chat', 'with', 'you', 'in', 'fact', 'i', 'need', 'money', 'can', 'you', 'raise', 'me']\n",
      "After stop words removal: ['always', 'chat', 'fact', 'need', 'money', 'raise']\n",
      "After stemming with porters algorithm: ['alwai', 'chat', 'fact', 'need', 'monei', 'rais']\n",
      "Tokenized sentence: ['awww', 'dat', 'is', 'sweet', 'we', 'can', 'think', 'of', 'something', 'to', 'do', 'he', 'he', 'have', 'a', 'nice', 'time', 'tonight', 'ill', 'probably', 'txt', 'u', 'later', 'cos', 'im', 'lonely', 'xxx']\n",
      "After stop words removal: ['awww', 'dat', 'sweet', 'think', 'something', 'nice', 'time', 'tonight', 'ill', 'probably', 'txt', 'u', 'later', 'cos', 'im', 'lonely', 'xxx']\n",
      "someth\n",
      "After stemming with porters algorithm: ['awww', 'dat', 'sweet', 'think', 'somet', 'nice', 'time', 'tonight', 'ill', 'probab', 'txt', 'later', 'co', 'lone', 'xxx']\n",
      "Tokenized sentence: ['yup', 'wun', 'believe', 'wat', 'u', 'really', 'neva', 'c', 'e', 'msg', 'i', 'sent', 'shuhui']\n",
      "After stop words removal: ['yup', 'wun', 'believe', 'wat', 'u', 'really', 'neva', 'c', 'e', 'msg', 'sent', 'shuhui']\n",
      "After stemming with porters algorithm: ['yup', 'wun', 'believ', 'wat', 'realli', 'neva', 'msg', 'sent', 'shuhui']\n",
      "Tokenized sentence: ['captain', 'vijaykanth', 'is', 'doing', 'comedy', 'in', 'captain', 'tv', 'he', 'is', 'drunken']\n",
      "After stop words removal: ['captain', 'vijaykanth', 'comedy', 'captain', 'tv', 'drunken']\n",
      "After stemming with porters algorithm: ['captain', 'vijaykanth', 'comedi', 'captain', 'drunken']\n",
      "Tokenized sentence: ['hi', 'harish', 's', 'rent', 'has', 'been', 'transfred', 'to', 'ur', 'acnt']\n",
      "After stop words removal: ['hi', 'harish', 'rent', 'transfred', 'ur', 'acnt']\n",
      "After stemming with porters algorithm: ['harish', 'rent', 'transf', 'acnt']\n",
      "Tokenized sentence: ['me', 'fine', 'absolutly', 'fine']\n",
      "After stop words removal: ['fine', 'absolutly', 'fine']\n",
      "After stemming with porters algorithm: ['fine', 'absolutli', 'fine']\n",
      "Tokenized sentence: ['really', 'do', 'hope', 'the', 'work', 'doesnt', 'get', 'stressful', 'have', 'a', 'gr', 'day']\n",
      "After stop words removal: ['really', 'hope', 'work', 'doesnt', 'get', 'stressful', 'gr', 'day']\n",
      "After stemming with porters algorithm: ['realli', 'hope', 'work', 'doesnt', 'get', 'stress', 'dai']\n",
      "Tokenized sentence: ['reminder', 'from', 'o', 'to', 'get', 'pounds', 'free', 'call', 'credit', 'and', 'details', 'of', 'great', 'offers', 'pls', 'reply', 'this', 'text', 'with', 'your', 'valid', 'name', 'house', 'no', 'and', 'postcode']\n",
      "After stop words removal: ['reminder', 'get', 'pounds', 'free', 'call', 'credit', 'details', 'great', 'offers', 'pls', 'reply', 'text', 'valid', 'name', 'house', 'postcode']\n",
      "After stemming with porters algorithm: ['remind', 'get', 'pound', 'free', 'call', 'credit', 'detail', 'great', 'offer', 'pl', 'repli', 'text', 'valid', 'name', 'hous', 'postcod']\n",
      "Tokenized sentence: ['come', 'by', 'our', 'room', 'at', 'some', 'point', 'so', 'we', 'can', 'iron', 'out', 'the', 'plan', 'for', 'this', 'weekend']\n",
      "After stop words removal: ['come', 'room', 'point', 'iron', 'plan', 'weekend']\n",
      "After stemming with porters algorithm: ['come', 'room', 'point', 'iron', 'plan', 'weekend']\n",
      "Tokenized sentence: ['i', 'know', 'where', 'the', 'lt', 'gt', 'is', 'i', 'll', 'be', 'there', 'around']\n",
      "After stop words removal: ['know', 'lt', 'gt', 'around']\n",
      "After stemming with porters algorithm: ['know', 'around']\n",
      "Tokenized sentence: ['you', 'will', 'be', 'in', 'the', 'place', 'of', 'that', 'man']\n",
      "After stop words removal: ['place', 'man']\n",
      "After stemming with porters algorithm: ['place', 'man']\n",
      "Tokenized sentence: ['i', 'love', 'working', 'from', 'home']\n",
      "After stop words removal: ['love', 'working', 'home']\n",
      "work\n",
      "After stemming with porters algorithm: ['love', 'wor', 'home']\n",
      "Tokenized sentence: ['am', 'not', 'working', 'but', 'am', 'up', 'to', 'eyes', 'in', 'philosophy', 'so', 'will', 'text', 'u', 'later', 'when', 'a', 'bit', 'more', 'free', 'for', 'chat']\n",
      "After stop words removal: ['working', 'eyes', 'philosophy', 'text', 'u', 'later', 'bit', 'free', 'chat']\n",
      "work\n",
      "After stemming with porters algorithm: ['wor', 'ey', 'philosophi', 'text', 'later', 'bit', 'free', 'chat']\n",
      "Tokenized sentence: ['i', 'didnt', 'get', 'ur', 'full', 'msg', 'sometext', 'is', 'missing', 'send', 'it', 'again']\n",
      "After stop words removal: ['didnt', 'get', 'ur', 'full', 'msg', 'sometext', 'missing', 'send']\n",
      "miss\n",
      "After stemming with porters algorithm: ['didnt', 'get', 'full', 'msg', 'sometext', 'miss', 'send']\n",
      "Tokenized sentence: ['fyi', 'i', 'm', 'gonna', 'call', 'you', 'sporadically', 'starting', 'at', 'like', 'lt', 'gt', 'bc', 'we', 'are', 'not', 'not', 'doin', 'this', 'shit']\n",
      "After stop words removal: ['fyi', 'gonna', 'call', 'sporadically', 'starting', 'like', 'lt', 'gt', 'bc', 'doin', 'shit']\n",
      "start\n",
      "After stemming with porters algorithm: ['fyi', 'gonna', 'call', 'sporad', 'star', 'like', 'doin', 'shit']\n",
      "Tokenized sentence: ['waiting', 'my', 'tv', 'show', 'start', 'lor', 'u', 'leh', 'still', 'busy', 'doing', 'ur', 'report']\n",
      "After stop words removal: ['waiting', 'tv', 'show', 'start', 'lor', 'u', 'leh', 'still', 'busy', 'ur', 'report']\n",
      "wait\n",
      "After stemming with porters algorithm: ['wait', 'show', 'start', 'lor', 'leh', 'still', 'busi', 'report']\n",
      "Tokenized sentence: ['so', 'when', 'do', 'you', 'wanna', 'gym', 'harri']\n",
      "After stop words removal: ['wanna', 'gym', 'harri']\n",
      "After stemming with porters algorithm: ['wanna', 'gym', 'harri']\n",
      "Tokenized sentence: ['ok', 'c', 'then']\n",
      "After stop words removal: ['ok', 'c']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['no', 'objection', 'my', 'bf', 'not', 'coming']\n",
      "After stop words removal: ['objection', 'bf', 'coming']\n",
      "com\n",
      "After stemming with porters algorithm: ['object', 'come']\n",
      "Tokenized sentence: ['alright', 'omw', 'gotta', 'change', 'my', 'order', 'to', 'a', 'half', 'th']\n",
      "After stop words removal: ['alright', 'omw', 'gotta', 'change', 'order', 'half', 'th']\n",
      "After stemming with porters algorithm: ['alright', 'omw', 'gotta', 'chang', 'order', 'half']\n",
      "Tokenized sentence: ['yup', 'i', 'm', 'still', 'having', 'coffee', 'wif', 'my', 'frens', 'my', 'fren', 'drove', 'she', 'll', 'give', 'me', 'a', 'lift']\n",
      "After stop words removal: ['yup', 'still', 'coffee', 'wif', 'frens', 'fren', 'drove', 'give', 'lift']\n",
      "After stemming with porters algorithm: ['yup', 'still', 'coffe', 'wif', 'fren', 'fren', 'drove', 'give', 'lift']\n",
      "Tokenized sentence: ['thanks', 'for', 'sending', 'this', 'mental', 'ability', 'question']\n",
      "After stop words removal: ['thanks', 'sending', 'mental', 'ability', 'question']\n",
      "send\n",
      "After stemming with porters algorithm: ['thank', 'sen', 'mental', 'abil', 'quest']\n",
      "Tokenized sentence: ['oh', 'sorry', 'please', 'its', 'over']\n",
      "After stop words removal: ['oh', 'sorry', 'please']\n",
      "After stemming with porters algorithm: ['sorri', 'pleas']\n",
      "Tokenized sentence: ['what', 'are', 'youdoing', 'later', 'sar', 'xxx']\n",
      "After stop words removal: ['youdoing', 'later', 'sar', 'xxx']\n",
      "youdo\n",
      "After stemming with porters algorithm: ['youdo', 'later', 'sar', 'xxx']\n",
      "Tokenized sentence: ['s', 'no', 'competition', 'for', 'him']\n",
      "After stop words removal: ['competition']\n",
      "After stemming with porters algorithm: ['competit']\n",
      "Tokenized sentence: ['u', 'r', 'a', 'winner', 'u', 'ave', 'been', 'specially', 'selected', 'receive', 'cash', 'or', 'a', 'holiday', 'flights', 'inc', 'speak', 'to', 'a', 'live', 'operator', 'claim', 'p', 'min']\n",
      "After stop words removal: ['u', 'r', 'winner', 'u', 'ave', 'specially', 'selected', 'receive', 'cash', 'holiday', 'flights', 'inc', 'speak', 'live', 'operator', 'claim', 'p', 'min']\n",
      "After stemming with porters algorithm: ['winner', 'av', 'special', 'selec', 'receiv', 'cash', 'holidai', 'flight', 'inc', 'speak', 'live', 'oper', 'claim', 'min']\n",
      "Tokenized sentence: ['ok', 'i', 'wont', 'call', 'or', 'disturb', 'any', 'one', 'i', 'know', 'all', 'are', 'avoiding', 'me', 'i', 'am', 'a', 'burden', 'for', 'all']\n",
      "After stop words removal: ['ok', 'wont', 'call', 'disturb', 'one', 'know', 'avoiding', 'burden']\n",
      "avoid\n",
      "After stemming with porters algorithm: ['wont', 'call', 'disturb', 'on', 'know', 'avoid', 'burden']\n",
      "Tokenized sentence: ['fwiw', 'the', 'reason', 'i', 'm', 'only', 'around', 'when', 'it', 's', 'time', 'to', 'smoke', 'is', 'that', 'because', 'of', 'gas', 'i', 'can', 'only', 'afford', 'to', 'be', 'around', 'when', 'someone', 'tells', 'me', 'to', 'be', 'and', 'that', 'apparently', 'only', 'happens', 'when', 'somebody', 'wants', 'to', 'light', 'up']\n",
      "After stop words removal: ['fwiw', 'reason', 'around', 'time', 'smoke', 'gas', 'afford', 'around', 'someone', 'tells', 'apparently', 'happens', 'somebody', 'wants', 'light']\n",
      "After stemming with porters algorithm: ['fwiw', 'reason', 'around', 'time', 'smoke', 'ga', 'afford', 'around', 'someon', 'tell', 'appar', 'happen', 'somebodi', 'want', 'light']\n",
      "Tokenized sentence: ['wow', 'i', 'love', 'you', 'sooo', 'much', 'you', 'know', 'i', 'can', 'barely', 'stand', 'it', 'i', 'wonder', 'how', 'your', 'day', 'goes', 'and', 'if', 'you', 'are', 'well', 'my', 'love', 'i', 'think', 'of', 'you', 'and', 'miss', 'you']\n",
      "After stop words removal: ['wow', 'love', 'sooo', 'much', 'know', 'barely', 'stand', 'wonder', 'day', 'goes', 'well', 'love', 'think', 'miss']\n",
      "After stemming with porters algorithm: ['wow', 'love', 'sooo', 'much', 'know', 'bare', 'stand', 'wonder', 'dai', 'goe', 'well', 'love', 'think', 'miss']\n",
      "Tokenized sentence: ['i', 'm', 'reaching', 'in', 'another', 'stops']\n",
      "After stop words removal: ['reaching', 'another', 'stops']\n",
      "reach\n",
      "After stemming with porters algorithm: ['reac', 'anoth', 'stop']\n",
      "Tokenized sentence: ['staff', 'science', 'nus', 'edu', 'sg', 'phyhcmk', 'teaching', 'pc']\n",
      "After stop words removal: ['staff', 'science', 'nus', 'edu', 'sg', 'phyhcmk', 'teaching', 'pc']\n",
      "teach\n",
      "After stemming with porters algorithm: ['staff', 'scienc', 'nu', 'edu', 'phyhcmk', 'teac']\n",
      "Tokenized sentence: ['keep', 'my', 'payasam', 'there', 'if', 'rinu', 'brings']\n",
      "After stop words removal: ['keep', 'payasam', 'rinu', 'brings']\n",
      "After stemming with porters algorithm: ['keep', 'payasam', 'rinu', 'bring']\n",
      "Tokenized sentence: ['okey', 'dokey', 'swashbuckling', 'stuff', 'what', 'oh']\n",
      "After stop words removal: ['okey', 'dokey', 'swashbuckling', 'stuff', 'oh']\n",
      "swashbuckl\n",
      "After stemming with porters algorithm: ['okei', 'dokei', 'swashbuck', 'stuff']\n",
      "Tokenized sentence: ['pa', 'but', 'not', 'selected']\n",
      "After stop words removal: ['pa', 'selected']\n",
      "After stemming with porters algorithm: ['selec']\n",
      "Tokenized sentence: ['y', 'lei']\n",
      "After stop words removal: ['lei']\n",
      "After stemming with porters algorithm: ['lei']\n",
      "Tokenized sentence: ['can', 'you', 'use', 'foreign', 'stamps', 'for', 'whatever', 'you', 'send', 'them', 'off', 'for']\n",
      "After stop words removal: ['use', 'foreign', 'stamps', 'whatever', 'send']\n",
      "After stemming with porters algorithm: ['us', 'foreign', 'stamp', 'whatev', 'send']\n",
      "Tokenized sentence: ['please', 'don', 't', 'text', 'me', 'anymore', 'i', 'have', 'nothing', 'else', 'to', 'say']\n",
      "After stop words removal: ['please', 'text', 'anymore', 'nothing', 'else', 'say']\n",
      "noth\n",
      "After stemming with porters algorithm: ['pleas', 'text', 'anymor', 'not', 'els', 'sai']\n",
      "Tokenized sentence: ['i', 'remain', 'unconvinced', 'that', 'this', 'isn', 't', 'an', 'elaborate', 'test', 'of', 'my', 'willpower']\n",
      "After stop words removal: ['remain', 'unconvinced', 'elaborate', 'test', 'willpower']\n",
      "After stemming with porters algorithm: ['remain', 'unconvin', 'elabor', 'test', 'willpow']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later', 'in', 'meeting']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'meet']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'nokia', 'i', 'this', 'is', 'what', 'you', 'get', 'when', 'you', 'win', 'our', 'free', 'auction', 'to', 'take', 'part', 'send', 'nokia', 'to', 'now', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stop words removal: ['nokia', 'get', 'win', 'free', 'auction', 'take', 'part', 'send', 'nokia', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stemming with porters algorithm: ['nokia', 'get', 'win', 'free', 'auct', 'take', 'part', 'send', 'nokia', 'suit', 'land', 'row', 'jhl']\n",
      "Tokenized sentence: ['painful', 'words', 'i', 'thought', 'being', 'happy', 'was', 'the', 'most', 'toughest', 'thing', 'on', 'earth', 'but']\n",
      "After stop words removal: ['painful', 'words', 'thought', 'happy', 'toughest', 'thing', 'earth']\n",
      "After stemming with porters algorithm: ['pain', 'word', 'thought', 'happi', 'toughest', 'thing', 'earth']\n",
      "Tokenized sentence: ['i', 'can', 't', 'make', 'it', 'tonight']\n",
      "After stop words removal: ['make', 'tonight']\n",
      "After stemming with porters algorithm: ['make', 'tonight']\n",
      "Tokenized sentence: ['are', 'u', 'coming', 'to', 'the', 'funeral', 'home']\n",
      "After stop words removal: ['u', 'coming', 'funeral', 'home']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'funer', 'home']\n",
      "Tokenized sentence: ['yeah', 'confirmed', 'for', 'you', 'staying', 'at', 'that', 'weekend']\n",
      "After stop words removal: ['yeah', 'confirmed', 'staying', 'weekend']\n",
      "stay\n",
      "After stemming with porters algorithm: ['yeah', 'confir', 'stai', 'weekend']\n",
      "Tokenized sentence: ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "After stop words removal: ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "jok\n",
      "After stemming with porters algorithm: ['lar', 'joke', 'wif', 'oni']\n",
      "Tokenized sentence: ['okay', 'but', 'i', 'thought', 'you', 'were', 'the', 'expert']\n",
      "After stop words removal: ['okay', 'thought', 'expert']\n",
      "After stemming with porters algorithm: ['okai', 'thought', 'expert']\n",
      "Tokenized sentence: ['dunno', 'lei', 'all', 'decide', 'lor', 'how', 'abt', 'leona', 'oops', 'i', 'tot', 'ben', 'is', 'going', 'n', 'i', 'msg', 'him']\n",
      "After stop words removal: ['dunno', 'lei', 'decide', 'lor', 'abt', 'leona', 'oops', 'tot', 'ben', 'going', 'n', 'msg']\n",
      "go\n",
      "After stemming with porters algorithm: ['dunno', 'lei', 'decid', 'lor', 'abt', 'leona', 'oop', 'tot', 'ben', 'go', 'msg']\n",
      "Tokenized sentence: ['says', 'that', 'he', 's', 'quitting', 'at', 'least', 'times', 'a', 'day', 'so', 'i', 'wudn', 't', 'take', 'much', 'notice', 'of', 'that', 'nah', 'she', 'didn', 't', 'mind', 'are', 'you', 'gonna', 'see', 'him', 'again', 'do', 'you', 'want', 'to', 'come', 'to', 'taunton', 'tonight', 'u', 'can', 'tell', 'me', 'all', 'about']\n",
      "After stop words removal: ['says', 'quitting', 'least', 'times', 'day', 'wudn', 'take', 'much', 'notice', 'nah', 'mind', 'gonna', 'see', 'want', 'come', 'taunton', 'tonight', 'u', 'tell']\n",
      "quitt\n",
      "After stemming with porters algorithm: ['sai', 'quit', 'least', 'time', 'dai', 'wudn', 'take', 'much', 'notic', 'nah', 'mind', 'gonna', 'see', 'want', 'come', 'taunton', 'tonight', 'tell']\n",
      "Tokenized sentence: ['i', 'am', 'great', 'princess', 'what', 'are', 'you', 'thinking', 'about', 'me']\n",
      "After stop words removal: ['great', 'princess', 'thinking']\n",
      "think\n",
      "After stemming with porters algorithm: ['great', 'princess', 'thin']\n",
      "Tokenized sentence: ['haha', 'dont', 'be', 'angry', 'with', 'yourself', 'take', 'it', 'as', 'a', 'practice', 'for', 'the', 'real', 'thing']\n",
      "After stop words removal: ['haha', 'dont', 'angry', 'take', 'practice', 'real', 'thing']\n",
      "After stemming with porters algorithm: ['haha', 'dont', 'angri', 'take', 'practic', 'real', 'thing']\n",
      "Tokenized sentence: ['chile', 'please', 'it', 's', 'only', 'a', 'lt', 'decimal', 'gt', 'hour', 'drive', 'for', 'me', 'i', 'come', 'down', 'all', 'the', 'time', 'and', 'will', 'be', 'subletting', 'feb', 'april', 'for', 'audition', 'season']\n",
      "After stop words removal: ['chile', 'please', 'lt', 'decimal', 'gt', 'hour', 'drive', 'come', 'time', 'subletting', 'feb', 'april', 'audition', 'season']\n",
      "sublett\n",
      "After stemming with porters algorithm: ['chile', 'pleas', 'decim', 'hour', 'drive', 'come', 'time', 'sublet', 'feb', 'april', 'audit', 'season']\n",
      "Tokenized sentence: ['sorry', 'you', 'never', 'hear', 'unless', 'you', 'book', 'it', 'one', 'was', 'kinda', 'a', 'joke', 'thet', 'were', 'really', 'looking', 'for', 'skinny', 'white', 'girls', 'the', 'other', 'was', 'one', 'line', 'you', 'can', 'only', 'do', 'so', 'much', 'on', 'camera', 'with', 'that', 'something', 'like', 'that', 'they', 're', 'casting', 'on', 'the', 'look']\n",
      "After stop words removal: ['sorry', 'never', 'hear', 'unless', 'book', 'one', 'kinda', 'joke', 'thet', 'really', 'looking', 'skinny', 'white', 'girls', 'one', 'line', 'much', 'camera', 'something', 'like', 'casting', 'look']\n",
      "look\n",
      "someth\n",
      "cast\n",
      "After stemming with porters algorithm: ['sorri', 'never', 'hear', 'unless', 'book', 'on', 'kinda', 'joke', 'thet', 'realli', 'look', 'skinni', 'white', 'girl', 'on', 'line', 'much', 'camera', 'somet', 'like', 'cas', 'look']\n",
      "Tokenized sentence: ['gudnite', 'tc', 'practice', 'going', 'on']\n",
      "After stop words removal: ['gudnite', 'tc', 'practice', 'going']\n",
      "go\n",
      "After stemming with porters algorithm: ['gudnit', 'practic', 'go']\n",
      "Tokenized sentence: ['i', 'm', 'still', 'pretty', 'weak', 'today', 'bad', 'day']\n",
      "After stop words removal: ['still', 'pretty', 'weak', 'today', 'bad', 'day']\n",
      "After stemming with porters algorithm: ['still', 'pretti', 'weak', 'todai', 'bad', 'dai']\n",
      "Tokenized sentence: ['today', 'my', 'system', 'sh', 'get', 'ready', 'all', 'is', 'well', 'and', 'i', 'am', 'also', 'in', 'the', 'deep', 'well']\n",
      "After stop words removal: ['today', 'system', 'sh', 'get', 'ready', 'well', 'also', 'deep', 'well']\n",
      "After stemming with porters algorithm: ['todai', 'system', 'get', 'readi', 'well', 'also', 'deep', 'well']\n",
      "Tokenized sentence: ['i', 'not', 'free', 'today', 'i', 'haf', 'pick', 'my', 'parents', 'up', 'tonite']\n",
      "After stop words removal: ['free', 'today', 'haf', 'pick', 'parents', 'tonite']\n",
      "After stemming with porters algorithm: ['free', 'todai', 'haf', 'pick', 'parent', 'tonit']\n",
      "Tokenized sentence: ['gr', 'so', 'how', 'do', 'you', 'handle', 'the', 'victoria', 'island', 'traffic', 'plus', 'when', 's', 'the', 'album', 'due']\n",
      "After stop words removal: ['gr', 'handle', 'victoria', 'island', 'traffic', 'plus', 'album', 'due']\n",
      "After stemming with porters algorithm: ['handl', 'victoria', 'island', 'traffic', 'plu', 'album', 'due']\n",
      "Tokenized sentence: ['oh', 'all', 'have', 'to', 'come', 'ah']\n",
      "After stop words removal: ['oh', 'come', 'ah']\n",
      "After stemming with porters algorithm: ['come']\n",
      "Tokenized sentence: ['ok', 'how', 'you', 'dear', 'did', 'you', 'call', 'chechi']\n",
      "After stop words removal: ['ok', 'dear', 'call', 'chechi']\n",
      "After stemming with porters algorithm: ['dear', 'call', 'chechi']\n",
      "Tokenized sentence: ['where', 'are', 'you', 'what', 'are', 'you', 'doing', 'are', 'yuou', 'working', 'on', 'getting', 'the', 'pc', 'to', 'your', 'mom', 's', 'did', 'you', 'find', 'a', 'spot', 'that', 'it', 'would', 'work', 'i', 'need', 'you']\n",
      "After stop words removal: ['yuou', 'working', 'getting', 'pc', 'mom', 'find', 'spot', 'would', 'work', 'need']\n",
      "work\n",
      "gett\n",
      "After stemming with porters algorithm: ['yuou', 'wor', 'get', 'mom', 'find', 'spot', 'would', 'work', 'need']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['yes', 'princess', 'toledo']\n",
      "After stop words removal: ['yes', 'princess', 'toledo']\n",
      "After stemming with porters algorithm: ['ye', 'princess', 'toledo']\n",
      "Tokenized sentence: ['oooh', 'i', 'got', 'plenty', 'of', 'those']\n",
      "After stop words removal: ['oooh', 'got', 'plenty']\n",
      "After stemming with porters algorithm: ['oooh', 'got', 'plenti']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['hello', 'from', 'orange', 'for', 'month', 's', 'free', 'access', 'to', 'games', 'news', 'and', 'sport', 'plus', 'free', 'texts', 'and', 'photo', 'messages', 'reply', 'yes', 'terms', 'apply', 'www', 'orange', 'co', 'uk', 'ow']\n",
      "After stop words removal: ['hello', 'orange', 'month', 'free', 'access', 'games', 'news', 'sport', 'plus', 'free', 'texts', 'photo', 'messages', 'reply', 'yes', 'terms', 'apply', 'www', 'orange', 'co', 'uk', 'ow']\n",
      "After stemming with porters algorithm: ['hello', 'orang', 'month', 'free', 'access', 'game', 'new', 'sport', 'plu', 'free', 'text', 'photo', 'messag', 'repli', 'ye', 'term', 'appli', 'www', 'orang']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'grasp', 'your', 'pretty', 'booty']\n",
      "After stop words removal: ['want', 'grasp', 'pretty', 'booty']\n",
      "After stemming with porters algorithm: ['want', 'grasp', 'pretti', 'booti']\n",
      "Tokenized sentence: ['for', 'taking', 'part', 'in', 'our', 'mobile', 'survey', 'yesterday', 'you', 'can', 'now', 'have', 'texts', 'use', 'however', 'you', 'wish', 'get', 'txts', 'just', 'send', 'txt', 'to', 't', 'c', 'www', 'txt', 'com', 'p']\n",
      "After stop words removal: ['taking', 'part', 'mobile', 'survey', 'yesterday', 'texts', 'use', 'however', 'wish', 'get', 'txts', 'send', 'txt', 'c', 'www', 'txt', 'com', 'p']\n",
      "tak\n",
      "After stemming with porters algorithm: ['take', 'part', 'mobil', 'survei', 'yesterdai', 'text', 'us', 'howev', 'wish', 'get', 'txt', 'send', 'txt', 'www', 'txt', 'com']\n",
      "Tokenized sentence: ['ok', 'ok', 'ok', 'then', 'whats', 'ur', 'todays', 'plan']\n",
      "After stop words removal: ['ok', 'ok', 'ok', 'whats', 'ur', 'todays', 'plan']\n",
      "After stemming with porters algorithm: ['what', 'todai', 'plan']\n",
      "Tokenized sentence: ['i', 'can', 't', 'right', 'this', 'second', 'gotta', 'hit', 'people', 'up', 'first']\n",
      "After stop words removal: ['right', 'second', 'gotta', 'hit', 'people', 'first']\n",
      "After stemming with porters algorithm: ['right', 'second', 'gotta', 'hit', 'peopl', 'first']\n",
      "Tokenized sentence: ['am', 'i', 'the', 'only', 'one', 'who', 'doesn', 't', 'stalk', 'profiles']\n",
      "After stop words removal: ['one', 'stalk', 'profiles']\n",
      "After stemming with porters algorithm: ['on', 'stalk', 'profil']\n",
      "Tokenized sentence: ['well', 'done', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tcs', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stop words removal: ['well', 'done', 'costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tcs', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stemming with porters algorithm: ['well', 'done', 'costa', 'del', 'sol', 'holidai', 'await', 'collect', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'cost', 'max', 'min']\n",
      "Tokenized sentence: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stop words removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stemming with porters algorithm: ['free', 'week', 'nokia', 'tone', 'mob', 'everi', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'mate', 'www', 'getz', 'pobox', 'norm', 'tone']\n",
      "Tokenized sentence: ['there', 'll', 'be', 'a', 'minor', 'shindig', 'at', 'my', 'place', 'later', 'tonight', 'you', 'interested']\n",
      "After stop words removal: ['minor', 'shindig', 'place', 'later', 'tonight', 'interested']\n",
      "After stemming with porters algorithm: ['minor', 'shindig', 'place', 'later', 'tonight', 'interes']\n",
      "Tokenized sentence: ['yes', 'he', 'is', 'really', 'great', 'bhaji', 'told', 'kallis', 'best', 'cricketer', 'after', 'sachin', 'in', 'world', 'very', 'tough', 'to', 'get', 'out']\n",
      "After stop words removal: ['yes', 'really', 'great', 'bhaji', 'told', 'kallis', 'best', 'cricketer', 'sachin', 'world', 'tough', 'get']\n",
      "After stemming with porters algorithm: ['ye', 'realli', 'great', 'bhaji', 'told', 'kalli', 'best', 'cricket', 'sachin', 'world', 'tough', 'get']\n",
      "Tokenized sentence: ['she', 'is', 'our', 'sister', 'she', 'belongs', 'our', 'family', 'she', 'is', 'd', 'hope', 'of', 'tomorrow', 'pray', 'her', 'who', 'was', 'fated', 'd', 'shoranur', 'train', 'incident', 'lets', 'hold', 'our', 'hands', 'together', 'amp', 'fuelled', 'by', 'love', 'amp', 'concern', 'prior', 'her', 'grief', 'amp', 'pain', 'pls', 'join', 'in', 'dis', 'chain', 'amp', 'pass', 'it', 'stop', 'violence', 'against', 'women']\n",
      "After stop words removal: ['sister', 'belongs', 'family', 'hope', 'tomorrow', 'pray', 'fated', 'shoranur', 'train', 'incident', 'lets', 'hold', 'hands', 'together', 'amp', 'fuelled', 'love', 'amp', 'concern', 'prior', 'grief', 'amp', 'pain', 'pls', 'join', 'dis', 'chain', 'amp', 'pass', 'stop', 'violence', 'women']\n",
      "fate\n",
      "After stemming with porters algorithm: ['sister', 'belong', 'famili', 'hope', 'tomorrow', 'prai', 'fate', 'shoranur', 'train', 'incid', 'let', 'hold', 'hand', 'togeth', 'amp', 'fuell', 'love', 'amp', 'concern', 'prior', 'grief', 'amp', 'pain', 'pl', 'join', 'di', 'chain', 'amp', 'pass', 'stop', 'violenc', 'women']\n",
      "Tokenized sentence: ['what', 'time', 'is', 'ur', 'flight', 'tmr']\n",
      "After stop words removal: ['time', 'ur', 'flight', 'tmr']\n",
      "After stemming with porters algorithm: ['time', 'flight', 'tmr']\n",
      "Tokenized sentence: ['its', 'ur', 'luck', 'to', 'love', 'someone', 'its', 'ur', 'fortune', 'to', 'love', 'the', 'one', 'who', 'loves', 'u', 'but']\n",
      "After stop words removal: ['ur', 'luck', 'love', 'someone', 'ur', 'fortune', 'love', 'one', 'loves', 'u']\n",
      "After stemming with porters algorithm: ['luck', 'love', 'someon', 'fortun', 'love', 'on', 'love']\n",
      "Tokenized sentence: ['i', 'm', 'ok', 'will', 'do', 'my', 'part', 'tomorrow']\n",
      "After stop words removal: ['ok', 'part', 'tomorrow']\n",
      "After stemming with porters algorithm: ['part', 'tomorrow']\n",
      "Tokenized sentence: ['themob', 'check', 'out', 'our', 'newest', 'selection', 'of', 'content', 'games', 'tones', 'gossip', 'babes', 'and', 'sport', 'keep', 'your', 'mobile', 'fit', 'and', 'funky', 'text', 'wap', 'to']\n",
      "After stop words removal: ['themob', 'check', 'newest', 'selection', 'content', 'games', 'tones', 'gossip', 'babes', 'sport', 'keep', 'mobile', 'fit', 'funky', 'text', 'wap']\n",
      "After stemming with porters algorithm: ['themob', 'check', 'newest', 'select', 'content', 'game', 'tone', 'gossip', 'babe', 'sport', 'keep', 'mobil', 'fit', 'funki', 'text', 'wap']\n",
      "Tokenized sentence: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'of', 'd', 'day', 'my', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
      "After stop words removal: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'day', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
      "After stemming with porters algorithm: ['ummmmmaah', 'mani', 'mani', 'happi', 'return', 'dai', 'dear', 'sweet', 'heart', 'happi', 'birthdai', 'dear']\n",
      "Tokenized sentence: ['got', 'but', 'got', 'colours', 'lor', 'one', 'colour', 'is', 'quite', 'light', 'n', 'e', 'other', 'is', 'darker', 'lor', 'actually', 'i', 'm', 'done', 'she', 's', 'styling', 'my', 'hair', 'now']\n",
      "After stop words removal: ['got', 'got', 'colours', 'lor', 'one', 'colour', 'quite', 'light', 'n', 'e', 'darker', 'lor', 'actually', 'done', 'styling', 'hair']\n",
      "After stemming with porters algorithm: ['got', 'got', 'colour', 'lor', 'on', 'colour', 'quit', 'light', 'darker', 'lor', 'actual', 'done', 'styling', 'hair']\n",
      "Tokenized sentence: ['pls', 'send', 'me', 'your', 'address', 'sir']\n",
      "After stop words removal: ['pls', 'send', 'address', 'sir']\n",
      "After stemming with porters algorithm: ['pl', 'send', 'address', 'sir']\n",
      "Tokenized sentence: ['it', 'should', 'take', 'about', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['take', 'lt', 'gt', 'min']\n",
      "After stemming with porters algorithm: ['take', 'min']\n",
      "Tokenized sentence: ['good', 'afternoon', 'my', 'boytoy', 'how', 'are', 'you', 'feeling', 'today', 'better', 'i', 'hope', 'are', 'you', 'being', 'my', 'good', 'boy', 'are', 'you', 'my', 'obedient', 'slave', 'do', 'you', 'please', 'your', 'queen']\n",
      "After stop words removal: ['good', 'afternoon', 'boytoy', 'feeling', 'today', 'better', 'hope', 'good', 'boy', 'obedient', 'slave', 'please', 'queen']\n",
      "feel\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'boytoi', 'feel', 'todai', 'better', 'hope', 'good', 'boi', 'obedi', 'slave', 'pleas', 'queen']\n",
      "Tokenized sentence: ['was', 'just', 'about', 'to', 'ask', 'will', 'keep', 'this', 'one', 'maybe', 'that', 's', 'why', 'you', 'didn', 't', 'get', 'all', 'the', 'messages', 'we', 'sent', 'you', 'on', 'glo']\n",
      "After stop words removal: ['ask', 'keep', 'one', 'maybe', 'get', 'messages', 'sent', 'glo']\n",
      "After stemming with porters algorithm: ['ask', 'keep', 'on', 'mayb', 'get', 'messag', 'sent', 'glo']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'see', 'your', 'pretty', 'pussy']\n",
      "After stop words removal: ['want', 'see', 'pretty', 'pussy']\n",
      "After stemming with porters algorithm: ['want', 'see', 'pretti', 'pussi']\n",
      "Tokenized sentence: ['ok', 'darlin', 'i', 'supose', 'it', 'was', 'ok', 'i', 'just', 'worry', 'too', 'much', 'i', 'have', 'to', 'do', 'some', 'film', 'stuff', 'my', 'mate', 'and', 'then', 'have', 'to', 'babysit', 'again', 'but', 'you', 'can', 'call', 'me', 'there', 'xx']\n",
      "After stop words removal: ['ok', 'darlin', 'supose', 'ok', 'worry', 'much', 'film', 'stuff', 'mate', 'babysit', 'call', 'xx']\n",
      "After stemming with porters algorithm: ['darlin', 'supos', 'worri', 'much', 'film', 'stuff', 'mate', 'babysit', 'call']\n",
      "Tokenized sentence: ['camera', 'you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stop words removal: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stemming with porters algorithm: ['camera', 'awar', 'sipix', 'digit', 'camera', 'call', 'fromm', 'landlin', 'deliveri', 'within', 'dai']\n",
      "Tokenized sentence: ['hurry', 'up', 'i', 've', 'been', 'weed', 'deficient', 'for', 'like', 'three', 'days']\n",
      "After stop words removal: ['hurry', 'weed', 'deficient', 'like', 'three', 'days']\n",
      "After stemming with porters algorithm: ['hurri', 'weed', 'defici', 'like', 'three', 'dai']\n",
      "Tokenized sentence: ['dhoni', 'have', 'luck', 'to', 'win', 'some', 'big', 'title', 'so', 'we', 'will', 'win']\n",
      "After stop words removal: ['dhoni', 'luck', 'win', 'big', 'title', 'win']\n",
      "After stemming with porters algorithm: ['dhoni', 'luck', 'win', 'big', 'titl', 'win']\n",
      "Tokenized sentence: ['not', 'tel', 'software', 'name']\n",
      "After stop words removal: ['tel', 'software', 'name']\n",
      "After stemming with porters algorithm: ['tel', 'softwar', 'name']\n",
      "Tokenized sentence: ['come', 'to', 'my', 'home', 'for', 'one', 'last', 'time', 'i', 'wont', 'do', 'anything', 'trust', 'me']\n",
      "After stop words removal: ['come', 'home', 'one', 'last', 'time', 'wont', 'anything', 'trust']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['come', 'home', 'on', 'last', 'time', 'wont', 'anyt', 'trust']\n",
      "Tokenized sentence: ['tell', 'dear', 'what', 'happen', 'to', 'you', 'why', 'you', 'talking', 'to', 'me', 'like', 'an', 'alian']\n",
      "After stop words removal: ['tell', 'dear', 'happen', 'talking', 'like', 'alian']\n",
      "talk\n",
      "After stemming with porters algorithm: ['tell', 'dear', 'happen', 'tal', 'like', 'alian']\n",
      "Tokenized sentence: ['is', 'ur', 'paper', 'in', 'e', 'morn', 'or', 'aft', 'tmr']\n",
      "After stop words removal: ['ur', 'paper', 'e', 'morn', 'aft', 'tmr']\n",
      "After stemming with porters algorithm: ['paper', 'morn', 'aft', 'tmr']\n",
      "Tokenized sentence: ['i', 'anything', 'lor']\n",
      "After stop words removal: ['anything', 'lor']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'lor']\n",
      "Tokenized sentence: ['for', 'me', 'the', 'love', 'should', 'start', 'with', 'attraction', 'i', 'should', 'feel', 'that', 'i', 'need', 'her', 'every', 'time', 'around', 'me', 'she', 'should', 'be', 'the', 'first', 'thing', 'which', 'comes', 'in', 'my', 'thoughts', 'i', 'would', 'start', 'the', 'day', 'and', 'end', 'it', 'with', 'her', 'she', 'should', 'be', 'there', 'every', 'time', 'i', 'dream', 'love', 'will', 'be', 'then', 'when', 'my', 'every', 'breath', 'has', 'her', 'name', 'my', 'life', 'should', 'happen', 'around', 'her', 'my', 'life', 'will', 'be', 'named', 'to', 'her', 'i', 'would', 'cry', 'for', 'her', 'will', 'give', 'all', 'my', 'happiness', 'and', 'take', 'all', 'her', 'sorrows', 'i', 'will', 'be', 'ready', 'to', 'fight', 'with', 'anyone', 'for', 'her', 'i', 'will', 'be', 'in', 'love', 'when', 'i', 'will', 'be', 'doing', 'the', 'craziest', 'things', 'for', 'her', 'love', 'will', 'be', 'when', 'i', 'don', 't', 'have', 'to', 'proove', 'anyone', 'that', 'my', 'girl', 'is', 'the', 'most', 'beautiful', 'lady', 'on', 'the', 'whole', 'planet', 'i', 'will', 'always', 'be', 'singing', 'praises', 'for', 'her', 'love', 'will', 'be', 'when', 'i', 'start', 'up', 'making', 'chicken', 'curry', 'and', 'end', 'up', 'makiing', 'sambar', 'life', 'will', 'be', 'the', 'most', 'beautiful', 'then', 'will', 'get', 'every', 'morning', 'and', 'thank', 'god', 'for', 'the', 'day', 'because', 'she', 'is', 'with', 'me', 'i', 'would', 'like', 'to', 'say', 'a', 'lot', 'will', 'tell', 'later']\n",
      "After stop words removal: ['love', 'start', 'attraction', 'feel', 'need', 'every', 'time', 'around', 'first', 'thing', 'comes', 'thoughts', 'would', 'start', 'day', 'end', 'every', 'time', 'dream', 'love', 'every', 'breath', 'name', 'life', 'happen', 'around', 'life', 'named', 'would', 'cry', 'give', 'happiness', 'take', 'sorrows', 'ready', 'fight', 'anyone', 'love', 'craziest', 'things', 'love', 'proove', 'anyone', 'girl', 'beautiful', 'lady', 'whole', 'planet', 'always', 'singing', 'praises', 'love', 'start', 'making', 'chicken', 'curry', 'end', 'makiing', 'sambar', 'life', 'beautiful', 'get', 'every', 'morning', 'thank', 'god', 'day', 'would', 'like', 'say', 'lot', 'tell', 'later']\n",
      "sing\n",
      "mak\n",
      "maki\n",
      "morn\n",
      "After stemming with porters algorithm: ['love', 'start', 'attract', 'feel', 'need', 'everi', 'time', 'around', 'first', 'thing', 'come', 'thought', 'would', 'start', 'dai', 'end', 'everi', 'time', 'dream', 'love', 'everi', 'breath', 'name', 'life', 'happen', 'around', 'life', 'name', 'would', 'cry', 'give', 'happi', 'take', 'sorrow', 'readi', 'fight', 'anyon', 'love', 'craziest', 'thing', 'love', 'proov', 'anyon', 'girl', 'beauti', 'ladi', 'whole', 'planet', 'alwai', 'sin', 'prais', 'love', 'start', 'make', 'chicken', 'curri', 'end', 'maki', 'sambar', 'life', 'beauti', 'get', 'everi', 'mor', 'thank', 'god', 'dai', 'would', 'like', 'sai', 'lot', 'tell', 'later']\n",
      "Tokenized sentence: ['hey', 'gals', 'anyone', 'of', 'u', 'going', 'down', 'to', 'e', 'driving', 'centre', 'tmr']\n",
      "After stop words removal: ['hey', 'gals', 'anyone', 'u', 'going', 'e', 'driving', 'centre', 'tmr']\n",
      "go\n",
      "driv\n",
      "After stemming with porters algorithm: ['hei', 'gal', 'anyon', 'go', 'drive', 'centr', 'tmr']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'was', 'awarded', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'our', 'final', 'try', 'to', 'contact', 'u', 'call', 'from', 'landline', 'box', 'wr', 'c', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'awarded', 'bonus', 'caller', 'prize', 'final', 'try', 'contact', 'u', 'call', 'landline', 'box', 'wr', 'c', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'awar', 'bonu', 'caller', 'priz', 'final', 'try', 'contact', 'call', 'landlin', 'box', 'ppm']\n",
      "Tokenized sentence: ['i', 'accidentally', 'deleted', 'the', 'message', 'resend', 'please']\n",
      "After stop words removal: ['accidentally', 'deleted', 'message', 'resend', 'please']\n",
      "After stemming with porters algorithm: ['accid', 'delet', 'messag', 'resend', 'pleas']\n",
      "Tokenized sentence: ['dear', 'voucher', 'holder', 'to', 'claim', 'this', 'weeks', 'offer', 'at', 'you', 'pc', 'please', 'go', 'to', 'http', 'www', 'e', 'tlp', 'co', 'uk', 'expressoffer', 'ts', 'cs', 'apply', 'to', 'stop', 'texts', 'txt', 'stop', 'to']\n",
      "After stop words removal: ['dear', 'voucher', 'holder', 'claim', 'weeks', 'offer', 'pc', 'please', 'go', 'http', 'www', 'e', 'tlp', 'co', 'uk', 'expressoffer', 'ts', 'cs', 'apply', 'stop', 'texts', 'txt', 'stop']\n",
      "After stemming with porters algorithm: ['dear', 'voucher', 'holder', 'claim', 'week', 'offer', 'pleas', 'http', 'www', 'tlp', 'expressoff', 'appli', 'stop', 'text', 'txt', 'stop']\n",
      "Tokenized sentence: ['okay', 'lor', 'wah', 'like', 'that', 'def', 'they', 'wont', 'let', 'us', 'go', 'haha', 'what', 'did', 'they', 'say', 'in', 'the', 'terms', 'and', 'conditions']\n",
      "After stop words removal: ['okay', 'lor', 'wah', 'like', 'def', 'wont', 'let', 'us', 'go', 'haha', 'say', 'terms', 'conditions']\n",
      "After stemming with porters algorithm: ['okai', 'lor', 'wah', 'like', 'def', 'wont', 'let', 'haha', 'sai', 'term', 'condit']\n",
      "Tokenized sentence: ['alright', 'took', 'the', 'morphine', 'back', 'in', 'yo']\n",
      "After stop words removal: ['alright', 'took', 'morphine', 'back', 'yo']\n",
      "After stemming with porters algorithm: ['alright', 'took', 'morphin', 'back']\n",
      "Tokenized sentence: ['ok', 'going', 'to', 'sleep', 'hope', 'i', 'can', 'meet', 'her']\n",
      "After stop words removal: ['ok', 'going', 'sleep', 'hope', 'meet']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'sleep', 'hope', 'meet']\n",
      "Tokenized sentence: ['pls', 'give', 'her', 'prometazine', 'syrup', 'mls', 'then', 'lt', 'gt', 'mins', 'later', 'feed']\n",
      "After stop words removal: ['pls', 'give', 'prometazine', 'syrup', 'mls', 'lt', 'gt', 'mins', 'later', 'feed']\n",
      "After stemming with porters algorithm: ['pl', 'give', 'prometazin', 'syrup', 'ml', 'min', 'later', 'feed']\n",
      "Tokenized sentence: ['aiya', 'we', 'discuss', 'later', 'lar', 'pick', 'up', 'at', 'is', 'it']\n",
      "After stop words removal: ['aiya', 'discuss', 'later', 'lar', 'pick']\n",
      "After stemming with porters algorithm: ['aiya', 'discuss', 'later', 'lar', 'pick']\n",
      "Tokenized sentence: ['re', 'your', 'call', 'you', 'didn', 't', 'see', 'my', 'facebook', 'huh']\n",
      "After stop words removal: ['call', 'see', 'facebook', 'huh']\n",
      "After stemming with porters algorithm: ['call', 'see', 'facebook', 'huh']\n",
      "Tokenized sentence: ['yo', 'theres', 'no', 'class', 'tmrw', 'right']\n",
      "After stop words removal: ['yo', 'theres', 'class', 'tmrw', 'right']\n",
      "After stemming with porters algorithm: ['there', 'class', 'tmrw', 'right']\n",
      "Tokenized sentence: ['dont', 'make', 'ne', 'plans', 'for', 'nxt', 'wknd', 'coz', 'she', 'wants', 'us', 'to', 'come', 'down', 'then', 'ok']\n",
      "After stop words removal: ['dont', 'make', 'ne', 'plans', 'nxt', 'wknd', 'coz', 'wants', 'us', 'come', 'ok']\n",
      "After stemming with porters algorithm: ['dont', 'make', 'plan', 'nxt', 'wknd', 'coz', 'want', 'come']\n",
      "Tokenized sentence: ['freemsg', 'hey', 'i', 'm', 'buffy', 'and', 'love', 'to', 'satisfy', 'men', 'home', 'alone', 'feeling', 'randy', 'reply', 'c', 'my', 'pix', 'qlynnbv', 'help', 'p', 'a', 'msg', 'send', 'stop', 'to', 'stop', 'txts']\n",
      "After stop words removal: ['freemsg', 'hey', 'buffy', 'love', 'satisfy', 'men', 'home', 'alone', 'feeling', 'randy', 'reply', 'c', 'pix', 'qlynnbv', 'help', 'p', 'msg', 'send', 'stop', 'stop', 'txts']\n",
      "feel\n",
      "After stemming with porters algorithm: ['freemsg', 'hei', 'buffi', 'love', 'satisfi', 'men', 'home', 'alon', 'feel', 'randi', 'repli', 'pix', 'qlynnbv', 'help', 'msg', 'send', 'stop', 'stop', 'txt']\n",
      "Tokenized sentence: ['hmmm', 'guess', 'we', 'can', 'go', 'kb', 'n', 'power', 'yoga', 'haha', 'dunno', 'we', 'can', 'tahan', 'power', 'yoga', 'anot', 'thk', 'got', 'lo', 'oso', 'forgot', 'liao']\n",
      "After stop words removal: ['hmmm', 'guess', 'go', 'kb', 'n', 'power', 'yoga', 'haha', 'dunno', 'tahan', 'power', 'yoga', 'anot', 'thk', 'got', 'lo', 'oso', 'forgot', 'liao']\n",
      "After stemming with porters algorithm: ['hmmm', 'guess', 'power', 'yoga', 'haha', 'dunno', 'tahan', 'power', 'yoga', 'anot', 'thk', 'got', 'oso', 'forgot', 'liao']\n",
      "Tokenized sentence: ['hi', 'where', 'are', 'you', 'we', 're', 'at', 'and', 'they', 're', 'not', 'keen', 'to', 'go', 'out', 'i', 'kind', 'of', 'am', 'but', 'feel', 'i', 'shouldn', 't', 'so', 'can', 'we', 'go', 'out', 'tomo', 'don', 't', 'mind', 'do', 'you']\n",
      "After stop words removal: ['hi', 'keen', 'go', 'kind', 'feel', 'go', 'tomo', 'mind']\n",
      "After stemming with porters algorithm: ['keen', 'kind', 'feel', 'tomo', 'mind']\n",
      "Tokenized sentence: ['beautiful', 'truth', 'against', 'gravity', 'read', 'carefully', 'our', 'heart', 'feels', 'light', 'when', 'someone', 'is', 'in', 'it', 'but', 'it', 'feels', 'very', 'heavy', 'when', 'someone', 'leaves', 'it', 'goodmorning']\n",
      "After stop words removal: ['beautiful', 'truth', 'gravity', 'read', 'carefully', 'heart', 'feels', 'light', 'someone', 'feels', 'heavy', 'someone', 'leaves', 'goodmorning']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['beauti', 'truth', 'graviti', 'read', 'carefulli', 'heart', 'feel', 'light', 'someon', 'feel', 'heavi', 'someon', 'leav', 'goodmor']\n",
      "Tokenized sentence: ['win', 'the', 'newest', 'harry', 'potter', 'and', 'the', 'order', 'of', 'the', 'phoenix', 'book', 'reply', 'harry', 'answer', 'questions', 'chance', 'to', 'be', 'the', 'first', 'among', 'readers']\n",
      "After stop words removal: ['win', 'newest', 'harry', 'potter', 'order', 'phoenix', 'book', 'reply', 'harry', 'answer', 'questions', 'chance', 'first', 'among', 'readers']\n",
      "After stemming with porters algorithm: ['win', 'newest', 'harri', 'potter', 'order', 'phoenix', 'book', 'repli', 'harri', 'answer', 'quest', 'chanc', 'first', 'among', 'reader']\n",
      "Tokenized sentence: ['he', 'has', 'lots', 'of', 'used', 'ones', 'babe', 'but', 'the', 'model', 'doesn', 't', 'help', 'youi', 'have', 'to', 'bring', 'it', 'over', 'and', 'he', 'll', 'match', 'it', 'up']\n",
      "After stop words removal: ['lots', 'used', 'ones', 'babe', 'model', 'help', 'youi', 'bring', 'match']\n",
      "After stemming with porters algorithm: ['lot', 'us', 'on', 'babe', 'model', 'help', 'youi', 'bring', 'match']\n",
      "Tokenized sentence: ['not', 'much', 'just', 'some', 'textin', 'how', 'bout', 'you']\n",
      "After stop words removal: ['much', 'textin', 'bout']\n",
      "After stemming with porters algorithm: ['much', 'textin', 'bout']\n",
      "Tokenized sentence: ['another', 'month', 'i', 'need', 'chocolate', 'weed', 'and', 'alcohol']\n",
      "After stop words removal: ['another', 'month', 'need', 'chocolate', 'weed', 'alcohol']\n",
      "After stemming with porters algorithm: ['anoth', 'month', 'need', 'chocol', 'weed', 'alcohol']\n",
      "Tokenized sentence: ['i', 'hav', 'almost', 'reached', 'call', 'i', 'm', 'unable', 'to', 'connect', 'u']\n",
      "After stop words removal: ['hav', 'almost', 'reached', 'call', 'unable', 'connect', 'u']\n",
      "After stemming with porters algorithm: ['hav', 'almost', 'reac', 'call', 'unab', 'connect']\n",
      "Tokenized sentence: ['haha', 'hope', 'can', 'hear', 'the', 'receipt', 'sound', 'gd', 'luck']\n",
      "After stop words removal: ['haha', 'hope', 'hear', 'receipt', 'sound', 'gd', 'luck']\n",
      "After stemming with porters algorithm: ['haha', 'hope', 'hear', 'receipt', 'sound', 'luck']\n",
      "Tokenized sentence: ['who', 'were', 'those', 'people', 'were', 'you', 'in', 'a', 'tour', 'i', 'thought', 'you', 'were', 'doing', 'that', 'sofa', 'thing', 'you', 'sent', 'me', 'your', 'curious', 'sugar']\n",
      "After stop words removal: ['people', 'tour', 'thought', 'sofa', 'thing', 'sent', 'curious', 'sugar']\n",
      "After stemming with porters algorithm: ['peopl', 'tour', 'thought', 'sofa', 'thing', 'sent', 'curiou', 'sugar']\n",
      "Tokenized sentence: ['aah', 'a', 'cuddle', 'would', 'be', 'lush', 'i', 'd', 'need', 'lots', 'of', 'tea', 'and', 'soup', 'before', 'any', 'kind', 'of', 'fumbling']\n",
      "After stop words removal: ['aah', 'cuddle', 'would', 'lush', 'need', 'lots', 'tea', 'soup', 'kind', 'fumbling']\n",
      "fumbl\n",
      "fumble\n",
      "After stemming with porters algorithm: ['aah', 'cuddl', 'would', 'lush', 'need', 'lot', 'tea', 'soup', 'kind', 'fumbl']\n",
      "Tokenized sentence: ['for', 'my', 'family', 'happiness']\n",
      "After stop words removal: ['family', 'happiness']\n",
      "After stemming with porters algorithm: ['famili', 'happi']\n",
      "Tokenized sentence: ['finally', 'it', 'has', 'happened', 'aftr', 'decades', 'beer', 'is', 'now', 'cheaper', 'than', 'petrol', 'the', 'goverment', 'expects', 'us', 'to', 'drink', 'but', 'don', 't', 'drive']\n",
      "After stop words removal: ['finally', 'happened', 'aftr', 'decades', 'beer', 'cheaper', 'petrol', 'goverment', 'expects', 'us', 'drink', 'drive']\n",
      "After stemming with porters algorithm: ['final', 'happen', 'aftr', 'decad', 'beer', 'cheaper', 'petrol', 'gover', 'expect', 'drink', 'drive']\n",
      "Tokenized sentence: ['call', 'me', 'when', 'u', 're', 'done']\n",
      "After stop words removal: ['call', 'u', 'done']\n",
      "After stemming with porters algorithm: ['call', 'done']\n",
      "Tokenized sentence: ['great', 'p', 'diddy', 'is', 'my', 'neighbor', 'and', 'comes', 'for', 'toothpaste', 'every', 'morning']\n",
      "After stop words removal: ['great', 'p', 'diddy', 'neighbor', 'comes', 'toothpaste', 'every', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['great', 'diddi', 'neighbor', 'come', 'toothpast', 'everi', 'mor']\n",
      "Tokenized sentence: ['all', 'was', 'well', 'until', 'slightly', 'disastrous', 'class', 'this', 'pm', 'with', 'my', 'fav', 'darlings', 'hope', 'day', 'off', 'ok', 'coffee', 'wld', 'be', 'good', 'as', 'can', 't', 'stay', 'late', 'tomorrow', 'same', 'time', 'place', 'as', 'always']\n",
      "After stop words removal: ['well', 'slightly', 'disastrous', 'class', 'pm', 'fav', 'darlings', 'hope', 'day', 'ok', 'coffee', 'wld', 'good', 'stay', 'late', 'tomorrow', 'time', 'place', 'always']\n",
      "darl\n",
      "After stemming with porters algorithm: ['well', 'slightli', 'disastr', 'class', 'fav', 'darl', 'hope', 'dai', 'coffe', 'wld', 'good', 'stai', 'late', 'tomorrow', 'time', 'place', 'alwai']\n",
      "Tokenized sentence: ['missed', 'your', 'call', 'cause', 'i', 'was', 'yelling', 'at', 'scrappy', 'miss', 'u', 'can', 't', 'wait', 'for', 'u', 'to', 'come', 'home', 'i', 'm', 'so', 'lonely', 'today']\n",
      "After stop words removal: ['missed', 'call', 'cause', 'yelling', 'scrappy', 'miss', 'u', 'wait', 'u', 'come', 'home', 'lonely', 'today']\n",
      "yell\n",
      "After stemming with porters algorithm: ['miss', 'call', 'caus', 'yell', 'scrappi', 'miss', 'wait', 'come', 'home', 'lone', 'todai']\n",
      "Tokenized sentence: ['we', 'have', 'all', 'rounder', 'so', 'not', 'required']\n",
      "After stop words removal: ['rounder', 'required']\n",
      "After stemming with porters algorithm: ['rounder', 'requir']\n",
      "Tokenized sentence: ['where']\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['dont', 'know', 'supports', 'ass', 'and', 'srt', 'i', 'thnk', 'i', 'think', 'ps', 'can', 'play', 'through', 'usb', 'too']\n",
      "After stop words removal: ['dont', 'know', 'supports', 'ass', 'srt', 'thnk', 'think', 'ps', 'play', 'usb']\n",
      "After stemming with porters algorithm: ['dont', 'know', 'support', 'ass', 'srt', 'thnk', 'think', 'plai', 'usb']\n",
      "Tokenized sentence: ['he', 'fucking', 'chickened', 'out', 'he', 'messaged', 'me', 'he', 'would', 'be', 'late', 'and', 'woould', 'buzz', 'me', 'and', 'then', 'i', 'didn', 't', 'hear', 'a', 'word', 'from', 'him']\n",
      "After stop words removal: ['fucking', 'chickened', 'messaged', 'would', 'late', 'woould', 'buzz', 'hear', 'word']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['fuc', 'chicken', 'messag', 'would', 'late', 'woould', 'buzz', 'hear', 'word']\n",
      "Tokenized sentence: ['lemme', 'know', 'when', 'i', 'can', 'swing', 'by', 'and', 'pick', 'up', 'i', 'm', 'free', 'basically', 'any', 'time', 'after', 'all', 'this', 'semester']\n",
      "After stop words removal: ['lemme', 'know', 'swing', 'pick', 'free', 'basically', 'time', 'semester']\n",
      "After stemming with porters algorithm: ['lemm', 'know', 'swing', 'pick', 'free', 'basic', 'time', 'semest']\n",
      "Tokenized sentence: ['oh', 'right', 'ok', 'i', 'll', 'make', 'sure', 'that', 'i', 'do', 'loads', 'of', 'work', 'during', 'the', 'day', 'got', 'a', 'really', 'nasty', 'cough', 'today', 'and', 'is', 'dry', 'n', 'shot', 'so', 'that', 'should', 'really', 'help', 'it']\n",
      "After stop words removal: ['oh', 'right', 'ok', 'make', 'sure', 'loads', 'work', 'day', 'got', 'really', 'nasty', 'cough', 'today', 'dry', 'n', 'shot', 'really', 'help']\n",
      "After stemming with porters algorithm: ['right', 'make', 'sure', 'load', 'work', 'dai', 'got', 'realli', 'nasti', 'cough', 'todai', 'dry', 'shot', 'realli', 'help']\n",
      "Tokenized sentence: ['erm', 'ill', 'pick', 'you', 'up', 'at', 'about', 'pm', 'that', 'll', 'give', 'enough', 'time', 'to', 'get', 'there', 'park', 'and', 'that']\n",
      "After stop words removal: ['erm', 'ill', 'pick', 'pm', 'give', 'enough', 'time', 'get', 'park']\n",
      "After stemming with porters algorithm: ['erm', 'ill', 'pick', 'give', 'enough', 'time', 'get', 'park']\n",
      "Tokenized sentence: ['i', 'uploaded', 'mine', 'to', 'facebook']\n",
      "After stop words removal: ['uploaded', 'mine', 'facebook']\n",
      "After stemming with porters algorithm: ['upload', 'mine', 'facebook']\n",
      "Tokenized sentence: ['thank', 'you', 'i', 'like', 'you', 'as', 'well']\n",
      "After stop words removal: ['thank', 'like', 'well']\n",
      "After stemming with porters algorithm: ['thank', 'like', 'well']\n",
      "Tokenized sentence: ['do', 'u', 'still', 'have', 'plumbers', 'tape', 'and', 'a', 'wrench', 'we', 'could', 'borrow']\n",
      "After stop words removal: ['u', 'still', 'plumbers', 'tape', 'wrench', 'could', 'borrow']\n",
      "After stemming with porters algorithm: ['still', 'plumber', 'tape', 'wrench', 'could', 'borrow']\n",
      "Tokenized sentence: ['single', 'line', 'with', 'a', 'big', 'meaning', 'miss', 'anything', 'ur', 'best', 'life', 'but']\n",
      "After stop words removal: ['single', 'line', 'big', 'meaning', 'miss', 'anything', 'ur', 'best', 'life']\n",
      "mean\n",
      "anyth\n",
      "After stemming with porters algorithm: ['singl', 'line', 'big', 'mean', 'miss', 'anyt', 'best', 'life']\n",
      "Tokenized sentence: ['call', 'from', 'tells', 'u', 'call', 'to', 'claim', 'prize', 'u', 'have', 'enter', 'all', 'ur', 'mobile', 'personal', 'details', 'the', 'prompts', 'careful']\n",
      "After stop words removal: ['call', 'tells', 'u', 'call', 'claim', 'prize', 'u', 'enter', 'ur', 'mobile', 'personal', 'details', 'prompts', 'careful']\n",
      "After stemming with porters algorithm: ['call', 'tell', 'call', 'claim', 'priz', 'enter', 'mobil', 'person', 'detail', 'prompt', 'care']\n",
      "Tokenized sentence: ['on', 'the', 'road', 'so', 'cant', 'txt']\n",
      "After stop words removal: ['road', 'cant', 'txt']\n",
      "After stemming with porters algorithm: ['road', 'cant', 'txt']\n",
      "Tokenized sentence: ['leave', 'it', 'wif', 'me', 'lar', 'wan', 'to', 'carry', 'meh', 'so', 'heavy', 'is', 'da', 'num', 'familiar', 'to']\n",
      "After stop words removal: ['leave', 'wif', 'lar', 'wan', 'carry', 'meh', 'heavy', 'da', 'num', 'familiar']\n",
      "After stemming with porters algorithm: ['leav', 'wif', 'lar', 'wan', 'carri', 'meh', 'heavi', 'num', 'familiar']\n",
      "Tokenized sentence: ['i', 'think', 'we', 're', 'going', 'to', 'finn', 's', 'now', 'come']\n",
      "After stop words removal: ['think', 'going', 'finn', 'come']\n",
      "go\n",
      "After stemming with porters algorithm: ['think', 'go', 'finn', 'come']\n",
      "Tokenized sentence: ['ok', 'i', 'thk', 'i', 'got', 'it', 'then', 'u', 'wan', 'me', 'come', 'now', 'or', 'wat']\n",
      "After stop words removal: ['ok', 'thk', 'got', 'u', 'wan', 'come', 'wat']\n",
      "After stemming with porters algorithm: ['thk', 'got', 'wan', 'come', 'wat']\n",
      "Tokenized sentence: ['hi', 'this', 'is', 'mandy', 'sullivan', 'calling', 'from', 'hotmix', 'fm', 'you', 'are', 'chosen', 'to', 'receive', 'in', 'our', 'easter', 'prize', 'draw', 'please', 'telephone', 'to', 'claim', 'before', 'or', 'your', 'prize', 'will', 'be', 'transferred', 'to', 'someone', 'else']\n",
      "After stop words removal: ['hi', 'mandy', 'sullivan', 'calling', 'hotmix', 'fm', 'chosen', 'receive', 'easter', 'prize', 'draw', 'please', 'telephone', 'claim', 'prize', 'transferred', 'someone', 'else']\n",
      "call\n",
      "After stemming with porters algorithm: ['mandi', 'sullivan', 'call', 'hotmix', 'chosen', 'receiv', 'easter', 'priz', 'draw', 'pleas', 'telephon', 'claim', 'priz', 'transfer', 'someon', 'els']\n",
      "Tokenized sentence: ['hey', 'hey', 'werethe', 'monkeespeople', 'say', 'we', 'monkeyaround', 'howdy', 'gorgeous']\n",
      "After stop words removal: ['hey', 'hey', 'werethe', 'monkeespeople', 'say', 'monkeyaround', 'howdy', 'gorgeous']\n",
      "After stemming with porters algorithm: ['hei', 'hei', 'wereth', 'monkeespeop', 'sai', 'monkeyaround', 'howdi', 'gorgeou']\n",
      "Tokenized sentence: ['want', 'to', 'funk', 'up', 'ur', 'fone', 'with', 'a', 'weekly', 'new', 'tone', 'reply', 'tones', 'u', 'this', 'text', 'www', 'ringtones', 'co', 'uk', 'the', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stop words removal: ['want', 'funk', 'ur', 'fone', 'weekly', 'new', 'tone', 'reply', 'tones', 'u', 'text', 'www', 'ringtones', 'co', 'uk', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stemming with porters algorithm: ['want', 'funk', 'fone', 'weekli', 'new', 'tone', 'repli', 'tone', 'text', 'www', 'rington', 'origin', 'best', 'tone', 'gbp', 'network', 'oper', 'rate', 'appli']\n",
      "Tokenized sentence: ['when', 'the', 'first', 'strike', 'is', 'a', 'red', 'one', 'the', 'bird', 'antelope', 'begin', 'toplay', 'in', 'the', 'fieldof', 'selfindependence', 'believe', 'this', 'the', 'flower', 'of', 'contention', 'will', 'grow', 'random']\n",
      "After stop words removal: ['first', 'strike', 'red', 'one', 'bird', 'antelope', 'begin', 'toplay', 'fieldof', 'selfindependence', 'believe', 'flower', 'contention', 'grow', 'random']\n",
      "After stemming with porters algorithm: ['first', 'strike', 'red', 'on', 'bird', 'antelop', 'begin', 'toplai', 'fieldof', 'selfindepend', 'believ', 'flower', 'content', 'grow', 'random']\n",
      "Tokenized sentence: ['sat', 'right', 'okay', 'thanks']\n",
      "After stop words removal: ['sat', 'right', 'okay', 'thanks']\n",
      "After stemming with porters algorithm: ['sat', 'right', 'okai', 'thank']\n",
      "Tokenized sentence: ['i', 'promise', 'to', 'take', 'good', 'care', 'of', 'you', 'princess', 'i', 'have', 'to', 'run', 'now', 'please', 'send', 'pics', 'when', 'you', 'get', 'a', 'chance', 'ttyl']\n",
      "After stop words removal: ['promise', 'take', 'good', 'care', 'princess', 'run', 'please', 'send', 'pics', 'get', 'chance', 'ttyl']\n",
      "After stemming with porters algorithm: ['promis', 'take', 'good', 'care', 'princess', 'run', 'pleas', 'send', 'pic', 'get', 'chanc', 'ttyl']\n",
      "Tokenized sentence: ['guess', 'what', 'somebody', 'you', 'know', 'secretly', 'fancies', 'you', 'wanna', 'find', 'out', 'who', 'it', 'is', 'give', 'us', 'a', 'call', 'on', 'from', 'landline', 'datebox', 'essexcm', 'xn', 'p', 'min']\n",
      "After stop words removal: ['guess', 'somebody', 'know', 'secretly', 'fancies', 'wanna', 'find', 'give', 'us', 'call', 'landline', 'datebox', 'essexcm', 'xn', 'p', 'min']\n",
      "After stemming with porters algorithm: ['guess', 'somebodi', 'know', 'secretli', 'fanci', 'wanna', 'find', 'give', 'call', 'landlin', 'datebox', 'essexcm', 'min']\n",
      "Tokenized sentence: ['probably', 'not', 'still', 'going', 'over', 'some', 'stuff', 'here']\n",
      "After stop words removal: ['probably', 'still', 'going', 'stuff']\n",
      "go\n",
      "After stemming with porters algorithm: ['probab', 'still', 'go', 'stuff']\n",
      "Tokenized sentence: ['let', 'me', 'know', 'if', 'you', 'need', 'anything', 'else', 'salad', 'or', 'desert', 'or', 'something', 'how', 'many', 'beers', 'shall', 'i', 'get']\n",
      "After stop words removal: ['let', 'know', 'need', 'anything', 'else', 'salad', 'desert', 'something', 'many', 'beers', 'shall', 'get']\n",
      "anyth\n",
      "someth\n",
      "After stemming with porters algorithm: ['let', 'know', 'need', 'anyt', 'els', 'salad', 'desert', 'somet', 'mani', 'beer', 'shall', 'get']\n",
      "Tokenized sentence: ['hi', 'its', 'kate', 'how', 'is', 'your', 'evening', 'i', 'hope', 'i', 'can', 'see', 'you', 'tomorrow', 'for', 'a', 'bit', 'but', 'i', 'have', 'to', 'bloody', 'babyjontet', 'txt', 'back', 'if', 'u', 'can', 'xxx']\n",
      "After stop words removal: ['hi', 'kate', 'evening', 'hope', 'see', 'tomorrow', 'bit', 'bloody', 'babyjontet', 'txt', 'back', 'u', 'xxx']\n",
      "even\n",
      "After stemming with porters algorithm: ['kate', 'even', 'hope', 'see', 'tomorrow', 'bit', 'bloodi', 'babyjontet', 'txt', 'back', 'xxx']\n",
      "Tokenized sentence: ['i', 'cant', 'wait', 'to', 'see', 'you', 'how', 'were', 'the', 'photos', 'were', 'useful']\n",
      "After stop words removal: ['cant', 'wait', 'see', 'photos', 'useful']\n",
      "After stemming with porters algorithm: ['cant', 'wait', 'see', 'photo', 'us']\n",
      "Tokenized sentence: ['we', 'got', 'a', 'divorce', 'lol', 'she', 's', 'here']\n",
      "After stop words removal: ['got', 'divorce', 'lol']\n",
      "After stemming with porters algorithm: ['got', 'divorc', 'lol']\n",
      "Tokenized sentence: ['tell', 'me', 'pa', 'how', 'is', 'pain', 'de']\n",
      "After stop words removal: ['tell', 'pa', 'pain', 'de']\n",
      "After stemming with porters algorithm: ['tell', 'pain']\n",
      "Tokenized sentence: ['where', 'are', 'you', 'when', 'wil', 'you', 'reach', 'here']\n",
      "After stop words removal: ['wil', 'reach']\n",
      "After stemming with porters algorithm: ['wil', 'reach']\n",
      "Tokenized sentence: ['university', 'of', 'southern', 'california']\n",
      "After stop words removal: ['university', 'southern', 'california']\n",
      "After stemming with porters algorithm: ['univers', 'southern', 'california']\n",
      "Tokenized sentence: ['just', 'hopeing', 'that', 'wasn', 't', 'too', 'pissed', 'up', 'to', 'remember', 'and', 'has', 'gone', 'off', 'to', 'his', 'sisters', 'or', 'something']\n",
      "After stop words removal: ['hopeing', 'pissed', 'remember', 'gone', 'sisters', 'something']\n",
      "hope\n",
      "someth\n",
      "After stemming with porters algorithm: ['hope', 'piss', 'rememb', 'gone', 'sister', 'somet']\n",
      "Tokenized sentence: ['carlos', 'll', 'be', 'here', 'in', 'a', 'minute', 'if', 'you', 'still', 'need', 'to', 'buy']\n",
      "After stop words removal: ['carlos', 'minute', 'still', 'need', 'buy']\n",
      "After stemming with porters algorithm: ['carlo', 'minut', 'still', 'need', 'bui']\n",
      "Tokenized sentence: ['just', 'taste', 'fish', 'curry', 'p']\n",
      "After stop words removal: ['taste', 'fish', 'curry', 'p']\n",
      "After stemming with porters algorithm: ['tast', 'fish', 'curri']\n",
      "Tokenized sentence: ['chinatown', 'got', 'porridge', 'claypot', 'rice', 'yam', 'cake', 'fishhead', 'beehoon', 'either', 'we', 'eat', 'cheap', 'den', 'go', 'cafe', 'n', 'tok', 'or', 'go', 'nydc', 'or', 'somethin']\n",
      "After stop words removal: ['chinatown', 'got', 'porridge', 'claypot', 'rice', 'yam', 'cake', 'fishhead', 'beehoon', 'either', 'eat', 'cheap', 'den', 'go', 'cafe', 'n', 'tok', 'go', 'nydc', 'somethin']\n",
      "After stemming with porters algorithm: ['chinatown', 'got', 'porridg', 'claypot', 'rice', 'yam', 'cake', 'fishhead', 'beehoon', 'either', 'eat', 'cheap', 'den', 'cafe', 'tok', 'nydc', 'somethin']\n",
      "Tokenized sentence: ['thats', 'cool', 'how', 'was', 'your', 'day']\n",
      "After stop words removal: ['thats', 'cool', 'day']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'dai']\n",
      "Tokenized sentence: ['no', 'probably', 'lt', 'gt']\n",
      "After stop words removal: ['probably', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['probab']\n",
      "Tokenized sentence: ['ok', 'take', 'ur', 'time', 'n', 'enjoy', 'ur', 'dinner']\n",
      "After stop words removal: ['ok', 'take', 'ur', 'time', 'n', 'enjoy', 'ur', 'dinner']\n",
      "After stemming with porters algorithm: ['take', 'time', 'enjoi', 'dinner']\n",
      "Tokenized sentence: ['i', 'want', 'kfc', 'its', 'tuesday', 'only', 'buy', 'meals', 'only', 'no', 'gravy', 'only', 'mark']\n",
      "After stop words removal: ['want', 'kfc', 'tuesday', 'buy', 'meals', 'gravy', 'mark']\n",
      "After stemming with porters algorithm: ['want', 'kfc', 'tuesdai', 'bui', 'meal', 'gravi', 'mark']\n",
      "Tokenized sentence: ['just', 'getting', 'back', 'home']\n",
      "After stop words removal: ['getting', 'back', 'home']\n",
      "gett\n",
      "After stemming with porters algorithm: ['get', 'back', 'home']\n",
      "Tokenized sentence: ['do', 'you', 'always', 'celebrate', 'ny', 's', 'with', 'your', 'family']\n",
      "After stop words removal: ['always', 'celebrate', 'ny', 'family']\n",
      "After stemming with porters algorithm: ['alwai', 'celebr', 'famili']\n",
      "Tokenized sentence: ['still', 'work', 'going', 'on', 'it', 'is', 'very', 'small', 'house']\n",
      "After stop words removal: ['still', 'work', 'going', 'small', 'house']\n",
      "go\n",
      "After stemming with porters algorithm: ['still', 'work', 'go', 'small', 'hous']\n",
      "Tokenized sentence: ['new', 'textbuddy', 'chat', 'horny', 'guys', 'in', 'ur', 'area', 'just', 'p', 'free', 'receive', 'search', 'postcode', 'or', 'at', 'gaytextbuddy', 'com', 'txt', 'one', 'name', 'to', 'rpl', 'stop', 'cnl']\n",
      "After stop words removal: ['new', 'textbuddy', 'chat', 'horny', 'guys', 'ur', 'area', 'p', 'free', 'receive', 'search', 'postcode', 'gaytextbuddy', 'com', 'txt', 'one', 'name', 'rpl', 'stop', 'cnl']\n",
      "After stemming with porters algorithm: ['new', 'textbuddi', 'chat', 'horni', 'gui', 'area', 'free', 'receiv', 'search', 'postcod', 'gaytextbuddi', 'com', 'txt', 'on', 'name', 'rpl', 'stop', 'cnl']\n",
      "Tokenized sentence: ['mmmmmm', 'i', 'love', 'you', 'so', 'much', 'ahmad', 'i', 'can', 't', 'wait', 'for', 'this', 'year', 'to', 'begin', 'as', 'every', 'second', 'takes', 'me', 'closer', 'to', 'being', 'at', 'your', 'side', 'happy', 'new', 'year', 'my', 'love']\n",
      "After stop words removal: ['mmmmmm', 'love', 'much', 'ahmad', 'wait', 'year', 'begin', 'every', 'second', 'takes', 'closer', 'side', 'happy', 'new', 'year', 'love']\n",
      "After stemming with porters algorithm: ['mmmmmm', 'love', 'much', 'ahmad', 'wait', 'year', 'begin', 'everi', 'second', 'take', 'closer', 'side', 'happi', 'new', 'year', 'love']\n",
      "Tokenized sentence: ['send', 'a', 'logo', 'ur', 'lover', 'names', 'joined', 'by', 'a', 'heart', 'txt', 'love', 'name', 'name', 'mobno', 'eg', 'love', 'adam', 'eve', 'to', 'yahoo', 'pobox', 'w', 'wq', 'txtno', 'no', 'ads', 'p']\n",
      "After stop words removal: ['send', 'logo', 'ur', 'lover', 'names', 'joined', 'heart', 'txt', 'love', 'name', 'name', 'mobno', 'eg', 'love', 'adam', 'eve', 'yahoo', 'pobox', 'w', 'wq', 'txtno', 'ads', 'p']\n",
      "After stemming with porters algorithm: ['send', 'logo', 'lover', 'name', 'join', 'heart', 'txt', 'love', 'name', 'name', 'mobno', 'love', 'adam', 'ev', 'yahoo', 'pobox', 'txtno', 'ad']\n",
      "Tokenized sentence: ['sir', 'i', 'have', 'been', 'late', 'in', 'paying', 'rent', 'for', 'the', 'past', 'few', 'months', 'and', 'had', 'to', 'pay', 'a', 'lt', 'gt', 'charge', 'i', 'felt', 'it', 'would', 'be', 'inconsiderate', 'of', 'me', 'to', 'nag', 'about', 'something', 'you', 'give', 'at', 'great', 'cost', 'to', 'yourself', 'and', 'that', 's', 'why', 'i', 'didnt', 'speak', 'up', 'i', 'however', 'am', 'in', 'a', 'recession', 'and', 'wont', 'be', 'able', 'to', 'pay', 'the', 'charge', 'this', 'month', 'hence', 'my', 'askin', 'well', 'ahead', 'of', 'month', 's', 'end', 'can', 'you', 'please', 'help', 'thanks']\n",
      "After stop words removal: ['sir', 'late', 'paying', 'rent', 'past', 'months', 'pay', 'lt', 'gt', 'charge', 'felt', 'would', 'inconsiderate', 'nag', 'something', 'give', 'great', 'cost', 'didnt', 'speak', 'however', 'recession', 'wont', 'able', 'pay', 'charge', 'month', 'hence', 'askin', 'well', 'ahead', 'month', 'end', 'please', 'help', 'thanks']\n",
      "pay\n",
      "someth\n",
      "After stemming with porters algorithm: ['sir', 'late', 'pai', 'rent', 'past', 'month', 'pai', 'charg', 'felt', 'would', 'inconsider', 'nag', 'somet', 'give', 'great', 'cost', 'didnt', 'speak', 'howev', 'recess', 'wont', 'abl', 'pai', 'charg', 'month', 'henc', 'askin', 'well', 'ahead', 'month', 'end', 'pleas', 'help', 'thank']\n",
      "Tokenized sentence: ['orange', 'customer', 'you', 'may', 'now', 'claim', 'your', 'free', 'camera', 'phone', 'upgrade', 'for', 'your', 'loyalty', 'call', 'now', 'on', 'offer', 'ends', 'thmarch', 't', 'c', 's', 'apply', 'opt', 'out', 'availa']\n",
      "After stop words removal: ['orange', 'customer', 'may', 'claim', 'free', 'camera', 'phone', 'upgrade', 'loyalty', 'call', 'offer', 'ends', 'thmarch', 'c', 'apply', 'opt', 'availa']\n",
      "After stemming with porters algorithm: ['orang', 'custom', 'mai', 'claim', 'free', 'camera', 'phone', 'upgrad', 'loyalti', 'call', 'offer', 'end', 'thmarch', 'appli', 'opt', 'availa']\n",
      "Tokenized sentence: ['i', 'm', 'nt', 'goin', 'got', 'somethin', 'on', 'unless', 'they', 'meetin', 'dinner', 'lor', 'haha', 'i', 'wonder', 'who', 'will', 'go', 'tis', 'time']\n",
      "After stop words removal: ['nt', 'goin', 'got', 'somethin', 'unless', 'meetin', 'dinner', 'lor', 'haha', 'wonder', 'go', 'tis', 'time']\n",
      "After stemming with porters algorithm: ['goin', 'got', 'somethin', 'unless', 'meetin', 'dinner', 'lor', 'haha', 'wonder', 'ti', 'time']\n",
      "Tokenized sentence: ['eat', 'at', 'old', 'airport', 'road', 'but', 'now', 'oredi', 'got', 'a', 'lot', 'of', 'pple']\n",
      "After stop words removal: ['eat', 'old', 'airport', 'road', 'oredi', 'got', 'lot', 'pple']\n",
      "After stemming with porters algorithm: ['eat', 'old', 'airport', 'road', 'oredi', 'got', 'lot', 'pple']\n",
      "Tokenized sentence: ['you', 'can', 'jot', 'down', 'things', 'you', 'want', 'to', 'remember', 'later']\n",
      "After stop words removal: ['jot', 'things', 'want', 'remember', 'later']\n",
      "After stemming with porters algorithm: ['jot', 'thing', 'want', 'rememb', 'later']\n",
      "Tokenized sentence: ['solve', 'd', 'case', 'a', 'man', 'was', 'found', 'murdered', 'on', 'lt', 'decimal', 'gt', 'lt', 'gt', 'afternoon', 'his', 'wife', 'called', 'police', 'police', 'questioned', 'everyone', 'wife', 'sir', 'i', 'was', 'sleeping', 'when', 'the', 'murder', 'took', 'place', 'cook', 'i', 'was', 'cooking', 'gardener', 'i', 'was', 'picking', 'vegetables', 'house', 'maid', 'i', 'went', 'd', 'post', 'office', 'children', 'we', 'went', 'play', 'neighbour', 'we', 'went', 'a', 'marriage', 'police', 'arrested', 'd', 'murderer', 'immediately', 'who', 's', 'it', 'reply', 'with', 'reason', 'if', 'u', 'r', 'brilliant']\n",
      "After stop words removal: ['solve', 'case', 'man', 'found', 'murdered', 'lt', 'decimal', 'gt', 'lt', 'gt', 'afternoon', 'wife', 'called', 'police', 'police', 'questioned', 'everyone', 'wife', 'sir', 'sleeping', 'murder', 'took', 'place', 'cook', 'cooking', 'gardener', 'picking', 'vegetables', 'house', 'maid', 'went', 'post', 'office', 'children', 'went', 'play', 'neighbour', 'went', 'marriage', 'police', 'arrested', 'murderer', 'immediately', 'reply', 'reason', 'u', 'r', 'brilliant']\n",
      "sleep\n",
      "cook\n",
      "pick\n",
      "After stemming with porters algorithm: ['solv', 'case', 'man', 'found', 'murder', 'decim', 'afternoon', 'wife', 'call', 'polic', 'polic', 'quest', 'everyon', 'wife', 'sir', 'sleep', 'murder', 'took', 'place', 'cook', 'cook', 'garden', 'pic', 'veget', 'hous', 'maid', 'went', 'post', 'offic', 'children', 'went', 'plai', 'neighbour', 'went', 'marriag', 'polic', 'arres', 'murder', 'immedi', 'repli', 'reason', 'brilliant']\n",
      "Tokenized sentence: ['freemsg', 'our', 'records', 'indicate', 'you', 'may', 'be', 'entitled', 'to', 'pounds', 'for', 'the', 'accident', 'you', 'had', 'to', 'claim', 'for', 'free', 'reply', 'with', 'yes', 'to', 'this', 'msg', 'to', 'opt', 'out', 'text', 'stop']\n",
      "After stop words removal: ['freemsg', 'records', 'indicate', 'may', 'entitled', 'pounds', 'accident', 'claim', 'free', 'reply', 'yes', 'msg', 'opt', 'text', 'stop']\n",
      "After stemming with porters algorithm: ['freemsg', 'record', 'indic', 'mai', 'entit', 'pound', 'accid', 'claim', 'free', 'repli', 'ye', 'msg', 'opt', 'text', 'stop']\n",
      "Tokenized sentence: ['really', 'dun', 'bluff', 'me', 'leh', 'u', 'sleep', 'early', 'too', 'nite']\n",
      "After stop words removal: ['really', 'dun', 'bluff', 'leh', 'u', 'sleep', 'early', 'nite']\n",
      "After stemming with porters algorithm: ['realli', 'dun', 'bluff', 'leh', 'sleep', 'earli', 'nite']\n",
      "Tokenized sentence: ['then', 'i', 'buy']\n",
      "After stop words removal: ['buy']\n",
      "After stemming with porters algorithm: ['bui']\n",
      "Tokenized sentence: ['i', 'havent', 'add', 'yet', 'right']\n",
      "After stop words removal: ['havent', 'add', 'yet', 'right']\n",
      "After stemming with porters algorithm: ['havent', 'add', 'yet', 'right']\n",
      "Tokenized sentence: ['k', 'i', 'll', 'be', 'there', 'before']\n",
      "After stop words removal: ['k']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['i', 'm', 'e', 'person', 'who', 's', 'doing', 'e', 'sms', 'survey']\n",
      "After stop words removal: ['e', 'person', 'e', 'sms', 'survey']\n",
      "After stemming with porters algorithm: ['person', 'sm', 'survei']\n",
      "Tokenized sentence: ['on', 'the', 'way', 'to', 'office', 'da']\n",
      "After stop words removal: ['way', 'office', 'da']\n",
      "After stemming with porters algorithm: ['wai', 'offic']\n",
      "Tokenized sentence: ['dunno', 'y', 'u', 'ask', 'me']\n",
      "After stop words removal: ['dunno', 'u', 'ask']\n",
      "After stemming with porters algorithm: ['dunno', 'ask']\n",
      "Tokenized sentence: ['buy', 'space', 'invaders', 'a', 'chance', 'win', 'orig', 'arcade', 'game', 'console', 'press', 'for', 'games', 'arcade', 'std', 'wap', 'charge', 'see', 'o', 'co', 'uk', 'games', 'terms', 'settings', 'no', 'purchase']\n",
      "After stop words removal: ['buy', 'space', 'invaders', 'chance', 'win', 'orig', 'arcade', 'game', 'console', 'press', 'games', 'arcade', 'std', 'wap', 'charge', 'see', 'co', 'uk', 'games', 'terms', 'settings', 'purchase']\n",
      "sett\n",
      "After stemming with porters algorithm: ['bui', 'space', 'invad', 'chanc', 'win', 'orig', 'arcad', 'game', 'consol', 'press', 'game', 'arcad', 'std', 'wap', 'charg', 'see', 'game', 'term', 'set', 'purchas']\n",
      "Tokenized sentence: ['hi', 'hope', 'u', 'r', 'both', 'ok', 'he', 'said', 'he', 'would', 'text', 'and', 'he', 'hasn', 't', 'have', 'u', 'seen', 'him', 'let', 'me', 'down', 'gently', 'please']\n",
      "After stop words removal: ['hi', 'hope', 'u', 'r', 'ok', 'said', 'would', 'text', 'u', 'seen', 'let', 'gently', 'please']\n",
      "After stemming with porters algorithm: ['hope', 'said', 'would', 'text', 'seen', 'let', 'gentli', 'pleas']\n",
      "Tokenized sentence: ['anything', 'lor', 'juz', 'both', 'of', 'us', 'lor']\n",
      "After stop words removal: ['anything', 'lor', 'juz', 'us', 'lor']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'lor', 'juz', 'lor']\n",
      "Tokenized sentence: ['mah', 'b', 'i', 'll', 'pick', 'it', 'up', 'tomorrow']\n",
      "After stop words removal: ['mah', 'b', 'pick', 'tomorrow']\n",
      "After stemming with porters algorithm: ['mah', 'pick', 'tomorrow']\n",
      "Tokenized sentence: ['u', 'ned', 'to', 'convince', 'him', 'tht', 'its', 'not', 'possible', 'witot', 'hurting', 'his', 'feeling', 'its', 'the', 'main']\n",
      "After stop words removal: ['u', 'ned', 'convince', 'tht', 'possible', 'witot', 'hurting', 'feeling', 'main']\n",
      "hurt\n",
      "feel\n",
      "After stemming with porters algorithm: ['ned', 'convinc', 'tht', 'possib', 'witot', 'hur', 'feel', 'main']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['congrats', 'mobile', 'g', 'videophones', 'r', 'yours', 'call', 'now', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl', 'bx', 'ip', 'we', 'p']\n",
      "After stop words removal: ['congrats', 'mobile', 'g', 'videophones', 'r', 'call', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl', 'bx', 'ip', 'p']\n",
      "After stemming with porters algorithm: ['congrat', 'mobil', 'videophon', 'call', 'videochat', 'wid', 'mate', 'plai', 'java', 'game', 'dload', 'polyph', 'music', 'nolin', 'rentl']\n",
      "Tokenized sentence: ['no', 'but', 'we', 'll', 'do', 'medical', 'missions', 'to', 'nigeria']\n",
      "After stop words removal: ['medical', 'missions', 'nigeria']\n",
      "After stemming with porters algorithm: ['medic', 'mission', 'nigeria']\n",
      "Tokenized sentence: ['symptoms', 'when', 'u', 'are', 'in', 'love', 'u', 'like', 'listening', 'songs', 'u', 'get', 'stopped', 'where', 'u', 'see', 'the', 'name', 'of', 'your', 'beloved', 'u', 'won', 't', 'get', 'angry', 'when', 'your']\n",
      "After stop words removal: ['symptoms', 'u', 'love', 'u', 'like', 'listening', 'songs', 'u', 'get', 'stopped', 'u', 'see', 'name', 'beloved', 'u', 'get', 'angry']\n",
      "listen\n",
      "After stemming with porters algorithm: ['symptom', 'love', 'like', 'listen', 'song', 'get', 'stop', 'see', 'name', 'belov', 'get', 'angri']\n",
      "Tokenized sentence: ['have', 'a', 'safe', 'trip', 'to', 'nigeria', 'wish', 'you', 'happiness', 'and', 'very', 'soon', 'company', 'to', 'share', 'moments', 'with']\n",
      "After stop words removal: ['safe', 'trip', 'nigeria', 'wish', 'happiness', 'soon', 'company', 'share', 'moments']\n",
      "After stemming with porters algorithm: ['safe', 'trip', 'nigeria', 'wish', 'happi', 'soon', 'compani', 'share', 'moment']\n",
      "Tokenized sentence: ['sad', 'story', 'of', 'a', 'man', 'last', 'week', 'was', 'my', 'b', 'day', 'my', 'wife', 'did', 'nt', 'wish', 'me', 'my', 'parents', 'forgot', 'n', 'so', 'did', 'my', 'kids', 'i', 'went', 'to', 'work', 'even', 'my', 'colleagues', 'did', 'not', 'wish']\n",
      "After stop words removal: ['sad', 'story', 'man', 'last', 'week', 'b', 'day', 'wife', 'nt', 'wish', 'parents', 'forgot', 'n', 'kids', 'went', 'work', 'even', 'colleagues', 'wish']\n",
      "After stemming with porters algorithm: ['sad', 'stori', 'man', 'last', 'week', 'dai', 'wife', 'wish', 'parent', 'forgot', 'kid', 'went', 'work', 'even', 'colleagu', 'wish']\n",
      "Tokenized sentence: ['they', 'just', 'talking', 'thats', 'it', 'de', 'they', 'wont', 'any', 'other']\n",
      "After stop words removal: ['talking', 'thats', 'de', 'wont']\n",
      "talk\n",
      "After stemming with porters algorithm: ['tal', 'that', 'wont']\n",
      "Tokenized sentence: ['ok', 'finishing', 'soon']\n",
      "After stop words removal: ['ok', 'finishing', 'soon']\n",
      "finish\n",
      "After stemming with porters algorithm: ['finis', 'soon']\n",
      "Tokenized sentence: ['just', 'got', 'to', 'lt', 'gt']\n",
      "After stop words removal: ['got', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['got']\n",
      "Tokenized sentence: ['interflora', 'it', 's', 'not', 'too', 'late', 'to', 'order', 'interflora', 'flowers', 'for', 'christmas', 'call', 'to', 'place', 'your', 'order', 'before', 'midnight', 'tomorrow']\n",
      "After stop words removal: ['interflora', 'late', 'order', 'interflora', 'flowers', 'christmas', 'call', 'place', 'order', 'midnight', 'tomorrow']\n",
      "After stemming with porters algorithm: ['interflora', 'late', 'order', 'interflora', 'flower', 'christma', 'call', 'place', 'order', 'midnight', 'tomorrow']\n",
      "Tokenized sentence: ['umma', 'did', 'she', 'say', 'anything']\n",
      "After stop words removal: ['umma', 'say', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['umma', 'sai', 'anyt']\n",
      "Tokenized sentence: ['you', 'at', 'mu', 'you', 'should', 'try', 'to', 'figure', 'out', 'how', 'much', 'money', 'everyone', 'has', 'for', 'gas', 'and', 'alcohol', 'jay', 'and', 'i', 'are', 'trying', 'to', 'figure', 'out', 'our', 'weed', 'budget']\n",
      "After stop words removal: ['mu', 'try', 'figure', 'much', 'money', 'everyone', 'gas', 'alcohol', 'jay', 'trying', 'figure', 'weed', 'budget']\n",
      "After stemming with porters algorithm: ['try', 'figur', 'much', 'monei', 'everyon', 'ga', 'alcohol', 'jai', 'trying', 'figur', 'weed', 'budget']\n",
      "Tokenized sentence: ['well', 'you', 'know', 'what', 'i', 'mean', 'texting']\n",
      "After stop words removal: ['well', 'know', 'mean', 'texting']\n",
      "text\n",
      "After stemming with porters algorithm: ['well', 'know', 'mean', 'tex']\n",
      "Tokenized sentence: ['i', 'am', 'not', 'at', 'all', 'happy', 'with', 'what', 'you', 'saying', 'or', 'doing']\n",
      "After stop words removal: ['happy', 'saying']\n",
      "say\n",
      "After stemming with porters algorithm: ['happi', 'sai']\n",
      "Tokenized sentence: ['thought', 'praps', 'you', 'meant', 'another', 'one', 'goodo', 'i', 'll', 'look', 'tomorrow']\n",
      "After stop words removal: ['thought', 'praps', 'meant', 'another', 'one', 'goodo', 'look', 'tomorrow']\n",
      "After stemming with porters algorithm: ['thought', 'prap', 'meant', 'anoth', 'on', 'goodo', 'look', 'tomorrow']\n",
      "Tokenized sentence: ['alright', 'i', 'll', 'head', 'out', 'in', 'a', 'few', 'minutes', 'text', 'me', 'where', 'to', 'meet', 'you']\n",
      "After stop words removal: ['alright', 'head', 'minutes', 'text', 'meet']\n",
      "After stemming with porters algorithm: ['alright', 'head', 'minut', 'text', 'meet']\n",
      "Tokenized sentence: ['please', 'ask', 'mummy', 'to', 'call', 'father']\n",
      "After stop words removal: ['please', 'ask', 'mummy', 'call', 'father']\n",
      "After stemming with porters algorithm: ['pleas', 'ask', 'mummi', 'call', 'father']\n",
      "Tokenized sentence: ['i', 'm', 'vivek', 'i', 'got', 'call', 'from', 'your', 'number']\n",
      "After stop words removal: ['vivek', 'got', 'call', 'number']\n",
      "After stemming with porters algorithm: ['vivek', 'got', 'call', 'number']\n",
      "Tokenized sentence: ['hi', 'you', 'just', 'spoke', 'to', 'maneesha', 'v', 'we', 'd', 'like', 'to', 'know', 'if', 'you', 'were', 'satisfied', 'with', 'the', 'experience', 'reply', 'toll', 'free', 'with', 'yes', 'or', 'no']\n",
      "After stop words removal: ['hi', 'spoke', 'maneesha', 'v', 'like', 'know', 'satisfied', 'experience', 'reply', 'toll', 'free', 'yes']\n",
      "After stemming with porters algorithm: ['spoke', 'maneesha', 'like', 'know', 'satisfi', 'experi', 'repli', 'toll', 'free', 'ye']\n",
      "Tokenized sentence: ['i', 'just', 'saw', 'ron', 'burgundy', 'captaining', 'a', 'party', 'boat', 'so', 'yeah']\n",
      "After stop words removal: ['saw', 'ron', 'burgundy', 'captaining', 'party', 'boat', 'yeah']\n",
      "captain\n",
      "After stemming with porters algorithm: ['saw', 'ron', 'burgundi', 'captain', 'parti', 'boat', 'yeah']\n",
      "Tokenized sentence: ['well', 'imma', 'definitely', 'need', 'to', 'restock', 'before', 'thanksgiving', 'i', 'll', 'let', 'you', 'know', 'when', 'i', 'm', 'out']\n",
      "After stop words removal: ['well', 'imma', 'definitely', 'need', 'restock', 'thanksgiving', 'let', 'know']\n",
      "thanksgiv\n",
      "After stemming with porters algorithm: ['well', 'imma', 'definit', 'need', 'restock', 'thanksgiv', 'let', 'know']\n",
      "Tokenized sentence: ['are', 'you', 'happy', 'baby', 'are', 'you', 'alright', 'did', 'you', 'take', 'that', 'job', 'i', 'hope', 'your', 'fine', 'i', 'send', 'you', 'a', 'kiss', 'to', 'make', 'you', 'smile', 'from', 'across', 'the', 'sea', 'kiss', 'kiss']\n",
      "After stop words removal: ['happy', 'baby', 'alright', 'take', 'job', 'hope', 'fine', 'send', 'kiss', 'make', 'smile', 'across', 'sea', 'kiss', 'kiss']\n",
      "After stemming with porters algorithm: ['happi', 'babi', 'alright', 'take', 'job', 'hope', 'fine', 'send', 'kiss', 'make', 'smile', 'across', 'sea', 'kiss', 'kiss']\n",
      "Tokenized sentence: ['you', 'said', 'not', 'now', 'no', 'problem', 'when', 'you', 'can', 'let', 'me', 'know']\n",
      "After stop words removal: ['said', 'problem', 'let', 'know']\n",
      "After stemming with porters algorithm: ['said', 'problem', 'let', 'know']\n",
      "Tokenized sentence: ['i', 'said', 'its', 'okay', 'sorry']\n",
      "After stop words removal: ['said', 'okay', 'sorry']\n",
      "After stemming with porters algorithm: ['said', 'okai', 'sorri']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'home', 'n', 'ready']\n",
      "After stop words removal: ['home', 'n', 'ready']\n",
      "After stemming with porters algorithm: ['home', 'readi']\n",
      "Tokenized sentence: ['why', 'are', 'u', 'up', 'so', 'early']\n",
      "After stop words removal: ['u', 'early']\n",
      "After stemming with porters algorithm: ['earli']\n",
      "Tokenized sentence: ['done', 'it', 'but', 'internet', 'connection', 'v', 'slow', 'and', 'can', 't', 'send', 'it', 'will', 'try', 'again', 'later', 'or', 'first', 'thing', 'tomo']\n",
      "After stop words removal: ['done', 'internet', 'connection', 'v', 'slow', 'send', 'try', 'later', 'first', 'thing', 'tomo']\n",
      "After stemming with porters algorithm: ['done', 'internet', 'connect', 'slow', 'send', 'try', 'later', 'first', 'thing', 'tomo']\n",
      "Tokenized sentence: ['how', 's', 'ur', 'paper']\n",
      "After stop words removal: ['ur', 'paper']\n",
      "After stemming with porters algorithm: ['paper']\n",
      "Tokenized sentence: ['you', 'always', 'make', 'things', 'bigger', 'than', 'they', 'are']\n",
      "After stop words removal: ['always', 'make', 'things', 'bigger']\n",
      "After stemming with porters algorithm: ['alwai', 'make', 'thing', 'bigger']\n",
      "Tokenized sentence: ['joy', 's', 'father', 'is', 'john', 'then', 'john', 'is', 'the', 'name', 'of', 'joy', 's', 'father', 'mandan']\n",
      "After stop words removal: ['joy', 'father', 'john', 'john', 'name', 'joy', 'father', 'mandan']\n",
      "After stemming with porters algorithm: ['joi', 'father', 'john', 'john', 'name', 'joi', 'father', 'mandan']\n",
      "Tokenized sentence: ['i', 'am', 'taking', 'half', 'day', 'leave', 'bec', 'i', 'am', 'not', 'well']\n",
      "After stop words removal: ['taking', 'half', 'day', 'leave', 'bec', 'well']\n",
      "tak\n",
      "After stemming with porters algorithm: ['take', 'half', 'dai', 'leav', 'bec', 'well']\n",
      "Tokenized sentence: ['do', 'you', 'mind', 'if', 'i', 'ask', 'what', 'happened', 'you', 'dont', 'have', 'to', 'say', 'if', 'it', 'is', 'uncomfortable']\n",
      "After stop words removal: ['mind', 'ask', 'happened', 'dont', 'say', 'uncomfortable']\n",
      "After stemming with porters algorithm: ['mind', 'ask', 'happen', 'dont', 'sai', 'uncomfort']\n",
      "Tokenized sentence: ['teach', 'me', 'apps', 'da', 'when', 'you', 'come', 'to', 'college']\n",
      "After stop words removal: ['teach', 'apps', 'da', 'come', 'college']\n",
      "After stemming with porters algorithm: ['teach', 'app', 'come', 'colleg']\n",
      "Tokenized sentence: ['get', 'ready', 'to', 'put', 'on', 'your', 'excellent', 'sub', 'face']\n",
      "After stop words removal: ['get', 'ready', 'put', 'excellent', 'sub', 'face']\n",
      "After stemming with porters algorithm: ['get', 'readi', 'put', 'excel', 'sub', 'face']\n",
      "Tokenized sentence: ['love', 'it', 'daddy', 'will', 'make', 'you', 'scream', 'with', 'pleasure', 'i', 'am', 'going', 'to', 'slap', 'your', 'ass', 'with', 'my', 'dick']\n",
      "After stop words removal: ['love', 'daddy', 'make', 'scream', 'pleasure', 'going', 'slap', 'ass', 'dick']\n",
      "go\n",
      "After stemming with porters algorithm: ['love', 'daddi', 'make', 'scream', 'pleasur', 'go', 'slap', 'ass', 'dick']\n",
      "Tokenized sentence: ['oh', 'well', 'c', 'u', 'later']\n",
      "After stop words removal: ['oh', 'well', 'c', 'u', 'later']\n",
      "After stemming with porters algorithm: ['well', 'later']\n",
      "Tokenized sentence: ['it', 'will', 'stop', 'on', 'itself', 'i', 'however', 'suggest', 'she', 'stays', 'with', 'someone', 'that', 'will', 'be', 'able', 'to', 'give', 'ors', 'for', 'every', 'stool']\n",
      "After stop words removal: ['stop', 'however', 'suggest', 'stays', 'someone', 'able', 'give', 'ors', 'every', 'stool']\n",
      "After stemming with porters algorithm: ['stop', 'howev', 'suggest', 'stai', 'someon', 'abl', 'give', 'or', 'everi', 'stool']\n",
      "Tokenized sentence: ['hmmm', 'thk', 'sure', 'got', 'time', 'to', 'hop', 'ard', 'ya', 'can', 'go', 'free', 'abt', 'muz', 'call', 'u', 'to', 'discuss', 'liao']\n",
      "After stop words removal: ['hmmm', 'thk', 'sure', 'got', 'time', 'hop', 'ard', 'ya', 'go', 'free', 'abt', 'muz', 'call', 'u', 'discuss', 'liao']\n",
      "After stemming with porters algorithm: ['hmmm', 'thk', 'sure', 'got', 'time', 'hop', 'ard', 'free', 'abt', 'muz', 'call', 'discuss', 'liao']\n",
      "Tokenized sentence: ['by', 'march', 'ending', 'i', 'should', 'be', 'ready', 'but', 'will', 'call', 'you', 'for', 'sure', 'the', 'problem', 'is', 'that', 'my', 'capital', 'never', 'complete', 'how', 'far', 'with', 'you', 'how', 's', 'work', 'and', 'the', 'ladies']\n",
      "After stop words removal: ['march', 'ending', 'ready', 'call', 'sure', 'problem', 'capital', 'never', 'complete', 'far', 'work', 'ladies']\n",
      "end\n",
      "After stemming with porters algorithm: ['march', 'en', 'readi', 'call', 'sure', 'problem', 'capit', 'never', 'complet', 'far', 'work', 'ladi']\n",
      "Tokenized sentence: ['huh', 'but', 'i', 'got', 'lesson', 'at', 'lei', 'n', 'i', 'was', 'thinkin', 'of', 'going', 'to', 'sch', 'earlier', 'n', 'i', 'tot', 'of', 'parkin', 'at', 'kent', 'vale']\n",
      "After stop words removal: ['huh', 'got', 'lesson', 'lei', 'n', 'thinkin', 'going', 'sch', 'earlier', 'n', 'tot', 'parkin', 'kent', 'vale']\n",
      "go\n",
      "After stemming with porters algorithm: ['huh', 'got', 'lesson', 'lei', 'thinkin', 'go', 'sch', 'earlier', 'tot', 'parkin', 'kent', 'vale']\n",
      "Tokenized sentence: ['why', 'don', 't', 'you', 'wait', 'til', 'at', 'least', 'wednesday', 'to', 'see', 'if', 'you', 'get', 'your']\n",
      "After stop words removal: ['wait', 'til', 'least', 'wednesday', 'see', 'get']\n",
      "After stemming with porters algorithm: ['wait', 'til', 'least', 'wednesdai', 'see', 'get']\n",
      "Tokenized sentence: ['how', 'much', 'she', 'payed', 'suganya']\n",
      "After stop words removal: ['much', 'payed', 'suganya']\n",
      "After stemming with porters algorithm: ['much', 'pai', 'suganya']\n",
      "Tokenized sentence: ['thanks', 'again', 'for', 'your', 'reply', 'today', 'when', 'is', 'ur', 'visa', 'coming', 'in', 'and', 'r', 'u', 'still', 'buying', 'the', 'gucci', 'and', 'bags', 'my', 'sister', 'things', 'are', 'not', 'easy', 'uncle', 'john', 'also', 'has', 'his', 'own', 'bills', 'so', 'i', 'really', 'need', 'to', 'think', 'about', 'how', 'to', 'make', 'my', 'own', 'money', 'later', 'sha']\n",
      "After stop words removal: ['thanks', 'reply', 'today', 'ur', 'visa', 'coming', 'r', 'u', 'still', 'buying', 'gucci', 'bags', 'sister', 'things', 'easy', 'uncle', 'john', 'also', 'bills', 'really', 'need', 'think', 'make', 'money', 'later', 'sha']\n",
      "com\n",
      "buy\n",
      "After stemming with porters algorithm: ['thank', 'repli', 'todai', 'visa', 'come', 'still', 'bui', 'gucci', 'bag', 'sister', 'thing', 'easi', 'uncl', 'john', 'also', 'bill', 'realli', 'need', 'think', 'make', 'monei', 'later', 'sha']\n",
      "Tokenized sentence: ['err', 'cud', 'do', 'i', 'm', 'going', 'to', 'at', 'pm', 'i', 'haven', 't', 'got', 'a', 'way', 'to', 'contact', 'him', 'until', 'then']\n",
      "After stop words removal: ['err', 'cud', 'going', 'pm', 'got', 'way', 'contact']\n",
      "go\n",
      "After stemming with porters algorithm: ['err', 'cud', 'go', 'got', 'wai', 'contact']\n",
      "Tokenized sentence: ['good', 'morning', 'plz', 'call', 'me', 'sir']\n",
      "After stop words removal: ['good', 'morning', 'plz', 'call', 'sir']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'plz', 'call', 'sir']\n",
      "Tokenized sentence: ['happy', 'birthday', 'may', 'all', 'ur', 'dreams', 'come', 'true']\n",
      "After stop words removal: ['happy', 'birthday', 'may', 'ur', 'dreams', 'come', 'true']\n",
      "After stemming with porters algorithm: ['happi', 'birthdai', 'mai', 'dream', 'come', 'true']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['ready', 'then', 'call', 'me']\n",
      "After stop words removal: ['ready', 'call']\n",
      "After stemming with porters algorithm: ['readi', 'call']\n",
      "Tokenized sentence: ['are', 'you', 'willing', 'to', 'go', 'for', 'apps', 'class']\n",
      "After stop words removal: ['willing', 'go', 'apps', 'class']\n",
      "will\n",
      "After stemming with porters algorithm: ['will', 'app', 'class']\n",
      "Tokenized sentence: ['sary', 'just', 'need', 'tim', 'in', 'the', 'bollox', 'it', 'hurt', 'him', 'a', 'lot', 'so', 'he', 'tol', 'me']\n",
      "After stop words removal: ['sary', 'need', 'tim', 'bollox', 'hurt', 'lot', 'tol']\n",
      "After stemming with porters algorithm: ['sari', 'need', 'tim', 'bollox', 'hurt', 'lot', 'tol']\n",
      "Tokenized sentence: ['what', 'today', 'sunday', 'sunday', 'is', 'holiday', 'so', 'no', 'work']\n",
      "After stop words removal: ['today', 'sunday', 'sunday', 'holiday', 'work']\n",
      "After stemming with porters algorithm: ['todai', 'sundai', 'sundai', 'holidai', 'work']\n",
      "Tokenized sentence: ['then', 'ur', 'physics', 'get', 'a']\n",
      "After stop words removal: ['ur', 'physics', 'get']\n",
      "After stemming with porters algorithm: ['physic', 'get']\n",
      "Tokenized sentence: ['aldrine', 'rakhesh', 'ex', 'rtm', 'here', 'pls', 'call', 'urgent']\n",
      "After stop words removal: ['aldrine', 'rakhesh', 'ex', 'rtm', 'pls', 'call', 'urgent']\n",
      "After stemming with porters algorithm: ['aldrin', 'rakhesh', 'rtm', 'pl', 'call', 'urgent']\n",
      "Tokenized sentence: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'zed', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stop words removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'zed', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stemming with porters algorithm: ['free', 'week', 'nokia', 'tone', 'mob', 'everi', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'mate', 'zed', 'pobox', 'norm', 'tone']\n",
      "Tokenized sentence: ['thanx', 'puttin', 'da', 'fone', 'down', 'on', 'me']\n",
      "After stop words removal: ['thanx', 'puttin', 'da', 'fone']\n",
      "After stemming with porters algorithm: ['thanx', 'puttin', 'fone']\n",
      "Tokenized sentence: ['hai', 'ana', 'tomarrow', 'am', 'coming', 'on', 'morning', 'lt', 'decimal', 'gt', 'ill', 'be', 'there', 'in', 'sathy', 'then', 'we', 'll', 'go', 'to', 'rto', 'office', 'reply', 'me', 'after', 'came', 'to', 'home']\n",
      "After stop words removal: ['hai', 'ana', 'tomarrow', 'coming', 'morning', 'lt', 'decimal', 'gt', 'ill', 'sathy', 'go', 'rto', 'office', 'reply', 'came', 'home']\n",
      "com\n",
      "morn\n",
      "After stemming with porters algorithm: ['hai', 'ana', 'tomarrow', 'come', 'mor', 'decim', 'ill', 'sathi', 'rto', 'offic', 'repli', 'came', 'home']\n",
      "Tokenized sentence: ['mm', 'yes', 'dear', 'look', 'how', 'i', 'am', 'hugging', 'you', 'both', 'p']\n",
      "After stop words removal: ['mm', 'yes', 'dear', 'look', 'hugging', 'p']\n",
      "hugg\n",
      "After stemming with porters algorithm: ['ye', 'dear', 'look', 'hug']\n",
      "Tokenized sentence: ['multiply', 'the', 'numbers', 'independently', 'and', 'count', 'decimal', 'points', 'then', 'for', 'the', 'division', 'push', 'the', 'decimal', 'places', 'like', 'i', 'showed', 'you']\n",
      "After stop words removal: ['multiply', 'numbers', 'independently', 'count', 'decimal', 'points', 'division', 'push', 'decimal', 'places', 'like', 'showed']\n",
      "After stemming with porters algorithm: ['multipli', 'number', 'independ', 'count', 'decim', 'point', 'divis', 'push', 'decim', 'place', 'like', 'showe']\n",
      "Tokenized sentence: ['yunny', 'i', 'm', 'walking', 'in', 'citylink', 'now', 'faster', 'come', 'down', 'me', 'very', 'hungry']\n",
      "After stop words removal: ['yunny', 'walking', 'citylink', 'faster', 'come', 'hungry']\n",
      "walk\n",
      "After stemming with porters algorithm: ['yunni', 'wal', 'citylink', 'faster', 'come', 'hungri']\n",
      "Tokenized sentence: ['zoe', 'it', 'just', 'hit', 'me', 'im', 'fucking', 'shitin', 'myself', 'il', 'defo', 'try', 'my', 'hardest', 'cum', 'morow', 'luv', 'u', 'millions', 'lekdog']\n",
      "After stop words removal: ['zoe', 'hit', 'im', 'fucking', 'shitin', 'il', 'defo', 'try', 'hardest', 'cum', 'morow', 'luv', 'u', 'millions', 'lekdog']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['zoe', 'hit', 'fuc', 'shitin', 'defo', 'try', 'hardest', 'cum', 'morow', 'luv', 'million', 'lekdog']\n",
      "Tokenized sentence: ['im', 'gonna', 'miss', 'u', 'so', 'much']\n",
      "After stop words removal: ['im', 'gonna', 'miss', 'u', 'much']\n",
      "After stemming with porters algorithm: ['gonna', 'miss', 'much']\n",
      "Tokenized sentence: ['great', 'news', 'call', 'freefone', 'to', 'claim', 'your', 'guaranteed', 'cash', 'or', 'gift', 'speak', 'to', 'a', 'live', 'operator', 'now']\n",
      "After stop words removal: ['great', 'news', 'call', 'freefone', 'claim', 'guaranteed', 'cash', 'gift', 'speak', 'live', 'operator']\n",
      "After stemming with porters algorithm: ['great', 'new', 'call', 'freefon', 'claim', 'guaranteed', 'cash', 'gift', 'speak', 'live', 'oper']\n",
      "Tokenized sentence: ['hey', 'are', 'you', 'angry', 'with', 'me', 'reply', 'me', 'dr']\n",
      "After stop words removal: ['hey', 'angry', 'reply', 'dr']\n",
      "After stemming with porters algorithm: ['hei', 'angri', 'repli']\n",
      "Tokenized sentence: ['the', 'new', 'deus', 'ex', 'game', 'comin', 'early', 'next', 'yr']\n",
      "After stop words removal: ['new', 'deus', 'ex', 'game', 'comin', 'early', 'next', 'yr']\n",
      "After stemming with porters algorithm: ['new', 'deu', 'game', 'comin', 'earli', 'next']\n",
      "Tokenized sentence: ['tell', 'her', 'i', 'said', 'eat', 'shit']\n",
      "After stop words removal: ['tell', 'said', 'eat', 'shit']\n",
      "After stemming with porters algorithm: ['tell', 'said', 'eat', 'shit']\n",
      "Tokenized sentence: ['oops', 'i', 'did', 'have', 'it', 'lt', 'gt']\n",
      "After stop words removal: ['oops', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['oop']\n",
      "Tokenized sentence: ['she', 'said', 'do', 'u', 'mind', 'if', 'i', 'go', 'into', 'the', 'bedroom', 'for', 'a', 'minute', 'ok', 'i', 'sed', 'in', 'a', 'sexy', 'mood', 'she', 'came', 'out', 'minuts', 'latr', 'wid', 'a', 'cake', 'n', 'my', 'wife']\n",
      "After stop words removal: ['said', 'u', 'mind', 'go', 'bedroom', 'minute', 'ok', 'sed', 'sexy', 'mood', 'came', 'minuts', 'latr', 'wid', 'cake', 'n', 'wife']\n",
      "After stemming with porters algorithm: ['said', 'mind', 'bedroom', 'minut', 'sed', 'sexi', 'mood', 'came', 'minut', 'latr', 'wid', 'cake', 'wife']\n",
      "Tokenized sentence: ['i', 'm', 'gonna', 'rip', 'out', 'my', 'uterus']\n",
      "After stop words removal: ['gonna', 'rip', 'uterus']\n",
      "After stemming with porters algorithm: ['gonna', 'rip', 'uteru']\n",
      "Tokenized sentence: ['pls', 'speak', 'to', 'that', 'customer', 'machan']\n",
      "After stop words removal: ['pls', 'speak', 'customer', 'machan']\n",
      "After stemming with porters algorithm: ['pl', 'speak', 'custom', 'machan']\n",
      "Tokenized sentence: ['its', 'ok', 'if', 'anybody', 'asks', 'abt', 'me', 'u', 'tel', 'them', 'p']\n",
      "After stop words removal: ['ok', 'anybody', 'asks', 'abt', 'u', 'tel', 'p']\n",
      "After stemming with porters algorithm: ['anybodi', 'ask', 'abt', 'tel']\n",
      "Tokenized sentence: ['final', 'chance', 'claim', 'ur', 'worth', 'of', 'discount', 'vouchers', 'today', 'text', 'yes', 'to', 'now', 'savamob', 'member', 'offers', 'mobile', 't', 'cs', 'savamob', 'pobox', 'm', 'uz', 'subs']\n",
      "After stop words removal: ['final', 'chance', 'claim', 'ur', 'worth', 'discount', 'vouchers', 'today', 'text', 'yes', 'savamob', 'member', 'offers', 'mobile', 'cs', 'savamob', 'pobox', 'uz', 'subs']\n",
      "After stemming with porters algorithm: ['final', 'chanc', 'claim', 'worth', 'discount', 'voucher', 'todai', 'text', 'ye', 'savamob', 'member', 'offer', 'mobil', 'savamob', 'pobox', 'sub']\n",
      "Tokenized sentence: ['latest', 'news', 'police', 'station', 'toilet', 'stolen', 'cops', 'have', 'nothing', 'to', 'go', 'on']\n",
      "After stop words removal: ['latest', 'news', 'police', 'station', 'toilet', 'stolen', 'cops', 'nothing', 'go']\n",
      "noth\n",
      "After stemming with porters algorithm: ['latest', 'new', 'polic', 'stat', 'toilet', 'stolen', 'cop', 'not']\n",
      "Tokenized sentence: ['hello', 'they', 'are', 'going', 'to', 'the', 'village', 'pub', 'at', 'so', 'either', 'come', 'here', 'or', 'there', 'accordingly', 'ok']\n",
      "After stop words removal: ['hello', 'going', 'village', 'pub', 'either', 'come', 'accordingly', 'ok']\n",
      "go\n",
      "After stemming with porters algorithm: ['hello', 'go', 'villag', 'pub', 'either', 'come', 'accordingli']\n",
      "Tokenized sentence: ['lol', 'i', 'have', 'to', 'take', 'it', 'member', 'how', 'i', 'said', 'my', 'aunt', 'flow', 'didn', 't', 'visit', 'for', 'months', 'it', 's', 'cause', 'i', 'developed', 'ovarian', 'cysts', 'bc', 'is', 'the', 'only', 'way', 'to', 'shrink', 'them']\n",
      "After stop words removal: ['lol', 'take', 'member', 'said', 'aunt', 'flow', 'visit', 'months', 'cause', 'developed', 'ovarian', 'cysts', 'bc', 'way', 'shrink']\n",
      "After stemming with porters algorithm: ['lol', 'take', 'member', 'said', 'aunt', 'flow', 'visit', 'month', 'caus', 'develop', 'ovarian', 'cyst', 'wai', 'shrink']\n",
      "Tokenized sentence: ['single', 'line', 'with', 'a', 'big', 'meaning', 'miss', 'anything', 'ur', 'best', 'life', 'but']\n",
      "After stop words removal: ['single', 'line', 'big', 'meaning', 'miss', 'anything', 'ur', 'best', 'life']\n",
      "mean\n",
      "anyth\n",
      "After stemming with porters algorithm: ['singl', 'line', 'big', 'mean', 'miss', 'anyt', 'best', 'life']\n",
      "Tokenized sentence: ['bring', 'home', 'some', 'wendy', 'd']\n",
      "After stop words removal: ['bring', 'home', 'wendy']\n",
      "After stemming with porters algorithm: ['bring', 'home', 'wendi']\n",
      "Tokenized sentence: ['babe', 'have', 'you', 'got', 'enough', 'money', 'to', 'pick', 'up', 'bread', 'and', 'milk', 'and', 'i', 'll', 'give', 'you', 'it', 'back', 'when', 'you', 'get', 'home']\n",
      "After stop words removal: ['babe', 'got', 'enough', 'money', 'pick', 'bread', 'milk', 'give', 'back', 'get', 'home']\n",
      "After stemming with porters algorithm: ['babe', 'got', 'enough', 'monei', 'pick', 'bread', 'milk', 'give', 'back', 'get', 'home']\n",
      "Tokenized sentence: ['usf', 'i', 'guess', 'might', 'as', 'well', 'take', 'car']\n",
      "After stop words removal: ['usf', 'guess', 'might', 'well', 'take', 'car']\n",
      "After stemming with porters algorithm: ['usf', 'guess', 'might', 'well', 'take', 'car']\n",
      "Tokenized sentence: ['hmv', 'bonus', 'special', 'pounds', 'of', 'genuine', 'hmv', 'vouchers', 'to', 'be', 'won', 'just', 'answer', 'easy', 'questions', 'play', 'now', 'send', 'hmv', 'to', 'more', 'info', 'www', 'percent', 'real', 'com']\n",
      "After stop words removal: ['hmv', 'bonus', 'special', 'pounds', 'genuine', 'hmv', 'vouchers', 'answer', 'easy', 'questions', 'play', 'send', 'hmv', 'info', 'www', 'percent', 'real', 'com']\n",
      "After stemming with porters algorithm: ['hmv', 'bonu', 'special', 'pound', 'genuin', 'hmv', 'voucher', 'answer', 'easi', 'quest', 'plai', 'send', 'hmv', 'info', 'www', 'percent', 'real', 'com']\n",
      "Tokenized sentence: ['mon', 'okie', 'lor', 'haha', 'best', 'is', 'cheap', 'n', 'gd', 'food', 'la', 'ex', 'oso', 'okie', 'depends', 'on', 'whether', 'wana', 'eat', 'western', 'or', 'chinese', 'food', 'den', 'which', 'u', 'prefer']\n",
      "After stop words removal: ['mon', 'okie', 'lor', 'haha', 'best', 'cheap', 'n', 'gd', 'food', 'la', 'ex', 'oso', 'okie', 'depends', 'whether', 'wana', 'eat', 'western', 'chinese', 'food', 'den', 'u', 'prefer']\n",
      "After stemming with porters algorithm: ['mon', 'oki', 'lor', 'haha', 'best', 'cheap', 'food', 'oso', 'oki', 'depend', 'whether', 'wana', 'eat', 'western', 'chines', 'food', 'den', 'prefer']\n",
      "Tokenized sentence: ['hey', 'what', 'happened', 'u', 'switch', 'off', 'ur', 'cell', 'd', 'whole', 'day', 'this', 'isnt', 'good', 'now', 'if', 'u', 'do', 'care', 'give', 'me', 'a', 'call', 'tomorrow']\n",
      "After stop words removal: ['hey', 'happened', 'u', 'switch', 'ur', 'cell', 'whole', 'day', 'isnt', 'good', 'u', 'care', 'give', 'call', 'tomorrow']\n",
      "After stemming with porters algorithm: ['hei', 'happen', 'switch', 'cell', 'whole', 'dai', 'isnt', 'good', 'care', 'give', 'call', 'tomorrow']\n",
      "Tokenized sentence: ['i', 've', 'reached', 'already']\n",
      "After stop words removal: ['reached', 'already']\n",
      "After stemming with porters algorithm: ['reac', 'alreadi']\n",
      "Tokenized sentence: ['then', 'u', 'drive', 'lor']\n",
      "After stop words removal: ['u', 'drive', 'lor']\n",
      "After stemming with porters algorithm: ['drive', 'lor']\n",
      "Tokenized sentence: ['i', 'will', 'be', 'outside', 'office', 'take', 'all', 'from', 'there']\n",
      "After stop words removal: ['outside', 'office', 'take']\n",
      "After stemming with porters algorithm: ['outsid', 'offic', 'take']\n",
      "Tokenized sentence: ['i', 'dunno', 'until', 'when', 'lets', 'go', 'learn', 'pilates']\n",
      "After stop words removal: ['dunno', 'lets', 'go', 'learn', 'pilates']\n",
      "After stemming with porters algorithm: ['dunno', 'let', 'learn', 'pilat']\n",
      "Tokenized sentence: ['yes', 'the', 'only', 'place', 'in', 'town', 'to', 'meet', 'exciting', 'adult', 'singles', 'is', 'now', 'in', 'the', 'uk', 'txt', 'chat', 'to', 'now', 'p', 'msg']\n",
      "After stop words removal: ['yes', 'place', 'town', 'meet', 'exciting', 'adult', 'singles', 'uk', 'txt', 'chat', 'p', 'msg']\n",
      "excit\n",
      "After stemming with porters algorithm: ['ye', 'place', 'town', 'meet', 'excit', 'adult', 'singl', 'txt', 'chat', 'msg']\n",
      "Tokenized sentence: ['free', 'ringtone', 'reply', 'real']\n",
      "After stop words removal: ['free', 'ringtone', 'reply', 'real']\n",
      "After stemming with porters algorithm: ['free', 'rington', 'repli', 'real']\n",
      "Tokenized sentence: ['oh', 'thats', 'late', 'well', 'have', 'a', 'good', 'night', 'and', 'i', 'will', 'give', 'u', 'a', 'call', 'tomorrow', 'iam', 'now', 'going', 'to', 'go', 'to', 'sleep', 'night', 'night']\n",
      "After stop words removal: ['oh', 'thats', 'late', 'well', 'good', 'night', 'give', 'u', 'call', 'tomorrow', 'iam', 'going', 'go', 'sleep', 'night', 'night']\n",
      "go\n",
      "After stemming with porters algorithm: ['that', 'late', 'well', 'good', 'night', 'give', 'call', 'tomorrow', 'iam', 'go', 'sleep', 'night', 'night']\n",
      "Tokenized sentence: ['hows', 'my', 'favourite', 'person', 'today', 'r', 'u', 'workin', 'hard', 'couldn', 't', 'sleep', 'again', 'last', 'nite', 'nearly', 'rang', 'u', 'at']\n",
      "After stop words removal: ['hows', 'favourite', 'person', 'today', 'r', 'u', 'workin', 'hard', 'sleep', 'last', 'nite', 'nearly', 'rang', 'u']\n",
      "After stemming with porters algorithm: ['how', 'favourit', 'person', 'todai', 'workin', 'hard', 'sleep', 'last', 'nite', 'nearli', 'rang']\n",
      "Tokenized sentence: ['i', 'pocked', 'you', 'up', 'there', 'before']\n",
      "After stop words removal: ['pocked']\n",
      "After stemming with porters algorithm: ['poc']\n",
      "Tokenized sentence: ['going', 'on', 'nothing', 'great', 'bye']\n",
      "After stop words removal: ['going', 'nothing', 'great', 'bye']\n",
      "go\n",
      "noth\n",
      "After stemming with porters algorithm: ['go', 'not', 'great', 'bye']\n",
      "Tokenized sentence: ['congrats', 'treat', 'pending', 'i', 'am', 'not', 'on', 'mail', 'for', 'days', 'will', 'mail', 'once', 'thru', 'respect', 'mother', 'at', 'home', 'check', 'mails']\n",
      "After stop words removal: ['congrats', 'treat', 'pending', 'mail', 'days', 'mail', 'thru', 'respect', 'mother', 'home', 'check', 'mails']\n",
      "pend\n",
      "After stemming with porters algorithm: ['congrat', 'treat', 'pen', 'mail', 'dai', 'mail', 'thru', 'respect', 'mother', 'home', 'check', 'mail']\n",
      "Tokenized sentence: ['u', 'know', 'we', 'watchin', 'at', 'lido']\n",
      "After stop words removal: ['u', 'know', 'watchin', 'lido']\n",
      "After stemming with porters algorithm: ['know', 'watchin', 'lido']\n",
      "Tokenized sentence: ['o', 'was', 'not', 'into', 'fps', 'then']\n",
      "After stop words removal: ['fps']\n",
      "After stemming with porters algorithm: ['fp']\n",
      "Tokenized sentence: ['k', 'k', 'i', 'm', 'going', 'to', 'tirunelvali', 'this', 'week', 'to', 'see', 'my', 'uncle', 'i', 'already', 'spend', 'the', 'amount', 'by', 'taking', 'dress', 'so', 'only', 'i', 'want', 'money', 'i', 'will', 'give', 'it', 'on', 'feb']\n",
      "After stop words removal: ['k', 'k', 'going', 'tirunelvali', 'week', 'see', 'uncle', 'already', 'spend', 'amount', 'taking', 'dress', 'want', 'money', 'give', 'feb']\n",
      "go\n",
      "tak\n",
      "After stemming with porters algorithm: ['go', 'tirunelvali', 'week', 'see', 'uncl', 'alreadi', 'spend', 'amount', 'take', 'dress', 'want', 'monei', 'give', 'feb']\n",
      "Tokenized sentence: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'it', 'seriously', 'coz', 'being', 'angry', 'is', 'd', 'most', 'childish', 'n', 'true', 'way', 'of', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'have', 'nice', 'day', 'da']\n",
      "After stop words removal: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'seriously', 'coz', 'angry', 'childish', 'n', 'true', 'way', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'nice', 'day', 'da']\n",
      "show\n",
      "After stemming with porters algorithm: ['wen', 'lovab', 'bcum', 'angri', 'wid', 'dnt', 'take', 'serious', 'coz', 'angri', 'childish', 'true', 'wai', 'showe', 'deep', 'affect', 'care', 'luv', 'kettoda', 'manda', 'nice', 'dai']\n",
      "Tokenized sentence: ['your', 'account', 'for', 'xxxxxxxxx', 'shows', 'unredeemed', 'points', 'to', 'claim', 'call', 'identifier', 'code', 'xxxxx', 'expires']\n",
      "After stop words removal: ['account', 'xxxxxxxxx', 'shows', 'unredeemed', 'points', 'claim', 'call', 'identifier', 'code', 'xxxxx', 'expires']\n",
      "After stemming with porters algorithm: ['account', 'xxxxxxxxx', 'show', 'unredeem', 'point', 'claim', 'call', 'identifi', 'code', 'xxxxx', 'expir']\n",
      "Tokenized sentence: ['hey', 'r', 'still', 'online', 'i', 've', 'finished', 'the', 'formatting']\n",
      "After stop words removal: ['hey', 'r', 'still', 'online', 'finished', 'formatting']\n",
      "formatt\n",
      "After stemming with porters algorithm: ['hei', 'still', 'onlin', 'finis', 'format']\n",
      "Tokenized sentence: ['moji', 'just', 'informed', 'me', 'that', 'you', 'saved', 'our', 'lives', 'thanks']\n",
      "After stop words removal: ['moji', 'informed', 'saved', 'lives', 'thanks']\n",
      "After stemming with porters algorithm: ['moji', 'infor', 'save', 'live', 'thank']\n",
      "Tokenized sentence: ['i', 'will', 'come', 'with', 'karnan', 'car', 'please', 'wait', 'till', 'pm', 'will', 'directly', 'goto', 'doctor']\n",
      "After stop words removal: ['come', 'karnan', 'car', 'please', 'wait', 'till', 'pm', 'directly', 'goto', 'doctor']\n",
      "After stemming with porters algorithm: ['come', 'karnan', 'car', 'pleas', 'wait', 'till', 'directli', 'goto', 'doctor']\n",
      "Tokenized sentence: ['was', 'actually', 'about', 'to', 'send', 'you', 'a', 'reminder', 'today', 'have', 'a', 'wonderful', 'weekend']\n",
      "After stop words removal: ['actually', 'send', 'reminder', 'today', 'wonderful', 'weekend']\n",
      "After stemming with porters algorithm: ['actual', 'send', 'remind', 'todai', 'wonder', 'weekend']\n",
      "Tokenized sentence: ['oh', 'god', 'i', 'am', 'happy', 'to', 'see', 'your', 'message', 'after', 'days']\n",
      "After stop words removal: ['oh', 'god', 'happy', 'see', 'message', 'days']\n",
      "After stemming with porters algorithm: ['god', 'happi', 'see', 'messag', 'dai']\n",
      "Tokenized sentence: ['haven', 't', 'seen', 'my', 'facebook', 'huh', 'lol']\n",
      "After stop words removal: ['seen', 'facebook', 'huh', 'lol']\n",
      "After stemming with porters algorithm: ['seen', 'facebook', 'huh', 'lol']\n",
      "Tokenized sentence: ['as', 'per', 'your', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', 'to', 'copy', 'your', 'friends', 'callertune']\n",
      "After stop words removal: ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'callers', 'press', 'copy', 'friends', 'callertune']\n",
      "After stemming with porters algorithm: ['per', 'request', 'mell', 'mell', 'oru', 'minnaminungint', 'nurungu', 'vettam', 'set', 'callertun', 'caller', 'press', 'copi', 'friend', 'callertun']\n",
      "Tokenized sentence: ['i', 'was', 'up', 'all', 'night', 'too', 'worrying', 'about', 'this', 'appt', 'it', 's', 'a', 'shame', 'we', 'missed', 'a', 'girls', 'night', 'out', 'with', 'quizzes', 'popcorn', 'and', 'you', 'doing', 'my', 'hair']\n",
      "After stop words removal: ['night', 'worrying', 'appt', 'shame', 'missed', 'girls', 'night', 'quizzes', 'popcorn', 'hair']\n",
      "worry\n",
      "After stemming with porters algorithm: ['night', 'worr', 'appt', 'shame', 'miss', 'girl', 'night', 'quizz', 'popcorn', 'hair']\n",
      "Tokenized sentence: ['can', 'u', 'look', 'me', 'in', 'da', 'lib', 'i', 'got', 'stuff', 'havent', 'finish', 'yet']\n",
      "After stop words removal: ['u', 'look', 'da', 'lib', 'got', 'stuff', 'havent', 'finish', 'yet']\n",
      "After stemming with porters algorithm: ['look', 'lib', 'got', 'stuff', 'havent', 'finish', 'yet']\n",
      "Tokenized sentence: ['k', 'tell', 'me', 'anything', 'about', 'you']\n",
      "After stop words removal: ['k', 'tell', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['tell', 'anyt']\n",
      "Tokenized sentence: ['what', 'you', 'doing', 'how', 'are', 'you']\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['mmm', 'so', 'yummy', 'babe', 'nice', 'jolt', 'to', 'the', 'suzy']\n",
      "After stop words removal: ['mmm', 'yummy', 'babe', 'nice', 'jolt', 'suzy']\n",
      "After stemming with porters algorithm: ['mmm', 'yummi', 'babe', 'nice', 'jolt', 'suzi']\n",
      "Tokenized sentence: ['dear', 'i', 'am', 'not', 'denying', 'your', 'words', 'please']\n",
      "After stop words removal: ['dear', 'denying', 'words', 'please']\n",
      "deny\n",
      "After stemming with porters algorithm: ['dear', 'den', 'word', 'pleas']\n",
      "Tokenized sentence: ['today', 'am', 'going', 'to', 'college', 'so', 'am', 'not', 'able', 'to', 'atten', 'the', 'class']\n",
      "After stop words removal: ['today', 'going', 'college', 'able', 'atten', 'class']\n",
      "go\n",
      "After stemming with porters algorithm: ['todai', 'go', 'colleg', 'abl', 'atten', 'class']\n",
      "Tokenized sentence: ['hello', 'u', 'call', 'wen', 'u', 'finish', 'wrk', 'i', 'fancy', 'meetin', 'up', 'wiv', 'u', 'all', 'tonite', 'as', 'i', 'need', 'a', 'break', 'from', 'dabooks', 'did', 'hrs', 'last', 'nite', 'today', 'of', 'wrk']\n",
      "After stop words removal: ['hello', 'u', 'call', 'wen', 'u', 'finish', 'wrk', 'fancy', 'meetin', 'wiv', 'u', 'tonite', 'need', 'break', 'dabooks', 'hrs', 'last', 'nite', 'today', 'wrk']\n",
      "After stemming with porters algorithm: ['hello', 'call', 'wen', 'finish', 'wrk', 'fanci', 'meetin', 'wiv', 'tonit', 'need', 'break', 'dabook', 'hr', 'last', 'nite', 'todai', 'wrk']\n",
      "Tokenized sentence: ['rt', 'king', 'pro', 'video', 'club', 'need', 'help', 'info', 'ringtoneking', 'co', 'uk', 'or', 'call', 'you', 'must', 'be', 'club', 'credits', 'redeemable', 'at', 'www', 'ringtoneking', 'co', 'uk', 'enjoy']\n",
      "After stop words removal: ['rt', 'king', 'pro', 'video', 'club', 'need', 'help', 'info', 'ringtoneking', 'co', 'uk', 'call', 'must', 'club', 'credits', 'redeemable', 'www', 'ringtoneking', 'co', 'uk', 'enjoy']\n",
      "ringtonek\n",
      "ringtonek\n",
      "After stemming with porters algorithm: ['king', 'pro', 'video', 'club', 'need', 'help', 'info', 'ringtonek', 'call', 'must', 'club', 'credit', 'redeem', 'www', 'ringtonek', 'enjoi']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['you', 'are', 'chosen', 'to', 'receive', 'a', 'award', 'pls', 'call', 'claim', 'number', 'to', 'collect', 'your', 'award', 'which', 'you', 'are', 'selected', 'to', 'receive', 'as', 'a', 'valued', 'mobile', 'customer']\n",
      "After stop words removal: ['chosen', 'receive', 'award', 'pls', 'call', 'claim', 'number', 'collect', 'award', 'selected', 'receive', 'valued', 'mobile', 'customer']\n",
      "After stemming with porters algorithm: ['chosen', 'receiv', 'award', 'pl', 'call', 'claim', 'number', 'collect', 'award', 'selec', 'receiv', 'valu', 'mobil', 'custom']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['customer', 'service', 'announcement', 'we', 'recently', 'tried', 'to', 'make', 'a', 'delivery', 'to', 'you', 'but', 'were', 'unable', 'to', 'do', 'so', 'please', 'call', 'to', 're', 'schedule', 'ref']\n",
      "After stop words removal: ['customer', 'service', 'announcement', 'recently', 'tried', 'make', 'delivery', 'unable', 'please', 'call', 'schedule', 'ref']\n",
      "After stemming with porters algorithm: ['custom', 'servic', 'announc', 'recent', 'tri', 'make', 'deliveri', 'unab', 'pleas', 'call', 'schedul', 'ref']\n",
      "Tokenized sentence: ['at', 'bruce', 'b', 'downs', 'amp', 'fletcher', 'now']\n",
      "After stop words removal: ['bruce', 'b', 'downs', 'amp', 'fletcher']\n",
      "After stemming with porters algorithm: ['bruce', 'down', 'amp', 'fletcher']\n",
      "Tokenized sentence: ['no', 'need', 'to', 'say', 'anything', 'to', 'me', 'i', 'know', 'i', 'am', 'an', 'outsider']\n",
      "After stop words removal: ['need', 'say', 'anything', 'know', 'outsider']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['need', 'sai', 'anyt', 'know', 'outsid']\n",
      "Tokenized sentence: ['what', 'do', 'u', 'reckon', 'as', 'need', 'arrange', 'transport', 'if', 'u', 'can', 't', 'do', 'it', 'thanks']\n",
      "After stop words removal: ['u', 'reckon', 'need', 'arrange', 'transport', 'u', 'thanks']\n",
      "After stemming with porters algorithm: ['reckon', 'need', 'arrang', 'transport', 'thank']\n",
      "Tokenized sentence: ['free', 'text', 'msgs', 'just', 'text', 'ok', 'to', 'and', 'we', 'll', 'credit', 'your', 'account']\n",
      "After stop words removal: ['free', 'text', 'msgs', 'text', 'ok', 'credit', 'account']\n",
      "After stemming with porters algorithm: ['free', 'text', 'msg', 'text', 'credit', 'account']\n",
      "Tokenized sentence: ['dai', 'i', 'downloaded', 'but', 'there', 'is', 'only', 'exe', 'file', 'which', 'i', 'can', 'only', 'run', 'that', 'exe', 'after', 'installing']\n",
      "After stop words removal: ['dai', 'downloaded', 'exe', 'file', 'run', 'exe', 'installing']\n",
      "install\n",
      "After stemming with porters algorithm: ['dai', 'download', 'ex', 'file', 'run', 'ex', 'instal']\n",
      "Tokenized sentence: ['y', 'wan', 'to', 'go', 'there', 'c', 'doctor']\n",
      "After stop words removal: ['wan', 'go', 'c', 'doctor']\n",
      "After stemming with porters algorithm: ['wan', 'doctor']\n",
      "Tokenized sentence: ['brainless', 'baby', 'doll', 'd', 'vehicle', 'sariyag', 'drive', 'madoke', 'barolla']\n",
      "After stop words removal: ['brainless', 'baby', 'doll', 'vehicle', 'sariyag', 'drive', 'madoke', 'barolla']\n",
      "After stemming with porters algorithm: ['brainless', 'babi', 'doll', 'vehic', 'sariyag', 'drive', 'madok', 'barolla']\n",
      "Tokenized sentence: ['would', 'really', 'appreciate', 'if', 'you', 'call', 'me', 'just', 'need', 'someone', 'to', 'talk', 'to']\n",
      "After stop words removal: ['would', 'really', 'appreciate', 'call', 'need', 'someone', 'talk']\n",
      "After stemming with porters algorithm: ['would', 'realli', 'appreci', 'call', 'need', 'someon', 'talk']\n",
      "Tokenized sentence: ['call', 'to', 'the', 'number', 'which', 'is', 'available', 'in', 'appointment', 'and', 'ask', 'to', 'connect', 'the', 'call', 'to', 'waheed', 'fathima']\n",
      "After stop words removal: ['call', 'number', 'available', 'appointment', 'ask', 'connect', 'call', 'waheed', 'fathima']\n",
      "After stemming with porters algorithm: ['call', 'number', 'avail', 'appoint', 'ask', 'connect', 'call', 'wahe', 'fathima']\n",
      "Tokenized sentence: ['want', 'a', 'new', 'video', 'phone', 'anytime', 'any', 'network', 'mins', 'half', 'price', 'line', 'rental', 'free', 'text', 'for', 'months', 'reply', 'or', 'call', 'for', 'free', 'delivery']\n",
      "After stop words removal: ['want', 'new', 'video', 'phone', 'anytime', 'network', 'mins', 'half', 'price', 'line', 'rental', 'free', 'text', 'months', 'reply', 'call', 'free', 'delivery']\n",
      "After stemming with porters algorithm: ['want', 'new', 'video', 'phone', 'anytim', 'network', 'min', 'half', 'price', 'line', 'rental', 'free', 'text', 'month', 'repli', 'call', 'free', 'deliveri']\n",
      "Tokenized sentence: ['had', 'the', 'money', 'issue', 'weigh', 'me', 'down', 'but', 'thanks', 'to', 'you', 'i', 'can', 'breathe', 'easier', 'now', 'i', 'll', 'make', 'sure', 'you', 'dont', 'regret', 'it', 'thanks']\n",
      "After stop words removal: ['money', 'issue', 'weigh', 'thanks', 'breathe', 'easier', 'make', 'sure', 'dont', 'regret', 'thanks']\n",
      "After stemming with porters algorithm: ['monei', 'issu', 'weigh', 'thank', 'breath', 'easier', 'make', 'sure', 'dont', 'regret', 'thank']\n",
      "Tokenized sentence: ['of', 'cos', 'can', 'lar', 'i', 'm', 'not', 'so', 'ba', 'dao', 'ok', 'pm', 'lor', 'y', 'u', 'never', 'ask', 'where', 'we', 'go', 'ah', 'i', 'said', 'u', 'would', 'ask', 'on', 'fri', 'but', 'he', 'said', 'u', 'will', 'ask', 'today']\n",
      "After stop words removal: ['cos', 'lar', 'ba', 'dao', 'ok', 'pm', 'lor', 'u', 'never', 'ask', 'go', 'ah', 'said', 'u', 'would', 'ask', 'fri', 'said', 'u', 'ask', 'today']\n",
      "After stemming with porters algorithm: ['co', 'lar', 'dao', 'lor', 'never', 'ask', 'said', 'would', 'ask', 'fri', 'said', 'ask', 'todai']\n",
      "Tokenized sentence: ['tmr', 'timin', 'still', 'da', 'same', 'wat', 'cos', 'i', 'got', 'lesson', 'until']\n",
      "After stop words removal: ['tmr', 'timin', 'still', 'da', 'wat', 'cos', 'got', 'lesson']\n",
      "After stemming with porters algorithm: ['tmr', 'timin', 'still', 'wat', 'co', 'got', 'lesson']\n",
      "Tokenized sentence: ['i', 'm', 'not', 'sure', 'if', 'its', 'still', 'available', 'though']\n",
      "After stop words removal: ['sure', 'still', 'available', 'though']\n",
      "After stemming with porters algorithm: ['sure', 'still', 'avail', 'though']\n",
      "Tokenized sentence: ['i', 'know', 'you', 'are', 'can', 'you', 'pls', 'open', 'the', 'back']\n",
      "After stop words removal: ['know', 'pls', 'open', 'back']\n",
      "After stemming with porters algorithm: ['know', 'pl', 'open', 'back']\n",
      "Tokenized sentence: ['i', 'know', 'dat', 'feelin', 'had', 'it', 'with', 'pete', 'wuld', 'get', 'with', 'em', 'nuther', 'place', 'nuther', 'time', 'mayb']\n",
      "After stop words removal: ['know', 'dat', 'feelin', 'pete', 'wuld', 'get', 'em', 'nuther', 'place', 'nuther', 'time', 'mayb']\n",
      "After stemming with porters algorithm: ['know', 'dat', 'feelin', 'pete', 'wuld', 'get', 'nuther', 'place', 'nuther', 'time', 'mayb']\n",
      "Tokenized sentence: ['aight', 'tomorrow', 'around', 'lt', 'gt', 'it', 'is']\n",
      "After stop words removal: ['aight', 'tomorrow', 'around', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['aight', 'tomorrow', 'around']\n",
      "Tokenized sentence: ['the', 'gas', 'station', 'is', 'like', 'a', 'block', 'away', 'from', 'my', 'house', 'you', 'll', 'drive', 'right', 'by', 'it', 'since', 'armenia', 'ends', 'at', 'swann', 'and', 'you', 'have', 'to', 'take', 'howard']\n",
      "After stop words removal: ['gas', 'station', 'like', 'block', 'away', 'house', 'drive', 'right', 'since', 'armenia', 'ends', 'swann', 'take', 'howard']\n",
      "After stemming with porters algorithm: ['ga', 'stat', 'like', 'block', 'awai', 'hous', 'drive', 'right', 'sinc', 'armenia', 'end', 'swann', 'take', 'howard']\n",
      "Tokenized sentence: ['yes', 'now', 'only', 'saw', 'your', 'message']\n",
      "After stop words removal: ['yes', 'saw', 'message']\n",
      "After stemming with porters algorithm: ['ye', 'saw', 'messag']\n",
      "Tokenized sentence: ['hi', 'do', 'u', 'want', 'to', 'join', 'me', 'with', 'sts', 'later', 'meeting', 'them', 'at', 'five', 'call', 'u', 'after', 'class']\n",
      "After stop words removal: ['hi', 'u', 'want', 'join', 'sts', 'later', 'meeting', 'five', 'call', 'u', 'class']\n",
      "meet\n",
      "After stemming with porters algorithm: ['want', 'join', 'st', 'later', 'meet', 'five', 'call', 'class']\n",
      "Tokenized sentence: ['getting', 'tickets', 'walsall', 'tue', 'th', 'march', 'my', 'mate', 'is', 'getting', 'me', 'them', 'on', 'sat', 'ill', 'pay', 'my', 'treat', 'want', 'go', 'txt', 'bak', 'terry']\n",
      "After stop words removal: ['getting', 'tickets', 'walsall', 'tue', 'th', 'march', 'mate', 'getting', 'sat', 'ill', 'pay', 'treat', 'want', 'go', 'txt', 'bak', 'terry']\n",
      "gett\n",
      "gett\n",
      "After stemming with porters algorithm: ['get', 'ticket', 'walsal', 'tue', 'march', 'mate', 'get', 'sat', 'ill', 'pai', 'treat', 'want', 'txt', 'bak', 'terri']\n",
      "Tokenized sentence: ['orange', 'brings', 'you', 'ringtones', 'from', 'all', 'time', 'chart', 'heroes', 'with', 'a', 'free', 'hit', 'each', 'week', 'go', 'to', 'ringtones', 'pics', 'on', 'wap', 'to', 'stop', 'receiving', 'these', 'tips', 'reply', 'stop']\n",
      "After stop words removal: ['orange', 'brings', 'ringtones', 'time', 'chart', 'heroes', 'free', 'hit', 'week', 'go', 'ringtones', 'pics', 'wap', 'stop', 'receiving', 'tips', 'reply', 'stop']\n",
      "receiv\n",
      "After stemming with porters algorithm: ['orang', 'bring', 'rington', 'time', 'chart', 'hero', 'free', 'hit', 'week', 'rington', 'pic', 'wap', 'stop', 'receiv', 'tip', 'repli', 'stop']\n",
      "Tokenized sentence: ['mobile', 'club', 'choose', 'any', 'of', 'the', 'top', 'quality', 'items', 'for', 'your', 'mobile', 'cfca', 'a']\n",
      "After stop words removal: ['mobile', 'club', 'choose', 'top', 'quality', 'items', 'mobile', 'cfca']\n",
      "After stemming with porters algorithm: ['mobil', 'club', 'choos', 'top', 'qualiti', 'item', 'mobil', 'cfca']\n",
      "Tokenized sentence: ['i', 'm', 'done', 'c', 'there']\n",
      "After stop words removal: ['done', 'c']\n",
      "After stemming with porters algorithm: ['done']\n",
      "Tokenized sentence: ['no', 'drama', 'pls', 'i', 'have', 'had', 'enough', 'from', 'you', 'and', 'family', 'while', 'i', 'am', 'struggling', 'in', 'the', 'hot', 'sun', 'in', 'a', 'strange', 'place', 'no', 'reason', 'why', 'there', 'should', 'be', 'an', 'ego', 'of', 'not', 'going', 'if', 'not', 'invited', 'when', 'actually', 'its', 'necessity', 'to', 'go', 'wait', 'for', 'very', 'serious', 'reppurcussions']\n",
      "After stop words removal: ['drama', 'pls', 'enough', 'family', 'struggling', 'hot', 'sun', 'strange', 'place', 'reason', 'ego', 'going', 'invited', 'actually', 'necessity', 'go', 'wait', 'serious', 'reppurcussions']\n",
      "struggl\n",
      "go\n",
      "After stemming with porters algorithm: ['drama', 'pl', 'enough', 'famili', 'struggl', 'hot', 'sun', 'strang', 'place', 'reason', 'ego', 'go', 'invit', 'actual', 'necess', 'wait', 'seriou', 'reppurcuss']\n",
      "Tokenized sentence: ['what', 's', 'your', 'room', 'number', 'again', 'wanna', 'make', 'sure', 'i', 'm', 'knocking', 'on', 'the', 'right', 'door']\n",
      "After stop words removal: ['room', 'number', 'wanna', 'make', 'sure', 'knocking', 'right', 'door']\n",
      "knock\n",
      "After stemming with porters algorithm: ['room', 'number', 'wanna', 'make', 'sure', 'knoc', 'right', 'door']\n",
      "Tokenized sentence: ['sounds', 'better', 'than', 'my', 'evening', 'im', 'just', 'doing', 'my', 'costume', 'im', 'not', 'sure', 'what', 'time', 'i', 'finish', 'tomorrow', 'but', 'i', 'will', 'txt', 'you', 'at', 'the', 'end']\n",
      "After stop words removal: ['sounds', 'better', 'evening', 'im', 'costume', 'im', 'sure', 'time', 'finish', 'tomorrow', 'txt', 'end']\n",
      "even\n",
      "After stemming with porters algorithm: ['sound', 'better', 'even', 'costum', 'sure', 'time', 'finish', 'tomorrow', 'txt', 'end']\n",
      "Tokenized sentence: ['you', 'bad', 'girl', 'i', 'can', 'still', 'remember', 'them']\n",
      "After stop words removal: ['bad', 'girl', 'still', 'remember']\n",
      "After stemming with porters algorithm: ['bad', 'girl', 'still', 'rememb']\n",
      "Tokenized sentence: ['hm', 'good', 'morning', 'headache', 'anyone']\n",
      "After stop words removal: ['hm', 'good', 'morning', 'headache', 'anyone']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'headach', 'anyon']\n",
      "Tokenized sentence: ['sorry', 'sent', 'blank', 'msg', 'again', 'yup', 'but', 'trying', 'do', 'some', 'serious', 'studying', 'now']\n",
      "After stop words removal: ['sorry', 'sent', 'blank', 'msg', 'yup', 'trying', 'serious', 'studying']\n",
      "study\n",
      "After stemming with porters algorithm: ['sorri', 'sent', 'blank', 'msg', 'yup', 'trying', 'seriou', 'stud']\n",
      "Tokenized sentence: ['tell', 'me', 'whos', 'this', 'pls']\n",
      "After stop words removal: ['tell', 'whos', 'pls']\n",
      "After stemming with porters algorithm: ['tell', 'who', 'pl']\n",
      "Tokenized sentence: ['god', 'picked', 'up', 'a', 'flower', 'and', 'dippeditinadew', 'lovingly', 'touched', 'itwhichturnedinto', 'u', 'and', 'the', 'he', 'gifted', 'tomeandsaid', 'this', 'friend', 'is', 'u']\n",
      "After stop words removal: ['god', 'picked', 'flower', 'dippeditinadew', 'lovingly', 'touched', 'itwhichturnedinto', 'u', 'gifted', 'tomeandsaid', 'friend', 'u']\n",
      "After stemming with porters algorithm: ['god', 'pic', 'flower', 'dippeditinadew', 'lovingli', 'touc', 'itwhichturnedinto', 'gif', 'tomeandsaid', 'friend']\n",
      "Tokenized sentence: ['are', 'you', 'willing', 'to', 'go', 'for', 'aptitude', 'class']\n",
      "After stop words removal: ['willing', 'go', 'aptitude', 'class']\n",
      "will\n",
      "After stemming with porters algorithm: ['will', 'aptitud', 'class']\n",
      "Tokenized sentence: ['funny', 'fact', 'nobody', 'teaches', 'volcanoes', 'erupt', 'tsunamis', 'arise', 'hurricanes', 'sway', 'aroundn', 'no', 'teaches', 'hw', 'choose', 'a', 'wife', 'natural', 'disasters', 'just', 'happens']\n",
      "After stop words removal: ['funny', 'fact', 'nobody', 'teaches', 'volcanoes', 'erupt', 'tsunamis', 'arise', 'hurricanes', 'sway', 'aroundn', 'teaches', 'hw', 'choose', 'wife', 'natural', 'disasters', 'happens']\n",
      "After stemming with porters algorithm: ['funni', 'fact', 'nobodi', 'teach', 'volcano', 'erupt', 'tsunami', 'aris', 'hurrican', 'swai', 'aroundn', 'teach', 'choos', 'wife', 'natur', 'disast', 'happen']\n",
      "Tokenized sentence: ['his', 'bday', 'real', 'is', 'in', 'april']\n",
      "After stop words removal: ['bday', 'real', 'april']\n",
      "After stemming with porters algorithm: ['bdai', 'real', 'april']\n",
      "Tokenized sentence: ['he', 'is', 'impossible', 'to', 'argue', 'with', 'and', 'he', 'always', 'treats', 'me', 'like', 'his', 'sub', 'like', 'he', 'never', 'released', 'me', 'which', 'he', 'did', 'and', 'i', 'will', 'remind', 'him', 'of', 'that', 'if', 'necessary']\n",
      "After stop words removal: ['impossible', 'argue', 'always', 'treats', 'like', 'sub', 'like', 'never', 'released', 'remind', 'necessary']\n",
      "After stemming with porters algorithm: ['imposs', 'argu', 'alwai', 'treat', 'like', 'sub', 'like', 'never', 'releas', 'remind', 'necessari']\n",
      "Tokenized sentence: ['gud', 'mrng', 'dear', 'hav', 'a', 'nice', 'day']\n",
      "After stop words removal: ['gud', 'mrng', 'dear', 'hav', 'nice', 'day']\n",
      "After stemming with porters algorithm: ['gud', 'mrng', 'dear', 'hav', 'nice', 'dai']\n",
      "Tokenized sentence: ['haha', 'sounds', 'crazy', 'dunno', 'can', 'tahan', 'anot']\n",
      "After stop words removal: ['haha', 'sounds', 'crazy', 'dunno', 'tahan', 'anot']\n",
      "After stemming with porters algorithm: ['haha', 'sound', 'crazi', 'dunno', 'tahan', 'anot']\n",
      "Tokenized sentence: ['i', 'am', 'late', 'i', 'will', 'be', 'there', 'at']\n",
      "After stop words removal: ['late']\n",
      "After stemming with porters algorithm: ['late']\n",
      "Tokenized sentence: ['gud', 'ni', 'dear', 'slp', 'well', 'take', 'care', 'swt', 'dreams', 'muah']\n",
      "After stop words removal: ['gud', 'ni', 'dear', 'slp', 'well', 'take', 'care', 'swt', 'dreams', 'muah']\n",
      "After stemming with porters algorithm: ['gud', 'dear', 'slp', 'well', 'take', 'care', 'swt', 'dream', 'muah']\n",
      "Tokenized sentence: ['what', 'i', 'm', 'saying', 'is', 'if', 'you', 'haven', 't', 'explicitly', 'told', 'nora', 'i', 'know', 'someone', 'i', 'm', 'probably', 'just', 'not', 'gonna', 'bother']\n",
      "After stop words removal: ['saying', 'explicitly', 'told', 'nora', 'know', 'someone', 'probably', 'gonna', 'bother']\n",
      "say\n",
      "After stemming with porters algorithm: ['sai', 'explicitli', 'told', 'nora', 'know', 'someon', 'probab', 'gonna', 'bother']\n",
      "Tokenized sentence: ['hmm', 'too', 'many', 'of', 'them', 'unfortunately', 'pics', 'obviously', 'arent', 'hot', 'cakes', 'its', 'kinda', 'fun', 'tho']\n",
      "After stop words removal: ['hmm', 'many', 'unfortunately', 'pics', 'obviously', 'arent', 'hot', 'cakes', 'kinda', 'fun', 'tho']\n",
      "After stemming with porters algorithm: ['hmm', 'mani', 'unfortun', 'pic', 'obvious', 'arent', 'hot', 'cake', 'kinda', 'fun', 'tho']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'reveal', 'who', 'thinks', 'u', 'r', 'so', 'special', 'call', 'to', 'opt', 'out', 'reply', 'reveal', 'stop', 'per', 'msg', 'recd', 'cust', 'care']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'reveal', 'thinks', 'u', 'r', 'special', 'call', 'opt', 'reply', 'reveal', 'stop', 'per', 'msg', 'recd', 'cust', 'care']\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'reveal', 'think', 'special', 'call', 'opt', 'repli', 'reveal', 'stop', 'per', 'msg', 'recd', 'cust', 'care']\n",
      "Tokenized sentence: ['about', 'lt', 'gt', 'bucks', 'the', 'banks', 'fees', 'are', 'fixed', 'better', 'to', 'call', 'the', 'bank', 'and', 'find', 'out']\n",
      "After stop words removal: ['lt', 'gt', 'bucks', 'banks', 'fees', 'fixed', 'better', 'call', 'bank', 'find']\n",
      "After stemming with porters algorithm: ['buck', 'bank', 'fee', 'fix', 'better', 'call', 'bank', 'find']\n",
      "Tokenized sentence: ['win', 'the', 'newest', 'harry', 'potter', 'and', 'the', 'order', 'of', 'the', 'phoenix', 'book', 'reply', 'harry', 'answer', 'questions', 'chance', 'to', 'be', 'the', 'first', 'among', 'readers']\n",
      "After stop words removal: ['win', 'newest', 'harry', 'potter', 'order', 'phoenix', 'book', 'reply', 'harry', 'answer', 'questions', 'chance', 'first', 'among', 'readers']\n",
      "After stemming with porters algorithm: ['win', 'newest', 'harri', 'potter', 'order', 'phoenix', 'book', 'repli', 'harri', 'answer', 'quest', 'chanc', 'first', 'among', 'reader']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later', 'ok', 'bye']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'ok', 'bye']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'bye']\n",
      "Tokenized sentence: ['mind', 'blastin', 'no', 'more', 'tsunamis', 'will', 'occur', 'from', 'now', 'on', 'rajnikant', 'stopped', 'swimming', 'in', 'indian', 'ocean', 'd']\n",
      "After stop words removal: ['mind', 'blastin', 'tsunamis', 'occur', 'rajnikant', 'stopped', 'swimming', 'indian', 'ocean']\n",
      "swimm\n",
      "After stemming with porters algorithm: ['mind', 'blastin', 'tsunami', 'occur', 'rajnik', 'stop', 'swim', 'indian', 'ocean']\n",
      "Tokenized sentence: ['ok', 'lor', 'then', 'we', 'go', 'tog', 'lor']\n",
      "After stop words removal: ['ok', 'lor', 'go', 'tog', 'lor']\n",
      "After stemming with porters algorithm: ['lor', 'tog', 'lor']\n",
      "Tokenized sentence: ['juz', 'now', 'havent', 'woke', 'up', 'so', 'a', 'bit', 'blur', 'blur', 'can', 'dad', 'went', 'out', 'liao', 'i', 'cant', 'cum', 'now', 'oso']\n",
      "After stop words removal: ['juz', 'havent', 'woke', 'bit', 'blur', 'blur', 'dad', 'went', 'liao', 'cant', 'cum', 'oso']\n",
      "After stemming with porters algorithm: ['juz', 'havent', 'woke', 'bit', 'blur', 'blur', 'dad', 'went', 'liao', 'cant', 'cum', 'oso']\n",
      "Tokenized sentence: ['no', 'da', 'vijay', 'going', 'to', 'talk', 'in', 'jaya', 'tv']\n",
      "After stop words removal: ['da', 'vijay', 'going', 'talk', 'jaya', 'tv']\n",
      "go\n",
      "After stemming with porters algorithm: ['vijai', 'go', 'talk', 'jaya']\n",
      "Tokenized sentence: ['your', 'free', 'ringtone', 'is', 'waiting', 'to', 'be', 'collected', 'simply', 'text', 'the', 'password', 'mix', 'to', 'to', 'verify', 'get', 'usher', 'and', 'britney', 'fml']\n",
      "After stop words removal: ['free', 'ringtone', 'waiting', 'collected', 'simply', 'text', 'password', 'mix', 'verify', 'get', 'usher', 'britney', 'fml']\n",
      "wait\n",
      "After stemming with porters algorithm: ['free', 'rington', 'wait', 'collec', 'simpli', 'text', 'password', 'mix', 'verifi', 'get', 'usher', 'britnei', 'fml']\n",
      "Tokenized sentence: ['sorry', 'light', 'turned', 'green', 'i', 'meant', 'another', 'friend', 'wanted', 'lt', 'gt', 'worth', 'but', 'he', 'may', 'not', 'be', 'around']\n",
      "After stop words removal: ['sorry', 'light', 'turned', 'green', 'meant', 'another', 'friend', 'wanted', 'lt', 'gt', 'worth', 'may', 'around']\n",
      "After stemming with porters algorithm: ['sorri', 'light', 'tur', 'green', 'meant', 'anoth', 'friend', 'wan', 'worth', 'mai', 'around']\n",
      "Tokenized sentence: ['yes', 'i', 'trust', 'u', 'to', 'buy', 'new', 'stuff', 'asap', 'so', 'i', 'can', 'try', 'it', 'out']\n",
      "After stop words removal: ['yes', 'trust', 'u', 'buy', 'new', 'stuff', 'asap', 'try']\n",
      "After stemming with porters algorithm: ['ye', 'trust', 'bui', 'new', 'stuff', 'asap', 'try']\n",
      "Tokenized sentence: ['got', 'fujitsu', 'ibm', 'hp', 'toshiba', 'got', 'a', 'lot', 'of', 'model', 'how', 'to', 'say']\n",
      "After stop words removal: ['got', 'fujitsu', 'ibm', 'hp', 'toshiba', 'got', 'lot', 'model', 'say']\n",
      "After stemming with porters algorithm: ['got', 'fujitsu', 'ibm', 'toshiba', 'got', 'lot', 'model', 'sai']\n",
      "Tokenized sentence: ['k', 'wat', 's', 'tht', 'incident']\n",
      "After stop words removal: ['k', 'wat', 'tht', 'incident']\n",
      "After stemming with porters algorithm: ['wat', 'tht', 'incid']\n",
      "Tokenized sentence: ['roger', 'that', 'we', 're', 'probably', 'going', 'to', 'rem', 'in', 'about']\n",
      "After stop words removal: ['roger', 'probably', 'going', 'rem']\n",
      "go\n",
      "After stemming with porters algorithm: ['roger', 'probab', 'go', 'rem']\n",
      "Tokenized sentence: ['yeah', 'i', 'got', 'a', 'list', 'with', 'only', 'u', 'and', 'joanna', 'if', 'i', 'm', 'feeling', 'really', 'anti', 'social']\n",
      "After stop words removal: ['yeah', 'got', 'list', 'u', 'joanna', 'feeling', 'really', 'anti', 'social']\n",
      "feel\n",
      "After stemming with porters algorithm: ['yeah', 'got', 'list', 'joanna', 'feel', 'realli', 'anti', 'social']\n",
      "Tokenized sentence: ['if', 'you', 'can', 'make', 'it', 'any', 'time', 'tonight', 'or', 'whenever', 'you', 'can', 'it', 's', 'cool', 'just', 'text', 'me', 'whenever', 'you', 're', 'around']\n",
      "After stop words removal: ['make', 'time', 'tonight', 'whenever', 'cool', 'text', 'whenever', 'around']\n",
      "After stemming with porters algorithm: ['make', 'time', 'tonight', 'whenev', 'cool', 'text', 'whenev', 'around']\n",
      "Tokenized sentence: ['i', 'think', 'chennai', 'well', 'settled']\n",
      "After stop words removal: ['think', 'chennai', 'well', 'settled']\n",
      "After stemming with porters algorithm: ['think', 'chennai', 'well', 'settl']\n",
      "Tokenized sentence: ['your', 'brother', 'is', 'a', 'genius']\n",
      "After stop words removal: ['brother', 'genius']\n",
      "After stemming with porters algorithm: ['brother', 'geniu']\n",
      "Tokenized sentence: ['whore', 'you', 'are', 'unbelievable']\n",
      "After stop words removal: ['whore', 'unbelievable']\n",
      "After stemming with porters algorithm: ['whore', 'unbeliev']\n",
      "Tokenized sentence: ['that', 'sucks', 'so', 'what', 'do', 'you', 'got', 'planned', 'for', 'your', 'yo', 'valentine', 'i', 'am', 'your', 'yo', 'valentine', 'aren', 't', 'i']\n",
      "After stop words removal: ['sucks', 'got', 'planned', 'yo', 'valentine', 'yo', 'valentine']\n",
      "After stemming with porters algorithm: ['suck', 'got', 'plan', 'valentin', 'valentin']\n",
      "Tokenized sentence: ['watching', 'ajith', 'film', 'ah']\n",
      "After stop words removal: ['watching', 'ajith', 'film', 'ah']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'ajith', 'film']\n",
      "Tokenized sentence: ['and', 'popping', 'lt', 'gt', 'ibuprofens', 'was', 'no', 'help']\n",
      "After stop words removal: ['popping', 'lt', 'gt', 'ibuprofens', 'help']\n",
      "popp\n",
      "After stemming with porters algorithm: ['pop', 'ibuprofen', 'help']\n",
      "Tokenized sentence: ['serious', 'what', 'like', 'proper', 'tongued', 'her']\n",
      "After stop words removal: ['serious', 'like', 'proper', 'tongued']\n",
      "After stemming with porters algorithm: ['seriou', 'like', 'proper', 'tongu']\n",
      "Tokenized sentence: ['k', 'will', 'do', 'addie', 'amp', 'i', 'are', 'doing', 'some', 'art', 'so', 'i', 'll', 'be', 'here', 'when', 'you', 'get', 'home']\n",
      "After stop words removal: ['k', 'addie', 'amp', 'art', 'get', 'home']\n",
      "After stemming with porters algorithm: ['addi', 'amp', 'art', 'get', 'home']\n",
      "Tokenized sentence: ['y', 'she', 'dun', 'believe', 'leh', 'i', 'tot', 'i', 'told', 'her', 'it', 's', 'true', 'already', 'i', 'thk', 'she', 'muz', 'c', 'us', 'tog', 'then', 'she', 'believe']\n",
      "After stop words removal: ['dun', 'believe', 'leh', 'tot', 'told', 'true', 'already', 'thk', 'muz', 'c', 'us', 'tog', 'believe']\n",
      "After stemming with porters algorithm: ['dun', 'believ', 'leh', 'tot', 'told', 'true', 'alreadi', 'thk', 'muz', 'tog', 'believ']\n",
      "Tokenized sentence: ['don', 't', 'worry', 'though', 'i', 'understand', 'how', 'important', 'it', 'is', 'that', 'i', 'be', 'put', 'in', 'my', 'place', 'with', 'a', 'poorly', 'thought', 'out', 'punishment', 'in', 'the', 'face', 'of', 'the', 'worst', 'thing', 'that', 'has', 'ever', 'happened', 'to', 'me', 'brb', 'gonna', 'go', 'kill', 'myself']\n",
      "After stop words removal: ['worry', 'though', 'understand', 'important', 'put', 'place', 'poorly', 'thought', 'punishment', 'face', 'worst', 'thing', 'ever', 'happened', 'brb', 'gonna', 'go', 'kill']\n",
      "After stemming with porters algorithm: ['worri', 'though', 'understand', 'import', 'put', 'place', 'poorli', 'thought', 'punish', 'face', 'worst', 'thing', 'ever', 'happen', 'brb', 'gonna', 'kill']\n",
      "Tokenized sentence: ['ok', 'not', 'sure', 'what', 'time', 'tho', 'as', 'not', 'sure', 'if', 'can', 'get', 'to', 'library', 'before', 'class', 'will', 'try', 'see', 'you', 'at', 'some', 'point', 'have', 'good', 'eve']\n",
      "After stop words removal: ['ok', 'sure', 'time', 'tho', 'sure', 'get', 'library', 'class', 'try', 'see', 'point', 'good', 'eve']\n",
      "After stemming with porters algorithm: ['sure', 'time', 'tho', 'sure', 'get', 'librari', 'class', 'try', 'see', 'point', 'good', 'ev']\n",
      "Tokenized sentence: ['then', 'wat', 'r', 'u', 'doing', 'now', 'busy', 'wif', 'work']\n",
      "After stop words removal: ['wat', 'r', 'u', 'busy', 'wif', 'work']\n",
      "After stemming with porters algorithm: ['wat', 'busi', 'wif', 'work']\n",
      "Tokenized sentence: ['onum', 'ela', 'pa', 'normal', 'than']\n",
      "After stop words removal: ['onum', 'ela', 'pa', 'normal']\n",
      "After stemming with porters algorithm: ['onum', 'ela', 'normal']\n",
      "Tokenized sentence: ['lol', 'oops', 'sorry', 'have', 'fun']\n",
      "After stop words removal: ['lol', 'oops', 'sorry', 'fun']\n",
      "After stemming with porters algorithm: ['lol', 'oop', 'sorri', 'fun']\n",
      "Tokenized sentence: ['nope', 'thats', 'fine', 'i', 'might', 'have', 'a', 'nap', 'tho']\n",
      "After stop words removal: ['nope', 'thats', 'fine', 'might', 'nap', 'tho']\n",
      "After stemming with porters algorithm: ['nope', 'that', 'fine', 'might', 'nap', 'tho']\n",
      "Tokenized sentence: ['at', 'let', 's', 'go', 'to', 'bill', 'millers']\n",
      "After stop words removal: ['let', 'go', 'bill', 'millers']\n",
      "After stemming with porters algorithm: ['let', 'bill', 'miller']\n",
      "Tokenized sentence: ['call', 'me', 'when', 'you', 'get', 'the', 'chance', 'plz', 'lt']\n",
      "After stop words removal: ['call', 'get', 'chance', 'plz', 'lt']\n",
      "After stemming with porters algorithm: ['call', 'get', 'chanc', 'plz']\n",
      "Tokenized sentence: ['i', 'asked', 'sen', 'to', 'come', 'chennai', 'and', 'search', 'for', 'job']\n",
      "After stop words removal: ['asked', 'sen', 'come', 'chennai', 'search', 'job']\n",
      "After stemming with porters algorithm: ['as', 'sen', 'come', 'chennai', 'search', 'job']\n",
      "Tokenized sentence: ['thx', 'all', 'will', 'be', 'well', 'in', 'a', 'few', 'months']\n",
      "After stop words removal: ['thx', 'well', 'months']\n",
      "After stemming with porters algorithm: ['thx', 'well', 'month']\n",
      "Tokenized sentence: ['sir', 'you', 'will', 'receive', 'the', 'account', 'no', 'another', 'hr', 'time', 'sorry', 'for', 'the', 'delay']\n",
      "After stop words removal: ['sir', 'receive', 'account', 'another', 'hr', 'time', 'sorry', 'delay']\n",
      "After stemming with porters algorithm: ['sir', 'receiv', 'account', 'anoth', 'time', 'sorri', 'delai']\n",
      "Tokenized sentence: ['no', 'no', 'i', 'will', 'check', 'all', 'rooms', 'befor', 'activities']\n",
      "After stop words removal: ['check', 'rooms', 'befor', 'activities']\n",
      "After stemming with porters algorithm: ['check', 'room', 'befor', 'activ']\n",
      "Tokenized sentence: ['get', 'ur', 'st', 'ringtone', 'free', 'now', 'reply', 'to', 'this', 'msg', 'with', 'tone', 'gr', 'top', 'tones', 'to', 'your', 'phone', 'every', 'week', 'just', 'per', 'wk', 'opt', 'out', 'send', 'stop']\n",
      "After stop words removal: ['get', 'ur', 'st', 'ringtone', 'free', 'reply', 'msg', 'tone', 'gr', 'top', 'tones', 'phone', 'every', 'week', 'per', 'wk', 'opt', 'send', 'stop']\n",
      "After stemming with porters algorithm: ['get', 'rington', 'free', 'repli', 'msg', 'tone', 'top', 'tone', 'phone', 'everi', 'week', 'per', 'opt', 'send', 'stop']\n",
      "Tokenized sentence: ['nothing', 'spl', 'wat', 'abt', 'u', 'and', 'whr', 'ru']\n",
      "After stop words removal: ['nothing', 'spl', 'wat', 'abt', 'u', 'whr', 'ru']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not', 'spl', 'wat', 'abt', 'whr']\n",
      "Tokenized sentence: ['that', 's', 'alrite', 'girl', 'u', 'know', 'gail', 'is', 'neva', 'wrong', 'take', 'care', 'sweet', 'and', 'don', 't', 'worry', 'c', 'u', 'l', 'tr', 'hun', 'love', 'yaxxx']\n",
      "After stop words removal: ['alrite', 'girl', 'u', 'know', 'gail', 'neva', 'wrong', 'take', 'care', 'sweet', 'worry', 'c', 'u', 'l', 'tr', 'hun', 'love', 'yaxxx']\n",
      "After stemming with porters algorithm: ['alrit', 'girl', 'know', 'gail', 'neva', 'wrong', 'take', 'care', 'sweet', 'worri', 'hun', 'love', 'yaxxx']\n",
      "Tokenized sentence: ['warner', 'village', 'c', 'colin', 'farrell', 'in', 'swat', 'this', 'wkend', 'warner', 'village', 'get', 'free', 'med', 'popcorn', 'just', 'show', 'msg', 'ticket', 'kiosk', 'valid', 'c', 't', 'c', 'kiosk', 'reply', 'sony', 'mre', 'film', 'offers']\n",
      "After stop words removal: ['warner', 'village', 'c', 'colin', 'farrell', 'swat', 'wkend', 'warner', 'village', 'get', 'free', 'med', 'popcorn', 'show', 'msg', 'ticket', 'kiosk', 'valid', 'c', 'c', 'kiosk', 'reply', 'sony', 'mre', 'film', 'offers']\n",
      "After stemming with porters algorithm: ['warner', 'villag', 'colin', 'farrel', 'swat', 'wkend', 'warner', 'villag', 'get', 'free', 'med', 'popcorn', 'show', 'msg', 'ticket', 'kiosk', 'valid', 'kiosk', 'repli', 'soni', 'mre', 'film', 'offer']\n",
      "Tokenized sentence: ['ball', 'is', 'moving', 'a', 'lot', 'will', 'spin', 'in', 'last', 'so', 'very', 'difficult', 'to', 'bat']\n",
      "After stop words removal: ['ball', 'moving', 'lot', 'spin', 'last', 'difficult', 'bat']\n",
      "mov\n",
      "After stemming with porters algorithm: ['ball', 'move', 'lot', 'spin', 'last', 'difficult', 'bat']\n",
      "Tokenized sentence: ['we', 'regret', 'to', 'inform', 'u', 'that', 'the', 'nhs', 'has', 'made', 'a', 'mistake', 'u', 'were', 'never', 'actually', 'born', 'please', 'report', 'yor', 'local', 'hospital', 'b', 'terminated', 'we', 'r', 'sorry', 'the', 'inconvenience']\n",
      "After stop words removal: ['regret', 'inform', 'u', 'nhs', 'made', 'mistake', 'u', 'never', 'actually', 'born', 'please', 'report', 'yor', 'local', 'hospital', 'b', 'terminated', 'r', 'sorry', 'inconvenience']\n",
      "terminate\n",
      "After stemming with porters algorithm: ['regret', 'inform', 'nh', 'made', 'mistak', 'never', 'actual', 'born', 'pleas', 'report', 'yor', 'local', 'hospit', 'termin', 'sorri', 'inconveni']\n",
      "Tokenized sentence: ['lol', 'what', 'happens', 'in', 'vegas', 'stays', 'in', 'vegas']\n",
      "After stop words removal: ['lol', 'happens', 'vegas', 'stays', 'vegas']\n",
      "After stemming with porters algorithm: ['lol', 'happen', 'vega', 'stai', 'vega']\n",
      "Tokenized sentence: ['call', 'me', 'da', 'i', 'am', 'waiting', 'for', 'your', 'call']\n",
      "After stop words removal: ['call', 'da', 'waiting', 'call']\n",
      "wait\n",
      "After stemming with porters algorithm: ['call', 'wait', 'call']\n",
      "Tokenized sentence: ['wen', 'did', 'you', 'get', 'so', 'spiritual', 'and', 'deep', 'that', 's', 'great']\n",
      "After stop words removal: ['wen', 'get', 'spiritual', 'deep', 'great']\n",
      "After stemming with porters algorithm: ['wen', 'get', 'spiritu', 'deep', 'great']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['you', 'are', 'now', 'unsubscribed', 'all', 'services', 'get', 'tons', 'of', 'sexy', 'babes', 'or', 'hunks', 'straight', 'to', 'your', 'phone', 'go', 'to', 'http', 'gotbabes', 'co', 'uk', 'no', 'subscriptions']\n",
      "After stop words removal: ['unsubscribed', 'services', 'get', 'tons', 'sexy', 'babes', 'hunks', 'straight', 'phone', 'go', 'http', 'gotbabes', 'co', 'uk', 'subscriptions']\n",
      "After stemming with porters algorithm: ['unsubscrib', 'servic', 'get', 'ton', 'sexi', 'babe', 'hunk', 'straight', 'phone', 'http', 'gotbab', 'subscript']\n",
      "Tokenized sentence: ['it', 's', 'wylie', 'you', 'in', 'tampa', 'or', 'sarasota']\n",
      "After stop words removal: ['wylie', 'tampa', 'sarasota']\n",
      "After stemming with porters algorithm: ['wylie', 'tampa', 'sarasota']\n",
      "Tokenized sentence: ['i', 'am', 'at', 'a', 'party', 'with', 'alex', 'nichols']\n",
      "After stop words removal: ['party', 'alex', 'nichols']\n",
      "After stemming with porters algorithm: ['parti', 'alex', 'nichol']\n",
      "Tokenized sentence: ['sorry', 'chikku', 'my', 'cell', 'got', 'some', 'problem', 'thts', 'y', 'i', 'was', 'nt', 'able', 'to', 'reply', 'u', 'or', 'msg', 'u']\n",
      "After stop words removal: ['sorry', 'chikku', 'cell', 'got', 'problem', 'thts', 'nt', 'able', 'reply', 'u', 'msg', 'u']\n",
      "After stemming with porters algorithm: ['sorri', 'chikku', 'cell', 'got', 'problem', 'tht', 'abl', 'repli', 'msg']\n",
      "Tokenized sentence: ['happy', 'new', 'years', 'melody']\n",
      "After stop words removal: ['happy', 'new', 'years', 'melody']\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year', 'melodi']\n",
      "Tokenized sentence: ['yo', 'do', 'you', 'know', 'anyone', 'lt', 'gt', 'or', 'otherwise', 'able', 'to', 'buy', 'liquor', 'our', 'guy', 'flaked', 'and', 'right', 'now', 'if', 'we', 'don', 't', 'get', 'a', 'hold', 'of', 'somebody', 'its', 'just', 'loko', 'all', 'night']\n",
      "After stop words removal: ['yo', 'know', 'anyone', 'lt', 'gt', 'otherwise', 'able', 'buy', 'liquor', 'guy', 'flaked', 'right', 'get', 'hold', 'somebody', 'loko', 'night']\n",
      "After stemming with porters algorithm: ['know', 'anyon', 'otherwis', 'abl', 'bui', 'liquor', 'gui', 'flake', 'right', 'get', 'hold', 'somebodi', 'loko', 'night']\n",
      "Tokenized sentence: ['why', 'i', 'come', 'in', 'between', 'you', 'people']\n",
      "After stop words removal: ['come', 'people']\n",
      "After stemming with porters algorithm: ['come', 'peopl']\n",
      "Tokenized sentence: ['it', 's', 'only', 'ard', 'rest', 'all', 'ard', 'at', 'least', 'which', 'is', 'price', 'bedrm']\n",
      "After stop words removal: ['ard', 'rest', 'ard', 'least', 'price', 'bedrm']\n",
      "After stemming with porters algorithm: ['ard', 'rest', 'ard', 'least', 'price', 'bedrm']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'subscription', 'to', 'ringtone', 'uk', 'your', 'mobile', 'will', 'be', 'charged', 'month', 'please', 'confirm', 'by', 'replying', 'yes', 'or', 'no', 'if', 'you', 'reply', 'no', 'you', 'will', 'not', 'be', 'charged']\n",
      "After stop words removal: ['thanks', 'subscription', 'ringtone', 'uk', 'mobile', 'charged', 'month', 'please', 'confirm', 'replying', 'yes', 'reply', 'charged']\n",
      "reply\n",
      "After stemming with porters algorithm: ['thank', 'subscript', 'rington', 'mobil', 'char', 'month', 'pleas', 'confirm', 'repl', 'ye', 'repli', 'char']\n",
      "Tokenized sentence: ['i', 'told', 'that', 'am', 'coming', 'on', 'wednesday']\n",
      "After stop words removal: ['told', 'coming', 'wednesday']\n",
      "com\n",
      "After stemming with porters algorithm: ['told', 'come', 'wednesdai']\n",
      "Tokenized sentence: ['you', 'are', 'guaranteed', 'the', 'latests', 'nokia', 'phone', 'a', 'gb', 'ipod', 'mp', 'player', 'or', 'a', 'prize', 'txt', 'word', 'collect', 'to', 'no', 'tc', 'llc', 'ny', 'usa', 'p', 'mt', 'msgrcvd']\n",
      "After stop words removal: ['guaranteed', 'latests', 'nokia', 'phone', 'gb', 'ipod', 'mp', 'player', 'prize', 'txt', 'word', 'collect', 'tc', 'llc', 'ny', 'usa', 'p', 'mt', 'msgrcvd']\n",
      "After stemming with porters algorithm: ['guaranteed', 'latest', 'nokia', 'phone', 'ipod', 'player', 'priz', 'txt', 'word', 'collect', 'llc', 'usa', 'msgrcvd']\n",
      "Tokenized sentence: ['and', 'my', 'man', 'carlos', 'is', 'definitely', 'coming', 'by', 'mu', 'tonight', 'no', 'excuses']\n",
      "After stop words removal: ['man', 'carlos', 'definitely', 'coming', 'mu', 'tonight', 'excuses']\n",
      "com\n",
      "After stemming with porters algorithm: ['man', 'carlo', 'definit', 'come', 'tonight', 'excus']\n",
      "Tokenized sentence: ['beautiful', 'truth', 'expression', 'of', 'the', 'face', 'could', 'be', 'seen', 'by', 'everyone', 'but', 'the', 'depression', 'of', 'heart', 'could', 'be', 'understood', 'only', 'by', 'the', 'loved', 'ones', 'gud', 'ni']\n",
      "After stop words removal: ['beautiful', 'truth', 'expression', 'face', 'could', 'seen', 'everyone', 'depression', 'heart', 'could', 'understood', 'loved', 'ones', 'gud', 'ni']\n",
      "After stemming with porters algorithm: ['beauti', 'truth', 'express', 'face', 'could', 'seen', 'everyon', 'depress', 'heart', 'could', 'understood', 'love', 'on', 'gud']\n",
      "Tokenized sentence: ['just', 'gettin', 'a', 'bit', 'arty', 'with', 'my', 'collages', 'at', 'the', 'mo', 'well', 'tryin', 'ne', 'way', 'got', 'a', 'roast', 'in', 'a', 'min', 'lovely', 'i', 'shall', 'enjoy', 'that']\n",
      "After stop words removal: ['gettin', 'bit', 'arty', 'collages', 'mo', 'well', 'tryin', 'ne', 'way', 'got', 'roast', 'min', 'lovely', 'shall', 'enjoy']\n",
      "After stemming with porters algorithm: ['gettin', 'bit', 'arti', 'collag', 'well', 'tryin', 'wai', 'got', 'roast', 'min', 'love', 'shall', 'enjoi']\n",
      "Tokenized sentence: ['win', 'urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only', 'ppm']\n",
      "After stop words removal: ['win', 'urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs', 'ppm']\n",
      "After stemming with porters algorithm: ['win', 'urgent', 'mobil', 'number', 'awar', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr', 'ppm']\n",
      "Tokenized sentence: ['true', 'dear', 'i', 'sat', 'to', 'pray', 'evening', 'and', 'felt', 'so', 'so', 'i', 'sms', 'd', 'you', 'in', 'some', 'time']\n",
      "After stop words removal: ['true', 'dear', 'sat', 'pray', 'evening', 'felt', 'sms', 'time']\n",
      "even\n",
      "After stemming with porters algorithm: ['true', 'dear', 'sat', 'prai', 'even', 'felt', 'sm', 'time']\n",
      "Tokenized sentence: ['are', 'you', 'going', 'to', 'wipro', 'interview', 'today']\n",
      "After stop words removal: ['going', 'wipro', 'interview', 'today']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'wipro', 'interview', 'todai']\n",
      "Tokenized sentence: ['i', 'wanted', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'and', 'i', 'wanted', 'to', 'talk', 'to', 'you', 'about', 'some', 'legal', 'advice', 'to', 'do', 'with', 'when', 'gary', 'and', 'i', 'split', 'but', 'in', 'person', 'i', 'll', 'make', 'a', 'trip', 'to', 'ptbo', 'for', 'that', 'i', 'hope', 'everything', 'is', 'good', 'with', 'you', 'babe', 'and', 'i', 'love', 'ya']\n",
      "After stop words removal: ['wanted', 'wish', 'happy', 'new', 'year', 'wanted', 'talk', 'legal', 'advice', 'gary', 'split', 'person', 'make', 'trip', 'ptbo', 'hope', 'everything', 'good', 'babe', 'love', 'ya']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['wan', 'wish', 'happi', 'new', 'year', 'wan', 'talk', 'legal', 'advic', 'gari', 'split', 'person', 'make', 'trip', 'ptbo', 'hope', 'everyt', 'good', 'babe', 'love']\n",
      "Tokenized sentence: ['my', 'house', 'here', 'e', 'sky', 'quite', 'dark', 'liao', 'if', 'raining', 'then', 'got', 'excuse', 'not', 'run', 'already', 'rite', 'hee']\n",
      "After stop words removal: ['house', 'e', 'sky', 'quite', 'dark', 'liao', 'raining', 'got', 'excuse', 'run', 'already', 'rite', 'hee']\n",
      "rain\n",
      "After stemming with porters algorithm: ['hous', 'sky', 'quit', 'dark', 'liao', 'rain', 'got', 'excus', 'run', 'alreadi', 'rite', 'hee']\n",
      "Tokenized sentence: ['yeah', 'that', 's', 'what', 'i', 'thought', 'lemme', 'know', 'if', 'anything', 's', 'goin', 'on', 'later']\n",
      "After stop words removal: ['yeah', 'thought', 'lemme', 'know', 'anything', 'goin', 'later']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['yeah', 'thought', 'lemm', 'know', 'anyt', 'goin', 'later']\n",
      "Tokenized sentence: ['how', 'do', 'you', 'plan', 'to', 'manage', 'that']\n",
      "After stop words removal: ['plan', 'manage']\n",
      "After stemming with porters algorithm: ['plan', 'manag']\n",
      "Tokenized sentence: ['someonone', 'you', 'know', 'is', 'trying', 'to', 'contact', 'you', 'via', 'our', 'dating', 'service', 'to', 'find', 'out', 'who', 'it', 'could', 'be', 'call', 'from', 'your', 'mobile', 'or', 'landline', 'box', 'sk', 'ch']\n",
      "After stop words removal: ['someonone', 'know', 'trying', 'contact', 'via', 'dating', 'service', 'find', 'could', 'call', 'mobile', 'landline', 'box', 'sk', 'ch']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someonon', 'know', 'trying', 'contact', 'via', 'date', 'servic', 'find', 'could', 'call', 'mobil', 'landlin', 'box']\n",
      "Tokenized sentence: ['plz', 'note', 'if', 'anyone', 'calling', 'from', 'a', 'mobile', 'co', 'amp', 'asks', 'u', 'to', 'type', 'lt', 'gt', 'or', 'lt', 'gt', 'do', 'not', 'do', 'so', 'disconnect', 'the', 'call', 'coz', 'it', 'iz', 'an', 'attempt', 'of', 'terrorist', 'to', 'make', 'use', 'of', 'the', 'sim', 'card', 'no', 'itz', 'confirmd', 'by', 'nokia', 'n', 'motorola', 'n', 'has', 'been', 'verified', 'by', 'cnn', 'ibn']\n",
      "After stop words removal: ['plz', 'note', 'anyone', 'calling', 'mobile', 'co', 'amp', 'asks', 'u', 'type', 'lt', 'gt', 'lt', 'gt', 'disconnect', 'call', 'coz', 'iz', 'attempt', 'terrorist', 'make', 'use', 'sim', 'card', 'itz', 'confirmd', 'nokia', 'n', 'motorola', 'n', 'verified', 'cnn', 'ibn']\n",
      "call\n",
      "After stemming with porters algorithm: ['plz', 'note', 'anyon', 'call', 'mobil', 'amp', 'ask', 'type', 'disconnect', 'call', 'coz', 'attempt', 'terrorist', 'make', 'us', 'sim', 'card', 'itz', 'confirmd', 'nokia', 'motorola', 'verifi', 'cnn', 'ibn']\n",
      "Tokenized sentence: ['yes', 'but', 'i', 'don', 't', 'care', 'cause', 'i', 'know', 'its', 'there']\n",
      "After stop words removal: ['yes', 'care', 'cause', 'know']\n",
      "After stemming with porters algorithm: ['ye', 'care', 'caus', 'know']\n",
      "Tokenized sentence: ['have', 'a', 'great', 'trip', 'to', 'india', 'and', 'bring', 'the', 'light', 'to', 'everyone', 'not', 'just', 'with', 'the', 'project', 'but', 'with', 'everyone', 'that', 'is', 'lucky', 'to', 'see', 'you', 'smile', 'bye', 'abiola']\n",
      "After stop words removal: ['great', 'trip', 'india', 'bring', 'light', 'everyone', 'project', 'everyone', 'lucky', 'see', 'smile', 'bye', 'abiola']\n",
      "After stemming with porters algorithm: ['great', 'trip', 'india', 'bring', 'light', 'everyon', 'project', 'everyon', 'lucki', 'see', 'smile', 'bye', 'abiola']\n",
      "Tokenized sentence: ['its', 'hard', 'to', 'believe', 'things', 'like', 'this', 'all', 'can', 'say', 'lie', 'but', 'think', 'twice', 'before', 'saying', 'anything', 'to', 'me']\n",
      "After stop words removal: ['hard', 'believe', 'things', 'like', 'say', 'lie', 'think', 'twice', 'saying', 'anything']\n",
      "say\n",
      "anyth\n",
      "After stemming with porters algorithm: ['hard', 'believ', 'thing', 'like', 'sai', 'lie', 'think', 'twice', 'sai', 'anyt']\n",
      "Tokenized sentence: ['i', 'm', 'outside', 'islands', 'head', 'towards', 'hard', 'rock', 'and', 'you', 'll', 'run', 'into', 'me']\n",
      "After stop words removal: ['outside', 'islands', 'head', 'towards', 'hard', 'rock', 'run']\n",
      "After stemming with porters algorithm: ['outsid', 'island', 'head', 'toward', 'hard', 'rock', 'run']\n",
      "Tokenized sentence: ['no', 'no', 'i', 'will', 'check', 'all', 'rooms', 'befor', 'activities']\n",
      "After stop words removal: ['check', 'rooms', 'befor', 'activities']\n",
      "After stemming with porters algorithm: ['check', 'room', 'befor', 'activ']\n",
      "Tokenized sentence: ['sorry', 'to', 'trouble', 'u', 'again', 'can', 'buy', 'd', 'for', 'my', 'dad', 'again', 'all', 'big', 'small', 'sat', 'n', 'sun', 'thanx']\n",
      "After stop words removal: ['sorry', 'trouble', 'u', 'buy', 'dad', 'big', 'small', 'sat', 'n', 'sun', 'thanx']\n",
      "After stemming with porters algorithm: ['sorri', 'troubl', 'bui', 'dad', 'big', 'small', 'sat', 'sun', 'thanx']\n",
      "Tokenized sentence: ['say', 'thanks']\n",
      "After stop words removal: ['say', 'thanks']\n",
      "After stemming with porters algorithm: ['sai', 'thank']\n",
      "Tokenized sentence: ['full', 'heat', 'pa', 'i', 'have', 'applyed', 'oil', 'pa']\n",
      "After stop words removal: ['full', 'heat', 'pa', 'applyed', 'oil', 'pa']\n",
      "After stemming with porters algorithm: ['full', 'heat', 'appl', 'oil']\n",
      "Tokenized sentence: ['then', 'u', 'going', 'ikea', 'str', 'aft', 'dat']\n",
      "After stop words removal: ['u', 'going', 'ikea', 'str', 'aft', 'dat']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'ikea', 'str', 'aft', 'dat']\n",
      "Tokenized sentence: ['no', 'but', 'you', 'told', 'me', 'you', 'were', 'going', 'before', 'you', 'got', 'drunk']\n",
      "After stop words removal: ['told', 'going', 'got', 'drunk']\n",
      "go\n",
      "After stemming with porters algorithm: ['told', 'go', 'got', 'drunk']\n",
      "Tokenized sentence: ['yup', 'but', 'not', 'studying', 'surfing', 'lor', 'i', 'm', 'in', 'e', 'lazy', 'mode', 'today']\n",
      "After stop words removal: ['yup', 'studying', 'surfing', 'lor', 'e', 'lazy', 'mode', 'today']\n",
      "study\n",
      "surf\n",
      "After stemming with porters algorithm: ['yup', 'stud', 'sur', 'lor', 'lazi', 'mode', 'todai']\n",
      "Tokenized sentence: ['ok', 'try', 'to', 'do', 'week', 'end', 'course', 'in', 'coimbatore']\n",
      "After stop words removal: ['ok', 'try', 'week', 'end', 'course', 'coimbatore']\n",
      "After stemming with porters algorithm: ['try', 'week', 'end', 'cours', 'coimbator']\n",
      "Tokenized sentence: ['i', 'only', 'haf', 'msn', 'it', 's', 'yijue', 'hotmail', 'com']\n",
      "After stop words removal: ['haf', 'msn', 'yijue', 'hotmail', 'com']\n",
      "After stemming with porters algorithm: ['haf', 'msn', 'yiju', 'hotmail', 'com']\n",
      "Tokenized sentence: ['yar', 'but', 'they', 'say', 'got', 'some', 'error']\n",
      "After stop words removal: ['yar', 'say', 'got', 'error']\n",
      "After stemming with porters algorithm: ['yar', 'sai', 'got', 'error']\n",
      "Tokenized sentence: ['party', 's', 'at', 'my', 'place', 'at', 'usf', 'no', 'charge', 'but', 'if', 'you', 'can', 'contribute', 'in', 'any', 'way', 'it', 'is', 'greatly', 'appreciated', 'and', 'yeah', 'we', 'got', 'room', 'for', 'one', 'more']\n",
      "After stop words removal: ['party', 'place', 'usf', 'charge', 'contribute', 'way', 'greatly', 'appreciated', 'yeah', 'got', 'room', 'one']\n",
      "appreciate\n",
      "After stemming with porters algorithm: ['parti', 'place', 'usf', 'charg', 'contribut', 'wai', 'greatli', 'appreci', 'yeah', 'got', 'room', 'on']\n",
      "Tokenized sentence: ['i', 'dunno', 'lei', 'like', 'dun', 'haf']\n",
      "After stop words removal: ['dunno', 'lei', 'like', 'dun', 'haf']\n",
      "After stemming with porters algorithm: ['dunno', 'lei', 'like', 'dun', 'haf']\n",
      "Tokenized sentence: ['sorry', 'i', 'flaked', 'last', 'night', 'shit', 's', 'seriously', 'goin', 'down', 'with', 'my', 'roommate', 'what', 'you', 'up', 'to', 'tonight']\n",
      "After stop words removal: ['sorry', 'flaked', 'last', 'night', 'shit', 'seriously', 'goin', 'roommate', 'tonight']\n",
      "After stemming with porters algorithm: ['sorri', 'flake', 'last', 'night', 'shit', 'serious', 'goin', 'roommat', 'tonight']\n",
      "Tokenized sentence: ['show', 'ur', 'colours', 'euro', 'offer', 'get', 'an', 'england', 'flag', 'lions', 'tone', 'on', 'ur', 'phone', 'click', 'on', 'the', 'following', 'service', 'message', 'for', 'info']\n",
      "After stop words removal: ['show', 'ur', 'colours', 'euro', 'offer', 'get', 'england', 'flag', 'lions', 'tone', 'ur', 'phone', 'click', 'following', 'service', 'message', 'info']\n",
      "follow\n",
      "After stemming with porters algorithm: ['show', 'colour', 'euro', 'offer', 'get', 'england', 'flag', 'lion', 'tone', 'phone', 'click', 'follow', 'servic', 'messag', 'info']\n",
      "Tokenized sentence: ['cool', 'we', 'will', 'have', 'fun', 'practicing', 'making', 'babies']\n",
      "After stop words removal: ['cool', 'fun', 'practicing', 'making', 'babies']\n",
      "practic\n",
      "mak\n",
      "After stemming with porters algorithm: ['cool', 'fun', 'practic', 'make', 'babi']\n",
      "Tokenized sentence: ['evening', 'v', 'good', 'if', 'somewhat', 'event', 'laden', 'will', 'fill', 'you', 'in', 'don', 't', 'you', 'worry', 'head', 'ok', 'but', 'throat', 'wrecked', 'see', 'you', 'at', 'six', 'then']\n",
      "After stop words removal: ['evening', 'v', 'good', 'somewhat', 'event', 'laden', 'fill', 'worry', 'head', 'ok', 'throat', 'wrecked', 'see', 'six']\n",
      "even\n",
      "After stemming with porters algorithm: ['even', 'good', 'somewhat', 'event', 'laden', 'fill', 'worri', 'head', 'throat', 'wrec', 'see', 'six']\n",
      "Tokenized sentence: ['yar', 'else', 'i', 'll', 'thk', 'of', 'all', 'sorts', 'of', 'funny', 'things']\n",
      "After stop words removal: ['yar', 'else', 'thk', 'sorts', 'funny', 'things']\n",
      "After stemming with porters algorithm: ['yar', 'els', 'thk', 'sort', 'funni', 'thing']\n",
      "Tokenized sentence: ['you', 'didnt', 'complete', 'your', 'gist', 'oh']\n",
      "After stop words removal: ['didnt', 'complete', 'gist', 'oh']\n",
      "After stemming with porters algorithm: ['didnt', 'complet', 'gist']\n",
      "Tokenized sentence: ['crucify', 'is', 'c', 'not', 's', 'you', 'should', 'have', 'told', 'me', 'earlier']\n",
      "After stop words removal: ['crucify', 'c', 'told', 'earlier']\n",
      "After stemming with porters algorithm: ['crucifi', 'told', 'earlier']\n",
      "Tokenized sentence: ['update', 'your', 'face', 'book', 'status', 'frequently']\n",
      "After stop words removal: ['update', 'face', 'book', 'status', 'frequently']\n",
      "After stemming with porters algorithm: ['updat', 'face', 'book', 'statu', 'frequent']\n",
      "Tokenized sentence: ['madam', 'regret', 'disturbance', 'might', 'receive', 'a', 'reference', 'check', 'from', 'dlf', 'premarica', 'kindly', 'be', 'informed', 'rgds', 'rakhesh', 'kerala']\n",
      "After stop words removal: ['madam', 'regret', 'disturbance', 'might', 'receive', 'reference', 'check', 'dlf', 'premarica', 'kindly', 'informed', 'rgds', 'rakhesh', 'kerala']\n",
      "After stemming with porters algorithm: ['madam', 'regret', 'disturb', 'might', 'receiv', 'refer', 'check', 'dlf', 'premarica', 'kindli', 'infor', 'rgd', 'rakhesh', 'kerala']\n",
      "Tokenized sentence: ['wot', 'u', 'up', 'j']\n",
      "After stop words removal: ['wot', 'u', 'j']\n",
      "After stemming with porters algorithm: ['wot']\n",
      "Tokenized sentence: ['aathi', 'where', 'are', 'you', 'dear']\n",
      "After stop words removal: ['aathi', 'dear']\n",
      "After stemming with porters algorithm: ['aathi', 'dear']\n",
      "Tokenized sentence: ['he', 'telling', 'not', 'to', 'tell', 'any', 'one', 'if', 'so', 'treat', 'for', 'me', 'hi', 'hi', 'hi']\n",
      "After stop words removal: ['telling', 'tell', 'one', 'treat', 'hi', 'hi', 'hi']\n",
      "tell\n",
      "After stemming with porters algorithm: ['tell', 'tell', 'on', 'treat']\n",
      "Tokenized sentence: ['double', 'your', 'mins', 'txts', 'on', 'orange', 'or', 'price', 'linerental', 'motorola', 'and', 'sonyericsson', 'with', 'b', 'tooth', 'free', 'nokia', 'free', 'call', 'mobileupd', 'on', 'or', 'optout', 'hv', 'd']\n",
      "After stop words removal: ['double', 'mins', 'txts', 'orange', 'price', 'linerental', 'motorola', 'sonyericsson', 'b', 'tooth', 'free', 'nokia', 'free', 'call', 'mobileupd', 'optout', 'hv']\n",
      "After stemming with porters algorithm: ['doubl', 'min', 'txt', 'orang', 'price', 'liner', 'motorola', 'sonyericsson', 'tooth', 'free', 'nokia', 'free', 'call', 'mobileupd', 'optout']\n",
      "Tokenized sentence: ['this', 'is', 'hoping', 'you', 'enjoyed', 'your', 'game', 'yesterday', 'sorry', 'i', 've', 'not', 'been', 'in', 'touch', 'but', 'pls', 'know', 'that', 'you', 'are', 'fondly', 'bein', 'thot', 'off', 'have', 'a', 'great', 'week', 'abiola']\n",
      "After stop words removal: ['hoping', 'enjoyed', 'game', 'yesterday', 'sorry', 'touch', 'pls', 'know', 'fondly', 'bein', 'thot', 'great', 'week', 'abiola']\n",
      "hop\n",
      "After stemming with porters algorithm: ['hope', 'enjoi', 'game', 'yesterdai', 'sorri', 'touch', 'pl', 'know', 'fondli', 'bein', 'thot', 'great', 'week', 'abiola']\n",
      "Tokenized sentence: ['correct', 'so', 'how', 'was', 'work', 'today']\n",
      "After stop words removal: ['correct', 'work', 'today']\n",
      "After stemming with porters algorithm: ['correct', 'work', 'todai']\n",
      "Tokenized sentence: ['watever', 'relation', 'u', 'built', 'up', 'in', 'dis', 'world', 'only', 'thing', 'which', 'remains', 'atlast', 'iz', 'lonlines', 'with', 'lotz', 'n', 'lot', 'memories', 'feeling']\n",
      "After stop words removal: ['watever', 'relation', 'u', 'built', 'dis', 'world', 'thing', 'remains', 'atlast', 'iz', 'lonlines', 'lotz', 'n', 'lot', 'memories', 'feeling']\n",
      "feel\n",
      "After stemming with porters algorithm: ['watev', 'relat', 'built', 'di', 'world', 'thing', 'remain', 'atlast', 'lonlin', 'lotz', 'lot', 'memori', 'feel']\n",
      "Tokenized sentence: ['hello', 'sort', 'of', 'out', 'in', 'town', 'already', 'that', 'so', 'dont', 'rush', 'home', 'i', 'am', 'eating', 'nachos', 'will', 'let', 'you', 'know', 'eta']\n",
      "After stop words removal: ['hello', 'sort', 'town', 'already', 'dont', 'rush', 'home', 'eating', 'nachos', 'let', 'know', 'eta']\n",
      "eat\n",
      "eate\n",
      "After stemming with porters algorithm: ['hello', 'sort', 'town', 'alreadi', 'dont', 'rush', 'home', 'eat', 'nacho', 'let', 'know', 'eta']\n",
      "Tokenized sentence: ['em', 'its', 'olowoyey', 'usc', 'edu', 'have', 'a', 'great', 'time', 'in', 'argentina', 'not', 'sad', 'about', 'secretary', 'everything', 'is', 'a', 'blessing']\n",
      "After stop words removal: ['em', 'olowoyey', 'usc', 'edu', 'great', 'time', 'argentina', 'sad', 'secretary', 'everything', 'blessing']\n",
      "everyth\n",
      "bless\n",
      "After stemming with porters algorithm: ['olowoyei', 'usc', 'edu', 'great', 'time', 'argentina', 'sad', 'secretari', 'everyt', 'bless']\n",
      "Tokenized sentence: ['come', 'to', 'me', 'right', 'now', 'ahmad']\n",
      "After stop words removal: ['come', 'right', 'ahmad']\n",
      "After stemming with porters algorithm: ['come', 'right', 'ahmad']\n",
      "Tokenized sentence: ['xmas', 'offer', 'latest', 'motorola', 'sonyericsson', 'nokia', 'free', 'bluetooth', 'or', 'dvd', 'double', 'mins', 'txt', 'on', 'orange', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'qf']\n",
      "After stop words removal: ['xmas', 'offer', 'latest', 'motorola', 'sonyericsson', 'nokia', 'free', 'bluetooth', 'dvd', 'double', 'mins', 'txt', 'orange', 'call', 'mobileupd', 'call', 'optout', 'qf']\n",
      "After stemming with porters algorithm: ['xma', 'offer', 'latest', 'motorola', 'sonyericsson', 'nokia', 'free', 'bluetooth', 'dvd', 'doubl', 'min', 'txt', 'orang', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['still', 'otside', 'le', 'u', 'come', 'morrow', 'maga']\n",
      "After stop words removal: ['still', 'otside', 'le', 'u', 'come', 'morrow', 'maga']\n",
      "After stemming with porters algorithm: ['still', 'otsid', 'come', 'morrow', 'maga']\n",
      "Tokenized sentence: ['would', 'u', 'fuckin', 'believe', 'it', 'they', 'didnt', 'know', 'i', 'had', 'thurs', 'pre', 'booked', 'off', 'so', 'they', 're', 'cancelled', 'me', 'again', 'that', 'needs', 'to', 'b', 'sacked']\n",
      "After stop words removal: ['would', 'u', 'fuckin', 'believe', 'didnt', 'know', 'thurs', 'pre', 'booked', 'cancelled', 'needs', 'b', 'sacked']\n",
      "After stemming with porters algorithm: ['would', 'fuckin', 'believ', 'didnt', 'know', 'thur', 'pre', 'book', 'cancel', 'need', 'sac']\n",
      "Tokenized sentence: ['k', 'k', 'where', 'are', 'you', 'how', 'did', 'you', 'performed']\n",
      "After stop words removal: ['k', 'k', 'performed']\n",
      "After stemming with porters algorithm: ['perfor']\n",
      "Tokenized sentence: ['my', 'stomach', 'has', 'been', 'thru', 'so', 'much', 'trauma', 'i', 'swear', 'i', 'just', 'can', 't', 'eat', 'i', 'better', 'lose', 'weight']\n",
      "After stop words removal: ['stomach', 'thru', 'much', 'trauma', 'swear', 'eat', 'better', 'lose', 'weight']\n",
      "After stemming with porters algorithm: ['stomach', 'thru', 'much', 'trauma', 'swear', 'eat', 'better', 'lose', 'weight']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'tirupur', 'call', 'you', 'da']\n",
      "After stop words removal: ['tirupur', 'call', 'da']\n",
      "After stemming with porters algorithm: ['tirupur', 'call']\n",
      "Tokenized sentence: ['lol', 'ok', 'i', 'll', 'snatch', 'her', 'purse', 'too']\n",
      "After stop words removal: ['lol', 'ok', 'snatch', 'purse']\n",
      "After stemming with porters algorithm: ['lol', 'snatch', 'purs']\n",
      "Tokenized sentence: ['need', 'a', 'coffee', 'run', 'tomo', 'can', 't', 'believe', 'it', 's', 'that', 'time', 'of', 'week', 'already']\n",
      "After stop words removal: ['need', 'coffee', 'run', 'tomo', 'believe', 'time', 'week', 'already']\n",
      "After stemming with porters algorithm: ['need', 'coffe', 'run', 'tomo', 'believ', 'time', 'week', 'alreadi']\n",
      "Tokenized sentence: ['you', 'do', 'got', 'a', 'shitload', 'of', 'diamonds', 'though']\n",
      "After stop words removal: ['got', 'shitload', 'diamonds', 'though']\n",
      "After stemming with porters algorithm: ['got', 'shitload', 'diamond', 'though']\n",
      "Tokenized sentence: ['total', 'disappointment', 'when', 'i', 'texted', 'you', 'was', 'the', 'craziest', 'shit', 'got']\n",
      "After stop words removal: ['total', 'disappointment', 'texted', 'craziest', 'shit', 'got']\n",
      "After stemming with porters algorithm: ['total', 'disappoint', 'tex', 'craziest', 'shit', 'got']\n",
      "Tokenized sentence: ['no', 'problem', 'baby', 'is', 'this', 'is', 'a', 'good', 'time', 'to', 'talk', 'i', 'called', 'and', 'left', 'a', 'message']\n",
      "After stop words removal: ['problem', 'baby', 'good', 'time', 'talk', 'called', 'left', 'message']\n",
      "After stemming with porters algorithm: ['problem', 'babi', 'good', 'time', 'talk', 'call', 'left', 'messag']\n",
      "Tokenized sentence: ['later', 'i', 'guess', 'i', 'needa', 'do', 'mcat', 'study', 'too']\n",
      "After stop words removal: ['later', 'guess', 'needa', 'mcat', 'study']\n",
      "After stemming with porters algorithm: ['later', 'guess', 'needa', 'mcat', 'studi']\n",
      "Tokenized sentence: ['yeh', 'indians', 'was', 'nice', 'tho', 'it', 'did', 'kane', 'me', 'off', 'a', 'bit', 'he', 'he', 'we', 'shud', 'go', 'out', 'a', 'drink', 'sometime', 'soon', 'mite', 'hav', 'go', 'da', 'works', 'a', 'laugh', 'soon', 'love', 'pete', 'x', 'x']\n",
      "After stop words removal: ['yeh', 'indians', 'nice', 'tho', 'kane', 'bit', 'shud', 'go', 'drink', 'sometime', 'soon', 'mite', 'hav', 'go', 'da', 'works', 'laugh', 'soon', 'love', 'pete', 'x', 'x']\n",
      "After stemming with porters algorithm: ['yeh', 'indian', 'nice', 'tho', 'kane', 'bit', 'shud', 'drink', 'sometim', 'soon', 'mite', 'hav', 'work', 'laugh', 'soon', 'love', 'pete']\n",
      "Tokenized sentence: ['my', 'friend', 'just', 'got', 'here', 'and', 'says', 'he', 's', 'upping', 'his', 'order', 'by', 'a', 'few', 'grams', 'he', 's', 'got', 'lt', 'gt', 'when', 'can', 'you', 'get', 'here']\n",
      "After stop words removal: ['friend', 'got', 'says', 'upping', 'order', 'grams', 'got', 'lt', 'gt', 'get']\n",
      "upp\n",
      "After stemming with porters algorithm: ['friend', 'got', 'sai', 'up', 'order', 'gram', 'got', 'get']\n",
      "Tokenized sentence: ['awesome', 'how', 'do', 'i', 'deal', 'with', 'the', 'gate', 'charles', 'told', 'me', 'last', 'night', 'but', 'uh', 'yeah']\n",
      "After stop words removal: ['awesome', 'deal', 'gate', 'charles', 'told', 'last', 'night', 'uh', 'yeah']\n",
      "After stemming with porters algorithm: ['awesom', 'deal', 'gate', 'charl', 'told', 'last', 'night', 'yeah']\n",
      "Tokenized sentence: ['ummma', 'will', 'call', 'after', 'check', 'in', 'our', 'life', 'will', 'begin', 'from', 'qatar', 'so', 'pls', 'pray', 'very', 'hard']\n",
      "After stop words removal: ['ummma', 'call', 'check', 'life', 'begin', 'qatar', 'pls', 'pray', 'hard']\n",
      "After stemming with porters algorithm: ['ummma', 'call', 'check', 'life', 'begin', 'qatar', 'pl', 'prai', 'hard']\n",
      "Tokenized sentence: ['if', 'i', 'let', 'you', 'do', 'this', 'i', 'want', 'you', 'in', 'the', 'house', 'by', 'am']\n",
      "After stop words removal: ['let', 'want', 'house']\n",
      "After stemming with porters algorithm: ['let', 'want', 'hous']\n",
      "Tokenized sentence: ['you', 'know', 'my', 'old', 'dom', 'i', 'told', 'you', 'about', 'yesterday', 'his', 'name', 'is', 'roger', 'he', 'got', 'in', 'touch', 'with', 'me', 'last', 'night', 'and', 'wants', 'me', 'to', 'meet', 'him', 'today', 'at', 'pm']\n",
      "After stop words removal: ['know', 'old', 'dom', 'told', 'yesterday', 'name', 'roger', 'got', 'touch', 'last', 'night', 'wants', 'meet', 'today', 'pm']\n",
      "After stemming with porters algorithm: ['know', 'old', 'dom', 'told', 'yesterdai', 'name', 'roger', 'got', 'touch', 'last', 'night', 'want', 'meet', 'todai']\n",
      "Tokenized sentence: ['purity', 'of', 'friendship', 'between', 'two', 'is', 'not', 'about', 'smiling', 'after', 'reading', 'the', 'forwarded', 'message', 'its', 'about', 'smiling', 'just', 'by', 'seeing', 'the', 'name', 'gud', 'evng']\n",
      "After stop words removal: ['purity', 'friendship', 'two', 'smiling', 'reading', 'forwarded', 'message', 'smiling', 'seeing', 'name', 'gud', 'evng']\n",
      "smil\n",
      "read\n",
      "smil\n",
      "see\n",
      "After stemming with porters algorithm: ['puriti', 'friendship', 'two', 'smile', 'read', 'forwar', 'messag', 'smile', 'see', 'name', 'gud', 'evng']\n",
      "Tokenized sentence: ['whos', 'this', 'am', 'in', 'class']\n",
      "After stop words removal: ['whos', 'class']\n",
      "After stemming with porters algorithm: ['who', 'class']\n",
      "Tokenized sentence: ['anyway', 'seriously', 'hit', 'me', 'up', 'when', 'you', 're', 'back', 'because', 'otherwise', 'i', 'have', 'to', 'light', 'up', 'with', 'armand', 'and', 'he', 'always', 'has', 'shit', 'and', 'or', 'is', 'vomiting']\n",
      "After stop words removal: ['anyway', 'seriously', 'hit', 'back', 'otherwise', 'light', 'armand', 'always', 'shit', 'vomiting']\n",
      "vomit\n",
      "After stemming with porters algorithm: ['anywai', 'serious', 'hit', 'back', 'otherwis', 'light', 'armand', 'alwai', 'shit', 'vomit']\n",
      "Tokenized sentence: ['nite', 'nite', 'pocay', 'wocay', 'luv', 'u', 'more', 'than', 'n', 'e', 'thing', 'eva', 'i', 'promise', 'ring', 'u', 'morrowxxxx']\n",
      "After stop words removal: ['nite', 'nite', 'pocay', 'wocay', 'luv', 'u', 'n', 'e', 'thing', 'eva', 'promise', 'ring', 'u', 'morrowxxxx']\n",
      "After stemming with porters algorithm: ['nite', 'nite', 'pocai', 'wocai', 'luv', 'thing', 'eva', 'promis', 'ring', 'morrowxxxx']\n",
      "Tokenized sentence: ['rose', 'for', 'red', 'red', 'for', 'blood', 'blood', 'for', 'heart', 'heart', 'for', 'u', 'but', 'u', 'for', 'me', 'send', 'tis', 'to', 'all', 'ur', 'friends', 'including', 'me', 'if', 'u', 'like', 'me', 'if', 'u', 'get', 'back', 'u', 'r', 'poor', 'in', 'relation', 'u', 'need', 'some', 'to', 'support', 'u', 'r', 'frnd', 'many', 'some', 'luvs', 'u', 'some', 'is', 'praying', 'god', 'to', 'marry', 'u', 'try', 'it']\n",
      "After stop words removal: ['rose', 'red', 'red', 'blood', 'blood', 'heart', 'heart', 'u', 'u', 'send', 'tis', 'ur', 'friends', 'including', 'u', 'like', 'u', 'get', 'back', 'u', 'r', 'poor', 'relation', 'u', 'need', 'support', 'u', 'r', 'frnd', 'many', 'luvs', 'u', 'praying', 'god', 'marry', 'u', 'try']\n",
      "includ\n",
      "pray\n",
      "After stemming with porters algorithm: ['rose', 'red', 'red', 'blood', 'blood', 'heart', 'heart', 'send', 'ti', 'friend', 'includ', 'like', 'get', 'back', 'poor', 'relat', 'need', 'support', 'frnd', 'mani', 'luv', 'prai', 'god', 'marri', 'try']\n",
      "Tokenized sentence: ['hungry', 'gay', 'guys', 'feeling', 'hungry', 'and', 'up', 'it', 'now', 'call', 'just', 'p', 'min', 'to', 'stop', 'texts', 'call', 'p', 'min']\n",
      "After stop words removal: ['hungry', 'gay', 'guys', 'feeling', 'hungry', 'call', 'p', 'min', 'stop', 'texts', 'call', 'p', 'min']\n",
      "feel\n",
      "After stemming with porters algorithm: ['hungri', 'gai', 'gui', 'feel', 'hungri', 'call', 'min', 'stop', 'text', 'call', 'min']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['the', 'beauty', 'of', 'life', 'is', 'in', 'next', 'second', 'which', 'hides', 'thousands', 'of', 'secrets', 'i', 'wish', 'every', 'second', 'will', 'be', 'wonderful', 'in', 'ur', 'life', 'gud', 'n']\n",
      "After stop words removal: ['beauty', 'life', 'next', 'second', 'hides', 'thousands', 'secrets', 'wish', 'every', 'second', 'wonderful', 'ur', 'life', 'gud', 'n']\n",
      "After stemming with porters algorithm: ['beauti', 'life', 'next', 'second', 'hide', 'thousand', 'secret', 'wish', 'everi', 'second', 'wonder', 'life', 'gud']\n",
      "Tokenized sentence: ['romcapspam', 'everyone', 'around', 'should', 'be', 'responding', 'well', 'to', 'your', 'presence', 'since', 'you', 'are', 'so', 'warm', 'and', 'outgoing', 'you', 'are', 'bringing', 'in', 'a', 'real', 'breath', 'of', 'sunshine']\n",
      "After stop words removal: ['romcapspam', 'everyone', 'around', 'responding', 'well', 'presence', 'since', 'warm', 'outgoing', 'bringing', 'real', 'breath', 'sunshine']\n",
      "respond\n",
      "outgo\n",
      "bring\n",
      "After stemming with porters algorithm: ['romcapspam', 'everyon', 'around', 'respon', 'well', 'presenc', 'sinc', 'warm', 'outgo', 'brin', 'real', 'breath', 'sunshin']\n",
      "Tokenized sentence: ['it', 'wont', 'b', 'until', 'as', 'trying', 'sort', 'house', 'out', 'is', 'that', 'ok']\n",
      "After stop words removal: ['wont', 'b', 'trying', 'sort', 'house', 'ok']\n",
      "After stemming with porters algorithm: ['wont', 'trying', 'sort', 'hous']\n",
      "Tokenized sentence: ['it', 's', 'cool', 'let', 'me', 'know', 'before', 'it', 'kicks', 'off', 'around', 'lt', 'gt', 'i', 'll', 'be', 'out', 'and', 'about', 'all', 'day']\n",
      "After stop words removal: ['cool', 'let', 'know', 'kicks', 'around', 'lt', 'gt', 'day']\n",
      "After stemming with porters algorithm: ['cool', 'let', 'know', 'kick', 'around', 'dai']\n",
      "Tokenized sentence: ['whats', 'the', 'staff', 'name', 'who', 'is', 'taking', 'class', 'for', 'us']\n",
      "After stop words removal: ['whats', 'staff', 'name', 'taking', 'class', 'us']\n",
      "tak\n",
      "After stemming with porters algorithm: ['what', 'staff', 'name', 'take', 'class']\n",
      "Tokenized sentence: ['i', 'don', 't', 'quite', 'know', 'what', 'to', 'do', 'i', 'still', 'can', 't', 'get', 'hold', 'of', 'anyone', 'i', 'cud', 'pick', 'you', 'up', 'bout', 'pm', 'and', 'we', 'can', 'see', 'if', 'they', 're', 'in', 'the', 'pub']\n",
      "After stop words removal: ['quite', 'know', 'still', 'get', 'hold', 'anyone', 'cud', 'pick', 'bout', 'pm', 'see', 'pub']\n",
      "After stemming with porters algorithm: ['quit', 'know', 'still', 'get', 'hold', 'anyon', 'cud', 'pick', 'bout', 'see', 'pub']\n",
      "Tokenized sentence: ['yes', 'rent', 'is', 'very', 'expensive', 'so', 'its', 'the', 'way', 'we', 'save']\n",
      "After stop words removal: ['yes', 'rent', 'expensive', 'way', 'save']\n",
      "After stemming with porters algorithm: ['ye', 'rent', 'expens', 'wai', 'save']\n",
      "Tokenized sentence: ['my', 'sister', 'got', 'placed', 'in', 'birla', 'soft', 'da']\n",
      "After stop words removal: ['sister', 'got', 'placed', 'birla', 'soft', 'da']\n",
      "After stemming with porters algorithm: ['sister', 'got', 'place', 'birla', 'soft']\n",
      "Tokenized sentence: ['bored', 'housewives', 'chat', 'n', 'date', 'now', 'bt', 'national', 'rate', 'p', 'min', 'only', 'from', 'landlines']\n",
      "After stop words removal: ['bored', 'housewives', 'chat', 'n', 'date', 'bt', 'national', 'rate', 'p', 'min', 'landlines']\n",
      "After stemming with porters algorithm: ['bore', 'housew', 'chat', 'date', 'nat', 'rate', 'min', 'landlin']\n",
      "Tokenized sentence: ['days', 'to', 'euro', 'kickoff', 'u', 'will', 'be', 'kept', 'informed', 'of', 'all', 'the', 'latest', 'news', 'and', 'results', 'daily', 'unsubscribe', 'send', 'get', 'euro', 'stop', 'to']\n",
      "After stop words removal: ['days', 'euro', 'kickoff', 'u', 'kept', 'informed', 'latest', 'news', 'results', 'daily', 'unsubscribe', 'send', 'get', 'euro', 'stop']\n",
      "After stemming with porters algorithm: ['dai', 'euro', 'kickoff', 'kept', 'infor', 'latest', 'new', 'result', 'daili', 'unsubscrib', 'send', 'get', 'euro', 'stop']\n",
      "Tokenized sentence: ['but', 'that', 's', 'on', 'ebay', 'it', 'might', 'be', 'less', 'elsewhere']\n",
      "After stop words removal: ['ebay', 'might', 'less', 'elsewhere']\n",
      "After stemming with porters algorithm: ['ebai', 'might', 'less', 'elsewher']\n",
      "Tokenized sentence: ['wa', 'u', 'so', 'efficient', 'gee', 'thanx']\n",
      "After stop words removal: ['wa', 'u', 'efficient', 'gee', 'thanx']\n",
      "After stemming with porters algorithm: ['effici', 'gee', 'thanx']\n",
      "Tokenized sentence: ['please', 'dont', 'say', 'like', 'that', 'hi', 'hi', 'hi']\n",
      "After stop words removal: ['please', 'dont', 'say', 'like', 'hi', 'hi', 'hi']\n",
      "After stemming with porters algorithm: ['pleas', 'dont', 'sai', 'like']\n",
      "Tokenized sentence: ['get', 'ready', 'for', 'lt', 'gt', 'inches', 'of', 'pleasure']\n",
      "After stop words removal: ['get', 'ready', 'lt', 'gt', 'inches', 'pleasure']\n",
      "After stemming with porters algorithm: ['get', 'readi', 'inch', 'pleasur']\n",
      "Tokenized sentence: ['ringtone', 'club', 'get', 'the', 'uk', 'singles', 'chart', 'on', 'your', 'mobile', 'each', 'week', 'and', 'choose', 'any', 'top', 'quality', 'ringtone', 'this', 'message', 'is', 'free', 'of', 'charge']\n",
      "After stop words removal: ['ringtone', 'club', 'get', 'uk', 'singles', 'chart', 'mobile', 'week', 'choose', 'top', 'quality', 'ringtone', 'message', 'free', 'charge']\n",
      "After stemming with porters algorithm: ['rington', 'club', 'get', 'singl', 'chart', 'mobil', 'week', 'choos', 'top', 'qualiti', 'rington', 'messag', 'free', 'charg']\n",
      "Tokenized sentence: ['i', 'm', 'not', 'sure', 'i', 'was', 'just', 'checking', 'out', 'what', 'was', 'happening', 'around', 'the', 'area']\n",
      "After stop words removal: ['sure', 'checking', 'happening', 'around', 'area']\n",
      "check\n",
      "happen\n",
      "After stemming with porters algorithm: ['sure', 'chec', 'happen', 'around', 'area']\n",
      "Tokenized sentence: ['asked', 'mobile', 'if', 'chatlines', 'inclu', 'in', 'free', 'mins', 'india', 'cust', 'servs', 'sed', 'yes', 'l', 'er', 'got', 'mega', 'bill', 'dont', 'giv', 'a', 'shit', 'bailiff', 'due', 'in', 'days', 'i', 'o', 'want']\n",
      "After stop words removal: ['asked', 'mobile', 'chatlines', 'inclu', 'free', 'mins', 'india', 'cust', 'servs', 'sed', 'yes', 'l', 'er', 'got', 'mega', 'bill', 'dont', 'giv', 'shit', 'bailiff', 'due', 'days', 'want']\n",
      "After stemming with porters algorithm: ['as', 'mobil', 'chatlin', 'inclu', 'free', 'min', 'india', 'cust', 'serv', 'sed', 'ye', 'got', 'mega', 'bill', 'dont', 'giv', 'shit', 'bailiff', 'due', 'dai', 'want']\n",
      "Tokenized sentence: ['tell', 'me', 'something', 'thats', 'okay']\n",
      "After stop words removal: ['tell', 'something', 'thats', 'okay']\n",
      "someth\n",
      "After stemming with porters algorithm: ['tell', 'somet', 'that', 'okai']\n",
      "Tokenized sentence: ['princess', 'is', 'your', 'kitty', 'shaved', 'or', 'natural']\n",
      "After stop words removal: ['princess', 'kitty', 'shaved', 'natural']\n",
      "After stemming with porters algorithm: ['princess', 'kitti', 'shave', 'natur']\n",
      "Tokenized sentence: ['k', 'ill', 'drink', 'pa', 'then', 'what', 'doing', 'i', 'need', 'srs', 'model', 'pls', 'send', 'it', 'to', 'my', 'mail', 'id', 'pa']\n",
      "After stop words removal: ['k', 'ill', 'drink', 'pa', 'need', 'srs', 'model', 'pls', 'send', 'mail', 'id', 'pa']\n",
      "After stemming with porters algorithm: ['ill', 'drink', 'need', 'sr', 'model', 'pl', 'send', 'mail']\n",
      "Tokenized sentence: ['we', 'don', 'call', 'like', 'lt', 'gt', 'times', 'oh', 'no', 'give', 'us', 'hypertension', 'oh']\n",
      "After stop words removal: ['call', 'like', 'lt', 'gt', 'times', 'oh', 'give', 'us', 'hypertension', 'oh']\n",
      "After stemming with porters algorithm: ['call', 'like', 'time', 'give', 'hypertens']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'nokia', 'i', 'colour', 'phone', 'delivered', 'tomorrow', 'with', 'free', 'minutes', 'to', 'any', 'mobile', 'free', 'text', 'free', 'camcorder', 'reply', 'or', 'call']\n",
      "After stop words removal: ['want', 'new', 'nokia', 'colour', 'phone', 'delivered', 'tomorrow', 'free', 'minutes', 'mobile', 'free', 'text', 'free', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['want', 'new', 'nokia', 'colour', 'phone', 'deliv', 'tomorrow', 'free', 'minut', 'mobil', 'free', 'text', 'free', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['just', 'got', 'outta', 'class', 'gonna', 'go', 'gym']\n",
      "After stop words removal: ['got', 'outta', 'class', 'gonna', 'go', 'gym']\n",
      "After stemming with porters algorithm: ['got', 'outta', 'class', 'gonna', 'gym']\n",
      "Tokenized sentence: ['i', 'm', 'doing', 'da', 'intro', 'covers', 'energy', 'trends', 'n', 'pros', 'n', 'cons', 'brief', 'description', 'of', 'nuclear', 'fusion', 'n', 'oso', 'brief', 'history', 'of', 'iter', 'n', 'jet', 'got', 'abt', 'n', 'half', 'pages']\n",
      "After stop words removal: ['da', 'intro', 'covers', 'energy', 'trends', 'n', 'pros', 'n', 'cons', 'brief', 'description', 'nuclear', 'fusion', 'n', 'oso', 'brief', 'history', 'iter', 'n', 'jet', 'got', 'abt', 'n', 'half', 'pages']\n",
      "After stemming with porters algorithm: ['intro', 'cover', 'energi', 'trend', 'pro', 'con', 'brief', 'descript', 'nuclear', 'fusion', 'oso', 'brief', 'histori', 'iter', 'jet', 'got', 'abt', 'half', 'page']\n",
      "Tokenized sentence: ['what', 's', 'nannys', 'address']\n",
      "After stop words removal: ['nannys', 'address']\n",
      "After stemming with porters algorithm: ['nanni', 'address']\n",
      "Tokenized sentence: ['last', 'chance', 'claim', 'ur', 'worth', 'of', 'discount', 'vouchers', 'text', 'yes', 'to', 'now', 'savamob', 'member', 'offers', 'mobile', 't', 'cs', 'sub', 'remove', 'txt', 'x', 'or', 'stop']\n",
      "After stop words removal: ['last', 'chance', 'claim', 'ur', 'worth', 'discount', 'vouchers', 'text', 'yes', 'savamob', 'member', 'offers', 'mobile', 'cs', 'sub', 'remove', 'txt', 'x', 'stop']\n",
      "After stemming with porters algorithm: ['last', 'chanc', 'claim', 'worth', 'discount', 'voucher', 'text', 'ye', 'savamob', 'member', 'offer', 'mobil', 'sub', 'remov', 'txt', 'stop']\n",
      "Tokenized sentence: ['my', 'sis', 'is', 'catching', 'e', 'show', 'in', 'e', 'afternoon', 'so', 'i', 'm', 'not', 'watching', 'w', 'her', 'so', 'c', 'u', 'wan', 'watch', 'today', 'or', 'tmr', 'lor']\n",
      "After stop words removal: ['sis', 'catching', 'e', 'show', 'e', 'afternoon', 'watching', 'w', 'c', 'u', 'wan', 'watch', 'today', 'tmr', 'lor']\n",
      "catch\n",
      "watch\n",
      "After stemming with porters algorithm: ['si', 'catc', 'show', 'afternoon', 'watc', 'wan', 'watch', 'todai', 'tmr', 'lor']\n",
      "Tokenized sentence: ['pls', 'call', 'me', 'da', 'what', 'happen']\n",
      "After stop words removal: ['pls', 'call', 'da', 'happen']\n",
      "After stemming with porters algorithm: ['pl', 'call', 'happen']\n",
      "Tokenized sentence: ['sorry', 'that', 'was', 'my', 'uncle', 'i', 'll', 'keep', 'in', 'touch']\n",
      "After stop words removal: ['sorry', 'uncle', 'keep', 'touch']\n",
      "After stemming with porters algorithm: ['sorri', 'uncl', 'keep', 'touch']\n",
      "Tokenized sentence: ['only', 'students', 'solved', 'this', 'cat', 'question', 'in', 'xam', 'lt', 'gt', 'lt', 'gt', 'lt', 'gt', 'then', 'tell', 'me', 'the', 'answer', 'if', 'u', 'r', 'brilliant', 'thing', 'i', 'got', 'd', 'answr']\n",
      "After stop words removal: ['students', 'solved', 'cat', 'question', 'xam', 'lt', 'gt', 'lt', 'gt', 'lt', 'gt', 'tell', 'answer', 'u', 'r', 'brilliant', 'thing', 'got', 'answr']\n",
      "After stemming with porters algorithm: ['student', 'sol', 'cat', 'quest', 'xam', 'tell', 'answer', 'brilliant', 'thing', 'got', 'answr']\n",
      "Tokenized sentence: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stop words removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stemming with porters algorithm: ['free', 'week', 'nokia', 'tone', 'mob', 'everi', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'mate', 'www', 'getz', 'pobox', 'norm', 'tone']\n",
      "Tokenized sentence: ['hey', 'so', 'whats', 'the', 'plan', 'this', 'sat']\n",
      "After stop words removal: ['hey', 'whats', 'plan', 'sat']\n",
      "After stemming with porters algorithm: ['hei', 'what', 'plan', 'sat']\n",
      "Tokenized sentence: ['yeah', 'do', 'don', 't', 'stand', 'to', 'close', 'tho', 'you', 'll', 'catch', 'something']\n",
      "After stop words removal: ['yeah', 'stand', 'close', 'tho', 'catch', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['yeah', 'stand', 'close', 'tho', 'catch', 'somet']\n",
      "Tokenized sentence: ['the', 'basket', 's', 'gettin', 'full', 'so', 'i', 'might', 'be', 'by', 'tonight']\n",
      "After stop words removal: ['basket', 'gettin', 'full', 'might', 'tonight']\n",
      "After stemming with porters algorithm: ['basket', 'gettin', 'full', 'might', 'tonight']\n",
      "Tokenized sentence: ['k', 'i', 'will', 'sent', 'it', 'again']\n",
      "After stop words removal: ['k', 'sent']\n",
      "After stemming with porters algorithm: ['sent']\n",
      "Tokenized sentence: ['if', 'anyone', 'calls', 'for', 'a', 'treadmill', 'say', 'you', 'll', 'buy', 'it', 'make', 'sure', 'its', 'working', 'i', 'found', 'an', 'ad', 'on', 'craigslist', 'selling', 'for', 'lt', 'gt']\n",
      "After stop words removal: ['anyone', 'calls', 'treadmill', 'say', 'buy', 'make', 'sure', 'working', 'found', 'ad', 'craigslist', 'selling', 'lt', 'gt']\n",
      "work\n",
      "sell\n",
      "After stemming with porters algorithm: ['anyon', 'call', 'treadmil', 'sai', 'bui', 'make', 'sure', 'wor', 'found', 'craigslist', 'sell']\n",
      "Tokenized sentence: ['thk', 'some', 'of', 'em', 'find', 'wtc', 'too', 'far', 'weiyi', 'not', 'goin', 'e', 'rest', 'i', 'dunno', 'yet', 'r', 'ur', 'goin', 'dinner', 'den', 'i', 'might', 'b', 'able', 'to', 'join']\n",
      "After stop words removal: ['thk', 'em', 'find', 'wtc', 'far', 'weiyi', 'goin', 'e', 'rest', 'dunno', 'yet', 'r', 'ur', 'goin', 'dinner', 'den', 'might', 'b', 'able', 'join']\n",
      "After stemming with porters algorithm: ['thk', 'find', 'wtc', 'far', 'weiyi', 'goin', 'rest', 'dunno', 'yet', 'goin', 'dinner', 'den', 'might', 'abl', 'join']\n",
      "Tokenized sentence: ['that', 'means', 'you', 'got', 'an', 'a', 'in', 'epi', 'she', 's', 'fine', 'she', 's', 'here', 'now']\n",
      "After stop words removal: ['means', 'got', 'epi', 'fine']\n",
      "After stemming with porters algorithm: ['mean', 'got', 'epi', 'fine']\n",
      "Tokenized sentence: ['dude', 'how', 'do', 'you', 'like', 'the', 'buff', 'wind']\n",
      "After stop words removal: ['dude', 'like', 'buff', 'wind']\n",
      "After stemming with porters algorithm: ['dude', 'like', 'buff', 'wind']\n",
      "Tokenized sentence: ['just', 'sleeping', 'and', 'surfing']\n",
      "After stop words removal: ['sleeping', 'surfing']\n",
      "sleep\n",
      "surf\n",
      "After stemming with porters algorithm: ['sleep', 'sur']\n",
      "Tokenized sentence: ['u', 'should', 'have', 'made', 'an', 'appointment']\n",
      "After stop words removal: ['u', 'made', 'appointment']\n",
      "After stemming with porters algorithm: ['made', 'appoint']\n",
      "Tokenized sentence: ['congratulations', 'ur', 'awarded', 'either', 'a', 'yrs', 'supply', 'of', 'cds', 'from', 'virgin', 'records', 'or', 'a', 'mystery', 'gift', 'guaranteed', 'call', 'ts', 'cs', 'www', 'smsco', 'net', 'pm', 'approx', 'mins']\n",
      "After stop words removal: ['congratulations', 'ur', 'awarded', 'either', 'yrs', 'supply', 'cds', 'virgin', 'records', 'mystery', 'gift', 'guaranteed', 'call', 'ts', 'cs', 'www', 'smsco', 'net', 'pm', 'approx', 'mins']\n",
      "After stemming with porters algorithm: ['congratul', 'awar', 'either', 'yr', 'suppli', 'cd', 'virgin', 'record', 'mysteri', 'gift', 'guaranteed', 'call', 'www', 'smsco', 'net', 'approx', 'min']\n",
      "Tokenized sentence: ['derp', 'which', 'is', 'worse', 'a', 'dude', 'who', 'always', 'wants', 'to', 'party', 'or', 'a', 'dude', 'who', 'files', 'a', 'complaint', 'about', 'the', 'three', 'drug', 'abusers', 'he', 'lives', 'with']\n",
      "After stop words removal: ['derp', 'worse', 'dude', 'always', 'wants', 'party', 'dude', 'files', 'complaint', 'three', 'drug', 'abusers', 'lives']\n",
      "After stemming with porters algorithm: ['derp', 'wors', 'dude', 'alwai', 'want', 'parti', 'dude', 'file', 'complaint', 'three', 'drug', 'abus', 'live']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'xxxxxx', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'xxxxxx', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'xxxxxx', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['or', 'better', 'still', 'can', 'you', 'catch', 'her', 'and', 'let', 'ask', 'her', 'if', 'she', 'can', 'sell', 'lt', 'gt', 'for', 'me']\n",
      "After stop words removal: ['better', 'still', 'catch', 'let', 'ask', 'sell', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['better', 'still', 'catch', 'let', 'ask', 'sell']\n",
      "Tokenized sentence: ['lol', 'its', 'ok', 'i', 'didn', 't', 'remember', 'til', 'last', 'nite']\n",
      "After stop words removal: ['lol', 'ok', 'remember', 'til', 'last', 'nite']\n",
      "After stemming with porters algorithm: ['lol', 'rememb', 'til', 'last', 'nite']\n",
      "Tokenized sentence: ['all', 'will', 'come', 'alive', 'better', 'correct', 'any', 'good', 'looking', 'figure', 'there', 'itself']\n",
      "After stop words removal: ['come', 'alive', 'better', 'correct', 'good', 'looking', 'figure']\n",
      "look\n",
      "After stemming with porters algorithm: ['come', 'aliv', 'better', 'correct', 'good', 'look', 'figur']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'guaranteed', 'award', 'or', 'even', 'cashto', 'claim', 'ur', 'award', 'call', 'free', 'on', 'stop', 'getstop', 'on', 'php']\n",
      "After stop words removal: ['guaranteed', 'award', 'even', 'cashto', 'claim', 'ur', 'award', 'call', 'free', 'stop', 'getstop', 'php']\n",
      "After stemming with porters algorithm: ['guaranteed', 'award', 'even', 'cashto', 'claim', 'award', 'call', 'free', 'stop', 'getstop', 'php']\n",
      "Tokenized sentence: ['ha', 'u', 'jus', 'ate', 'honey', 'ar', 'so', 'sweet']\n",
      "After stop words removal: ['ha', 'u', 'jus', 'ate', 'honey', 'ar', 'sweet']\n",
      "After stemming with porters algorithm: ['ju', 'at', 'honei', 'sweet']\n",
      "Tokenized sentence: ['ur', 'cash', 'balance', 'is', 'currently', 'pounds', 'to', 'maximize', 'ur', 'cash', 'in', 'now', 'send', 'go', 'to', 'only', 'p', 'meg', 'cc', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stop words removal: ['ur', 'cash', 'balance', 'currently', 'pounds', 'maximize', 'ur', 'cash', 'send', 'go', 'p', 'meg', 'cc', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stemming with porters algorithm: ['cash', 'balanc', 'current', 'pound', 'maxim', 'cash', 'send', 'meg', 'suit', 'land', 'row']\n",
      "Tokenized sentence: ['thats', 'cool', 'i', 'am', 'a', 'gentleman', 'and', 'will', 'treat', 'you', 'with', 'dignity', 'and', 'respect']\n",
      "After stop words removal: ['thats', 'cool', 'gentleman', 'treat', 'dignity', 'respect']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'gentleman', 'treat', 'digniti', 'respect']\n",
      "Tokenized sentence: ['no', 'shit', 'but', 'i', 'wasn', 't', 'that', 'surprised', 'so', 'i', 'went', 'and', 'spent', 'the', 'evening', 'with', 'that', 'french', 'guy', 'i', 'met', 'in', 'town', 'here', 'and', 'we', 'fooled', 'around', 'a', 'bit', 'but', 'i', 'didn', 't', 'let', 'him', 'fuck', 'me']\n",
      "After stop words removal: ['shit', 'surprised', 'went', 'spent', 'evening', 'french', 'guy', 'met', 'town', 'fooled', 'around', 'bit', 'let', 'fuck']\n",
      "even\n",
      "After stemming with porters algorithm: ['shit', 'surpris', 'went', 'spent', 'even', 'french', 'gui', 'met', 'town', 'fool', 'around', 'bit', 'let', 'fuck']\n",
      "Tokenized sentence: ['i', 'don', 't', 'know', 'same', 'thing', 'that', 's', 'wrong', 'everyso', 'often', 'he', 'panicks', 'starts', 'goin', 'on', 'bout', 'not', 'bein', 'good', 'enough']\n",
      "After stop words removal: ['know', 'thing', 'wrong', 'everyso', 'often', 'panicks', 'starts', 'goin', 'bout', 'bein', 'good', 'enough']\n",
      "After stemming with porters algorithm: ['know', 'thing', 'wrong', 'everyso', 'often', 'panick', 'start', 'goin', 'bout', 'bein', 'good', 'enough']\n",
      "Tokenized sentence: ['who', 'are', 'you', 'seeing']\n",
      "After stop words removal: ['seeing']\n",
      "see\n",
      "After stemming with porters algorithm: ['see']\n",
      "Tokenized sentence: ['and', 'are', 'premium', 'phone', 'services', 'call']\n",
      "After stop words removal: ['premium', 'phone', 'services', 'call']\n",
      "After stemming with porters algorithm: ['premium', 'phone', 'servic', 'call']\n",
      "Tokenized sentence: ['k', 'want', 'us', 'to', 'come', 'by', 'now']\n",
      "After stop words removal: ['k', 'want', 'us', 'come']\n",
      "After stemming with porters algorithm: ['want', 'come']\n",
      "Tokenized sentence: ['princess', 'i', 'like', 'to', 'make', 'love', 'lt', 'gt', 'times', 'per', 'night', 'hope', 'thats', 'not', 'a', 'problem']\n",
      "After stop words removal: ['princess', 'like', 'make', 'love', 'lt', 'gt', 'times', 'per', 'night', 'hope', 'thats', 'problem']\n",
      "After stemming with porters algorithm: ['princess', 'like', 'make', 'love', 'time', 'per', 'night', 'hope', 'that', 'problem']\n",
      "Tokenized sentence: ['aiyah', 'u', 'did', 'ok', 'already', 'lar', 'e', 'nydc', 'at', 'wheellock']\n",
      "After stop words removal: ['aiyah', 'u', 'ok', 'already', 'lar', 'e', 'nydc', 'wheellock']\n",
      "After stemming with porters algorithm: ['aiyah', 'alreadi', 'lar', 'nydc', 'wheellock']\n",
      "Tokenized sentence: ['rgent', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'contact', 'u', 'u', 'have', 'won', 'call', 'b', 't', 'csbcm', 'wc', 'n', 'xx', 'callcost', 'ppm', 'mobilesvary', 'max']\n",
      "After stop words removal: ['rgent', 'nd', 'attempt', 'contact', 'u', 'u', 'call', 'b', 'csbcm', 'wc', 'n', 'xx', 'callcost', 'ppm', 'mobilesvary', 'max']\n",
      "After stemming with porters algorithm: ['rgent', 'attempt', 'contact', 'call', 'csbcm', 'callcost', 'ppm', 'mobilesvari', 'max']\n",
      "Tokenized sentence: ['hey', 'i', 'will', 'be', 'late', 'ah', 'meet', 'you', 'at']\n",
      "After stop words removal: ['hey', 'late', 'ah', 'meet']\n",
      "After stemming with porters algorithm: ['hei', 'late', 'meet']\n",
      "Tokenized sentence: ['ok', 'which', 'your', 'another', 'number']\n",
      "After stop words removal: ['ok', 'another', 'number']\n",
      "After stemming with porters algorithm: ['anoth', 'number']\n",
      "Tokenized sentence: ['dunno', 'lei', 'i', 'thk', 'mum', 'lazy', 'to', 'go', 'out', 'i', 'neva', 'ask', 'her', 'yet']\n",
      "After stop words removal: ['dunno', 'lei', 'thk', 'mum', 'lazy', 'go', 'neva', 'ask', 'yet']\n",
      "After stemming with porters algorithm: ['dunno', 'lei', 'thk', 'mum', 'lazi', 'neva', 'ask', 'yet']\n",
      "Tokenized sentence: ['great', 'have', 'a', 'safe', 'trip', 'dont', 'panic', 'surrender', 'all']\n",
      "After stop words removal: ['great', 'safe', 'trip', 'dont', 'panic', 'surrender']\n",
      "After stemming with porters algorithm: ['great', 'safe', 'trip', 'dont', 'panic', 'surrend']\n",
      "Tokenized sentence: ['i', 'gotta', 'collect', 'da', 'car', 'at', 'lei']\n",
      "After stop words removal: ['gotta', 'collect', 'da', 'car', 'lei']\n",
      "After stemming with porters algorithm: ['gotta', 'collect', 'car', 'lei']\n",
      "Tokenized sentence: ['btw', 'regarding', 'that', 'we', 'should', 'really', 'try', 'to', 'see', 'if', 'anyone', 'else', 'can', 'be', 'our', 'th', 'guy', 'before', 'we', 'commit', 'to', 'a', 'random', 'dude']\n",
      "After stop words removal: ['btw', 'regarding', 'really', 'try', 'see', 'anyone', 'else', 'th', 'guy', 'commit', 'random', 'dude']\n",
      "regard\n",
      "After stemming with porters algorithm: ['btw', 'regar', 'realli', 'try', 'see', 'anyon', 'els', 'gui', 'commit', 'random', 'dude']\n",
      "Tokenized sentence: ['yes', 'here', 'tv', 'is', 'always', 'available', 'in', 'work', 'place']\n",
      "After stop words removal: ['yes', 'tv', 'always', 'available', 'work', 'place']\n",
      "After stemming with porters algorithm: ['ye', 'alwai', 'avail', 'work', 'place']\n",
      "Tokenized sentence: ['watching', 'cartoon', 'listening', 'music', 'amp', 'at', 'eve', 'had', 'to', 'go', 'temple', 'amp', 'church', 'what', 'about', 'u']\n",
      "After stop words removal: ['watching', 'cartoon', 'listening', 'music', 'amp', 'eve', 'go', 'temple', 'amp', 'church', 'u']\n",
      "watch\n",
      "listen\n",
      "After stemming with porters algorithm: ['watc', 'cartoon', 'listen', 'music', 'amp', 'ev', 'templ', 'amp', 'church']\n",
      "Tokenized sentence: ['nope', 'wif', 'my', 'sis', 'lor', 'aft', 'bathing', 'my', 'dog', 'then', 'i', 'can', 'bathe', 'looks', 'like', 'it', 's', 'going', 'rain', 'soon']\n",
      "After stop words removal: ['nope', 'wif', 'sis', 'lor', 'aft', 'bathing', 'dog', 'bathe', 'looks', 'like', 'going', 'rain', 'soon']\n",
      "bath\n",
      "go\n",
      "After stemming with porters algorithm: ['nope', 'wif', 'si', 'lor', 'aft', 'bat', 'dog', 'bath', 'look', 'like', 'go', 'rain', 'soon']\n",
      "Tokenized sentence: ['no', 'no', 'i', 'will', 'check', 'all', 'rooms', 'befor', 'activities']\n",
      "After stop words removal: ['check', 'rooms', 'befor', 'activities']\n",
      "After stemming with porters algorithm: ['check', 'room', 'befor', 'activ']\n",
      "Tokenized sentence: ['probably', 'a', 'couple', 'hours', 'tops']\n",
      "After stop words removal: ['probably', 'couple', 'hours', 'tops']\n",
      "After stemming with porters algorithm: ['probab', 'coupl', 'hour', 'top']\n",
      "Tokenized sentence: ['anything', 'lor', 'is', 'she', 'coming']\n",
      "After stop words removal: ['anything', 'lor', 'coming']\n",
      "anyth\n",
      "com\n",
      "After stemming with porters algorithm: ['anyt', 'lor', 'come']\n",
      "Tokenized sentence: ['left', 'dessert', 'u', 'wan', 'me', 'go', 'suntec', 'look', 'u']\n",
      "After stop words removal: ['left', 'dessert', 'u', 'wan', 'go', 'suntec', 'look', 'u']\n",
      "After stemming with porters algorithm: ['left', 'dessert', 'wan', 'suntec', 'look']\n",
      "Tokenized sentence: ['hi', 'ya', 'babe', 'x', 'u', 'goten', 'bout', 'me', 'scammers', 'getting', 'smart', 'though', 'this', 'is', 'a', 'regular', 'vodafone', 'no', 'if', 'you', 'respond', 'you', 'get', 'further', 'prem', 'rate', 'msg', 'subscription', 'other', 'nos', 'used', 'also', 'beware']\n",
      "After stop words removal: ['hi', 'ya', 'babe', 'x', 'u', 'goten', 'bout', 'scammers', 'getting', 'smart', 'though', 'regular', 'vodafone', 'respond', 'get', 'prem', 'rate', 'msg', 'subscription', 'nos', 'used', 'also', 'beware']\n",
      "gett\n",
      "After stemming with porters algorithm: ['babe', 'goten', 'bout', 'scammer', 'get', 'smart', 'though', 'regular', 'vodafon', 'respond', 'get', 'prem', 'rate', 'msg', 'subscript', 'no', 'us', 'also', 'bewar']\n",
      "Tokenized sentence: ['do', 'we', 'have', 'any', 'spare', 'power', 'supplies']\n",
      "After stop words removal: ['spare', 'power', 'supplies']\n",
      "After stemming with porters algorithm: ['spare', 'power', 'suppli']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'i', 'miss', 'you']\n",
      "After stop words removal: ['miss']\n",
      "After stemming with porters algorithm: ['miss']\n",
      "Tokenized sentence: ['yup', 'i', 'shd', 'haf', 'ard', 'pages', 'if', 'i', 'add', 'figures', 'all', 'got', 'how', 'many', 'pages']\n",
      "After stop words removal: ['yup', 'shd', 'haf', 'ard', 'pages', 'add', 'figures', 'got', 'many', 'pages']\n",
      "After stemming with porters algorithm: ['yup', 'shd', 'haf', 'ard', 'page', 'add', 'figur', 'got', 'mani', 'page']\n",
      "Tokenized sentence: ['i', 'had', 'askd', 'u', 'a', 'question', 'some', 'hours', 'before', 'its', 'answer']\n",
      "After stop words removal: ['askd', 'u', 'question', 'hours', 'answer']\n",
      "After stemming with porters algorithm: ['askd', 'quest', 'hour', 'answer']\n",
      "Tokenized sentence: ['you', 'do', 'what', 'all', 'you', 'like']\n",
      "After stop words removal: ['like']\n",
      "After stemming with porters algorithm: ['like']\n",
      "Tokenized sentence: ['horrible', 'bf', 'i', 'now', 'v', 'hungry']\n",
      "After stop words removal: ['horrible', 'bf', 'v', 'hungry']\n",
      "After stemming with porters algorithm: ['horrib', 'hungri']\n",
      "Tokenized sentence: ['think', 'ur', 'smart', 'win', 'this', 'week', 'in', 'our', 'weekly', 'quiz', 'text', 'play', 'to', 'now', 't', 'cs', 'winnersclub', 'po', 'box', 'm', 'uz', 'gbp', 'week']\n",
      "After stop words removal: ['think', 'ur', 'smart', 'win', 'week', 'weekly', 'quiz', 'text', 'play', 'cs', 'winnersclub', 'po', 'box', 'uz', 'gbp', 'week']\n",
      "After stemming with porters algorithm: ['think', 'smart', 'win', 'week', 'weekli', 'quiz', 'text', 'plai', 'winnersclub', 'box', 'gbp', 'week']\n",
      "Tokenized sentence: ['i', 'm', 'fine', 'hope', 'you', 'are', 'good', 'do', 'take', 'care']\n",
      "After stop words removal: ['fine', 'hope', 'good', 'take', 'care']\n",
      "After stemming with porters algorithm: ['fine', 'hope', 'good', 'take', 'care']\n",
      "Tokenized sentence: ['here', 'is', 'your', 'discount', 'code', 'rp', 'to', 'stop', 'further', 'messages', 'reply', 'stop', 'www', 'regalportfolio', 'co', 'uk', 'customer', 'services']\n",
      "After stop words removal: ['discount', 'code', 'rp', 'stop', 'messages', 'reply', 'stop', 'www', 'regalportfolio', 'co', 'uk', 'customer', 'services']\n",
      "After stemming with porters algorithm: ['discount', 'code', 'stop', 'messag', 'repli', 'stop', 'www', 'regalportfolio', 'custom', 'servic']\n",
      "Tokenized sentence: ['can', 'you', 'tell', 'shola', 'to', 'please', 'go', 'to', 'college', 'of', 'medicine', 'and', 'visit', 'the', 'academic', 'department', 'tell', 'the', 'academic', 'secretary', 'what', 'the', 'current', 'situation', 'is', 'and', 'ask', 'if', 'she', 'can', 'transfer', 'there', 'she', 'should', 'ask', 'someone', 'to', 'check', 'sagamu', 'for', 'the', 'same', 'thing', 'and', 'lautech', 'its', 'vital', 'she', 'completes', 'her', 'medical', 'education', 'in', 'nigeria', 'its', 'less', 'expensive', 'much', 'less', 'expensive', 'unless', 'she', 'will', 'be', 'getting', 'citizen', 'rates', 'in', 'new', 'zealand']\n",
      "After stop words removal: ['tell', 'shola', 'please', 'go', 'college', 'medicine', 'visit', 'academic', 'department', 'tell', 'academic', 'secretary', 'current', 'situation', 'ask', 'transfer', 'ask', 'someone', 'check', 'sagamu', 'thing', 'lautech', 'vital', 'completes', 'medical', 'education', 'nigeria', 'less', 'expensive', 'much', 'less', 'expensive', 'unless', 'getting', 'citizen', 'rates', 'new', 'zealand']\n",
      "gett\n",
      "After stemming with porters algorithm: ['tell', 'shola', 'pleas', 'colleg', 'medicin', 'visit', 'academ', 'depart', 'tell', 'academ', 'secretari', 'current', 'situat', 'ask', 'transfer', 'ask', 'someon', 'check', 'sagamu', 'thing', 'lautech', 'vital', 'complet', 'medic', 'educ', 'nigeria', 'less', 'expens', 'much', 'less', 'expens', 'unless', 'get', 'citizen', 'rate', 'new', 'zealand']\n",
      "Tokenized sentence: ['not', 'yet', 'just', 'i', 'd', 'like', 'to', 'keep', 'in', 'touch', 'and', 'it', 'will', 'be', 'the', 'easiest', 'way', 'to', 'do', 'that', 'from', 'barcelona', 'by', 'the', 'way', 'how', 'ru', 'and', 'how', 'is', 'the', 'house']\n",
      "After stop words removal: ['yet', 'like', 'keep', 'touch', 'easiest', 'way', 'barcelona', 'way', 'ru', 'house']\n",
      "After stemming with porters algorithm: ['yet', 'like', 'keep', 'touch', 'easiest', 'wai', 'barcelona', 'wai', 'hous']\n",
      "Tokenized sentence: ['new', 'theory', 'argument', 'wins', 'd', 'situation', 'but', 'loses', 'the', 'person', 'so', 'dont', 'argue', 'with', 'ur', 'friends', 'just', 'kick', 'them', 'amp', 'say', 'i', 'm', 'always', 'correct']\n",
      "After stop words removal: ['new', 'theory', 'argument', 'wins', 'situation', 'loses', 'person', 'dont', 'argue', 'ur', 'friends', 'kick', 'amp', 'say', 'always', 'correct']\n",
      "After stemming with porters algorithm: ['new', 'theori', 'argum', 'win', 'situat', 'lose', 'person', 'dont', 'argu', 'friend', 'kick', 'amp', 'sai', 'alwai', 'correct']\n",
      "Tokenized sentence: ['olol', 'i', 'printed', 'out', 'a', 'forum', 'post', 'by', 'a', 'guy', 'with', 'the', 'exact', 'same', 'prob', 'which', 'was', 'fixed', 'with', 'a', 'gpu', 'replacement', 'hopefully', 'they', 'dont', 'ignore', 'that']\n",
      "After stop words removal: ['olol', 'printed', 'forum', 'post', 'guy', 'exact', 'prob', 'fixed', 'gpu', 'replacement', 'hopefully', 'dont', 'ignore']\n",
      "After stemming with porters algorithm: ['olol', 'prin', 'forum', 'post', 'gui', 'exact', 'prob', 'fix', 'gpu', 'replac', 'hopefulli', 'dont', 'ignor']\n",
      "Tokenized sentence: ['check', 'audrey', 's', 'status', 'right', 'now']\n",
      "After stop words removal: ['check', 'audrey', 'status', 'right']\n",
      "After stemming with porters algorithm: ['check', 'audrei', 'statu', 'right']\n",
      "Tokenized sentence: ['i', 'm', 'aight', 'wat', 's', 'happening', 'on', 'your', 'side']\n",
      "After stop words removal: ['aight', 'wat', 'happening', 'side']\n",
      "happen\n",
      "After stemming with porters algorithm: ['aight', 'wat', 'happen', 'side']\n",
      "Tokenized sentence: ['set', 'a', 'place', 'for', 'me', 'in', 'your', 'heart', 'and', 'not', 'in', 'your', 'mind', 'as', 'the', 'mind', 'easily', 'forgets', 'but', 'the', 'heart', 'will', 'always', 'remember', 'wish', 'you', 'happy', 'valentines', 'day']\n",
      "After stop words removal: ['set', 'place', 'heart', 'mind', 'mind', 'easily', 'forgets', 'heart', 'always', 'remember', 'wish', 'happy', 'valentines', 'day']\n",
      "After stemming with porters algorithm: ['set', 'place', 'heart', 'mind', 'mind', 'easili', 'forget', 'heart', 'alwai', 'rememb', 'wish', 'happi', 'valentin', 'dai']\n",
      "Tokenized sentence: ['none', 'nowhere', 'ikno', 'doesdiscount', 'shitinnit']\n",
      "After stop words removal: ['none', 'nowhere', 'ikno', 'doesdiscount', 'shitinnit']\n",
      "After stemming with porters algorithm: ['none', 'nowher', 'ikno', 'doesdiscount', 'shitinnit']\n",
      "Tokenized sentence: ['and', 'smile', 'for', 'me', 'right', 'now', 'as', 'you', 'go', 'and', 'the', 'world', 'will', 'wonder', 'what', 'you', 'are', 'smiling', 'about', 'and', 'think', 'your', 'crazy', 'and', 'keep', 'away', 'from', 'you', 'grins']\n",
      "After stop words removal: ['smile', 'right', 'go', 'world', 'wonder', 'smiling', 'think', 'crazy', 'keep', 'away', 'grins']\n",
      "smil\n",
      "After stemming with porters algorithm: ['smile', 'right', 'world', 'wonder', 'smile', 'think', 'crazi', 'keep', 'awai', 'grin']\n",
      "Tokenized sentence: ['where', 'at', 'were', 'hungry', 'too']\n",
      "After stop words removal: ['hungry']\n",
      "After stemming with porters algorithm: ['hungri']\n",
      "Tokenized sentence: ['any', 'pain', 'on', 'urination', 'any', 'thing', 'else']\n",
      "After stop words removal: ['pain', 'urination', 'thing', 'else']\n",
      "After stemming with porters algorithm: ['pain', 'urin', 'thing', 'els']\n",
      "Tokenized sentence: ['are', 'you', 'in', 'the', 'pub']\n",
      "After stop words removal: ['pub']\n",
      "After stemming with porters algorithm: ['pub']\n",
      "Tokenized sentence: ['k', 'k', 'its', 'good', 'when', 'are', 'you', 'going']\n",
      "After stop words removal: ['k', 'k', 'good', 'going']\n",
      "go\n",
      "After stemming with porters algorithm: ['good', 'go']\n",
      "Tokenized sentence: ['i', 'cant', 'keep', 'talking', 'to', 'people', 'if', 'am', 'not', 'sure', 'i', 'can', 'pay', 'them', 'if', 'they', 'agree', 'to', 'price', 'so', 'pls', 'tell', 'me', 'what', 'you', 'want', 'to', 'really', 'buy', 'and', 'how', 'much', 'you', 'are', 'willing', 'to', 'pay']\n",
      "After stop words removal: ['cant', 'keep', 'talking', 'people', 'sure', 'pay', 'agree', 'price', 'pls', 'tell', 'want', 'really', 'buy', 'much', 'willing', 'pay']\n",
      "talk\n",
      "will\n",
      "After stemming with porters algorithm: ['cant', 'keep', 'tal', 'peopl', 'sure', 'pai', 'agre', 'price', 'pl', 'tell', 'want', 'realli', 'bui', 'much', 'will', 'pai']\n",
      "Tokenized sentence: ['how', 'u', 'doin', 'baby', 'girl', 'hope', 'u', 'are', 'okay', 'every', 'time', 'i', 'call', 'ure', 'phone', 'is', 'off', 'i', 'miss', 'u', 'get', 'in', 'touch']\n",
      "After stop words removal: ['u', 'doin', 'baby', 'girl', 'hope', 'u', 'okay', 'every', 'time', 'call', 'ure', 'phone', 'miss', 'u', 'get', 'touch']\n",
      "After stemming with porters algorithm: ['doin', 'babi', 'girl', 'hope', 'okai', 'everi', 'time', 'call', 'ur', 'phone', 'miss', 'get', 'touch']\n",
      "Tokenized sentence: ['honestly', 'i', 've', 'just', 'made', 'a', 'lovely', 'cup', 'of', 'tea', 'and', 'promptly', 'dropped', 'my', 'keys', 'in', 'it', 'and', 'then', 'burnt', 'my', 'fingers', 'getting', 'them', 'out']\n",
      "After stop words removal: ['honestly', 'made', 'lovely', 'cup', 'tea', 'promptly', 'dropped', 'keys', 'burnt', 'fingers', 'getting']\n",
      "gett\n",
      "After stemming with porters algorithm: ['honestli', 'made', 'love', 'cup', 'tea', 'promptli', 'drop', 'kei', 'burnt', 'finger', 'get']\n",
      "Tokenized sentence: ['good', 'afternon', 'my', 'love', 'how', 'are', 'today', 'i', 'hope', 'your', 'good', 'and', 'maybe', 'have', 'some', 'interviews', 'i', 'wake', 'and', 'miss', 'you', 'babe', 'a', 'passionate', 'kiss', 'from', 'across', 'the', 'sea']\n",
      "After stop words removal: ['good', 'afternon', 'love', 'today', 'hope', 'good', 'maybe', 'interviews', 'wake', 'miss', 'babe', 'passionate', 'kiss', 'across', 'sea']\n",
      "After stemming with porters algorithm: ['good', 'afternon', 'love', 'todai', 'hope', 'good', 'mayb', 'interview', 'wake', 'miss', 'babe', 'passion', 'kiss', 'across', 'sea']\n",
      "Tokenized sentence: ['how', 'to', 'make', 'a', 'girl', 'happy', 'it', 's', 'not', 'at', 'all', 'difficult', 'to', 'make', 'girls', 'happy', 'u', 'only', 'need', 'to', 'be', 'a', 'friend', 'companion', 'lover', 'chef', 'lt', 'gt', 'good', 'listener', 'lt', 'gt', 'organizer', 'lt', 'gt', 'good', 'boyfriend', 'lt', 'gt', 'very', 'clean', 'lt', 'gt', 'sympathetic', 'lt', 'gt', 'athletic', 'lt', 'gt', 'warm', 'lt', 'gt', 'courageous', 'lt', 'gt', 'determined', 'lt', 'gt', 'true', 'lt', 'gt', 'dependable', 'lt', 'gt', 'intelligent', 'lt', 'gt', 'psychologist', 'lt', 'gt', 'pest', 'exterminator', 'lt', 'gt', 'psychiatrist', 'lt', 'gt', 'healer', 'lt', 'gt', 'stylist', 'lt', 'gt', 'driver', 'aaniye', 'pudunga', 'venaam']\n",
      "After stop words removal: ['make', 'girl', 'happy', 'difficult', 'make', 'girls', 'happy', 'u', 'need', 'friend', 'companion', 'lover', 'chef', 'lt', 'gt', 'good', 'listener', 'lt', 'gt', 'organizer', 'lt', 'gt', 'good', 'boyfriend', 'lt', 'gt', 'clean', 'lt', 'gt', 'sympathetic', 'lt', 'gt', 'athletic', 'lt', 'gt', 'warm', 'lt', 'gt', 'courageous', 'lt', 'gt', 'determined', 'lt', 'gt', 'true', 'lt', 'gt', 'dependable', 'lt', 'gt', 'intelligent', 'lt', 'gt', 'psychologist', 'lt', 'gt', 'pest', 'exterminator', 'lt', 'gt', 'psychiatrist', 'lt', 'gt', 'healer', 'lt', 'gt', 'stylist', 'lt', 'gt', 'driver', 'aaniye', 'pudunga', 'venaam']\n",
      "After stemming with porters algorithm: ['make', 'girl', 'happi', 'difficult', 'make', 'girl', 'happi', 'need', 'friend', 'companion', 'lover', 'chef', 'good', 'listen', 'organiz', 'good', 'boyfriend', 'clean', 'sympathet', 'athlet', 'warm', 'courag', 'determin', 'true', 'depend', 'intellig', 'psychologist', 'pest', 'extermin', 'psychiatrist', 'healer', 'stylist', 'driver', 'aaniy', 'pudunga', 'venaam']\n",
      "Tokenized sentence: ['die', 'now', 'i', 'have', 'e', 'toot', 'fringe', 'again']\n",
      "After stop words removal: ['die', 'e', 'toot', 'fringe']\n",
      "After stemming with porters algorithm: ['die', 'toot', 'fring']\n",
      "Tokenized sentence: ['i', 'didnt', 'get', 'anything', 'da']\n",
      "After stop words removal: ['didnt', 'get', 'anything', 'da']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['didnt', 'get', 'anyt']\n",
      "Tokenized sentence: ['you', 'still', 'at', 'grand', 'prix']\n",
      "After stop words removal: ['still', 'grand', 'prix']\n",
      "After stemming with porters algorithm: ['still', 'grand', 'prix']\n",
      "Tokenized sentence: ['shall', 'i', 'get', 'my', 'pouch']\n",
      "After stop words removal: ['shall', 'get', 'pouch']\n",
      "After stemming with porters algorithm: ['shall', 'get', 'pouch']\n",
      "Tokenized sentence: ['haha', 'yeah', 'i', 'see', 'that', 'now', 'be', 'there', 'in', 'a', 'sec']\n",
      "After stop words removal: ['haha', 'yeah', 'see', 'sec']\n",
      "After stemming with porters algorithm: ['haha', 'yeah', 'see', 'sec']\n",
      "Tokenized sentence: ['friendship', 'poem', 'dear', 'o', 'dear', 'u', 'r', 'not', 'near', 'but', 'i', 'can', 'hear', 'dont', 'get', 'fear', 'live', 'with', 'cheer', 'no', 'more', 'tear', 'u', 'r', 'always', 'my', 'dear', 'gud', 'ni']\n",
      "After stop words removal: ['friendship', 'poem', 'dear', 'dear', 'u', 'r', 'near', 'hear', 'dont', 'get', 'fear', 'live', 'cheer', 'tear', 'u', 'r', 'always', 'dear', 'gud', 'ni']\n",
      "After stemming with porters algorithm: ['friendship', 'poem', 'dear', 'dear', 'near', 'hear', 'dont', 'get', 'fear', 'live', 'cheer', 'tear', 'alwai', 'dear', 'gud']\n",
      "Tokenized sentence: ['can', 'i', 'get', 'your', 'opinion', 'on', 'something', 'first']\n",
      "After stop words removal: ['get', 'opinion', 'something', 'first']\n",
      "someth\n",
      "After stemming with porters algorithm: ['get', 'opinion', 'somet', 'first']\n",
      "Tokenized sentence: ['or', 'u', 'ask', 'they', 'all', 'if', 'next', 'sat', 'can', 'a', 'not', 'if', 'all', 'of', 'them', 'can', 'make', 'it', 'then', 'i', 'm', 'ok', 'lor']\n",
      "After stop words removal: ['u', 'ask', 'next', 'sat', 'make', 'ok', 'lor']\n",
      "After stemming with porters algorithm: ['ask', 'next', 'sat', 'make', 'lor']\n",
      "Tokenized sentence: ['this', 'weekend', 'is', 'fine', 'an', 'excuse', 'not', 'to', 'do', 'too', 'much', 'decorating']\n",
      "After stop words removal: ['weekend', 'fine', 'excuse', 'much', 'decorating']\n",
      "decorat\n",
      "decorate\n",
      "After stemming with porters algorithm: ['weekend', 'fine', 'excus', 'much', 'decor']\n",
      "Tokenized sentence: ['i', 've', 'got', 'lt', 'gt', 'any', 'way', 'i', 'could', 'pick', 'up']\n",
      "After stop words removal: ['got', 'lt', 'gt', 'way', 'could', 'pick']\n",
      "After stemming with porters algorithm: ['got', 'wai', 'could', 'pick']\n",
      "Tokenized sentence: ['congratulations', 'ur', 'awarded', 'either', 'of', 'cd', 'gift', 'vouchers', 'free', 'entry', 'our', 'weekly', 'draw', 'txt', 'music', 'to', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stop words removal: ['congratulations', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'weekly', 'draw', 'txt', 'music', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stemming with porters algorithm: ['congratul', 'awar', 'either', 'gift', 'voucher', 'free', 'entri', 'weekli', 'draw', 'txt', 'music', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'ag']\n",
      "Tokenized sentence: ['studying', 'but', 'i', 'll', 'be', 'free', 'next', 'weekend']\n",
      "After stop words removal: ['studying', 'free', 'next', 'weekend']\n",
      "study\n",
      "After stemming with porters algorithm: ['stud', 'free', 'next', 'weekend']\n",
      "Tokenized sentence: ['well', 'that', 'must', 'be', 'a', 'pain', 'to', 'catch']\n",
      "After stop words removal: ['well', 'must', 'pain', 'catch']\n",
      "After stemming with porters algorithm: ['well', 'must', 'pain', 'catch']\n",
      "Tokenized sentence: ['why', 'do', 'you', 'ask', 'princess']\n",
      "After stop words removal: ['ask', 'princess']\n",
      "After stemming with porters algorithm: ['ask', 'princess']\n",
      "Tokenized sentence: ['ur', 'tonexs', 'subscription', 'has', 'been', 'renewed', 'and', 'you', 'have', 'been', 'charged', 'you', 'can', 'choose', 'more', 'polys', 'this', 'month', 'www', 'clubzed', 'co', 'uk', 'billing', 'msg']\n",
      "After stop words removal: ['ur', 'tonexs', 'subscription', 'renewed', 'charged', 'choose', 'polys', 'month', 'www', 'clubzed', 'co', 'uk', 'billing', 'msg']\n",
      "bill\n",
      "After stemming with porters algorithm: ['tonex', 'subscript', 'renew', 'char', 'choos', 'poli', 'month', 'www', 'clubz', 'bill', 'msg']\n",
      "Tokenized sentence: ['tell', 'them', 'the', 'drug', 'dealer', 's', 'getting', 'impatient']\n",
      "After stop words removal: ['tell', 'drug', 'dealer', 'getting', 'impatient']\n",
      "gett\n",
      "After stemming with porters algorithm: ['tell', 'drug', 'dealer', 'get', 'impati']\n",
      "Tokenized sentence: ['got', 'what', 'it', 'takes', 'take', 'part', 'in', 'the', 'wrc', 'rally', 'in', 'oz', 'u', 'can', 'with', 'lucozade', 'energy', 'text', 'rally', 'le', 'to', 'p', 'see', 'packs', 'or', 'lucozade', 'co', 'uk', 'wrc', 'itcould', 'be', 'u']\n",
      "After stop words removal: ['got', 'takes', 'take', 'part', 'wrc', 'rally', 'oz', 'u', 'lucozade', 'energy', 'text', 'rally', 'le', 'p', 'see', 'packs', 'lucozade', 'co', 'uk', 'wrc', 'itcould', 'u']\n",
      "After stemming with porters algorithm: ['got', 'take', 'take', 'part', 'wrc', 'ralli', 'lucozad', 'energi', 'text', 'ralli', 'see', 'pack', 'lucozad', 'wrc', 'itcould']\n",
      "Tokenized sentence: ['my', 'life', 'means', 'a', 'lot', 'to', 'me', 'not', 'because', 'i', 'love', 'my', 'life', 'but', 'because', 'i', 'love', 'the', 'people', 'in', 'my', 'life', 'the', 'world', 'calls', 'them', 'friends', 'i', 'call', 'them', 'my', 'world', 'ge']\n",
      "After stop words removal: ['life', 'means', 'lot', 'love', 'life', 'love', 'people', 'life', 'world', 'calls', 'friends', 'call', 'world', 'ge']\n",
      "After stemming with porters algorithm: ['life', 'mean', 'lot', 'love', 'life', 'love', 'peopl', 'life', 'world', 'call', 'friend', 'call', 'world']\n",
      "Tokenized sentence: ['piss', 'is', 'talking', 'is', 'someone', 'that', 'realise', 'u', 'that', 'point', 'this', 'at', 'is', 'it', 'now', 'read', 'it', 'backwards']\n",
      "After stop words removal: ['piss', 'talking', 'someone', 'realise', 'u', 'point', 'read', 'backwards']\n",
      "talk\n",
      "After stemming with porters algorithm: ['piss', 'tal', 'someon', 'realis', 'point', 'read', 'backward']\n",
      "Tokenized sentence: ['alright', 'we', 're', 'hooked', 'up', 'where', 'you', 'guys', 'at']\n",
      "After stop words removal: ['alright', 'hooked', 'guys']\n",
      "After stemming with porters algorithm: ['alright', 'hook', 'gui']\n",
      "Tokenized sentence: ['then', 'any', 'special', 'there']\n",
      "After stop words removal: ['special']\n",
      "After stemming with porters algorithm: ['special']\n",
      "Tokenized sentence: ['gam', 'gone', 'after', 'outstanding', 'innings']\n",
      "After stop words removal: ['gam', 'gone', 'outstanding', 'innings']\n",
      "outstand\n",
      "inn\n",
      "After stemming with porters algorithm: ['gam', 'gone', 'outstan', 'in']\n",
      "Tokenized sentence: ['can', 'meh', 'thgt', 'some', 'will', 'clash', 'really', 'ah', 'i', 'dun', 'mind', 'i', 'dun', 'seen', 'to', 'have', 'lost', 'any', 'weight', 'gee']\n",
      "After stop words removal: ['meh', 'thgt', 'clash', 'really', 'ah', 'dun', 'mind', 'dun', 'seen', 'lost', 'weight', 'gee']\n",
      "After stemming with porters algorithm: ['meh', 'thgt', 'clash', 'realli', 'dun', 'mind', 'dun', 'seen', 'lost', 'weight', 'gee']\n",
      "Tokenized sentence: ['had', 'your', 'mobile', 'months', 'or', 'more', 'u', 'r', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'mobiles', 'with', 'camera', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']\n",
      "After stop words removal: ['mobile', 'months', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobiles', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
      "After stemming with porters algorithm: ['mobil', 'month', 'entit', 'updat', 'latest', 'colour', 'mobil', 'camera', 'free', 'call', 'mobil', 'updat', 'free']\n",
      "Tokenized sentence: ['we', 'will', 'meet', 'soon', 'princess', 'ttyl']\n",
      "After stop words removal: ['meet', 'soon', 'princess', 'ttyl']\n",
      "After stemming with porters algorithm: ['meet', 'soon', 'princess', 'ttyl']\n",
      "Tokenized sentence: ['ill', 'be', 'there', 'on', 'lt', 'gt', 'ok']\n",
      "After stop words removal: ['ill', 'lt', 'gt', 'ok']\n",
      "After stemming with porters algorithm: ['ill']\n",
      "Tokenized sentence: ['no', 'but', 'heard', 'abt', 'tat']\n",
      "After stop words removal: ['heard', 'abt', 'tat']\n",
      "After stemming with porters algorithm: ['heard', 'abt', 'tat']\n",
      "Tokenized sentence: ['hellogorgeous', 'hows', 'u', 'my', 'fone', 'was', 'on', 'charge', 'lst', 'nitw', 'wen', 'u', 'texd', 'me', 'hopeu', 'ad', 'a', 'nice', 'wkend', 'as', 'im', 'sure', 'u', 'did', 'lookin', 'ward', 'c', 'in', 'u', 'mrw', 'luv', 'jaz']\n",
      "After stop words removal: ['hellogorgeous', 'hows', 'u', 'fone', 'charge', 'lst', 'nitw', 'wen', 'u', 'texd', 'hopeu', 'ad', 'nice', 'wkend', 'im', 'sure', 'u', 'lookin', 'ward', 'c', 'u', 'mrw', 'luv', 'jaz']\n",
      "After stemming with porters algorithm: ['hellogorg', 'how', 'fone', 'charg', 'lst', 'nitw', 'wen', 'texd', 'hopeu', 'nice', 'wkend', 'sure', 'lookin', 'ward', 'mrw', 'luv', 'jaz']\n",
      "Tokenized sentence: ['good', 'morning', 'princess', 'have', 'a', 'great', 'day']\n",
      "After stop words removal: ['good', 'morning', 'princess', 'great', 'day']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'princess', 'great', 'dai']\n",
      "Tokenized sentence: ['will', 'purchase', 'd', 'stuff', 'today', 'and', 'mail', 'to', 'you', 'do', 'you', 'have', 'a', 'po', 'box', 'number']\n",
      "After stop words removal: ['purchase', 'stuff', 'today', 'mail', 'po', 'box', 'number']\n",
      "After stemming with porters algorithm: ['purchas', 'stuff', 'todai', 'mail', 'box', 'number']\n",
      "Tokenized sentence: ['when', 'people', 'see', 'my', 'msgs', 'they', 'think', 'iam', 'addicted', 'to', 'msging', 'they', 'are', 'wrong', 'bcoz', 'they', 'don', 't', 'know', 'that', 'iam', 'addicted', 'to', 'my', 'sweet', 'friends', 'bslvyl']\n",
      "After stop words removal: ['people', 'see', 'msgs', 'think', 'iam', 'addicted', 'msging', 'wrong', 'bcoz', 'know', 'iam', 'addicted', 'sweet', 'friends', 'bslvyl']\n",
      "After stemming with porters algorithm: ['peopl', 'see', 'msg', 'think', 'iam', 'addic', 'msging', 'wrong', 'bcoz', 'know', 'iam', 'addic', 'sweet', 'friend', 'bslvyl']\n",
      "Tokenized sentence: ['how', 'much', 'it', 'will', 'cost', 'approx', 'per', 'month']\n",
      "After stop words removal: ['much', 'cost', 'approx', 'per', 'month']\n",
      "After stemming with porters algorithm: ['much', 'cost', 'approx', 'per', 'month']\n",
      "Tokenized sentence: ['loan', 'for', 'any', 'purpose', 'homeowners', 'tenants', 'welcome', 'have', 'you', 'been', 'previously', 'refused', 'we', 'can', 'still', 'help', 'call', 'free', 'or', 'text', 'back', 'help']\n",
      "After stop words removal: ['loan', 'purpose', 'homeowners', 'tenants', 'welcome', 'previously', 'refused', 'still', 'help', 'call', 'free', 'text', 'back', 'help']\n",
      "After stemming with porters algorithm: ['loan', 'purpos', 'homeown', 'tenant', 'welcom', 'previous', 'refus', 'still', 'help', 'call', 'free', 'text', 'back', 'help']\n",
      "Tokenized sentence: ['good', 'stuff', 'will', 'do']\n",
      "After stop words removal: ['good', 'stuff']\n",
      "After stemming with porters algorithm: ['good', 'stuff']\n",
      "Tokenized sentence: ['haha', 'that', 'was', 'the', 'first', 'person', 'i', 'was', 'gonna', 'ask']\n",
      "After stop words removal: ['haha', 'first', 'person', 'gonna', 'ask']\n",
      "After stemming with porters algorithm: ['haha', 'first', 'person', 'gonna', 'ask']\n",
      "Tokenized sentence: ['i', 'dont', 'have', 'any', 'of', 'your', 'file', 'in', 'my', 'bag', 'i', 'was', 'in', 'work', 'when', 'you', 'called', 'me', 'i', 'll', 'tell', 'you', 'if', 'i', 'find', 'anything', 'in', 'my', 'room']\n",
      "After stop words removal: ['dont', 'file', 'bag', 'work', 'called', 'tell', 'find', 'anything', 'room']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['dont', 'file', 'bag', 'work', 'call', 'tell', 'find', 'anyt', 'room']\n",
      "Tokenized sentence: ['you', 'have', 'new', 'message', 'please', 'call']\n",
      "After stop words removal: ['new', 'message', 'please', 'call']\n",
      "After stemming with porters algorithm: ['new', 'messag', 'pleas', 'call']\n",
      "Tokenized sentence: ['hello', 'how', 'u', 'doing', 'what', 'u', 'been', 'up', 'when', 'will', 'u', 'b', 'moving', 'out', 'of', 'the', 'flat', 'cos', 'i', 'will', 'need', 'to', 'arrange', 'to', 'pick', 'up', 'the', 'lamp', 'etc', 'take', 'care', 'hello', 'caroline']\n",
      "After stop words removal: ['hello', 'u', 'u', 'u', 'b', 'moving', 'flat', 'cos', 'need', 'arrange', 'pick', 'lamp', 'etc', 'take', 'care', 'hello', 'caroline']\n",
      "mov\n",
      "After stemming with porters algorithm: ['hello', 'move', 'flat', 'co', 'need', 'arrang', 'pick', 'lamp', 'etc', 'take', 'care', 'hello', 'carolin']\n",
      "Tokenized sentence: ['i', 'borrow', 'ur', 'bag', 'ok']\n",
      "After stop words removal: ['borrow', 'ur', 'bag', 'ok']\n",
      "After stemming with porters algorithm: ['borrow', 'bag']\n",
      "Tokenized sentence: ['for', 'fear', 'of', 'fainting', 'with', 'the', 'of', 'all', 'that', 'housework', 'you', 'just', 'did', 'quick', 'have', 'a', 'cuppa']\n",
      "After stop words removal: ['fear', 'fainting', 'housework', 'quick', 'cuppa']\n",
      "faint\n",
      "After stemming with porters algorithm: ['fear', 'fain', 'housework', 'quick', 'cuppa']\n",
      "Tokenized sentence: ['how', 'many', 'licks', 'does', 'it', 'take', 'to', 'get', 'to', 'the', 'center', 'of', 'a', 'tootsie', 'pop']\n",
      "After stop words removal: ['many', 'licks', 'take', 'get', 'center', 'tootsie', 'pop']\n",
      "After stemming with porters algorithm: ['mani', 'lick', 'take', 'get', 'center', 'tootsi', 'pop']\n",
      "Tokenized sentence: ['had', 'your', 'mobile', 'mths', 'update', 'for', 'free', 'to', 'oranges', 'latest', 'colour', 'camera', 'mobiles', 'unlimited', 'weekend', 'calls', 'call', 'mobile', 'upd', 'on', 'freefone', 'or', 'stoptx']\n",
      "After stop words removal: ['mobile', 'mths', 'update', 'free', 'oranges', 'latest', 'colour', 'camera', 'mobiles', 'unlimited', 'weekend', 'calls', 'call', 'mobile', 'upd', 'freefone', 'stoptx']\n",
      "After stemming with porters algorithm: ['mobil', 'mth', 'updat', 'free', 'orang', 'latest', 'colour', 'camera', 'mobil', 'unlimit', 'weekend', 'call', 'call', 'mobil', 'upd', 'freefon', 'stoptx']\n",
      "Tokenized sentence: ['what', 'not', 'under', 'standing']\n",
      "After stop words removal: ['standing']\n",
      "stand\n",
      "After stemming with porters algorithm: ['stan']\n",
      "Tokenized sentence: ['i', 'can', 'make', 'it', 'up', 'there', 'squeezed', 'lt', 'gt', 'bucks', 'out', 'of', 'my', 'dad']\n",
      "After stop words removal: ['make', 'squeezed', 'lt', 'gt', 'bucks', 'dad']\n",
      "After stemming with porters algorithm: ['make', 'squeez', 'buck', 'dad']\n",
      "Tokenized sentence: ['am', 'in', 'film', 'ill', 'call', 'you', 'later']\n",
      "After stop words removal: ['film', 'ill', 'call', 'later']\n",
      "After stemming with porters algorithm: ['film', 'ill', 'call', 'later']\n",
      "Tokenized sentence: ['hello', 'madam', 'how', 'are', 'you']\n",
      "After stop words removal: ['hello', 'madam']\n",
      "After stemming with porters algorithm: ['hello', 'madam']\n",
      "Tokenized sentence: ['msgs', 'r', 'not', 'time', 'pass', 'they', 'silently', 'say', 'that', 'i', 'am', 'thinking', 'of', 'u', 'right', 'now', 'and', 'also', 'making', 'u', 'think', 'of', 'me', 'at', 'least', 'a', 'moment', 'gd', 'nt', 'swt', 'drms', 'shesil']\n",
      "After stop words removal: ['msgs', 'r', 'time', 'pass', 'silently', 'say', 'thinking', 'u', 'right', 'also', 'making', 'u', 'think', 'least', 'moment', 'gd', 'nt', 'swt', 'drms', 'shesil']\n",
      "think\n",
      "mak\n",
      "After stemming with porters algorithm: ['msg', 'time', 'pass', 'silent', 'sai', 'thin', 'right', 'also', 'make', 'think', 'least', 'moment', 'swt', 'drm', 'shesil']\n",
      "Tokenized sentence: ['night', 'night', 'see', 'you', 'tomorrow']\n",
      "After stop words removal: ['night', 'night', 'see', 'tomorrow']\n",
      "After stemming with porters algorithm: ['night', 'night', 'see', 'tomorrow']\n",
      "Tokenized sentence: ['somewhere', 'out', 'there', 'beneath', 'the', 'pale', 'moon', 'light', 'someone', 'think', 'in', 'of', 'u', 'some', 'where', 'out', 'there', 'where', 'dreams', 'come', 'true', 'goodnite', 'amp', 'sweet', 'dreams']\n",
      "After stop words removal: ['somewhere', 'beneath', 'pale', 'moon', 'light', 'someone', 'think', 'u', 'dreams', 'come', 'true', 'goodnite', 'amp', 'sweet', 'dreams']\n",
      "After stemming with porters algorithm: ['somewher', 'beneath', 'pale', 'moon', 'light', 'someon', 'think', 'dream', 'come', 'true', 'goodnit', 'amp', 'sweet', 'dream']\n",
      "Tokenized sentence: ['will', 'b', 'going', 'to', 'esplanade', 'fr', 'home']\n",
      "After stop words removal: ['b', 'going', 'esplanade', 'fr', 'home']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'esplanad', 'home']\n",
      "Tokenized sentence: ['it', 'didnt', 'work', 'again', 'oh', 'ok', 'goodnight', 'then', 'i', 'll', 'fix', 'and', 'have', 'it', 'ready', 'by', 'the', 'time', 'you', 'wake', 'up', 'you', 'are', 'very', 'dearly', 'missed', 'have', 'a', 'good', 'night', 'sleep']\n",
      "After stop words removal: ['didnt', 'work', 'oh', 'ok', 'goodnight', 'fix', 'ready', 'time', 'wake', 'dearly', 'missed', 'good', 'night', 'sleep']\n",
      "After stemming with porters algorithm: ['didnt', 'work', 'goodnight', 'fix', 'readi', 'time', 'wake', 'dearli', 'miss', 'good', 'night', 'sleep']\n",
      "Tokenized sentence: ['you', 'have', 'come', 'into', 'my', 'life', 'and', 'brought', 'the', 'sun', 'shiny', 'down', 'on', 'me', 'warming', 'my', 'heart', 'putting', 'a', 'constant', 'smile', 'on', 'my', 'face', 'making', 'me', 'feel', 'loved', 'and', 'cared', 'for']\n",
      "After stop words removal: ['come', 'life', 'brought', 'sun', 'shiny', 'warming', 'heart', 'putting', 'constant', 'smile', 'face', 'making', 'feel', 'loved', 'cared']\n",
      "warm\n",
      "putt\n",
      "mak\n",
      "After stemming with porters algorithm: ['come', 'life', 'brought', 'sun', 'shini', 'war', 'heart', 'put', 'constant', 'smile', 'face', 'make', 'feel', 'love', 'care']\n",
      "Tokenized sentence: ['tick', 'tick', 'tick', 'where', 'are', 'you', 'i', 'could', 'die', 'of', 'loneliness', 'you', 'know', 'pouts', 'stomps', 'feet', 'i', 'need', 'you']\n",
      "After stop words removal: ['tick', 'tick', 'tick', 'could', 'die', 'loneliness', 'know', 'pouts', 'stomps', 'feet', 'need']\n",
      "After stemming with porters algorithm: ['tick', 'tick', 'tick', 'could', 'die', 'loneli', 'know', 'pout', 'stomp', 'feet', 'need']\n",
      "Tokenized sentence: ['i', 'wont', 'get', 'concentration', 'dear', 'you', 'know', 'you', 'are', 'my', 'mind', 'and', 'everything']\n",
      "After stop words removal: ['wont', 'get', 'concentration', 'dear', 'know', 'mind', 'everything']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['wont', 'get', 'concentr', 'dear', 'know', 'mind', 'everyt']\n",
      "Tokenized sentence: ['if', 'i', 'get', 'there', 'before', 'you', 'after', 'your', 'ten', 'billion', 'calls', 'and', 'texts', 'so', 'help', 'me', 'god']\n",
      "After stop words removal: ['get', 'ten', 'billion', 'calls', 'texts', 'help', 'god']\n",
      "After stemming with porters algorithm: ['get', 'ten', 'billion', 'call', 'text', 'help', 'god']\n",
      "Tokenized sentence: ['haha', 'awesome', 'i', 've', 'been', 'to', 'u', 'a', 'couple', 'times', 'who', 'all', 's', 'coming']\n",
      "After stop words removal: ['haha', 'awesome', 'u', 'couple', 'times', 'coming']\n",
      "com\n",
      "After stemming with porters algorithm: ['haha', 'awesom', 'coupl', 'time', 'come']\n",
      "Tokenized sentence: ['dun', 'wan', 'to', 'watch', 'infernal', 'affair']\n",
      "After stop words removal: ['dun', 'wan', 'watch', 'infernal', 'affair']\n",
      "After stemming with porters algorithm: ['dun', 'wan', 'watch', 'infern', 'affair']\n",
      "Tokenized sentence: ['dude', 'we', 'should', 'go', 'sup', 'again']\n",
      "After stop words removal: ['dude', 'go', 'sup']\n",
      "After stemming with porters algorithm: ['dude', 'sup']\n",
      "Tokenized sentence: ['she', 'ran', 'off', 'with', 'a', 'younger', 'man', 'we', 'will', 'make', 'pretty', 'babies', 'together']\n",
      "After stop words removal: ['ran', 'younger', 'man', 'make', 'pretty', 'babies', 'together']\n",
      "After stemming with porters algorithm: ['ran', 'younger', 'man', 'make', 'pretti', 'babi', 'togeth']\n",
      "Tokenized sentence: ['nope', 'c', 'then']\n",
      "After stop words removal: ['nope', 'c']\n",
      "After stemming with porters algorithm: ['nope']\n",
      "Tokenized sentence: ['sorry', 'da', 'i', 'was', 'thought', 'of', 'calling', 'you', 'lot', 'of', 'times', 'lil', 'busy', 'i', 'will', 'call', 'you', 'at', 'noon']\n",
      "After stop words removal: ['sorry', 'da', 'thought', 'calling', 'lot', 'times', 'lil', 'busy', 'call', 'noon']\n",
      "call\n",
      "After stemming with porters algorithm: ['sorri', 'thought', 'call', 'lot', 'time', 'lil', 'busi', 'call', 'noon']\n",
      "Tokenized sentence: ['ok', 'i', 'msg', 'u', 'b', 'i', 'leave', 'my', 'house']\n",
      "After stop words removal: ['ok', 'msg', 'u', 'b', 'leave', 'house']\n",
      "After stemming with porters algorithm: ['msg', 'leav', 'hous']\n",
      "Tokenized sentence: ['you', 're', 'not', 'sure', 'that', 'i', 'm', 'not', 'trying', 'to', 'make', 'xavier', 'smoke', 'because', 'i', 'don', 't', 'want', 'to', 'smoke', 'after', 'being', 'told', 'i', 'smoke', 'too', 'much']\n",
      "After stop words removal: ['sure', 'trying', 'make', 'xavier', 'smoke', 'want', 'smoke', 'told', 'smoke', 'much']\n",
      "After stemming with porters algorithm: ['sure', 'trying', 'make', 'xavier', 'smoke', 'want', 'smoke', 'told', 'smoke', 'much']\n",
      "Tokenized sentence: ['hiya', 'hows', 'it', 'going', 'in', 'sunny', 'africa', 'hope', 'u', 'r', 'avin', 'a', 'good', 'time', 'give', 'that', 'big', 'old', 'silver', 'back', 'a', 'big', 'kiss', 'from', 'me']\n",
      "After stop words removal: ['hiya', 'hows', 'going', 'sunny', 'africa', 'hope', 'u', 'r', 'avin', 'good', 'time', 'give', 'big', 'old', 'silver', 'back', 'big', 'kiss']\n",
      "go\n",
      "After stemming with porters algorithm: ['hiya', 'how', 'go', 'sunni', 'africa', 'hope', 'avin', 'good', 'time', 'give', 'big', 'old', 'silver', 'back', 'big', 'kiss']\n",
      "Tokenized sentence: ['december', 'only', 'had', 'your', 'mobile', 'mths', 'you', 'are', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'camera', 'mobile', 'for', 'free', 'call', 'the', 'mobile', 'update', 'vco', 'free', 'on']\n",
      "After stop words removal: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'vco', 'free']\n",
      "After stemming with porters algorithm: ['decemb', 'mobil', 'mth', 'entit', 'updat', 'latest', 'colour', 'camera', 'mobil', 'free', 'call', 'mobil', 'updat', 'vco', 'free']\n",
      "Tokenized sentence: ['mode', 'men', 'or', 'have', 'you', 'left']\n",
      "After stop words removal: ['mode', 'men', 'left']\n",
      "After stemming with porters algorithm: ['mode', 'men', 'left']\n",
      "Tokenized sentence: ['really', 'i', 'tot', 'ur', 'paper', 'ended', 'long', 'ago', 'but', 'wat', 'u', 'copied', 'jus', 'now', 'got', 'use', 'u', 'happy', 'lar', 'i', 'still', 'haf', 'study']\n",
      "After stop words removal: ['really', 'tot', 'ur', 'paper', 'ended', 'long', 'ago', 'wat', 'u', 'copied', 'jus', 'got', 'use', 'u', 'happy', 'lar', 'still', 'haf', 'study']\n",
      "After stemming with porters algorithm: ['realli', 'tot', 'paper', 'en', 'long', 'ago', 'wat', 'copi', 'ju', 'got', 'us', 'happi', 'lar', 'still', 'haf', 'studi']\n",
      "Tokenized sentence: ['yup']\n",
      "After stop words removal: ['yup']\n",
      "After stemming with porters algorithm: ['yup']\n",
      "Tokenized sentence: ['tomarrow', 'final', 'hearing', 'on', 'my', 'laptop', 'case', 'so', 'i', 'cant']\n",
      "After stop words removal: ['tomarrow', 'final', 'hearing', 'laptop', 'case', 'cant']\n",
      "hear\n",
      "After stemming with porters algorithm: ['tomarrow', 'final', 'hear', 'laptop', 'case', 'cant']\n",
      "Tokenized sentence: ['did', 'you', 'say', 'bold', 'then', 'torch', 'later', 'or', 'one', 'torch', 'and', 'bold']\n",
      "After stop words removal: ['say', 'bold', 'torch', 'later', 'one', 'torch', 'bold']\n",
      "After stemming with porters algorithm: ['sai', 'bold', 'torch', 'later', 'on', 'torch', 'bold']\n",
      "Tokenized sentence: ['forgot', 'to', 'tell', 'smth', 'can', 'like', 'number', 'the', 'sections', 'so', 'that', 'it', 's', 'clearer']\n",
      "After stop words removal: ['forgot', 'tell', 'smth', 'like', 'number', 'sections', 'clearer']\n",
      "After stemming with porters algorithm: ['forgot', 'tell', 'smth', 'like', 'number', 'sect', 'clearer']\n",
      "Tokenized sentence: ['i', 'got', 'to', 'video', 'tape', 'pple', 'type', 'in', 'message', 'lor', 'u', 'so', 'free', 'wan', 'help', 'me', 'hee', 'cos', 'i', 'noe', 'u', 'wan', 'watch', 'infernal', 'affairs', 'so', 'ask', 'u', 'along', 'asking', 'shuhui', 'oso']\n",
      "After stop words removal: ['got', 'video', 'tape', 'pple', 'type', 'message', 'lor', 'u', 'free', 'wan', 'help', 'hee', 'cos', 'noe', 'u', 'wan', 'watch', 'infernal', 'affairs', 'ask', 'u', 'along', 'asking', 'shuhui', 'oso']\n",
      "ask\n",
      "After stemming with porters algorithm: ['got', 'video', 'tape', 'pple', 'type', 'messag', 'lor', 'free', 'wan', 'help', 'hee', 'co', 'noe', 'wan', 'watch', 'infern', 'affair', 'ask', 'along', 'as', 'shuhui', 'oso']\n",
      "Tokenized sentence: ['night', 'has', 'ended', 'for', 'another', 'day', 'morning', 'has', 'come', 'in', 'a', 'special', 'way', 'may', 'you', 'smile', 'like', 'the', 'sunny', 'rays', 'and', 'leaves', 'your', 'worries', 'at', 'the', 'blue', 'blue', 'bay', 'gud', 'mrng']\n",
      "After stop words removal: ['night', 'ended', 'another', 'day', 'morning', 'come', 'special', 'way', 'may', 'smile', 'like', 'sunny', 'rays', 'leaves', 'worries', 'blue', 'blue', 'bay', 'gud', 'mrng']\n",
      "morn\n",
      "After stemming with porters algorithm: ['night', 'en', 'anoth', 'dai', 'mor', 'come', 'special', 'wai', 'mai', 'smile', 'like', 'sunni', 'rai', 'leav', 'worri', 'blue', 'blue', 'bai', 'gud', 'mrng']\n",
      "Tokenized sentence: ['i', 'don', 't', 'think', 'so', 'you', 'don', 't', 'need', 'to', 'be', 'going', 'out', 'that', 'late', 'on', 'a', 'school', 'night', 'especially', 'when', 'the', 'one', 'class', 'you', 'have', 'is', 'the', 'one', 'you', 'missed', 'last', 'wednesday', 'and', 'probably', 'failed', 'a', 'test', 'in', 'on', 'friday']\n",
      "After stop words removal: ['think', 'need', 'going', 'late', 'school', 'night', 'especially', 'one', 'class', 'one', 'missed', 'last', 'wednesday', 'probably', 'failed', 'test', 'friday']\n",
      "go\n",
      "After stemming with porters algorithm: ['think', 'need', 'go', 'late', 'school', 'night', 'especi', 'on', 'class', 'on', 'miss', 'last', 'wednesdai', 'probab', 'fail', 'test', 'fridai']\n",
      "Tokenized sentence: ['come', 'to', 'me', 'slave', 'your', 'doing', 'it', 'again', 'going', 'into', 'your', 'shell', 'and', 'unconsciously', 'avoiding', 'me', 'you', 'are', 'making', 'me', 'unhappy']\n",
      "After stop words removal: ['come', 'slave', 'going', 'shell', 'unconsciously', 'avoiding', 'making', 'unhappy']\n",
      "go\n",
      "avoid\n",
      "mak\n",
      "After stemming with porters algorithm: ['come', 'slave', 'go', 'shell', 'unconsci', 'avoid', 'make', 'unhappi']\n",
      "Tokenized sentence: ['ding', 'me', 'on', 'ya', 'break', 'fassyole', 'blacko', 'from', 'londn']\n",
      "After stop words removal: ['ding', 'ya', 'break', 'fassyole', 'blacko', 'londn']\n",
      "After stemming with porters algorithm: ['ding', 'break', 'fassyol', 'blacko', 'londn']\n",
      "Tokenized sentence: ['i', 'll', 'be', 'at', 'mu', 'in', 'like', 'lt', 'gt', 'seconds']\n",
      "After stop words removal: ['mu', 'like', 'lt', 'gt', 'seconds']\n",
      "After stemming with porters algorithm: ['like', 'second']\n",
      "Tokenized sentence: ['all', 'day', 'working', 'day', 'except', 'saturday', 'and', 'sunday']\n",
      "After stop words removal: ['day', 'working', 'day', 'except', 'saturday', 'sunday']\n",
      "work\n",
      "After stemming with porters algorithm: ['dai', 'wor', 'dai', 'except', 'saturdai', 'sundai']\n",
      "Tokenized sentence: ['after', 'my', 'work', 'ah', 'den', 'plus', 'lor', 'u', 'workin', 'oso', 'rite', 'den', 'go', 'orchard', 'lor', 'no', 'other', 'place', 'to', 'go', 'liao']\n",
      "After stop words removal: ['work', 'ah', 'den', 'plus', 'lor', 'u', 'workin', 'oso', 'rite', 'den', 'go', 'orchard', 'lor', 'place', 'go', 'liao']\n",
      "After stemming with porters algorithm: ['work', 'den', 'plu', 'lor', 'workin', 'oso', 'rite', 'den', 'orchard', 'lor', 'place', 'liao']\n",
      "Tokenized sentence: ['will', 'it', 'help', 'if', 'we', 'propose', 'going', 'back', 'again', 'tomorrow']\n",
      "After stop words removal: ['help', 'propose', 'going', 'back', 'tomorrow']\n",
      "go\n",
      "After stemming with porters algorithm: ['help', 'propos', 'go', 'back', 'tomorrow']\n",
      "Tokenized sentence: ['have', 'you', 'seen', 'who', 's', 'back', 'at', 'holby']\n",
      "After stop words removal: ['seen', 'back', 'holby']\n",
      "After stemming with porters algorithm: ['seen', 'back', 'holbi']\n",
      "Tokenized sentence: ['k', 'all', 'the', 'best', 'congrats']\n",
      "After stop words removal: ['k', 'best', 'congrats']\n",
      "After stemming with porters algorithm: ['best', 'congrat']\n",
      "Tokenized sentence: ['motivate', 'behind', 'every', 'darkness', 'there', 'is', 'a', 'shining', 'light', 'waiting', 'for', 'you', 'to', 'find', 'it', 'behind', 'every', 'best', 'friend', 'there', 'is', 'always', 'trust', 'and', 'love', 'bslvyl']\n",
      "After stop words removal: ['motivate', 'behind', 'every', 'darkness', 'shining', 'light', 'waiting', 'find', 'behind', 'every', 'best', 'friend', 'always', 'trust', 'love', 'bslvyl']\n",
      "shin\n",
      "wait\n",
      "After stemming with porters algorithm: ['motiv', 'behind', 'everi', 'dark', 'shine', 'light', 'wait', 'find', 'behind', 'everi', 'best', 'friend', 'alwai', 'trust', 'love', 'bslvyl']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'was', 'awarded', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'our', 'nd', 'attempt', 'to', 'contact', 'you', 'call', 'box', 'qu']\n",
      "After stop words removal: ['urgent', 'mobile', 'awarded', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'contact', 'call', 'box', 'qu']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'awar', 'bonu', 'caller', 'priz', 'attempt', 'contact', 'call', 'box']\n",
      "Tokenized sentence: ['wat', 'makes', 'some', 'people', 'dearer', 'is', 'not', 'just', 'de', 'happiness', 'dat', 'u', 'feel', 'when', 'u', 'meet', 'them', 'but', 'de', 'pain', 'u', 'feel', 'when', 'u', 'miss', 'dem']\n",
      "After stop words removal: ['wat', 'makes', 'people', 'dearer', 'de', 'happiness', 'dat', 'u', 'feel', 'u', 'meet', 'de', 'pain', 'u', 'feel', 'u', 'miss', 'dem']\n",
      "After stemming with porters algorithm: ['wat', 'make', 'peopl', 'dearer', 'happi', 'dat', 'feel', 'meet', 'pain', 'feel', 'miss', 'dem']\n",
      "Tokenized sentence: ['reverse', 'is', 'cheating', 'that', 'is', 'not', 'mathematics']\n",
      "After stop words removal: ['reverse', 'cheating', 'mathematics']\n",
      "cheat\n",
      "cheate\n",
      "After stemming with porters algorithm: ['revers', 'cheat', 'mathemat']\n",
      "Tokenized sentence: ['from', 'lost', 'help']\n",
      "After stop words removal: ['lost', 'help']\n",
      "After stemming with porters algorithm: ['lost', 'help']\n",
      "Tokenized sentence: ['hhahhaahahah', 'rofl', 'wtf', 'nig', 'was', 'leonardo', 'in', 'your', 'room', 'or', 'something']\n",
      "After stop words removal: ['hhahhaahahah', 'rofl', 'wtf', 'nig', 'leonardo', 'room', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['hhahhaahahah', 'rofl', 'wtf', 'nig', 'leonardo', 'room', 'somet']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['get', 'me', 'out', 'of', 'this', 'dump', 'heap', 'my', 'mom', 'decided', 'to', 'come', 'to', 'lowes', 'boring']\n",
      "After stop words removal: ['get', 'dump', 'heap', 'mom', 'decided', 'come', 'lowes', 'boring']\n",
      "bor\n",
      "After stemming with porters algorithm: ['get', 'dump', 'heap', 'mom', 'decid', 'come', 'lowe', 'bore']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'tell', 'you', 'how', 'bad', 'i', 'feel', 'that', 'basically', 'the', 'only', 'times', 'i', 'text', 'you', 'lately', 'are', 'when', 'i', 'need', 'drugs']\n",
      "After stop words removal: ['want', 'tell', 'bad', 'feel', 'basically', 'times', 'text', 'lately', 'need', 'drugs']\n",
      "After stemming with porters algorithm: ['want', 'tell', 'bad', 'feel', 'basic', 'time', 'text', 'late', 'need', 'drug']\n",
      "Tokenized sentence: ['not', 'thought', 'bout', 'it', 'drink', 'in', 'tap', 'spile', 'at', 'seven', 'is', 'that', 'pub', 'on', 'gas', 'st', 'off', 'broad', 'st', 'by', 'canal', 'ok']\n",
      "After stop words removal: ['thought', 'bout', 'drink', 'tap', 'spile', 'seven', 'pub', 'gas', 'st', 'broad', 'st', 'canal', 'ok']\n",
      "After stemming with porters algorithm: ['thought', 'bout', 'drink', 'tap', 'spile', 'seven', 'pub', 'ga', 'broad', 'canal']\n",
      "Tokenized sentence: ['stop', 'knowing', 'me', 'so', 'well']\n",
      "After stop words removal: ['stop', 'knowing', 'well']\n",
      "know\n",
      "After stemming with porters algorithm: ['stop', 'knowe', 'well']\n",
      "Tokenized sentence: ['wat', 'u', 'doing', 'there']\n",
      "After stop words removal: ['wat', 'u']\n",
      "After stemming with porters algorithm: ['wat']\n",
      "Tokenized sentence: ['gran', 'onlyfound', 'out', 'afew', 'days', 'ago', 'cusoon', 'honi']\n",
      "After stop words removal: ['gran', 'onlyfound', 'afew', 'days', 'ago', 'cusoon', 'honi']\n",
      "After stemming with porters algorithm: ['gran', 'onlyfound', 'afew', 'dai', 'ago', 'cusoon', 'honi']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'video', 'handset', 'any', 'time', 'any', 'network', 'mins', 'unlimited', 'text', 'camcorder', 'reply', 'or', 'call', 'now', 'for', 'del', 'sat', 'am']\n",
      "After stop words removal: ['want', 'new', 'video', 'handset', 'time', 'network', 'mins', 'unlimited', 'text', 'camcorder', 'reply', 'call', 'del', 'sat']\n",
      "After stemming with porters algorithm: ['want', 'new', 'video', 'handset', 'time', 'network', 'min', 'unlimit', 'text', 'camcord', 'repli', 'call', 'del', 'sat']\n",
      "Tokenized sentence: ['moby', 'pub', 'quiz', 'win', 'a', 'high', 'street', 'prize', 'if', 'u', 'know', 'who', 'the', 'new', 'duchess', 'of', 'cornwall', 'will', 'be', 'txt', 'her', 'first', 'name', 'to', 'unsub', 'stop', 'sp']\n",
      "After stop words removal: ['moby', 'pub', 'quiz', 'win', 'high', 'street', 'prize', 'u', 'know', 'new', 'duchess', 'cornwall', 'txt', 'first', 'name', 'unsub', 'stop', 'sp']\n",
      "After stemming with porters algorithm: ['mobi', 'pub', 'quiz', 'win', 'high', 'street', 'priz', 'know', 'new', 'duchess', 'cornwal', 'txt', 'first', 'name', 'unsub', 'stop']\n",
      "Tokenized sentence: ['den', 'wat', 'will', 'e', 'schedule', 'b', 'lk', 'on', 'sun']\n",
      "After stop words removal: ['den', 'wat', 'e', 'schedule', 'b', 'lk', 'sun']\n",
      "After stemming with porters algorithm: ['den', 'wat', 'schedul', 'sun']\n",
      "Tokenized sentence: ['can', 'you', 'do', 'online', 'transaction']\n",
      "After stop words removal: ['online', 'transaction']\n",
      "After stemming with porters algorithm: ['onlin', 'transact']\n",
      "Tokenized sentence: ['carlos', 'says', 'we', 'can', 'pick', 'up', 'from', 'him', 'later', 'so', 'yeah', 'we', 're', 'set']\n",
      "After stop words removal: ['carlos', 'says', 'pick', 'later', 'yeah', 'set']\n",
      "After stemming with porters algorithm: ['carlo', 'sai', 'pick', 'later', 'yeah', 'set']\n",
      "Tokenized sentence: ['wait', 'me', 'in', 'sch', 'i', 'finish', 'ard']\n",
      "After stop words removal: ['wait', 'sch', 'finish', 'ard']\n",
      "After stemming with porters algorithm: ['wait', 'sch', 'finish', 'ard']\n",
      "Tokenized sentence: ['it', 'to', 'your', 'free', 'text', 'messages', 'are', 'valid', 'until', 'december']\n",
      "After stop words removal: ['free', 'text', 'messages', 'valid', 'december']\n",
      "After stemming with porters algorithm: ['free', 'text', 'messag', 'valid', 'decemb']\n",
      "Tokenized sentence: ['join', 'the', 'uk', 's', 'horniest', 'dogging', 'service', 'and', 'u', 'can', 'have', 'sex', 'nite', 'just', 'sign', 'up', 'and', 'follow', 'the', 'instructions', 'txt', 'entry', 'to', 'now', 'nyt', 'ec', 'a', 'lp', 'msg', 'p']\n",
      "After stop words removal: ['join', 'uk', 'horniest', 'dogging', 'service', 'u', 'sex', 'nite', 'sign', 'follow', 'instructions', 'txt', 'entry', 'nyt', 'ec', 'lp', 'msg', 'p']\n",
      "dogg\n",
      "After stemming with porters algorithm: ['join', 'horniest', 'dog', 'servic', 'sex', 'nite', 'sign', 'follow', 'instruct', 'txt', 'entri', 'nyt', 'msg']\n",
      "Tokenized sentence: ['ok', 'u', 'enjoy', 'ur', 'shows']\n",
      "After stop words removal: ['ok', 'u', 'enjoy', 'ur', 'shows']\n",
      "After stemming with porters algorithm: ['enjoi', 'show']\n",
      "Tokenized sentence: ['living', 'is', 'very', 'simple', 'loving', 'is', 'also', 'simple', 'laughing', 'is', 'too', 'simple', 'winning', 'is', 'tooo', 'simple', 'but', 'being', 'simple', 'is', 'very', 'difficult']\n",
      "After stop words removal: ['living', 'simple', 'loving', 'also', 'simple', 'laughing', 'simple', 'winning', 'tooo', 'simple', 'simple', 'difficult']\n",
      "liv\n",
      "lov\n",
      "laugh\n",
      "winn\n",
      "After stemming with porters algorithm: ['live', 'simpl', 'love', 'also', 'simpl', 'laug', 'simpl', 'win', 'tooo', 'simpl', 'simpl', 'difficult']\n",
      "Tokenized sentence: ['ok', 'not', 'a', 'problem', 'will', 'get', 'them', 'a', 'taxi', 'c', 'ing', 'tomorrow', 'and', 'tuesday', 'on', 'tuesday', 'think', 'we', 'r', 'all', 'going', 'to', 'the', 'cinema']\n",
      "After stop words removal: ['ok', 'problem', 'get', 'taxi', 'c', 'ing', 'tomorrow', 'tuesday', 'tuesday', 'think', 'r', 'going', 'cinema']\n",
      "go\n",
      "After stemming with porters algorithm: ['problem', 'get', 'taxi', 'ing', 'tomorrow', 'tuesdai', 'tuesdai', 'think', 'go', 'cinema']\n",
      "Tokenized sentence: ['apple', 'day', 'no', 'doctor', 'tulsi', 'leaf', 'day', 'no', 'cancer', 'lemon', 'day', 'no', 'fat', 'cup', 'milk', 'day', 'no', 'bone', 'problms', 'litres', 'watr', 'day', 'no', 'diseases', 'snd', 'ths', 'whom', 'u', 'care']\n",
      "After stop words removal: ['apple', 'day', 'doctor', 'tulsi', 'leaf', 'day', 'cancer', 'lemon', 'day', 'fat', 'cup', 'milk', 'day', 'bone', 'problms', 'litres', 'watr', 'day', 'diseases', 'snd', 'ths', 'u', 'care']\n",
      "After stemming with porters algorithm: ['appl', 'dai', 'doctor', 'tulsi', 'leaf', 'dai', 'cancer', 'lemon', 'dai', 'fat', 'cup', 'milk', 'dai', 'bone', 'problm', 'litr', 'watr', 'dai', 'diseas', 'snd', 'th', 'care']\n",
      "Tokenized sentence: ['k', 'k', 'how', 'is', 'your', 'business', 'now']\n",
      "After stop words removal: ['k', 'k', 'business']\n",
      "After stemming with porters algorithm: ['busi']\n",
      "Tokenized sentence: ['just', 'nw', 'i', 'came', 'to', 'hme', 'da']\n",
      "After stop words removal: ['nw', 'came', 'hme', 'da']\n",
      "After stemming with porters algorithm: ['came', 'hme']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['anything', 'lor', 'u', 'decide']\n",
      "After stop words removal: ['anything', 'lor', 'u', 'decide']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'lor', 'decid']\n",
      "Tokenized sentence: ['lt', 'gt', 'of', 'pple', 'marry', 'with', 'their', 'lovers', 'becz', 'they', 'hav', 'gud', 'undrstndng', 'dat', 'avoids', 'problems', 'i', 'sent', 'dis', 'u', 'u', 'wil', 'get', 'gud', 'news', 'on', 'friday', 'by', 'd', 'person', 'you', 'like', 'and', 'tomorrow', 'will', 'be', 'the', 'best', 'day', 'of', 'your', 'life', 'dont', 'break', 'this', 'chain', 'if', 'you', 'break', 'you', 'will', 'suffer', 'send', 'this', 'to', 'lt', 'gt', 'frnds', 'in', 'lt', 'gt', 'mins', 'whn', 'u', 'read']\n",
      "After stop words removal: ['lt', 'gt', 'pple', 'marry', 'lovers', 'becz', 'hav', 'gud', 'undrstndng', 'dat', 'avoids', 'problems', 'sent', 'dis', 'u', 'u', 'wil', 'get', 'gud', 'news', 'friday', 'person', 'like', 'tomorrow', 'best', 'day', 'life', 'dont', 'break', 'chain', 'break', 'suffer', 'send', 'lt', 'gt', 'frnds', 'lt', 'gt', 'mins', 'whn', 'u', 'read']\n",
      "After stemming with porters algorithm: ['pple', 'marri', 'lover', 'becz', 'hav', 'gud', 'undrstndng', 'dat', 'avoid', 'problem', 'sent', 'di', 'wil', 'get', 'gud', 'new', 'fridai', 'person', 'like', 'tomorrow', 'best', 'dai', 'life', 'dont', 'break', 'chain', 'break', 'suffer', 'send', 'frnd', 'min', 'whn', 'read']\n",
      "Tokenized sentence: ['oi', 'ami', 'parchi', 'na', 're', 'kicchu', 'kaaj', 'korte', 'iccha', 'korche', 'na', 'phone', 'ta', 'tul', 'na', 'plz', 'plz']\n",
      "After stop words removal: ['oi', 'ami', 'parchi', 'na', 'kicchu', 'kaaj', 'korte', 'iccha', 'korche', 'na', 'phone', 'ta', 'tul', 'na', 'plz', 'plz']\n",
      "After stemming with porters algorithm: ['ami', 'parchi', 'kicchu', 'kaaj', 'kort', 'iccha', 'korch', 'phone', 'tul', 'plz', 'plz']\n",
      "Tokenized sentence: ['win', 'we', 'have', 'a', 'winner', 'mr', 't', 'foley', 'won', 'an', 'ipod', 'more', 'exciting', 'prizes', 'soon', 'so', 'keep', 'an', 'eye', 'on', 'ur', 'mobile', 'or', 'visit', 'www', 'win', 'co', 'uk']\n",
      "After stop words removal: ['win', 'winner', 'mr', 'foley', 'ipod', 'exciting', 'prizes', 'soon', 'keep', 'eye', 'ur', 'mobile', 'visit', 'www', 'win', 'co', 'uk']\n",
      "excit\n",
      "After stemming with porters algorithm: ['win', 'winner', 'folei', 'ipod', 'excit', 'priz', 'soon', 'keep', 'ey', 'mobil', 'visit', 'www', 'win']\n",
      "Tokenized sentence: ['i', 'know', 'you', 'are', 'thinkin', 'malaria', 'but', 'relax', 'children', 'cant', 'handle', 'malaria', 'she', 'would', 'have', 'been', 'worse', 'and', 'its', 'gastroenteritis', 'if', 'she', 'takes', 'enough', 'to', 'replace', 'her', 'loss', 'her', 'temp', 'will', 'reduce', 'and', 'if', 'you', 'give', 'her', 'malaria', 'meds', 'now', 'she', 'will', 'just', 'vomit', 'its', 'a', 'self', 'limiting', 'illness', 'she', 'has', 'which', 'means', 'in', 'a', 'few', 'days', 'it', 'will', 'completely', 'stop']\n",
      "After stop words removal: ['know', 'thinkin', 'malaria', 'relax', 'children', 'cant', 'handle', 'malaria', 'would', 'worse', 'gastroenteritis', 'takes', 'enough', 'replace', 'loss', 'temp', 'reduce', 'give', 'malaria', 'meds', 'vomit', 'self', 'limiting', 'illness', 'means', 'days', 'completely', 'stop']\n",
      "limit\n",
      "After stemming with porters algorithm: ['know', 'thinkin', 'malaria', 'relax', 'children', 'cant', 'handl', 'malaria', 'would', 'wors', 'gastroenter', 'take', 'enough', 'replac', 'loss', 'temp', 'reduc', 'give', 'malaria', 'med', 'vomit', 'self', 'limit', 'ill', 'mean', 'dai', 'complet', 'stop']\n",
      "Tokenized sentence: ['purity', 'of', 'friendship', 'between', 'two', 'is', 'not', 'about', 'smiling', 'after', 'reading', 'the', 'forwarded', 'message', 'its', 'about', 'smiling', 'just', 'by', 'seeing', 'the', 'name', 'gud', 'evng']\n",
      "After stop words removal: ['purity', 'friendship', 'two', 'smiling', 'reading', 'forwarded', 'message', 'smiling', 'seeing', 'name', 'gud', 'evng']\n",
      "smil\n",
      "read\n",
      "smil\n",
      "see\n",
      "After stemming with porters algorithm: ['puriti', 'friendship', 'two', 'smile', 'read', 'forwar', 'messag', 'smile', 'see', 'name', 'gud', 'evng']\n",
      "Tokenized sentence: ['my', 'life', 'means', 'a', 'lot', 'to', 'me', 'not', 'because', 'i', 'love', 'my', 'life', 'but', 'because', 'i', 'love', 'the', 'people', 'in', 'my', 'life', 'the', 'world', 'calls', 'them', 'friends', 'i', 'call', 'them', 'my', 'world', 'ge']\n",
      "After stop words removal: ['life', 'means', 'lot', 'love', 'life', 'love', 'people', 'life', 'world', 'calls', 'friends', 'call', 'world', 'ge']\n",
      "After stemming with porters algorithm: ['life', 'mean', 'lot', 'love', 'life', 'love', 'peopl', 'life', 'world', 'call', 'friend', 'call', 'world']\n",
      "Tokenized sentence: ['oh', 'is', 'it', 'send', 'me', 'the', 'address']\n",
      "After stop words removal: ['oh', 'send', 'address']\n",
      "After stemming with porters algorithm: ['send', 'address']\n",
      "Tokenized sentence: ['you', 'are', 'being', 'contacted', 'by', 'our', 'dating', 'service', 'by', 'someone', 'you', 'know', 'to', 'find', 'out', 'who', 'it', 'is', 'call', 'from', 'a', 'land', 'line', 'pobox', 'w', 'tg', 'p']\n",
      "After stop words removal: ['contacted', 'dating', 'service', 'someone', 'know', 'find', 'call', 'land', 'line', 'pobox', 'w', 'tg', 'p']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['contac', 'date', 'servic', 'someon', 'know', 'find', 'call', 'land', 'line', 'pobox']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'chennai', 'velachery']\n",
      "After stop words removal: ['chennai', 'velachery']\n",
      "After stemming with porters algorithm: ['chennai', 'velacheri']\n",
      "Tokenized sentence: ['would', 'me', 'smoking', 'you', 'out', 'help', 'us', 'work', 'through', 'this', 'difficult', 'time']\n",
      "After stop words removal: ['would', 'smoking', 'help', 'us', 'work', 'difficult', 'time']\n",
      "smok\n",
      "After stemming with porters algorithm: ['would', 'smoke', 'help', 'work', 'difficult', 'time']\n",
      "Tokenized sentence: ['that', 'means', 'from', 'february', 'to', 'april', 'i', 'll', 'be', 'getting', 'a', 'place', 'to', 'stay', 'down', 'there', 'so', 'i', 'don', 't', 'have', 'to', 'hustle', 'back', 'and', 'forth', 'during', 'audition', 'season', 'as', 'i', 'have', 'since', 'my', 'sister', 'moved', 'away', 'from', 'harlem']\n",
      "After stop words removal: ['means', 'february', 'april', 'getting', 'place', 'stay', 'hustle', 'back', 'forth', 'audition', 'season', 'since', 'sister', 'moved', 'away', 'harlem']\n",
      "gett\n",
      "After stemming with porters algorithm: ['mean', 'februari', 'april', 'get', 'place', 'stai', 'hustl', 'back', 'forth', 'audit', 'season', 'sinc', 'sister', 'move', 'awai', 'harlem']\n",
      "Tokenized sentence: ['the', 'world', 's', 'most', 'happiest', 'frnds', 'never', 'have', 'the', 'same', 'characters', 'dey', 'just', 'have', 'the', 'best', 'understanding', 'of', 'their', 'differences']\n",
      "After stop words removal: ['world', 'happiest', 'frnds', 'never', 'characters', 'dey', 'best', 'understanding', 'differences']\n",
      "understand\n",
      "After stemming with porters algorithm: ['world', 'happiest', 'frnd', 'never', 'charact', 'dei', 'best', 'understan', 'differ']\n",
      "Tokenized sentence: ['all', 'done', 'all', 'handed', 'in', 'celebrations', 'in', 'full', 'swing', 'yet']\n",
      "After stop words removal: ['done', 'handed', 'celebrations', 'full', 'swing', 'yet']\n",
      "After stemming with porters algorithm: ['done', 'han', 'celebr', 'full', 'swing', 'yet']\n",
      "Tokenized sentence: ['thanks', 'for', 'yesterday', 'sir', 'you', 'have', 'been', 'wonderful', 'hope', 'you', 'enjoyed', 'the', 'burial', 'mojibiola']\n",
      "After stop words removal: ['thanks', 'yesterday', 'sir', 'wonderful', 'hope', 'enjoyed', 'burial', 'mojibiola']\n",
      "After stemming with porters algorithm: ['thank', 'yesterdai', 'sir', 'wonder', 'hope', 'enjoi', 'burial', 'mojibiola']\n",
      "Tokenized sentence: ['haha', 'good', 'to', 'hear', 'i', 'm', 'officially', 'paid', 'and', 'on', 'the', 'market', 'for', 'an', 'th']\n",
      "After stop words removal: ['haha', 'good', 'hear', 'officially', 'paid', 'market', 'th']\n",
      "After stemming with porters algorithm: ['haha', 'good', 'hear', 'offici', 'paid', 'market']\n",
      "Tokenized sentence: ['whenevr', 'ur', 'sad', 'whenevr', 'ur', 'gray', 'remembr', 'im', 'here', 'listn', 'watevr', 'u', 'wanna', 'say', 'jus', 'walk', 'wid', 'me', 'a', 'little', 'while', 'amp', 'i', 'promise', 'i', 'll', 'bring', 'back', 'ur', 'smile']\n",
      "After stop words removal: ['whenevr', 'ur', 'sad', 'whenevr', 'ur', 'gray', 'remembr', 'im', 'listn', 'watevr', 'u', 'wanna', 'say', 'jus', 'walk', 'wid', 'little', 'amp', 'promise', 'bring', 'back', 'ur', 'smile']\n",
      "After stemming with porters algorithm: ['whenevr', 'sad', 'whenevr', 'grai', 'remembr', 'listn', 'watevr', 'wanna', 'sai', 'ju', 'walk', 'wid', 'littl', 'amp', 'promis', 'bring', 'back', 'smile']\n",
      "Tokenized sentence: ['just', 'seeing', 'your', 'missed', 'call', 'my', 'dear', 'brother', 'do', 'have', 'a', 'gr', 'day']\n",
      "After stop words removal: ['seeing', 'missed', 'call', 'dear', 'brother', 'gr', 'day']\n",
      "see\n",
      "After stemming with porters algorithm: ['see', 'miss', 'call', 'dear', 'brother', 'dai']\n",
      "Tokenized sentence: ['aight', 'i', 've', 'been', 'set', 'free', 'think', 'you', 'could', 'text', 'me', 'blake', 's', 'address', 'it', 'occurs', 'to', 'me', 'i', 'm', 'not', 'quite', 'as', 'sure', 'what', 'i', 'm', 'doing', 'as', 'i', 'thought', 'i', 'was']\n",
      "After stop words removal: ['aight', 'set', 'free', 'think', 'could', 'text', 'blake', 'address', 'occurs', 'quite', 'sure', 'thought']\n",
      "After stemming with porters algorithm: ['aight', 'set', 'free', 'think', 'could', 'text', 'blake', 'address', 'occur', 'quit', 'sure', 'thought']\n",
      "Tokenized sentence: ['kallis', 'is', 'ready', 'for', 'bat', 'in', 'nd', 'innings']\n",
      "After stop words removal: ['kallis', 'ready', 'bat', 'nd', 'innings']\n",
      "inn\n",
      "After stemming with porters algorithm: ['kalli', 'readi', 'bat', 'in']\n",
      "Tokenized sentence: ['wish', 'u', 'many', 'many', 'returns', 'of', 'the', 'day', 'happy', 'birthday', 'vikky']\n",
      "After stop words removal: ['wish', 'u', 'many', 'many', 'returns', 'day', 'happy', 'birthday', 'vikky']\n",
      "After stemming with porters algorithm: ['wish', 'mani', 'mani', 'return', 'dai', 'happi', 'birthdai', 'vikki']\n",
      "Tokenized sentence: ['no', 'its', 'ful', 'of', 'song', 'lyrics']\n",
      "After stop words removal: ['ful', 'song', 'lyrics']\n",
      "After stemming with porters algorithm: ['ful', 'song', 'lyric']\n",
      "Tokenized sentence: ['gudnite', 'tc', 'practice', 'going', 'on']\n",
      "After stop words removal: ['gudnite', 'tc', 'practice', 'going']\n",
      "go\n",
      "After stemming with porters algorithm: ['gudnit', 'practic', 'go']\n",
      "Tokenized sentence: ['th', 'of', 'july']\n",
      "After stop words removal: ['th', 'july']\n",
      "After stemming with porters algorithm: ['juli']\n",
      "Tokenized sentence: ['boltblue', 'tones', 'for', 'p', 'reply', 'poly', 'or', 'mono', 'eg', 'poly', 'cha', 'cha', 'slide', 'yeah', 'slow', 'jamz', 'toxic', 'come', 'with', 'me', 'or', 'stop', 'more', 'tones', 'txt', 'more']\n",
      "After stop words removal: ['boltblue', 'tones', 'p', 'reply', 'poly', 'mono', 'eg', 'poly', 'cha', 'cha', 'slide', 'yeah', 'slow', 'jamz', 'toxic', 'come', 'stop', 'tones', 'txt']\n",
      "After stemming with porters algorithm: ['boltblu', 'tone', 'repli', 'poli', 'mono', 'poli', 'cha', 'cha', 'slide', 'yeah', 'slow', 'jamz', 'toxic', 'come', 'stop', 'tone', 'txt']\n",
      "Tokenized sentence: ['no', 'we', 'sell', 'it', 'all', 'so', 'we', 'll', 'have', 'tons', 'if', 'coins', 'then', 'sell', 'our', 'coins', 'to', 'someone', 'thru', 'paypal', 'voila', 'money', 'back', 'in', 'life', 'pockets']\n",
      "After stop words removal: ['sell', 'tons', 'coins', 'sell', 'coins', 'someone', 'thru', 'paypal', 'voila', 'money', 'back', 'life', 'pockets']\n",
      "After stemming with porters algorithm: ['sell', 'ton', 'coin', 'sell', 'coin', 'someon', 'thru', 'paypal', 'voila', 'monei', 'back', 'life', 'pocket']\n",
      "Tokenized sentence: ['no', 'promises', 'on', 'when', 'though', 'haven', 't', 'even', 'gotten', 'dinner', 'yet']\n",
      "After stop words removal: ['promises', 'though', 'even', 'gotten', 'dinner', 'yet']\n",
      "After stemming with porters algorithm: ['promis', 'though', 'even', 'gotten', 'dinner', 'yet']\n",
      "Tokenized sentence: ['k', 'k', 'yesterday', 'i', 'was', 'in', 'cbe']\n",
      "After stop words removal: ['k', 'k', 'yesterday', 'cbe']\n",
      "After stemming with porters algorithm: ['yesterdai', 'cbe']\n",
      "Tokenized sentence: ['no', 'da', 'he', 'is', 'stupid', 'da', 'always', 'sending', 'like', 'this', 'don', 'believe', 'any', 'of', 'those', 'message', 'pandy', 'is', 'a', 'mental']\n",
      "After stop words removal: ['da', 'stupid', 'da', 'always', 'sending', 'like', 'believe', 'message', 'pandy', 'mental']\n",
      "send\n",
      "After stemming with porters algorithm: ['stupid', 'alwai', 'sen', 'like', 'believ', 'messag', 'pandi', 'mental']\n",
      "Tokenized sentence: ['yes', 'nigh', 'you', 'cant', 'aha']\n",
      "After stop words removal: ['yes', 'nigh', 'cant', 'aha']\n",
      "After stemming with porters algorithm: ['ye', 'nigh', 'cant', 'aha']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['are', 'you', 'in', 'castor', 'you', 'need', 'to', 'see', 'something']\n",
      "After stop words removal: ['castor', 'need', 'see', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['castor', 'need', 'see', 'somet']\n",
      "Tokenized sentence: ['no', 'dear', 'i', 'was', 'sleeping', 'p']\n",
      "After stop words removal: ['dear', 'sleeping', 'p']\n",
      "sleep\n",
      "After stemming with porters algorithm: ['dear', 'sleep']\n",
      "Tokenized sentence: ['but', 'really', 'quite', 'funny', 'lor', 'wat', 'then', 'u', 'shd', 'haf', 'run', 'shorter', 'distance', 'wat']\n",
      "After stop words removal: ['really', 'quite', 'funny', 'lor', 'wat', 'u', 'shd', 'haf', 'run', 'shorter', 'distance', 'wat']\n",
      "After stemming with porters algorithm: ['realli', 'quit', 'funni', 'lor', 'wat', 'shd', 'haf', 'run', 'shorter', 'distanc', 'wat']\n",
      "Tokenized sentence: ['valentines', 'day', 'special', 'win', 'over', 'in', 'our', 'quiz', 'and', 'take', 'your', 'partner', 'on', 'the', 'trip', 'of', 'a', 'lifetime', 'send', 'go', 'to', 'now', 'p', 'msg', 'rcvd', 'custcare']\n",
      "After stop words removal: ['valentines', 'day', 'special', 'win', 'quiz', 'take', 'partner', 'trip', 'lifetime', 'send', 'go', 'p', 'msg', 'rcvd', 'custcare']\n",
      "After stemming with porters algorithm: ['valentin', 'dai', 'special', 'win', 'quiz', 'take', 'partner', 'trip', 'lifetim', 'send', 'msg', 'rcvd', 'custcar']\n",
      "Tokenized sentence: ['your', 'opinion', 'about', 'me', 'over', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'not', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stop words removal: ['opinion', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stemming with porters algorithm: ['opinion', 'jada', 'kusruthi', 'lovab', 'silent', 'spl', 'charact', 'matur', 'stylish', 'simpl', 'pl', 'repli']\n",
      "Tokenized sentence: ['pete', 'can', 'you', 'please', 'ring', 'meive', 'hardly', 'gotany', 'credit']\n",
      "After stop words removal: ['pete', 'please', 'ring', 'meive', 'hardly', 'gotany', 'credit']\n",
      "After stemming with porters algorithm: ['pete', 'pleas', 'ring', 'meiv', 'hardli', 'gotani', 'credit']\n",
      "Tokenized sentence: ['so', 'how', 's', 'the', 'weather', 'over', 'there']\n",
      "After stop words removal: ['weather']\n",
      "After stemming with porters algorithm: ['weather']\n",
      "Tokenized sentence: ['yay', 'can', 't', 'wait', 'to', 'party', 'together']\n",
      "After stop words removal: ['yay', 'wait', 'party', 'together']\n",
      "After stemming with porters algorithm: ['yai', 'wait', 'parti', 'togeth']\n",
      "Tokenized sentence: ['sorry', 'man', 'my', 'account', 's', 'dry', 'or', 'i', 'would', 'if', 'you', 'want', 'we', 'could', 'trade', 'back', 'half', 'or', 'i', 'could', 'buy', 'some', 'shit', 'with', 'my', 'credit', 'card']\n",
      "After stop words removal: ['sorry', 'man', 'account', 'dry', 'would', 'want', 'could', 'trade', 'back', 'half', 'could', 'buy', 'shit', 'credit', 'card']\n",
      "After stemming with porters algorithm: ['sorri', 'man', 'account', 'dry', 'would', 'want', 'could', 'trade', 'back', 'half', 'could', 'bui', 'shit', 'credit', 'card']\n",
      "Tokenized sentence: ['yay', 'you', 'better', 'not', 'have', 'told', 'that', 'to', 'other', 'girls', 'either']\n",
      "After stop words removal: ['yay', 'better', 'told', 'girls', 'either']\n",
      "After stemming with porters algorithm: ['yai', 'better', 'told', 'girl', 'either']\n",
      "Tokenized sentence: ['house', 'maid', 'is', 'the', 'murderer', 'coz', 'the', 'man', 'was', 'murdered', 'on', 'lt', 'gt', 'th', 'january', 'as', 'public', 'holiday', 'all', 'govt', 'instituitions', 'are', 'closed', 'including', 'post', 'office', 'understand']\n",
      "After stop words removal: ['house', 'maid', 'murderer', 'coz', 'man', 'murdered', 'lt', 'gt', 'th', 'january', 'public', 'holiday', 'govt', 'instituitions', 'closed', 'including', 'post', 'office', 'understand']\n",
      "includ\n",
      "After stemming with porters algorithm: ['hous', 'maid', 'murder', 'coz', 'man', 'murder', 'januari', 'public', 'holidai', 'govt', 'instituit', 'close', 'includ', 'post', 'offic', 'understand']\n",
      "Tokenized sentence: ['i', 'career', 'tel', 'have', 'added', 'u', 'as', 'a', 'contact', 'on', 'indyarocks', 'com', 'to', 'send', 'free', 'sms', 'to', 'remove', 'from', 'phonebook', 'sms', 'no', 'to', 'lt', 'gt']\n",
      "After stop words removal: ['career', 'tel', 'added', 'u', 'contact', 'indyarocks', 'com', 'send', 'free', 'sms', 'remove', 'phonebook', 'sms', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['career', 'tel', 'ad', 'contact', 'indyarock', 'com', 'send', 'free', 'sm', 'remov', 'phonebook', 'sm']\n",
      "Tokenized sentence: ['you', 'still', 'around', 'looking', 'to', 'pick', 'up', 'later']\n",
      "After stop words removal: ['still', 'around', 'looking', 'pick', 'later']\n",
      "look\n",
      "After stemming with porters algorithm: ['still', 'around', 'look', 'pick', 'later']\n",
      "Tokenized sentence: ['hi', 'wk', 'been', 'ok', 'on', 'hols', 'now', 'yes', 'on', 'for', 'a', 'bit', 'of', 'a', 'run', 'forgot', 'that', 'i', 'have', 'hairdressers', 'appointment', 'at', 'four', 'so', 'need', 'to', 'get', 'home', 'n', 'shower', 'beforehand', 'does', 'that', 'cause', 'prob', 'for', 'u', 'ham']\n",
      "After stop words removal: ['hi', 'wk', 'ok', 'hols', 'yes', 'bit', 'run', 'forgot', 'hairdressers', 'appointment', 'four', 'need', 'get', 'home', 'n', 'shower', 'beforehand', 'cause', 'prob', 'u', 'ham']\n",
      "After stemming with porters algorithm: ['hol', 'ye', 'bit', 'run', 'forgot', 'hairdress', 'appoint', 'four', 'need', 'get', 'home', 'shower', 'beforehand', 'caus', 'prob', 'ham']\n",
      "Tokenized sentence: ['yup', 'he', 'msg', 'me', 'is', 'tat', 'yijue', 'then', 'i', 'tot', 'it', 's', 'my', 'group', 'mate', 'cos', 'we', 'meeting', 'today', 'mah', 'i', 'm', 'askin', 'if', 'leaving', 'earlier', 'or', 'wat', 'mah', 'cos', 'mayb', 'haf', 'to', 'walk', 'v', 'far']\n",
      "After stop words removal: ['yup', 'msg', 'tat', 'yijue', 'tot', 'group', 'mate', 'cos', 'meeting', 'today', 'mah', 'askin', 'leaving', 'earlier', 'wat', 'mah', 'cos', 'mayb', 'haf', 'walk', 'v', 'far']\n",
      "meet\n",
      "leav\n",
      "After stemming with porters algorithm: ['yup', 'msg', 'tat', 'yiju', 'tot', 'group', 'mate', 'co', 'meet', 'todai', 'mah', 'askin', 'leav', 'earlier', 'wat', 'mah', 'co', 'mayb', 'haf', 'walk', 'far']\n",
      "Tokenized sentence: ['hows', 'that', 'watch', 'resizing']\n",
      "After stop words removal: ['hows', 'watch', 'resizing']\n",
      "resiz\n",
      "resize\n",
      "After stemming with porters algorithm: ['how', 'watch', 'resiz']\n",
      "Tokenized sentence: ['i', 'to', 'am', 'looking', 'forward', 'to', 'all', 'the', 'sex', 'cuddling', 'only', 'two', 'more', 'sleeps']\n",
      "After stop words removal: ['looking', 'forward', 'sex', 'cuddling', 'two', 'sleeps']\n",
      "look\n",
      "cuddl\n",
      "After stemming with porters algorithm: ['look', 'forward', 'sex', 'cuddl', 'two', 'sleep']\n",
      "Tokenized sentence: ['i', 'm', 'turning', 'off', 'my', 'phone', 'my', 'moms', 'telling', 'everyone', 'i', 'have', 'cancer', 'and', 'my', 'sister', 'won', 't', 'stop', 'calling', 'it', 'hurts', 'to', 'talk', 'can', 't', 'put', 'up', 'with', 'it', 'see', 'u', 'when', 'u', 'get', 'home', 'love', 'u']\n",
      "After stop words removal: ['turning', 'phone', 'moms', 'telling', 'everyone', 'cancer', 'sister', 'stop', 'calling', 'hurts', 'talk', 'put', 'see', 'u', 'u', 'get', 'home', 'love', 'u']\n",
      "turn\n",
      "tell\n",
      "call\n",
      "After stemming with porters algorithm: ['tur', 'phone', 'mom', 'tell', 'everyon', 'cancer', 'sister', 'stop', 'call', 'hurt', 'talk', 'put', 'see', 'get', 'home', 'love']\n",
      "Tokenized sentence: ['annoying', 'isn', 't', 'it']\n",
      "After stop words removal: ['annoying']\n",
      "annoy\n",
      "After stemming with porters algorithm: ['annoi']\n",
      "Tokenized sentence: ['dear', 'hero', 'i', 'am', 'leaving', 'to', 'qatar', 'tonite', 'for', 'an', 'apt', 'opportunity', 'pls', 'do', 'keep', 'in', 'touch', 'at', 'lt', 'email', 'gt', 'kerala']\n",
      "After stop words removal: ['dear', 'hero', 'leaving', 'qatar', 'tonite', 'apt', 'opportunity', 'pls', 'keep', 'touch', 'lt', 'email', 'gt', 'kerala']\n",
      "leav\n",
      "After stemming with porters algorithm: ['dear', 'hero', 'leav', 'qatar', 'tonit', 'apt', 'opportun', 'pl', 'keep', 'touch', 'email', 'kerala']\n",
      "Tokenized sentence: ['spjanuary', 'male', 'sale', 'hot', 'gay', 'chat', 'now', 'cheaper', 'call', 'national', 'rate', 'from', 'p', 'min', 'cheap', 'to', 'p', 'min', 'peak', 'to', 'stop', 'texts', 'call', 'p', 'min']\n",
      "After stop words removal: ['spjanuary', 'male', 'sale', 'hot', 'gay', 'chat', 'cheaper', 'call', 'national', 'rate', 'p', 'min', 'cheap', 'p', 'min', 'peak', 'stop', 'texts', 'call', 'p', 'min']\n",
      "After stemming with porters algorithm: ['spjanuari', 'male', 'sale', 'hot', 'gai', 'chat', 'cheaper', 'call', 'nat', 'rate', 'min', 'cheap', 'min', 'peak', 'stop', 'text', 'call', 'min']\n",
      "Tokenized sentence: ['aight', 'see', 'you', 'in', 'a', 'bit']\n",
      "After stop words removal: ['aight', 'see', 'bit']\n",
      "After stemming with porters algorithm: ['aight', 'see', 'bit']\n",
      "Tokenized sentence: ['i', 'need', 'an', 'th', 'but', 'i', 'm', 'off', 'campus', 'atm', 'could', 'i', 'pick', 'up', 'in', 'an', 'hour', 'or', 'two']\n",
      "After stop words removal: ['need', 'th', 'campus', 'atm', 'could', 'pick', 'hour', 'two']\n",
      "After stemming with porters algorithm: ['need', 'campu', 'atm', 'could', 'pick', 'hour', 'two']\n",
      "Tokenized sentence: ['this', 'single', 'single', 'answers', 'are', 'we', 'fighting', 'plus', 'i', 'said', 'am', 'broke', 'and', 'you', 'didnt', 'reply']\n",
      "After stop words removal: ['single', 'single', 'answers', 'fighting', 'plus', 'said', 'broke', 'didnt', 'reply']\n",
      "fight\n",
      "After stemming with porters algorithm: ['singl', 'singl', 'answer', 'figh', 'plu', 'said', 'broke', 'didnt', 'repli']\n",
      "Tokenized sentence: ['i', 'meant', 'middle', 'left', 'or', 'right']\n",
      "After stop words removal: ['meant', 'middle', 'left', 'right']\n",
      "After stemming with porters algorithm: ['meant', 'middl', 'left', 'right']\n",
      "Tokenized sentence: ['dont', 'know', 'you', 'bring', 'some', 'food']\n",
      "After stop words removal: ['dont', 'know', 'bring', 'food']\n",
      "After stemming with porters algorithm: ['dont', 'know', 'bring', 'food']\n",
      "Tokenized sentence: ['si', 'como', 'no', 'listened', 'the', 'plaid', 'album', 'quite', 'gd', 'the', 'new', 'air', 'which', 'is', 'hilarious', 'also', 'bought', 'braindance', 'a', 'comp', 'ofstuff', 'on', 'aphex', 's', 'abel']\n",
      "After stop words removal: ['si', 'como', 'listened', 'plaid', 'album', 'quite', 'gd', 'new', 'air', 'hilarious', 'also', 'bought', 'braindance', 'comp', 'ofstuff', 'aphex', 'abel']\n",
      "After stemming with porters algorithm: ['como', 'listen', 'plaid', 'album', 'quit', 'new', 'air', 'hilari', 'also', 'bought', 'braindanc', 'comp', 'ofstuff', 'aphex', 'abel']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'was', 'awarded', 'a', 'bonus', 'caller', 'prize', 'on', 'our', 'final', 'attempt', 'contact', 'u', 'call']\n",
      "After stop words removal: ['urgent', 'mobile', 'awarded', 'bonus', 'caller', 'prize', 'final', 'attempt', 'contact', 'u', 'call']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'awar', 'bonu', 'caller', 'priz', 'final', 'attempt', 'contact', 'call']\n",
      "Tokenized sentence: ['i', 've', 'been', 'trying', 'to', 'reach', 'him', 'without', 'success']\n",
      "After stop words removal: ['trying', 'reach', 'without', 'success']\n",
      "After stemming with porters algorithm: ['trying', 'reach', 'without', 'success']\n",
      "Tokenized sentence: ['ok', 'i', 'found', 'dis', 'pierre', 'cardin', 'one', 'which', 'looks', 'normal', 'costs', 'its', 'on', 'sale']\n",
      "After stop words removal: ['ok', 'found', 'dis', 'pierre', 'cardin', 'one', 'looks', 'normal', 'costs', 'sale']\n",
      "After stemming with porters algorithm: ['found', 'di', 'pierr', 'cardin', 'on', 'look', 'normal', 'cost', 'sale']\n",
      "Tokenized sentence: ['ard', 'like', 'dat', 'lor', 'we', 'juz', 'meet', 'in', 'mrt', 'station', 'then', 'dun', 'haf', 'to', 'come', 'out']\n",
      "After stop words removal: ['ard', 'like', 'dat', 'lor', 'juz', 'meet', 'mrt', 'station', 'dun', 'haf', 'come']\n",
      "After stemming with porters algorithm: ['ard', 'like', 'dat', 'lor', 'juz', 'meet', 'mrt', 'stat', 'dun', 'haf', 'come']\n",
      "Tokenized sentence: ['k', 'k', 'how', 'about', 'your', 'training', 'process']\n",
      "After stop words removal: ['k', 'k', 'training', 'process']\n",
      "train\n",
      "After stemming with porters algorithm: ['train', 'process']\n",
      "Tokenized sentence: ['you', 'need', 'to', 'get', 'up', 'now']\n",
      "After stop words removal: ['need', 'get']\n",
      "After stemming with porters algorithm: ['need', 'get']\n",
      "Tokenized sentence: ['don', 't', 'forget', 'who', 'owns', 'you', 'and', 'who', 's', 'private', 'property', 'you', 'are', 'and', 'be', 'my', 'good', 'boy', 'always', 'passionate', 'kiss']\n",
      "After stop words removal: ['forget', 'owns', 'private', 'property', 'good', 'boy', 'always', 'passionate', 'kiss']\n",
      "After stemming with porters algorithm: ['forget', 'own', 'privat', 'properti', 'good', 'boi', 'alwai', 'passion', 'kiss']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['urgent', 'ur', 'awarded', 'a', 'complimentary', 'trip', 'to', 'eurodisinc', 'trav', 'aco', 'entry', 'or', 'to', 'claim', 'txt', 'dis', 'to', 'morefrmmob', 'shracomorsglsuplt', 'ls', 'aj']\n",
      "After stop words removal: ['urgent', 'ur', 'awarded', 'complimentary', 'trip', 'eurodisinc', 'trav', 'aco', 'entry', 'claim', 'txt', 'dis', 'morefrmmob', 'shracomorsglsuplt', 'ls', 'aj']\n",
      "After stemming with porters algorithm: ['urgent', 'awar', 'complimentari', 'trip', 'eurodisinc', 'trav', 'aco', 'entri', 'claim', 'txt', 'di', 'morefrmmob', 'shracomorsglsuplt']\n",
      "Tokenized sentence: ['thank', 'u', 'it', 'better', 'work', 'out', 'cause', 'i', 'will', 'feel', 'used', 'otherwise']\n",
      "After stop words removal: ['thank', 'u', 'better', 'work', 'cause', 'feel', 'used', 'otherwise']\n",
      "After stemming with porters algorithm: ['thank', 'better', 'work', 'caus', 'feel', 'us', 'otherwis']\n",
      "Tokenized sentence: ['dear', 'regret', 'i', 'cudnt', 'pick', 'call', 'drove', 'down', 'frm', 'ctla', 'now', 'at', 'cochin', 'home', 'left', 'mobile', 'in', 'car', 'ente', 'style', 'ishtamayoo', 'happy', 'bakrid']\n",
      "After stop words removal: ['dear', 'regret', 'cudnt', 'pick', 'call', 'drove', 'frm', 'ctla', 'cochin', 'home', 'left', 'mobile', 'car', 'ente', 'style', 'ishtamayoo', 'happy', 'bakrid']\n",
      "After stemming with porters algorithm: ['dear', 'regret', 'cudnt', 'pick', 'call', 'drove', 'frm', 'ctla', 'cochin', 'home', 'left', 'mobil', 'car', 'ent', 'style', 'ishtamayoo', 'happi', 'bakrid']\n",
      "Tokenized sentence: ['i', 'love', 'u', 'babe', 'r', 'u', 'sure', 'everything', 'is', 'alrite', 'is', 'he', 'being', 'an', 'idiot', 'txt', 'bak', 'girlie']\n",
      "After stop words removal: ['love', 'u', 'babe', 'r', 'u', 'sure', 'everything', 'alrite', 'idiot', 'txt', 'bak', 'girlie']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['love', 'babe', 'sure', 'everyt', 'alrit', 'idiot', 'txt', 'bak', 'girli']\n",
      "Tokenized sentence: ['can', 'i', 'please', 'come', 'up', 'now', 'imin', 'town', 'dontmatter', 'if', 'urgoin', 'outl', 'r']\n",
      "After stop words removal: ['please', 'come', 'imin', 'town', 'dontmatter', 'urgoin', 'outl', 'r']\n",
      "After stemming with porters algorithm: ['pleas', 'come', 'imin', 'town', 'dontmatt', 'urgoin', 'outl']\n",
      "Tokenized sentence: ['shopping', 'lor', 'them', 'raining', 'mah', 'hard', 'leave', 'orchard']\n",
      "After stop words removal: ['shopping', 'lor', 'raining', 'mah', 'hard', 'leave', 'orchard']\n",
      "shopp\n",
      "rain\n",
      "After stemming with porters algorithm: ['shop', 'lor', 'rain', 'mah', 'hard', 'leav', 'orchard']\n",
      "Tokenized sentence: ['i', 'can', 'call', 'in', 'lt', 'gt', 'min', 'if', 'thats', 'ok']\n",
      "After stop words removal: ['call', 'lt', 'gt', 'min', 'thats', 'ok']\n",
      "After stemming with porters algorithm: ['call', 'min', 'that']\n",
      "Tokenized sentence: ['yes', 'when', 'is', 'the', 'appt', 'again']\n",
      "After stop words removal: ['yes', 'appt']\n",
      "After stemming with porters algorithm: ['ye', 'appt']\n",
      "Tokenized sentence: ['i', 'taught', 'that', 'ranjith', 'sir', 'called', 'me', 'so', 'only', 'i', 'sms', 'like', 'that', 'becaus', 'hes', 'verifying', 'about', 'project', 'prabu', 'told', 'today', 'so', 'only', 'pa', 'dont', 'mistake', 'me']\n",
      "After stop words removal: ['taught', 'ranjith', 'sir', 'called', 'sms', 'like', 'becaus', 'hes', 'verifying', 'project', 'prabu', 'told', 'today', 'pa', 'dont', 'mistake']\n",
      "verify\n",
      "After stemming with porters algorithm: ['taught', 'ranjith', 'sir', 'call', 'sm', 'like', 'becau', 'he', 'verif', 'project', 'prabu', 'told', 'todai', 'dont', 'mistak']\n",
      "Tokenized sentence: ['so', 'u', 'workin', 'overtime', 'nigpun']\n",
      "After stop words removal: ['u', 'workin', 'overtime', 'nigpun']\n",
      "After stemming with porters algorithm: ['workin', 'overtim', 'nigpun']\n",
      "Tokenized sentence: ['from', 'www', 'applausestore', 'com', 'monthlysubscription', 'p', 'msg', 'max', 'month', 't', 'csc', 'web', 'age', 'stop', 'txt', 'stop']\n",
      "After stop words removal: ['www', 'applausestore', 'com', 'monthlysubscription', 'p', 'msg', 'max', 'month', 'csc', 'web', 'age', 'stop', 'txt', 'stop']\n",
      "After stemming with porters algorithm: ['www', 'applausestor', 'com', 'monthlysubscript', 'msg', 'max', 'month', 'csc', 'web', 'ag', 'stop', 'txt', 'stop']\n",
      "Tokenized sentence: ['well', 'i', 'will', 'watch', 'shrek', 'in', 'd', 'b']\n",
      "After stop words removal: ['well', 'watch', 'shrek', 'b']\n",
      "After stemming with porters algorithm: ['well', 'watch', 'shrek']\n",
      "Tokenized sentence: ['open', 'rebtel', 'with', 'firefox', 'when', 'it', 'loads', 'just', 'put', 'plus', 'sign', 'in', 'the', 'user', 'name', 'place', 'and', 'it', 'will', 'show', 'you', 'two', 'numbers', 'the', 'lower', 'number', 'is', 'my', 'number', 'once', 'you', 'pick', 'that', 'number', 'the', 'pin', 'will', 'display', 'okay']\n",
      "After stop words removal: ['open', 'rebtel', 'firefox', 'loads', 'put', 'plus', 'sign', 'user', 'name', 'place', 'show', 'two', 'numbers', 'lower', 'number', 'number', 'pick', 'number', 'pin', 'display', 'okay']\n",
      "After stemming with porters algorithm: ['open', 'rebtel', 'firefox', 'load', 'put', 'plu', 'sign', 'user', 'name', 'place', 'show', 'two', 'number', 'lower', 'number', 'number', 'pick', 'number', 'pin', 'displai', 'okai']\n",
      "Tokenized sentence: []\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['r', 'comin', 'back', 'for', 'dinner']\n",
      "After stop words removal: ['r', 'comin', 'back', 'dinner']\n",
      "After stemming with porters algorithm: ['comin', 'back', 'dinner']\n",
      "Tokenized sentence: ['no', 'i', 'm', 'not', 'i', 'can', 't', 'give', 'you', 'everything', 'you', 'want', 'and', 'need', 'you', 'actually', 'could', 'do', 'better', 'for', 'yourself', 'on', 'yor', 'own', 'you', 've', 'got', 'more', 'money', 'than', 'i', 'do', 'i', 'can', 't', 'get', 'work', 'i', 'can', 't', 'get', 'a', 'man', 'i', 'can', 't', 'pay', 'the', 'rent', 'i', 'can', 't', 'even', 'fill', 'my', 'fucking', 'gas', 'tank', 'yes', 'i', 'm', 'stressed', 'and', 'depressed', 'i', 'didn', 't', 'even', 'call', 'home', 'for', 'thanksgiving', 'cuz', 'i', 'll', 'have', 'to', 'tell', 'them', 'i', 'm', 'up', 'to', 'nothing']\n",
      "After stop words removal: ['give', 'everything', 'want', 'need', 'actually', 'could', 'better', 'yor', 'got', 'money', 'get', 'work', 'get', 'man', 'pay', 'rent', 'even', 'fill', 'fucking', 'gas', 'tank', 'yes', 'stressed', 'depressed', 'even', 'call', 'home', 'thanksgiving', 'cuz', 'tell', 'nothing']\n",
      "everyth\n",
      "fuck\n",
      "thanksgiv\n",
      "noth\n",
      "After stemming with porters algorithm: ['give', 'everyt', 'want', 'need', 'actual', 'could', 'better', 'yor', 'got', 'monei', 'get', 'work', 'get', 'man', 'pai', 'rent', 'even', 'fill', 'fuc', 'ga', 'tank', 'ye', 'stress', 'depress', 'even', 'call', 'home', 'thanksgiv', 'cuz', 'tell', 'not']\n",
      "Tokenized sentence: ['cool', 'do', 'you', 'like', 'swimming', 'i', 'have', 'a', 'pool', 'and', 'jacuzzi', 'at', 'my', 'house']\n",
      "After stop words removal: ['cool', 'like', 'swimming', 'pool', 'jacuzzi', 'house']\n",
      "swimm\n",
      "After stemming with porters algorithm: ['cool', 'like', 'swim', 'pool', 'jacuzzi', 'hous']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['have', 'a', 'lovely', 'night', 'and', 'when', 'you', 'wake', 'up', 'to', 'see', 'this', 'message', 'i', 'hope', 'you', 'smile', 'knowing', 'all', 'is', 'as', 'should', 'be', 'have', 'a', 'great', 'morning']\n",
      "After stop words removal: ['lovely', 'night', 'wake', 'see', 'message', 'hope', 'smile', 'knowing', 'great', 'morning']\n",
      "know\n",
      "morn\n",
      "After stemming with porters algorithm: ['love', 'night', 'wake', 'see', 'messag', 'hope', 'smile', 'knowe', 'great', 'mor']\n",
      "Tokenized sentence: ['good', 'afternoon', 'starshine', 'how', 's', 'my', 'boytoy', 'does', 'he', 'crave', 'me', 'yet', 'ache', 'to', 'fuck', 'me', 'sips', 'cappuccino', 'i', 'miss', 'you', 'babe', 'teasing', 'kiss']\n",
      "After stop words removal: ['good', 'afternoon', 'starshine', 'boytoy', 'crave', 'yet', 'ache', 'fuck', 'sips', 'cappuccino', 'miss', 'babe', 'teasing', 'kiss']\n",
      "teas\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'starshin', 'boytoi', 'crave', 'yet', 'ach', 'fuck', 'sip', 'cappuccino', 'miss', 'babe', 'teas', 'kiss']\n",
      "Tokenized sentence: ['haha', 'awesome', 'i', 'might', 'need', 'to', 'take', 'you', 'up', 'on', 'that', 'what', 'you', 'doin', 'tonight']\n",
      "After stop words removal: ['haha', 'awesome', 'might', 'need', 'take', 'doin', 'tonight']\n",
      "After stemming with porters algorithm: ['haha', 'awesom', 'might', 'need', 'take', 'doin', 'tonight']\n",
      "Tokenized sentence: ['and', 'of', 'course', 'you', 'should', 'make', 'a', 'stink']\n",
      "After stop words removal: ['course', 'make', 'stink']\n",
      "After stemming with porters algorithm: ['cours', 'make', 'stink']\n",
      "Tokenized sentence: ['swhrt', 'how', 'u', 'dey', 'hope', 'ur', 'ok', 'tot', 'about', 'u', 'day', 'love', 'n', 'miss', 'take', 'care']\n",
      "After stop words removal: ['swhrt', 'u', 'dey', 'hope', 'ur', 'ok', 'tot', 'u', 'day', 'love', 'n', 'miss', 'take', 'care']\n",
      "After stemming with porters algorithm: ['swhrt', 'dei', 'hope', 'tot', 'dai', 'love', 'miss', 'take', 'care']\n",
      "Tokenized sentence: ['where', 'wuld', 'i', 'be', 'without', 'my', 'baby', 'the', 'thought', 'alone', 'mite', 'break', 'me', 'and', 'i', 'don', 't', 'wanna', 'go', 'crazy', 'but', 'everyboy', 'needs', 'his', 'lady', 'xxxxxxxx']\n",
      "After stop words removal: ['wuld', 'without', 'baby', 'thought', 'alone', 'mite', 'break', 'wanna', 'go', 'crazy', 'everyboy', 'needs', 'lady', 'xxxxxxxx']\n",
      "After stemming with porters algorithm: ['wuld', 'without', 'babi', 'thought', 'alon', 'mite', 'break', 'wanna', 'crazi', 'everyboi', 'need', 'ladi', 'xxxxxxxx']\n",
      "Tokenized sentence: ['its', 'good', 'to', 'hear', 'from', 'you']\n",
      "After stop words removal: ['good', 'hear']\n",
      "After stemming with porters algorithm: ['good', 'hear']\n",
      "Tokenized sentence: ['im', 'fine', 'babes', 'aint', 'been', 'up', 'much', 'tho', 'saw', 'scary', 'movie', 'yest', 'its', 'quite', 'funny', 'want', 'mrw', 'afternoon', 'at', 'town', 'or', 'mall', 'or', 'sumthin', 'xx']\n",
      "After stop words removal: ['im', 'fine', 'babes', 'aint', 'much', 'tho', 'saw', 'scary', 'movie', 'yest', 'quite', 'funny', 'want', 'mrw', 'afternoon', 'town', 'mall', 'sumthin', 'xx']\n",
      "After stemming with porters algorithm: ['fine', 'babe', 'aint', 'much', 'tho', 'saw', 'scari', 'movi', 'yest', 'quit', 'funni', 'want', 'mrw', 'afternoon', 'town', 'mall', 'sumthin']\n",
      "Tokenized sentence: ['yo', 'yo', 'yo', 'byatch', 'whassup']\n",
      "After stop words removal: ['yo', 'yo', 'yo', 'byatch', 'whassup']\n",
      "After stemming with porters algorithm: ['byatch', 'whassup']\n",
      "Tokenized sentence: ['stupid', 'its', 'not', 'possible']\n",
      "After stop words removal: ['stupid', 'possible']\n",
      "After stemming with porters algorithm: ['stupid', 'possib']\n",
      "Tokenized sentence: ['i', 'm', 'glad', 'you', 'are', 'following', 'your', 'dreams']\n",
      "After stop words removal: ['glad', 'following', 'dreams']\n",
      "follow\n",
      "After stemming with porters algorithm: ['glad', 'follow', 'dream']\n",
      "Tokenized sentence: ['ha', 'ha', 'cool', 'cool', 'chikku', 'chikku', 'db']\n",
      "After stop words removal: ['ha', 'ha', 'cool', 'cool', 'chikku', 'chikku', 'db']\n",
      "After stemming with porters algorithm: ['cool', 'cool', 'chikku', 'chikku']\n",
      "Tokenized sentence: ['that', 'would', 'be', 'good', 'i', 'll', 'phone', 'you', 'tomo', 'lunchtime', 'shall', 'i', 'to', 'organise', 'something']\n",
      "After stop words removal: ['would', 'good', 'phone', 'tomo', 'lunchtime', 'shall', 'organise', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['would', 'good', 'phone', 'tomo', 'lunchtim', 'shall', 'organis', 'somet']\n",
      "Tokenized sentence: ['you', 'have', 'new', 'message', 'please', 'call']\n",
      "After stop words removal: ['new', 'message', 'please', 'call']\n",
      "After stemming with porters algorithm: ['new', 'messag', 'pleas', 'call']\n",
      "Tokenized sentence: ['hey', 'great', 'deal', 'farm', 'tour', 'am', 'to', 'pm', 'pax', 'deposit', 'by', 'may']\n",
      "After stop words removal: ['hey', 'great', 'deal', 'farm', 'tour', 'pm', 'pax', 'deposit', 'may']\n",
      "After stemming with porters algorithm: ['hei', 'great', 'deal', 'farm', 'tour', 'pax', 'deposit', 'mai']\n",
      "Tokenized sentence: ['bbq', 'this', 'sat', 'at', 'mine', 'from', 'ish', 'ur', 'welcome', 'come']\n",
      "After stop words removal: ['bbq', 'sat', 'mine', 'ish', 'ur', 'welcome', 'come']\n",
      "After stemming with porters algorithm: ['bbq', 'sat', 'mine', 'ish', 'welcom', 'come']\n",
      "Tokenized sentence: ['no', 'other', 'valentines', 'huh', 'the', 'proof', 'is', 'on', 'your', 'fb', 'page', 'ugh', 'i', 'm', 'so', 'glad', 'i', 'really', 'didn', 't', 'watch', 'your', 'rupaul', 'show', 'you', 'tool']\n",
      "After stop words removal: ['valentines', 'huh', 'proof', 'fb', 'page', 'ugh', 'glad', 'really', 'watch', 'rupaul', 'show', 'tool']\n",
      "After stemming with porters algorithm: ['valentin', 'huh', 'proof', 'page', 'ugh', 'glad', 'realli', 'watch', 'rupaul', 'show', 'tool']\n",
      "Tokenized sentence: ['sorry', 'for', 'the', 'delay', 'yes', 'masters']\n",
      "After stop words removal: ['sorry', 'delay', 'yes', 'masters']\n",
      "After stemming with porters algorithm: ['sorri', 'delai', 'ye', 'master']\n",
      "Tokenized sentence: ['i', 'dont', 'know', 'oh', 'hopefully', 'this', 'month']\n",
      "After stop words removal: ['dont', 'know', 'oh', 'hopefully', 'month']\n",
      "After stemming with porters algorithm: ['dont', 'know', 'hopefulli', 'month']\n",
      "Tokenized sentence: ['not', 'sure', 'yet', 'still', 'trying', 'to', 'get', 'a', 'hold', 'of', 'him']\n",
      "After stop words removal: ['sure', 'yet', 'still', 'trying', 'get', 'hold']\n",
      "After stemming with porters algorithm: ['sure', 'yet', 'still', 'trying', 'get', 'hold']\n",
      "Tokenized sentence: ['everybody', 'had', 'fun', 'this', 'evening', 'miss', 'you']\n",
      "After stop words removal: ['everybody', 'fun', 'evening', 'miss']\n",
      "even\n",
      "After stemming with porters algorithm: ['everybodi', 'fun', 'even', 'miss']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['i', 'tot', 'it', 's', 'my', 'group', 'mate', 'lucky', 'i', 'havent', 'reply', 'wat', 'time', 'do', 'need', 'to', 'leave']\n",
      "After stop words removal: ['tot', 'group', 'mate', 'lucky', 'havent', 'reply', 'wat', 'time', 'need', 'leave']\n",
      "After stemming with porters algorithm: ['tot', 'group', 'mate', 'lucki', 'havent', 'repli', 'wat', 'time', 'need', 'leav']\n",
      "Tokenized sentence: ['sms', 'ac', 'blind', 'date', 'u', 'rodds', 'is', 'm', 'from', 'aberdeen', 'united', 'kingdom', 'check', 'him', 'out', 'http', 'img', 'sms', 'ac', 'w', 'icmb', 'cktz', 'r', 'no', 'blind', 'dates', 'send', 'hide']\n",
      "After stop words removal: ['sms', 'ac', 'blind', 'date', 'u', 'rodds', 'aberdeen', 'united', 'kingdom', 'check', 'http', 'img', 'sms', 'ac', 'w', 'icmb', 'cktz', 'r', 'blind', 'dates', 'send', 'hide']\n",
      "After stemming with porters algorithm: ['sm', 'blind', 'date', 'rodd', 'aberdeen', 'unit', 'kingdom', 'check', 'http', 'img', 'sm', 'icmb', 'cktz', 'blind', 'date', 'send', 'hide']\n",
      "Tokenized sentence: ['i', 'm', 'back', 'amp', 'we', 're', 'packing', 'the', 'car', 'now', 'i', 'll', 'let', 'you', 'know', 'if', 'there', 's', 'room']\n",
      "After stop words removal: ['back', 'amp', 'packing', 'car', 'let', 'know', 'room']\n",
      "pack\n",
      "After stemming with porters algorithm: ['back', 'amp', 'pac', 'car', 'let', 'know', 'room']\n",
      "Tokenized sentence: ['you', 'still', 'coming', 'tonight']\n",
      "After stop words removal: ['still', 'coming', 'tonight']\n",
      "com\n",
      "After stemming with porters algorithm: ['still', 'come', 'tonight']\n",
      "Tokenized sentence: ['so', 'anyways', 'you', 'can', 'just', 'go', 'to', 'your', 'gym', 'or', 'whatever', 'my', 'love', 'smiles', 'i', 'hope', 'your', 'ok', 'and', 'having', 'a', 'good', 'day', 'babe', 'i', 'miss', 'you', 'so', 'much', 'already']\n",
      "After stop words removal: ['anyways', 'go', 'gym', 'whatever', 'love', 'smiles', 'hope', 'ok', 'good', 'day', 'babe', 'miss', 'much', 'already']\n",
      "After stemming with porters algorithm: ['anywai', 'gym', 'whatev', 'love', 'smile', 'hope', 'good', 'dai', 'babe', 'miss', 'much', 'alreadi']\n",
      "Tokenized sentence: ['oh', 'u', 'must', 'have', 'taken', 'your', 'real', 'valentine', 'out', 'shopping', 'first']\n",
      "After stop words removal: ['oh', 'u', 'must', 'taken', 'real', 'valentine', 'shopping', 'first']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['must', 'taken', 'real', 'valentin', 'shop', 'first']\n",
      "Tokenized sentence: ['doing', 'project', 'w', 'frens', 'lor']\n",
      "After stop words removal: ['project', 'w', 'frens', 'lor']\n",
      "After stemming with porters algorithm: ['project', 'fren', 'lor']\n",
      "Tokenized sentence: ['there', 'are', 'some', 'nice', 'pubs', 'near', 'here', 'or', 'there', 'is', 'frankie', 'n', 'bennys', 'near', 'the', 'warner', 'cinema']\n",
      "After stop words removal: ['nice', 'pubs', 'near', 'frankie', 'n', 'bennys', 'near', 'warner', 'cinema']\n",
      "After stemming with porters algorithm: ['nice', 'pub', 'near', 'franki', 'benni', 'near', 'warner', 'cinema']\n",
      "Tokenized sentence: ['i', 'realise', 'you', 'are', 'a', 'busy', 'guy', 'and', 'i', 'm', 'trying', 'not', 'to', 'be', 'a', 'bother', 'i', 'have', 'to', 'get', 'some', 'exams', 'outta', 'the', 'way', 'and', 'then', 'try', 'the', 'cars', 'do', 'have', 'a', 'gr', 'day']\n",
      "After stop words removal: ['realise', 'busy', 'guy', 'trying', 'bother', 'get', 'exams', 'outta', 'way', 'try', 'cars', 'gr', 'day']\n",
      "After stemming with porters algorithm: ['realis', 'busi', 'gui', 'trying', 'bother', 'get', 'exam', 'outta', 'wai', 'try', 'car', 'dai']\n",
      "Tokenized sentence: ['oh', 'k', 'after', 'that', 'placement', 'there', 'ah']\n",
      "After stop words removal: ['oh', 'k', 'placement', 'ah']\n",
      "After stemming with porters algorithm: ['placem']\n",
      "Tokenized sentence: ['i', 'probably', 'won', 't', 'eat', 'at', 'all', 'today', 'i', 'think', 'i', 'm', 'gonna', 'pop', 'how', 'was', 'your', 'weekend', 'did', 'u', 'miss', 'me']\n",
      "After stop words removal: ['probably', 'eat', 'today', 'think', 'gonna', 'pop', 'weekend', 'u', 'miss']\n",
      "After stemming with porters algorithm: ['probab', 'eat', 'todai', 'think', 'gonna', 'pop', 'weekend', 'miss']\n",
      "Tokenized sentence: ['kind', 'of', 'took', 'it', 'to', 'garage', 'centre', 'part', 'of', 'exhaust', 'needs', 'replacing', 'part', 'ordered', 'n', 'taking', 'it', 'to', 'be', 'fixed', 'tomo', 'morning']\n",
      "After stop words removal: ['kind', 'took', 'garage', 'centre', 'part', 'exhaust', 'needs', 'replacing', 'part', 'ordered', 'n', 'taking', 'fixed', 'tomo', 'morning']\n",
      "replac\n",
      "tak\n",
      "morn\n",
      "After stemming with porters algorithm: ['kind', 'took', 'garag', 'centr', 'part', 'exhaust', 'need', 'replac', 'part', 'order', 'take', 'fix', 'tomo', 'mor']\n",
      "Tokenized sentence: ['k', 'eng', 'rocking', 'in', 'ashes']\n",
      "After stop words removal: ['k', 'eng', 'rocking', 'ashes']\n",
      "rock\n",
      "After stemming with porters algorithm: ['eng', 'roc', 'ash']\n",
      "Tokenized sentence: ['you', 'are', 'guaranteed', 'the', 'latest', 'nokia', 'phone', 'a', 'gb', 'ipod', 'mp', 'player', 'or', 'a', 'prize', 'txt', 'word', 'collect', 'to', 'no', 'ibhltd', 'ldnw', 'h', 'p', 'mtmsgrcvd']\n",
      "After stop words removal: ['guaranteed', 'latest', 'nokia', 'phone', 'gb', 'ipod', 'mp', 'player', 'prize', 'txt', 'word', 'collect', 'ibhltd', 'ldnw', 'h', 'p', 'mtmsgrcvd']\n",
      "After stemming with porters algorithm: ['guaranteed', 'latest', 'nokia', 'phone', 'ipod', 'player', 'priz', 'txt', 'word', 'collect', 'ibhltd', 'ldnw', 'mtmsgrcvd']\n",
      "Tokenized sentence: ['jesus', 'christ', 'bitch', 'i', 'm', 'trying', 'to', 'give', 'you', 'drugs', 'answer', 'your', 'fucking', 'phone']\n",
      "After stop words removal: ['jesus', 'christ', 'bitch', 'trying', 'give', 'drugs', 'answer', 'fucking', 'phone']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['jesu', 'christ', 'bitch', 'trying', 'give', 'drug', 'answer', 'fuc', 'phone']\n",
      "Tokenized sentence: ['oh', 'that', 'was', 'a', 'forwarded', 'message', 'i', 'thought', 'you', 'send', 'that', 'to', 'me']\n",
      "After stop words removal: ['oh', 'forwarded', 'message', 'thought', 'send']\n",
      "After stemming with porters algorithm: ['forwar', 'messag', 'thought', 'send']\n",
      "Tokenized sentence: ['urgh', 'coach', 'hot', 'smells', 'of', 'chip', 'fat', 'thanks', 'again', 'especially', 'for', 'the', 'duvet', 'not', 'a', 'predictive', 'text', 'word']\n",
      "After stop words removal: ['urgh', 'coach', 'hot', 'smells', 'chip', 'fat', 'thanks', 'especially', 'duvet', 'predictive', 'text', 'word']\n",
      "After stemming with porters algorithm: ['urgh', 'coach', 'hot', 'smell', 'chip', 'fat', 'thank', 'especi', 'duvet', 'predict', 'text', 'word']\n",
      "Tokenized sentence: ['if', 'he', 'started', 'searching', 'he', 'will', 'get', 'job', 'in', 'few', 'days', 'he', 'have', 'great', 'potential', 'and', 'talent']\n",
      "After stop words removal: ['started', 'searching', 'get', 'job', 'days', 'great', 'potential', 'talent']\n",
      "search\n",
      "After stemming with porters algorithm: ['star', 'searc', 'get', 'job', 'dai', 'great', 'potenti', 'talent']\n",
      "Tokenized sentence: ['what', 's', 'up', 'my', 'own', 'oga', 'left', 'my', 'phone', 'at', 'home', 'and', 'just', 'saw', 'ur', 'messages', 'hope', 'you', 'are', 'good', 'have', 'a', 'great', 'weekend']\n",
      "After stop words removal: ['oga', 'left', 'phone', 'home', 'saw', 'ur', 'messages', 'hope', 'good', 'great', 'weekend']\n",
      "After stemming with porters algorithm: ['oga', 'left', 'phone', 'home', 'saw', 'messag', 'hope', 'good', 'great', 'weekend']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'holding', 'up']\n",
      "After stop words removal: ['holding']\n",
      "hold\n",
      "After stemming with porters algorithm: ['hol']\n",
      "Tokenized sentence: ['does', 'cinema', 'plus', 'drink', 'appeal', 'tomo', 'is', 'a', 'fr', 'thriller', 'by', 'director', 'i', 'like', 'on', 'at', 'mac', 'at']\n",
      "After stop words removal: ['cinema', 'plus', 'drink', 'appeal', 'tomo', 'fr', 'thriller', 'director', 'like', 'mac']\n",
      "After stemming with porters algorithm: ['cinema', 'plu', 'drink', 'appeal', 'tomo', 'thriller', 'director', 'like', 'mac']\n",
      "Tokenized sentence: ['when', 'you', 'just', 'put', 'in', 'the', 'sign', 'choose', 'my', 'number', 'and', 'the', 'pin', 'will', 'show', 'right']\n",
      "After stop words removal: ['put', 'sign', 'choose', 'number', 'pin', 'show', 'right']\n",
      "After stemming with porters algorithm: ['put', 'sign', 'choos', 'number', 'pin', 'show', 'right']\n",
      "Tokenized sentence: ['which', 'channel']\n",
      "After stop words removal: ['channel']\n",
      "After stemming with porters algorithm: ['channel']\n",
      "Tokenized sentence: ['bears', 'pic', 'nick', 'and', 'tom', 'pete', 'and', 'dick', 'in', 'fact', 'all', 'types', 'try', 'gay', 'chat', 'with', 'photo', 'upload', 'call', 'p', 'min', 'stop', 'texts', 'call']\n",
      "After stop words removal: ['bears', 'pic', 'nick', 'tom', 'pete', 'dick', 'fact', 'types', 'try', 'gay', 'chat', 'photo', 'upload', 'call', 'p', 'min', 'stop', 'texts', 'call']\n",
      "After stemming with porters algorithm: ['bear', 'pic', 'nick', 'tom', 'pete', 'dick', 'fact', 'type', 'try', 'gai', 'chat', 'photo', 'upload', 'call', 'min', 'stop', 'text', 'call']\n",
      "Tokenized sentence: ['aiyah', 'then', 'i', 'wait', 'lor', 'then', 'u', 'entertain', 'me', 'hee']\n",
      "After stop words removal: ['aiyah', 'wait', 'lor', 'u', 'entertain', 'hee']\n",
      "After stemming with porters algorithm: ['aiyah', 'wait', 'lor', 'entertain', 'hee']\n",
      "Tokenized sentence: ['somewhere', 'out', 'there', 'beneath', 'the', 'pale', 'moon', 'light', 'someone', 'think', 'in', 'of', 'u', 'some', 'where', 'out', 'there', 'where', 'dreams', 'come', 'true', 'goodnite', 'amp', 'sweet', 'dreams']\n",
      "After stop words removal: ['somewhere', 'beneath', 'pale', 'moon', 'light', 'someone', 'think', 'u', 'dreams', 'come', 'true', 'goodnite', 'amp', 'sweet', 'dreams']\n",
      "After stemming with porters algorithm: ['somewher', 'beneath', 'pale', 'moon', 'light', 'someon', 'think', 'dream', 'come', 'true', 'goodnit', 'amp', 'sweet', 'dream']\n",
      "Tokenized sentence: ['were', 'gonna', 'go', 'get', 'some', 'tacos']\n",
      "After stop words removal: ['gonna', 'go', 'get', 'tacos']\n",
      "After stemming with porters algorithm: ['gonna', 'get', 'taco']\n",
      "Tokenized sentence: ['in', 'sch', 'but', 'neva', 'mind', 'u', 'eat', 'st', 'lor']\n",
      "After stop words removal: ['sch', 'neva', 'mind', 'u', 'eat', 'st', 'lor']\n",
      "After stemming with porters algorithm: ['sch', 'neva', 'mind', 'eat', 'lor']\n",
      "Tokenized sentence: ['sorry', 'da', 'today', 'i', 'wont', 'come', 'to', 'play', 'i', 'have', 'driving', 'clas']\n",
      "After stop words removal: ['sorry', 'da', 'today', 'wont', 'come', 'play', 'driving', 'clas']\n",
      "driv\n",
      "After stemming with porters algorithm: ['sorri', 'todai', 'wont', 'come', 'plai', 'drive', 'cla']\n",
      "Tokenized sentence: ['hows', 'the', 'street', 'where', 'the', 'end', 'of', 'library', 'walk', 'is']\n",
      "After stop words removal: ['hows', 'street', 'end', 'library', 'walk']\n",
      "After stemming with porters algorithm: ['how', 'street', 'end', 'librari', 'walk']\n",
      "Tokenized sentence: ['lol', 'yes', 'but', 'it', 'will', 'add', 'some', 'spice', 'to', 'your', 'day']\n",
      "After stop words removal: ['lol', 'yes', 'add', 'spice', 'day']\n",
      "After stemming with porters algorithm: ['lol', 'ye', 'add', 'spice', 'dai']\n",
      "Tokenized sentence: ['should', 'i', 'send', 'you', 'naughty', 'pix']\n",
      "After stop words removal: ['send', 'naughty', 'pix']\n",
      "After stemming with porters algorithm: ['send', 'naughti', 'pix']\n",
      "Tokenized sentence: ['living', 'is', 'very', 'simple', 'loving', 'is', 'also', 'simple', 'laughing', 'is', 'too', 'simple', 'winning', 'is', 'tooo', 'simple', 'but', 'being', 'simple', 'is', 'very', 'difficult', 'gud', 'nte']\n",
      "After stop words removal: ['living', 'simple', 'loving', 'also', 'simple', 'laughing', 'simple', 'winning', 'tooo', 'simple', 'simple', 'difficult', 'gud', 'nte']\n",
      "liv\n",
      "lov\n",
      "laugh\n",
      "winn\n",
      "After stemming with porters algorithm: ['live', 'simpl', 'love', 'also', 'simpl', 'laug', 'simpl', 'win', 'tooo', 'simpl', 'simpl', 'difficult', 'gud', 'nte']\n",
      "Tokenized sentence: ['hahaha', 'use', 'your', 'brain', 'dear']\n",
      "After stop words removal: ['hahaha', 'use', 'brain', 'dear']\n",
      "After stemming with porters algorithm: ['hahaha', 'us', 'brain', 'dear']\n",
      "Tokenized sentence: ['my', 'new', 'years', 'eve', 'was', 'ok', 'i', 'went', 'to', 'a', 'party', 'with', 'my', 'boyfriend', 'who', 'is', 'this', 'si', 'then', 'hey']\n",
      "After stop words removal: ['new', 'years', 'eve', 'ok', 'went', 'party', 'boyfriend', 'si', 'hey']\n",
      "After stemming with porters algorithm: ['new', 'year', 'ev', 'went', 'parti', 'boyfriend', 'hei']\n",
      "Tokenized sentence: ['hi', 'darlin', 'i', 'finish', 'at', 'do', 'u', 'pick', 'me', 'up', 'or', 'meet', 'me', 'text', 'back', 'on', 'this', 'number', 'luv', 'kate', 'xxx']\n",
      "After stop words removal: ['hi', 'darlin', 'finish', 'u', 'pick', 'meet', 'text', 'back', 'number', 'luv', 'kate', 'xxx']\n",
      "After stemming with porters algorithm: ['darlin', 'finish', 'pick', 'meet', 'text', 'back', 'number', 'luv', 'kate', 'xxx']\n",
      "Tokenized sentence: ['cold', 'dont', 'be', 'sad', 'dear']\n",
      "After stop words removal: ['cold', 'dont', 'sad', 'dear']\n",
      "After stemming with porters algorithm: ['cold', 'dont', 'sad', 'dear']\n",
      "Tokenized sentence: ['haha', 'where', 'got', 'so', 'fast', 'lose', 'weight', 'thk', 'muz', 'go', 'a', 'month', 'den', 'got', 'effect', 'gee', 'later', 'we', 'go', 'aust', 'put', 'bk', 'e', 'weight']\n",
      "After stop words removal: ['haha', 'got', 'fast', 'lose', 'weight', 'thk', 'muz', 'go', 'month', 'den', 'got', 'effect', 'gee', 'later', 'go', 'aust', 'put', 'bk', 'e', 'weight']\n",
      "After stemming with porters algorithm: ['haha', 'got', 'fast', 'lose', 'weight', 'thk', 'muz', 'month', 'den', 'got', 'effect', 'gee', 'later', 'aust', 'put', 'weight']\n",
      "Tokenized sentence: ['i', 'am', 'not', 'having', 'her', 'number', 'sir']\n",
      "After stop words removal: ['number', 'sir']\n",
      "After stemming with porters algorithm: ['number', 'sir']\n",
      "Tokenized sentence: ['aiyar', 'u', 'so', 'poor', 'thing', 'i', 'give', 'u', 'my', 'support', 'k', 'jia', 'you', 'i', 'll', 'think', 'of', 'u']\n",
      "After stop words removal: ['aiyar', 'u', 'poor', 'thing', 'give', 'u', 'support', 'k', 'jia', 'think', 'u']\n",
      "After stemming with porters algorithm: ['aiyar', 'poor', 'thing', 'give', 'support', 'jia', 'think']\n",
      "Tokenized sentence: ['lets', 'use', 'it', 'next', 'week', 'princess']\n",
      "After stop words removal: ['lets', 'use', 'next', 'week', 'princess']\n",
      "After stemming with porters algorithm: ['let', 'us', 'next', 'week', 'princess']\n",
      "Tokenized sentence: ['geeeee', 'your', 'internet', 'is', 'really', 'bad', 'today', 'eh']\n",
      "After stop words removal: ['geeeee', 'internet', 'really', 'bad', 'today', 'eh']\n",
      "After stemming with porters algorithm: ['geeeee', 'internet', 'realli', 'bad', 'todai']\n",
      "Tokenized sentence: ['kinda', 'first', 'one', 'gets', 'in', 'at', 'twelve', 'aah', 'speak', 'tomo']\n",
      "After stop words removal: ['kinda', 'first', 'one', 'gets', 'twelve', 'aah', 'speak', 'tomo']\n",
      "After stemming with porters algorithm: ['kinda', 'first', 'on', 'get', 'twelv', 'aah', 'speak', 'tomo']\n",
      "Tokenized sentence: ['yup', 'bathe', 'liao']\n",
      "After stop words removal: ['yup', 'bathe', 'liao']\n",
      "After stemming with porters algorithm: ['yup', 'bath', 'liao']\n",
      "Tokenized sentence: ['hey', 'mr', 'and', 'i', 'are', 'going', 'to', 'the', 'sea', 'view', 'and', 'having', 'a', 'couple', 'of', 'gays', 'i', 'mean', 'games', 'give', 'me', 'a', 'bell', 'when', 'ya', 'finish']\n",
      "After stop words removal: ['hey', 'mr', 'going', 'sea', 'view', 'couple', 'gays', 'mean', 'games', 'give', 'bell', 'ya', 'finish']\n",
      "go\n",
      "After stemming with porters algorithm: ['hei', 'go', 'sea', 'view', 'coupl', 'gai', 'mean', 'game', 'give', 'bell', 'finish']\n",
      "Tokenized sentence: ['dis', 'is', 'yijue', 'i', 'jus', 'saw', 'ur', 'mail', 'in', 'case', 'huiming', 'havent', 'sent', 'u', 'my', 'num', 'dis', 'is', 'my', 'num']\n",
      "After stop words removal: ['dis', 'yijue', 'jus', 'saw', 'ur', 'mail', 'case', 'huiming', 'havent', 'sent', 'u', 'num', 'dis', 'num']\n",
      "huim\n",
      "After stemming with porters algorithm: ['di', 'yiju', 'ju', 'saw', 'mail', 'case', 'huim', 'havent', 'sent', 'num', 'di', 'num']\n",
      "Tokenized sentence: ['i', 'got', 'like', 'lt', 'gt', 'i', 'can', 'get', 'some', 'more', 'later', 'though', 'get', 'whatever', 'you', 'feel', 'like']\n",
      "After stop words removal: ['got', 'like', 'lt', 'gt', 'get', 'later', 'though', 'get', 'whatever', 'feel', 'like']\n",
      "After stemming with porters algorithm: ['got', 'like', 'get', 'later', 'though', 'get', 'whatev', 'feel', 'like']\n",
      "Tokenized sentence: ['we', 'walked', 'from', 'my', 'moms', 'right', 'on', 'stagwood', 'pass', 'right', 'on', 'winterstone', 'left', 'on', 'victors', 'hill', 'address', 'is', 'lt', 'gt']\n",
      "After stop words removal: ['walked', 'moms', 'right', 'stagwood', 'pass', 'right', 'winterstone', 'left', 'victors', 'hill', 'address', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['wal', 'mom', 'right', 'stagwood', 'pass', 'right', 'winterston', 'left', 'victor', 'hill', 'address']\n",
      "Tokenized sentence: ['company', 'is', 'very', 'good', 'environment', 'is', 'terrific', 'and', 'food', 'is', 'really', 'nice']\n",
      "After stop words removal: ['company', 'good', 'environment', 'terrific', 'food', 'really', 'nice']\n",
      "After stemming with porters algorithm: ['compani', 'good', 'environ', 'terrif', 'food', 'realli', 'nice']\n",
      "Tokenized sentence: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'a', 'top', 'sony', 'dvd', 'player', 'if', 'u', 'know', 'which', 'country', 'the', 'algarve', 'is', 'in', 'txt', 'ansr', 'to', 'sp', 'tyrone']\n",
      "After stop words removal: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'top', 'sony', 'dvd', 'player', 'u', 'know', 'country', 'algarve', 'txt', 'ansr', 'sp', 'tyrone']\n",
      "After stemming with porters algorithm: ['sunshin', 'quiz', 'wkly', 'win', 'top', 'soni', 'dvd', 'player', 'know', 'countri', 'algarv', 'txt', 'ansr', 'tyrone']\n",
      "Tokenized sentence: ['uhhhhrmm', 'isnt', 'having', 'tb', 'test', 'bad', 'when', 'youre', 'sick']\n",
      "After stop words removal: ['uhhhhrmm', 'isnt', 'tb', 'test', 'bad', 'youre', 'sick']\n",
      "After stemming with porters algorithm: ['uhhhhrmm', 'isnt', 'test', 'bad', 'your', 'sick']\n",
      "Tokenized sentence: ['hi', 'dear', 'call', 'me', 'its', 'urgnt', 'i', 'don', 't', 'know', 'whats', 'your', 'problem', 'you', 'don', 't', 'want', 'to', 'work', 'or', 'if', 'you', 'have', 'any', 'other', 'problem', 'at', 'least', 'tell', 'me', 'wating', 'for', 'your', 'reply']\n",
      "After stop words removal: ['hi', 'dear', 'call', 'urgnt', 'know', 'whats', 'problem', 'want', 'work', 'problem', 'least', 'tell', 'wating', 'reply']\n",
      "wat\n",
      "wate\n",
      "After stemming with porters algorithm: ['dear', 'call', 'urgnt', 'know', 'what', 'problem', 'want', 'work', 'problem', 'least', 'tell', 'wate', 'repli']\n",
      "Tokenized sentence: ['goodmorning', 'today', 'i', 'am', 'late', 'for', 'lt', 'decimal', 'gt', 'min']\n",
      "After stop words removal: ['goodmorning', 'today', 'late', 'lt', 'decimal', 'gt', 'min']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['goodmor', 'todai', 'late', 'decim', 'min']\n",
      "Tokenized sentence: ['just', 'sing', 'hu', 'i', 'think', 'its', 'also', 'important', 'to', 'find', 'someone', 'female', 'that', 'know', 'the', 'place', 'well', 'preferably', 'a', 'citizen', 'that', 'is', 'also', 'smart', 'to', 'help', 'you', 'navigate', 'through', 'even', 'things', 'like', 'choosing', 'a', 'phone', 'plan', 'require', 'guidance', 'when', 'in', 'doubt', 'ask', 'especially', 'girls']\n",
      "After stop words removal: ['sing', 'hu', 'think', 'also', 'important', 'find', 'someone', 'female', 'know', 'place', 'well', 'preferably', 'citizen', 'also', 'smart', 'help', 'navigate', 'even', 'things', 'like', 'choosing', 'phone', 'plan', 'require', 'guidance', 'doubt', 'ask', 'especially', 'girls']\n",
      "choos\n",
      "After stemming with porters algorithm: ['sing', 'think', 'also', 'import', 'find', 'someon', 'femal', 'know', 'place', 'well', 'prefer', 'citizen', 'also', 'smart', 'help', 'navig', 'even', 'thing', 'like', 'choos', 'phone', 'plan', 'requir', 'guidanc', 'doubt', 'ask', 'especi', 'girl']\n",
      "Tokenized sentence: ['i', 'think', 'if', 'he', 'rule', 'tamilnadu', 'then', 'its', 'very', 'tough', 'for', 'our', 'people']\n",
      "After stop words removal: ['think', 'rule', 'tamilnadu', 'tough', 'people']\n",
      "After stemming with porters algorithm: ['think', 'rule', 'tamilnadu', 'tough', 'peopl']\n",
      "Tokenized sentence: ['england', 'v', 'macedonia', 'dont', 'miss', 'the', 'goals', 'team', 'news', 'txt', 'ur', 'national', 'team', 'to', 'eg', 'england', 'to', 'try', 'wales', 'scotland', 'txt', 'poboxox', 'w', 'wq']\n",
      "After stop words removal: ['england', 'v', 'macedonia', 'dont', 'miss', 'goals', 'team', 'news', 'txt', 'ur', 'national', 'team', 'eg', 'england', 'try', 'wales', 'scotland', 'txt', 'poboxox', 'w', 'wq']\n",
      "After stemming with porters algorithm: ['england', 'macedonia', 'dont', 'miss', 'goal', 'team', 'new', 'txt', 'nat', 'team', 'england', 'try', 'wale', 'scotland', 'txt', 'poboxox']\n",
      "Tokenized sentence: ['wat', 'makes', 'u', 'thk', 'i', 'll', 'fall', 'down', 'but', 'actually', 'i', 'thk', 'i', 'm', 'quite', 'prone', 'falls', 'lucky', 'my', 'dad', 'at', 'home', 'i', 'ask', 'him', 'come', 'n', 'fetch', 'me', 'already']\n",
      "After stop words removal: ['wat', 'makes', 'u', 'thk', 'fall', 'actually', 'thk', 'quite', 'prone', 'falls', 'lucky', 'dad', 'home', 'ask', 'come', 'n', 'fetch', 'already']\n",
      "After stemming with porters algorithm: ['wat', 'make', 'thk', 'fall', 'actual', 'thk', 'quit', 'prone', 'fall', 'lucki', 'dad', 'home', 'ask', 'come', 'fetch', 'alreadi']\n",
      "Tokenized sentence: ['hows', 'the', 'champ', 'just', 'leaving', 'glasgow']\n",
      "After stop words removal: ['hows', 'champ', 'leaving', 'glasgow']\n",
      "leav\n",
      "After stemming with porters algorithm: ['how', 'champ', 'leav', 'glasgow']\n",
      "Tokenized sentence: ['you', 'still', 'at', 'the', 'game']\n",
      "After stop words removal: ['still', 'game']\n",
      "After stemming with porters algorithm: ['still', 'game']\n",
      "Tokenized sentence: ['in', 'work', 'now', 'going', 'have', 'in', 'few', 'min']\n",
      "After stop words removal: ['work', 'going', 'min']\n",
      "go\n",
      "After stemming with porters algorithm: ['work', 'go', 'min']\n",
      "Tokenized sentence: ['delhi', 'and', 'chennai', 'still', 'silent']\n",
      "After stop words removal: ['delhi', 'chennai', 'still', 'silent']\n",
      "After stemming with porters algorithm: ['delhi', 'chennai', 'still', 'silent']\n",
      "Tokenized sentence: ['in', 'that', 'case', 'i', 'guess', 'i', 'll', 'see', 'you', 'at', 'campus', 'lodge']\n",
      "After stop words removal: ['case', 'guess', 'see', 'campus', 'lodge']\n",
      "After stemming with porters algorithm: ['case', 'guess', 'see', 'campu', 'lodg']\n",
      "Tokenized sentence: ['accordingly', 'i', 'repeat', 'just', 'text', 'the', 'word', 'ok', 'on', 'your', 'mobile', 'phone', 'and', 'send']\n",
      "After stop words removal: ['accordingly', 'repeat', 'text', 'word', 'ok', 'mobile', 'phone', 'send']\n",
      "After stemming with porters algorithm: ['accordingli', 'repeat', 'text', 'word', 'mobil', 'phone', 'send']\n",
      "Tokenized sentence: ['smsservices', 'for', 'yourinclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'with', 'stop', 'no', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stop words removal: ['smsservices', 'yourinclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'stop', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stemming with porters algorithm: ['smsservic', 'yourinclus', 'text', 'credit', 'pl', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscrib', 'stop', 'extra', 'charg', 'help', 'comuk']\n",
      "Tokenized sentence: ['new', 'car', 'and', 'house', 'for', 'my', 'parents', 'i', 'have', 'only', 'new', 'job', 'in', 'hand']\n",
      "After stop words removal: ['new', 'car', 'house', 'parents', 'new', 'job', 'hand']\n",
      "After stemming with porters algorithm: ['new', 'car', 'hous', 'parent', 'new', 'job', 'hand']\n",
      "Tokenized sentence: ['do', 'well', 'all', 'will', 'for', 'little', 'time', 'thing', 'of', 'good', 'times', 'ahead']\n",
      "After stop words removal: ['well', 'little', 'time', 'thing', 'good', 'times', 'ahead']\n",
      "After stemming with porters algorithm: ['well', 'littl', 'time', 'thing', 'good', 'time', 'ahead']\n",
      "Tokenized sentence: ['tf', 'p']\n",
      "After stop words removal: ['tf', 'p']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['that', 's', 'fine', 'i', 'll', 'bitch', 'at', 'you', 'about', 'it', 'later', 'then']\n",
      "After stop words removal: ['fine', 'bitch', 'later']\n",
      "After stemming with porters algorithm: ['fine', 'bitch', 'later']\n",
      "Tokenized sentence: ['cool', 'what', 'time', 'you', 'think', 'you', 'can', 'get', 'here']\n",
      "After stop words removal: ['cool', 'time', 'think', 'get']\n",
      "After stemming with porters algorithm: ['cool', 'time', 'think', 'get']\n",
      "Tokenized sentence: ['sorry', 'in', 'meeting', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'meeting', 'call', 'later']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'meet', 'call', 'later']\n",
      "Tokenized sentence: ['beautiful', 'truth', 'against', 'gravity', 'read', 'carefully', 'our', 'heart', 'feels', 'light', 'when', 'someone', 'is', 'in', 'it', 'but', 'it', 'feels', 'very', 'heavy', 'when', 'someone', 'leaves', 'it', 'goodmorning']\n",
      "After stop words removal: ['beautiful', 'truth', 'gravity', 'read', 'carefully', 'heart', 'feels', 'light', 'someone', 'feels', 'heavy', 'someone', 'leaves', 'goodmorning']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['beauti', 'truth', 'graviti', 'read', 'carefulli', 'heart', 'feel', 'light', 'someon', 'feel', 'heavi', 'someon', 'leav', 'goodmor']\n",
      "Tokenized sentence: ['i', 'have', 'to', 'take', 'exam', 'with', 'in', 'march']\n",
      "After stop words removal: ['take', 'exam', 'march']\n",
      "After stemming with porters algorithm: ['take', 'exam', 'march']\n",
      "Tokenized sentence: ['o', 'ic', 'lol', 'should', 'play', 'doors', 'sometime', 'yo']\n",
      "After stop words removal: ['ic', 'lol', 'play', 'doors', 'sometime', 'yo']\n",
      "After stemming with porters algorithm: ['lol', 'plai', 'door', 'sometim']\n",
      "Tokenized sentence: ['thinkin', 'about', 'someone', 'is', 'all', 'good', 'no', 'drugs', 'for', 'that']\n",
      "After stop words removal: ['thinkin', 'someone', 'good', 'drugs']\n",
      "After stemming with porters algorithm: ['thinkin', 'someon', 'good', 'drug']\n",
      "Tokenized sentence: ['cheers', 'lou', 'yeah', 'was', 'a', 'goodnite', 'shame', 'u', 'neva', 'came', 'c', 'ya', 'gailxx']\n",
      "After stop words removal: ['cheers', 'lou', 'yeah', 'goodnite', 'shame', 'u', 'neva', 'came', 'c', 'ya', 'gailxx']\n",
      "After stemming with porters algorithm: ['cheer', 'lou', 'yeah', 'goodnit', 'shame', 'neva', 'came', 'gailxx']\n",
      "Tokenized sentence: ['gonna', 'let', 'me', 'know', 'cos', 'comes', 'bak', 'from', 'holiday', 'that', 'day', 'is', 'coming', 'don', 't', 'get', 'text', 'me', 'number']\n",
      "After stop words removal: ['gonna', 'let', 'know', 'cos', 'comes', 'bak', 'holiday', 'day', 'coming', 'get', 'text', 'number']\n",
      "com\n",
      "After stemming with porters algorithm: ['gonna', 'let', 'know', 'co', 'come', 'bak', 'holidai', 'dai', 'come', 'get', 'text', 'number']\n",
      "Tokenized sentence: ['arr', 'birthday', 'today', 'i', 'wish', 'him', 'to', 'get', 'more', 'oscar']\n",
      "After stop words removal: ['arr', 'birthday', 'today', 'wish', 'get', 'oscar']\n",
      "After stemming with porters algorithm: ['arr', 'birthdai', 'todai', 'wish', 'get', 'oscar']\n",
      "Tokenized sentence: ['jus', 'chillaxin', 'what', 'up']\n",
      "After stop words removal: ['jus', 'chillaxin']\n",
      "After stemming with porters algorithm: ['ju', 'chillaxin']\n",
      "Tokenized sentence: ['you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'from', 'landline', 'delivery', 'within', 'days', 't', 'cs', 'box', 'm', 'bp', 'yr', 'warranty', 'ppm', 'p', 'p']\n",
      "After stop words removal: ['awarded', 'sipix', 'digital', 'camera', 'call', 'landline', 'delivery', 'within', 'days', 'cs', 'box', 'bp', 'yr', 'warranty', 'ppm', 'p', 'p']\n",
      "After stemming with porters algorithm: ['awar', 'sipix', 'digit', 'camera', 'call', 'landlin', 'deliveri', 'within', 'dai', 'box', 'warranti', 'ppm']\n",
      "Tokenized sentence: ['uncle', 'boye', 'i', 'need', 'movies', 'oh', 'guide', 'me', 'plus', 'you', 'know', 'torrents', 'are', 'not', 'particularly', 'legal', 'here', 'and', 'the', 'system', 'is', 'slowing', 'down', 'what', 'should', 'i', 'do', 'have', 'a', 'gr', 'day', 'plus', 'have', 'you', 'started', 'cos', 'i', 'dont', 'meet', 'you', 'online', 'how', 'was', 'the', 'honey', 'moon']\n",
      "After stop words removal: ['uncle', 'boye', 'need', 'movies', 'oh', 'guide', 'plus', 'know', 'torrents', 'particularly', 'legal', 'system', 'slowing', 'gr', 'day', 'plus', 'started', 'cos', 'dont', 'meet', 'online', 'honey', 'moon']\n",
      "slow\n",
      "After stemming with porters algorithm: ['uncl', 'boy', 'need', 'movi', 'guid', 'plu', 'know', 'torrent', 'particularli', 'legal', 'system', 'slowe', 'dai', 'plu', 'star', 'co', 'dont', 'meet', 'onlin', 'honei', 'moon']\n",
      "Tokenized sentence: ['yeah', 'hopefully', 'if', 'tyler', 'can', 't', 'do', 'it', 'i', 'could', 'maybe', 'ask', 'around', 'a', 'bit']\n",
      "After stop words removal: ['yeah', 'hopefully', 'tyler', 'could', 'maybe', 'ask', 'around', 'bit']\n",
      "After stemming with porters algorithm: ['yeah', 'hopefulli', 'tyler', 'could', 'mayb', 'ask', 'around', 'bit']\n",
      "Tokenized sentence: ['this', 'girl', 'does', 'not', 'stay', 'in', 'bed', 'this', 'girl', 'doesn', 't', 'need', 'recovery', 'time', 'id', 'rather', 'pass', 'out', 'while', 'having', 'fun', 'then', 'be', 'cooped', 'up', 'in', 'bed']\n",
      "After stop words removal: ['girl', 'stay', 'bed', 'girl', 'need', 'recovery', 'time', 'id', 'rather', 'pass', 'fun', 'cooped', 'bed']\n",
      "After stemming with porters algorithm: ['girl', 'stai', 'bed', 'girl', 'need', 'recoveri', 'time', 'rather', 'pass', 'fun', 'coop', 'bed']\n",
      "Tokenized sentence: ['tell', 'your', 'friends', 'what', 'you', 'plan', 'to', 'do', 'on', 'valentines', 'day', 'lt', 'url', 'gt']\n",
      "After stop words removal: ['tell', 'friends', 'plan', 'valentines', 'day', 'lt', 'url', 'gt']\n",
      "After stemming with porters algorithm: ['tell', 'friend', 'plan', 'valentin', 'dai', 'url']\n",
      "Tokenized sentence: ['good', 'morning', 'my', 'dear', 'have', 'a', 'great', 'amp', 'successful', 'day']\n",
      "After stop words removal: ['good', 'morning', 'dear', 'great', 'amp', 'successful', 'day']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'dear', 'great', 'amp', 'success', 'dai']\n",
      "Tokenized sentence: ['i', 'have', 'docs', 'appointments', 'next', 'week', 'i', 'm', 'tired', 'of', 'them', 'shoving', 'stuff', 'up', 'me', 'ugh', 'why', 'couldn', 't', 'i', 'have', 'had', 'a', 'normal', 'body']\n",
      "After stop words removal: ['docs', 'appointments', 'next', 'week', 'tired', 'shoving', 'stuff', 'ugh', 'normal', 'body']\n",
      "shov\n",
      "After stemming with porters algorithm: ['doc', 'appoint', 'next', 'week', 'tire', 'shove', 'stuff', 'ugh', 'normal', 'bodi']\n",
      "Tokenized sentence: ['goin', 'to', 'workout', 'lor', 'muz', 'lose', 'e', 'fats']\n",
      "After stop words removal: ['goin', 'workout', 'lor', 'muz', 'lose', 'e', 'fats']\n",
      "After stemming with porters algorithm: ['goin', 'workout', 'lor', 'muz', 'lose', 'fat']\n",
      "Tokenized sentence: ['long', 'beach', 'lor', 'expected', 'u', 'having', 'dinner', 'now']\n",
      "After stop words removal: ['long', 'beach', 'lor', 'expected', 'u', 'dinner']\n",
      "After stemming with porters algorithm: ['long', 'beach', 'lor', 'expec', 'dinner']\n",
      "Tokenized sentence: ['if', 'i', 'start', 'sending', 'blackberry', 'torch', 'to', 'nigeria', 'will', 'you', 'find', 'buyer', 'for', 'me', 'like', 'a', 'month', 'and', 'tell', 'dad', 'not', 'to', 'buy', 'bb', 'from', 'anyone', 'oh']\n",
      "After stop words removal: ['start', 'sending', 'blackberry', 'torch', 'nigeria', 'find', 'buyer', 'like', 'month', 'tell', 'dad', 'buy', 'bb', 'anyone', 'oh']\n",
      "send\n",
      "After stemming with porters algorithm: ['start', 'sen', 'blackberri', 'torch', 'nigeria', 'find', 'buyer', 'like', 'month', 'tell', 'dad', 'bui', 'anyon']\n",
      "Tokenized sentence: ['i', 'wonder', 'if', 'your', 'phone', 'battery', 'went', 'dead', 'i', 'had', 'to', 'tell', 'you', 'i', 'love', 'you', 'babe']\n",
      "After stop words removal: ['wonder', 'phone', 'battery', 'went', 'dead', 'tell', 'love', 'babe']\n",
      "After stemming with porters algorithm: ['wonder', 'phone', 'batteri', 'went', 'dead', 'tell', 'love', 'babe']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'nokia', 'i', 'this', 'is', 'what', 'you', 'get', 'when', 'you', 'win', 'our', 'free', 'auction', 'to', 'take', 'part', 'send', 'nokia', 'to', 'now', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stop words removal: ['nokia', 'get', 'win', 'free', 'auction', 'take', 'part', 'send', 'nokia', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stemming with porters algorithm: ['nokia', 'get', 'win', 'free', 'auct', 'take', 'part', 'send', 'nokia', 'suit', 'land', 'row', 'jhl']\n",
      "Tokenized sentence: ['congrats', 'year', 'special', 'cinema', 'pass', 'for', 'is', 'yours', 'call', 'now', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'all', 'free', 'bx', 'ip', 'we', 'pm', 'dont', 'miss', 'out']\n",
      "After stop words removal: ['congrats', 'year', 'special', 'cinema', 'pass', 'call', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'free', 'bx', 'ip', 'pm', 'dont', 'miss']\n",
      "After stemming with porters algorithm: ['congrat', 'year', 'special', 'cinema', 'pass', 'call', 'suprman', 'matrix', 'starwar', 'etc', 'free', 'dont', 'miss']\n",
      "Tokenized sentence: ['im', 'gonnamissu', 'so', 'much', 'i', 'would', 'say', 'il', 'send', 'u', 'a', 'postcard', 'buttheres', 'aboutas', 'much', 'chance', 'of', 'merememberin', 'asthere', 'is', 'ofsi', 'not', 'breakin', 'his', 'contract', 'luv', 'yaxx']\n",
      "After stop words removal: ['im', 'gonnamissu', 'much', 'would', 'say', 'il', 'send', 'u', 'postcard', 'buttheres', 'aboutas', 'much', 'chance', 'merememberin', 'asthere', 'ofsi', 'breakin', 'contract', 'luv', 'yaxx']\n",
      "After stemming with porters algorithm: ['gonnamissu', 'much', 'would', 'sai', 'send', 'postcard', 'butther', 'abouta', 'much', 'chanc', 'merememberin', 'asther', 'ofsi', 'breakin', 'contract', 'luv', 'yaxx']\n",
      "Tokenized sentence: ['yeah', 'in', 'fact', 'he', 'just', 'asked', 'if', 'we', 'needed', 'anything', 'like', 'an', 'hour', 'ago', 'when', 'and', 'how', 'much']\n",
      "After stop words removal: ['yeah', 'fact', 'asked', 'needed', 'anything', 'like', 'hour', 'ago', 'much']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['yeah', 'fact', 'as', 'need', 'anyt', 'like', 'hour', 'ago', 'much']\n",
      "Tokenized sentence: ['ok', 'i', 'also', 'wan', 'watch', 'e', 'pm', 'show']\n",
      "After stop words removal: ['ok', 'also', 'wan', 'watch', 'e', 'pm', 'show']\n",
      "After stemming with porters algorithm: ['also', 'wan', 'watch', 'show']\n",
      "Tokenized sentence: ['how', 'was', 'txting', 'and', 'driving']\n",
      "After stop words removal: ['txting', 'driving']\n",
      "driv\n",
      "After stemming with porters algorithm: ['txting', 'drive']\n",
      "Tokenized sentence: ['sorry', 'it', 's', 'a', 'lot', 'of', 'friend', 'of', 'a', 'friend', 'stuff', 'i', 'm', 'just', 'now', 'about', 'to', 'talk', 'to', 'the', 'actual', 'guy', 'who', 'wants', 'to', 'buy']\n",
      "After stop words removal: ['sorry', 'lot', 'friend', 'friend', 'stuff', 'talk', 'actual', 'guy', 'wants', 'buy']\n",
      "After stemming with porters algorithm: ['sorri', 'lot', 'friend', 'friend', 'stuff', 'talk', 'actual', 'gui', 'want', 'bui']\n",
      "Tokenized sentence: ['alright', 'thanks', 'for', 'the', 'advice', 'enjoy', 'your', 'night', 'out', 'i', 'ma', 'try', 'to', 'get', 'some', 'sleep']\n",
      "After stop words removal: ['alright', 'thanks', 'advice', 'enjoy', 'night', 'try', 'get', 'sleep']\n",
      "After stemming with porters algorithm: ['alright', 'thank', 'advic', 'enjoi', 'night', 'try', 'get', 'sleep']\n",
      "Tokenized sentence: ['my', 'love', 'how', 'come', 'it', 'took', 'you', 'so', 'long', 'to', 'leave', 'for', 'zaher', 's', 'i', 'got', 'your', 'words', 'on', 'ym', 'and', 'was', 'happy', 'to', 'see', 'them', 'but', 'was', 'sad', 'you', 'had', 'left', 'i', 'miss', 'you']\n",
      "After stop words removal: ['love', 'come', 'took', 'long', 'leave', 'zaher', 'got', 'words', 'ym', 'happy', 'see', 'sad', 'left', 'miss']\n",
      "After stemming with porters algorithm: ['love', 'come', 'took', 'long', 'leav', 'zaher', 'got', 'word', 'happi', 'see', 'sad', 'left', 'miss']\n",
      "Tokenized sentence: ['dont', 'gimme', 'that', 'lip', 'caveboy']\n",
      "After stop words removal: ['dont', 'gimme', 'lip', 'caveboy']\n",
      "After stemming with porters algorithm: ['dont', 'gimm', 'lip', 'caveboi']\n",
      "Tokenized sentence: ['mila', 'age', 'blonde', 'new', 'in', 'uk', 'i', 'look', 'sex', 'with', 'uk', 'guys', 'if', 'u', 'like', 'fun', 'with', 'me', 'text', 'mtalk', 'to', 'pp', 'txt', 'st', 'free', 'increments', 'help']\n",
      "After stop words removal: ['mila', 'age', 'blonde', 'new', 'uk', 'look', 'sex', 'uk', 'guys', 'u', 'like', 'fun', 'text', 'mtalk', 'pp', 'txt', 'st', 'free', 'increments', 'help']\n",
      "After stemming with porters algorithm: ['mila', 'ag', 'blond', 'new', 'look', 'sex', 'gui', 'like', 'fun', 'text', 'mtalk', 'txt', 'free', 'increm', 'help']\n",
      "Tokenized sentence: ['fffff', 'can', 'you', 'text', 'kadeem', 'or', 'are', 'you', 'too', 'far', 'gone']\n",
      "After stop words removal: ['fffff', 'text', 'kadeem', 'far', 'gone']\n",
      "After stemming with porters algorithm: ['fffff', 'text', 'kadeem', 'far', 'gone']\n",
      "Tokenized sentence: ['dear', 'why', 'you', 'mood', 'off', 'i', 'cant', 'drive', 'so', 'i', 'brother', 'to', 'drive']\n",
      "After stop words removal: ['dear', 'mood', 'cant', 'drive', 'brother', 'drive']\n",
      "After stemming with porters algorithm: ['dear', 'mood', 'cant', 'drive', 'brother', 'drive']\n",
      "Tokenized sentence: ['cthen', 'i', 'thk', 'shd', 'b', 'enuff', 'still', 'got', 'conclusion', 'n', 'contents', 'pg', 'n', 'references', 'i', 'll', 'b', 'doing', 'da', 'contents', 'pg', 'n', 'cover', 'pg']\n",
      "After stop words removal: ['cthen', 'thk', 'shd', 'b', 'enuff', 'still', 'got', 'conclusion', 'n', 'contents', 'pg', 'n', 'references', 'b', 'da', 'contents', 'pg', 'n', 'cover', 'pg']\n",
      "After stemming with porters algorithm: ['cthen', 'thk', 'shd', 'enuff', 'still', 'got', 'conclus', 'content', 'refer', 'content', 'cover']\n",
      "Tokenized sentence: ['gud', 'ni', 'swt', 'drms', 'take', 'care']\n",
      "After stop words removal: ['gud', 'ni', 'swt', 'drms', 'take', 'care']\n",
      "After stemming with porters algorithm: ['gud', 'swt', 'drm', 'take', 'care']\n",
      "Tokenized sentence: ['how', 'come']\n",
      "After stop words removal: ['come']\n",
      "After stemming with porters algorithm: ['come']\n",
      "Tokenized sentence: ['i', 'went', 'to', 'ur', 'hon', 'lab', 'but', 'no', 'one', 'is', 'there']\n",
      "After stop words removal: ['went', 'ur', 'hon', 'lab', 'one']\n",
      "After stemming with porters algorithm: ['went', 'hon', 'lab', 'on']\n",
      "Tokenized sentence: ['pls', 'help', 'me', 'tell', 'ashley', 'that', 'i', 'cant', 'find', 'her', 'number', 'oh']\n",
      "After stop words removal: ['pls', 'help', 'tell', 'ashley', 'cant', 'find', 'number', 'oh']\n",
      "After stemming with porters algorithm: ['pl', 'help', 'tell', 'ashlei', 'cant', 'find', 'number']\n",
      "Tokenized sentence: ['lol', 'grr', 'my', 'mom', 'is', 'taking', 'forever', 'with', 'my', 'prescription', 'pharmacy', 'is', 'like', 'minutes', 'away', 'ugh']\n",
      "After stop words removal: ['lol', 'grr', 'mom', 'taking', 'forever', 'prescription', 'pharmacy', 'like', 'minutes', 'away', 'ugh']\n",
      "tak\n",
      "After stemming with porters algorithm: ['lol', 'grr', 'mom', 'take', 'forev', 'prescript', 'pharmaci', 'like', 'minut', 'awai', 'ugh']\n",
      "Tokenized sentence: ['free', 'video', 'camera', 'phones', 'with', 'half', 'price', 'line', 'rental', 'for', 'mths', 'and', 'cross', 'ntwk', 'mins', 'txts', 'call', 'mobileupd', 'or', 'call', 'optout']\n",
      "After stop words removal: ['free', 'video', 'camera', 'phones', 'half', 'price', 'line', 'rental', 'mths', 'cross', 'ntwk', 'mins', 'txts', 'call', 'mobileupd', 'call', 'optout']\n",
      "After stemming with porters algorithm: ['free', 'video', 'camera', 'phone', 'half', 'price', 'line', 'rental', 'mth', 'cross', 'ntwk', 'min', 'txt', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['baaaaabe', 'i', 'misss', 'youuuuu', 'where', 'are', 'you', 'i', 'have', 'to', 'go', 'and', 'teach', 'my', 'class', 'at']\n",
      "After stop words removal: ['baaaaabe', 'misss', 'youuuuu', 'go', 'teach', 'class']\n",
      "After stemming with porters algorithm: ['baaaaab', 'misss', 'youuuuu', 'teach', 'class']\n",
      "Tokenized sentence: ['cool', 'i', 'am', 'lt', 'gt', 'inches', 'long', 'hope', 'you', 'like', 'them', 'big']\n",
      "After stop words removal: ['cool', 'lt', 'gt', 'inches', 'long', 'hope', 'like', 'big']\n",
      "After stemming with porters algorithm: ['cool', 'inch', 'long', 'hope', 'like', 'big']\n",
      "Tokenized sentence: ['draw', 'va', 'i', 'dont', 'think', 'so']\n",
      "After stop words removal: ['draw', 'va', 'dont', 'think']\n",
      "After stemming with porters algorithm: ['draw', 'dont', 'think']\n",
      "Tokenized sentence: ['that', 's', 'good', 'because', 'i', 'need', 'drugs']\n",
      "After stop words removal: ['good', 'need', 'drugs']\n",
      "After stemming with porters algorithm: ['good', 'need', 'drug']\n",
      "Tokenized sentence: ['we', 'are', 'both', 'fine', 'thanks']\n",
      "After stop words removal: ['fine', 'thanks']\n",
      "After stemming with porters algorithm: ['fine', 'thank']\n",
      "Tokenized sentence: ['wait', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['wait', 'lt', 'gt', 'min']\n",
      "After stemming with porters algorithm: ['wait', 'min']\n",
      "Tokenized sentence: ['lol', 'no', 'just', 'was', 'busy']\n",
      "After stop words removal: ['lol', 'busy']\n",
      "After stemming with porters algorithm: ['lol', 'busi']\n",
      "Tokenized sentence: ['i', 'm', 'going', 'out', 'to', 'buy', 'mum', 's', 'present', 'ar']\n",
      "After stop words removal: ['going', 'buy', 'mum', 'present', 'ar']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'bui', 'mum', 'present']\n",
      "Tokenized sentence: ['stupid', 'its', 'not', 'possible']\n",
      "After stop words removal: ['stupid', 'possible']\n",
      "After stemming with porters algorithm: ['stupid', 'possib']\n",
      "Tokenized sentence: ['bored', 'of', 'speed', 'dating', 'try', 'speedchat', 'txt', 'speedchat', 'to', 'if', 'you', 'don', 't', 'like', 'em', 'txt', 'swap', 'and', 'get', 'a', 'new', 'chatter', 'chat', 'pobox', 'w', 'wq', 'p', 'msg', 'rcd']\n",
      "After stop words removal: ['bored', 'speed', 'dating', 'try', 'speedchat', 'txt', 'speedchat', 'like', 'em', 'txt', 'swap', 'get', 'new', 'chatter', 'chat', 'pobox', 'w', 'wq', 'p', 'msg', 'rcd']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['bore', 'speed', 'date', 'try', 'speedchat', 'txt', 'speedchat', 'like', 'txt', 'swap', 'get', 'new', 'chatter', 'chat', 'pobox', 'msg', 'rcd']\n",
      "Tokenized sentence: ['i', 'm', 'reaching', 'home', 'in', 'min']\n",
      "After stop words removal: ['reaching', 'home', 'min']\n",
      "reach\n",
      "After stemming with porters algorithm: ['reac', 'home', 'min']\n",
      "Tokenized sentence: ['your', 'account', 'has', 'been', 'refilled', 'successfully', 'by', 'inr', 'lt', 'decimal', 'gt', 'your', 'keralacircle', 'prepaid', 'account', 'balance', 'is', 'rs', 'lt', 'decimal', 'gt', 'your', 'transaction', 'id', 'is', 'kr', 'lt', 'gt']\n",
      "After stop words removal: ['account', 'refilled', 'successfully', 'inr', 'lt', 'decimal', 'gt', 'keralacircle', 'prepaid', 'account', 'balance', 'rs', 'lt', 'decimal', 'gt', 'transaction', 'id', 'kr', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['account', 'refil', 'successfulli', 'inr', 'decim', 'keralacirc', 'prepaid', 'account', 'balanc', 'decim', 'transact']\n",
      "Tokenized sentence: ['shall', 'i', 'ask', 'one', 'thing', 'if', 'you', 'dont', 'mistake', 'me']\n",
      "After stop words removal: ['shall', 'ask', 'one', 'thing', 'dont', 'mistake']\n",
      "After stemming with porters algorithm: ['shall', 'ask', 'on', 'thing', 'dont', 'mistak']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'go', 'to', 'perumbavoor']\n",
      "After stop words removal: ['want', 'go', 'perumbavoor']\n",
      "After stemming with porters algorithm: ['want', 'perumbavoor']\n",
      "Tokenized sentence: ['also', 'track', 'down', 'any', 'lighters', 'you', 'can', 'find']\n",
      "After stop words removal: ['also', 'track', 'lighters', 'find']\n",
      "After stemming with porters algorithm: ['also', 'track', 'lighter', 'find']\n",
      "Tokenized sentence: ['hot', 'live', 'fantasies', 'call', 'now', 'just', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'k']\n",
      "After stop words removal: ['hot', 'live', 'fantasies', 'call', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'k']\n",
      "After stemming with porters algorithm: ['hot', 'live', 'fantasi', 'call', 'per', 'min', 'ntt', 'ltd', 'box', 'croydon']\n",
      "Tokenized sentence: ['i', 'am', 'ft', 'we', 'will', 'be', 'a', 'good', 'combination']\n",
      "After stop words removal: ['ft', 'good', 'combination']\n",
      "After stemming with porters algorithm: ['good', 'combin']\n",
      "Tokenized sentence: ['long', 'time', 'you', 'remember', 'me', 'today']\n",
      "After stop words removal: ['long', 'time', 'remember', 'today']\n",
      "After stemming with porters algorithm: ['long', 'time', 'rememb', 'todai']\n",
      "Tokenized sentence: ['thanx', 'sending', 'me', 'home']\n",
      "After stop words removal: ['thanx', 'sending', 'home']\n",
      "send\n",
      "After stemming with porters algorithm: ['thanx', 'sen', 'home']\n",
      "Tokenized sentence: ['ok', 'k', 'sry', 'i', 'knw', 'siva', 'tats', 'y', 'i', 'askd']\n",
      "After stop words removal: ['ok', 'k', 'sry', 'knw', 'siva', 'tats', 'askd']\n",
      "After stemming with porters algorithm: ['sry', 'knw', 'siva', 'tat', 'askd']\n",
      "Tokenized sentence: ['dont', 'put', 'your', 'phone', 'on', 'silent', 'mode', 'ok']\n",
      "After stop words removal: ['dont', 'put', 'phone', 'silent', 'mode', 'ok']\n",
      "After stemming with porters algorithm: ['dont', 'put', 'phone', 'silent', 'mode']\n",
      "Tokenized sentence: ['as', 'in', 'i', 'want', 'custom', 'officer', 'discount', 'oh']\n",
      "After stop words removal: ['want', 'custom', 'officer', 'discount', 'oh']\n",
      "After stemming with porters algorithm: ['want', 'custom', 'offic', 'discount']\n",
      "Tokenized sentence: ['i', 'm', 'home', 'my', 'love', 'if', 'your', 'still', 'awake', 'loving', 'kiss']\n",
      "After stop words removal: ['home', 'love', 'still', 'awake', 'loving', 'kiss']\n",
      "lov\n",
      "After stemming with porters algorithm: ['home', 'love', 'still', 'awak', 'love', 'kiss']\n",
      "Tokenized sentence: ['oh', 'oh', 'den', 'muz', 'change', 'plan', 'liao', 'go', 'back', 'have', 'to', 'yan', 'jiu', 'again']\n",
      "After stop words removal: ['oh', 'oh', 'den', 'muz', 'change', 'plan', 'liao', 'go', 'back', 'yan', 'jiu']\n",
      "After stemming with porters algorithm: ['den', 'muz', 'chang', 'plan', 'liao', 'back', 'yan', 'jiu']\n",
      "Tokenized sentence: ['hi', 'darlin', 'did', 'youphone', 'me', 'im', 'athome', 'if', 'youwanna', 'chat']\n",
      "After stop words removal: ['hi', 'darlin', 'youphone', 'im', 'athome', 'youwanna', 'chat']\n",
      "After stemming with porters algorithm: ['darlin', 'youphon', 'athom', 'youwanna', 'chat']\n",
      "Tokenized sentence: ['i', 'know', 'you', 'are', 'thinkin', 'malaria', 'but', 'relax', 'children', 'cant', 'handle', 'malaria', 'she', 'would', 'have', 'been', 'worse', 'and', 'its', 'gastroenteritis', 'if', 'she', 'takes', 'enough', 'to', 'replace', 'her', 'loss', 'her', 'temp', 'will', 'reduce', 'and', 'if', 'you', 'give', 'her', 'malaria', 'meds', 'now', 'she', 'will', 'just', 'vomit', 'its', 'a', 'self', 'limiting', 'illness', 'she', 'has', 'which', 'means', 'in', 'a', 'few', 'days', 'it', 'will', 'completely', 'stop']\n",
      "After stop words removal: ['know', 'thinkin', 'malaria', 'relax', 'children', 'cant', 'handle', 'malaria', 'would', 'worse', 'gastroenteritis', 'takes', 'enough', 'replace', 'loss', 'temp', 'reduce', 'give', 'malaria', 'meds', 'vomit', 'self', 'limiting', 'illness', 'means', 'days', 'completely', 'stop']\n",
      "limit\n",
      "After stemming with porters algorithm: ['know', 'thinkin', 'malaria', 'relax', 'children', 'cant', 'handl', 'malaria', 'would', 'wors', 'gastroenter', 'take', 'enough', 'replac', 'loss', 'temp', 'reduc', 'give', 'malaria', 'med', 'vomit', 'self', 'limit', 'ill', 'mean', 'dai', 'complet', 'stop']\n",
      "Tokenized sentence: ['cps', 'is', 'causing', 'the', 'outages', 'to', 'conserve', 'energy']\n",
      "After stop words removal: ['cps', 'causing', 'outages', 'conserve', 'energy']\n",
      "caus\n",
      "After stemming with porters algorithm: ['cp', 'caus', 'outag', 'conserv', 'energi']\n",
      "Tokenized sentence: ['please', 'call', 'amanda', 'with', 'regard', 'to', 'renewing', 'or', 'upgrading', 'your', 'current', 't', 'mobile', 'handset', 'free', 'of', 'charge', 'offer', 'ends', 'today', 'tel', 'subject', 'to', 't', 's', 'and', 'c', 's']\n",
      "After stop words removal: ['please', 'call', 'amanda', 'regard', 'renewing', 'upgrading', 'current', 'mobile', 'handset', 'free', 'charge', 'offer', 'ends', 'today', 'tel', 'subject', 'c']\n",
      "renew\n",
      "upgrad\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'amanda', 'regard', 'renew', 'upgrad', 'current', 'mobil', 'handset', 'free', 'charg', 'offer', 'end', 'todai', 'tel', 'subject']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'm', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['k', 'k', 'why', 'cant', 'you', 'come', 'here', 'and', 'search', 'job']\n",
      "After stop words removal: ['k', 'k', 'cant', 'come', 'search', 'job']\n",
      "After stemming with porters algorithm: ['cant', 'come', 'search', 'job']\n",
      "Tokenized sentence: ['yes', 'princess', 'i', 'want', 'to', 'catch', 'you', 'with', 'my', 'big', 'strong', 'hands']\n",
      "After stop words removal: ['yes', 'princess', 'want', 'catch', 'big', 'strong', 'hands']\n",
      "After stemming with porters algorithm: ['ye', 'princess', 'want', 'catch', 'big', 'strong', 'hand']\n",
      "Tokenized sentence: ['yes', 'just', 'finished', 'watching', 'days', 'of', 'our', 'lives', 'i', 'love', 'it']\n",
      "After stop words removal: ['yes', 'finished', 'watching', 'days', 'lives', 'love']\n",
      "watch\n",
      "After stemming with porters algorithm: ['ye', 'finis', 'watc', 'dai', 'live', 'love']\n",
      "Tokenized sentence: ['nothing', 'really', 'just', 'making', 'sure', 'everybody', 's', 'up', 'to', 'speed']\n",
      "After stop words removal: ['nothing', 'really', 'making', 'sure', 'everybody', 'speed']\n",
      "noth\n",
      "mak\n",
      "After stemming with porters algorithm: ['not', 'realli', 'make', 'sure', 'everybodi', 'speed']\n",
      "Tokenized sentence: ['ok', 'c', 'ya']\n",
      "After stop words removal: ['ok', 'c', 'ya']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['big', 'brother', 'alert', 'the', 'computer', 'has', 'selected', 'u', 'for', 'k', 'cash', 'or', 'voucher', 'call', 'ntt', 'po', 'box', 'cro', 'bt', 'landline', 'cost', 'ppm', 'mobiles', 'vary']\n",
      "After stop words removal: ['big', 'brother', 'alert', 'computer', 'selected', 'u', 'k', 'cash', 'voucher', 'call', 'ntt', 'po', 'box', 'cro', 'bt', 'landline', 'cost', 'ppm', 'mobiles', 'vary']\n",
      "After stemming with porters algorithm: ['big', 'brother', 'alert', 'comput', 'selec', 'cash', 'voucher', 'call', 'ntt', 'box', 'cro', 'landlin', 'cost', 'ppm', 'mobil', 'vari']\n",
      "Tokenized sentence: ['k', 'i', 'will', 'sent', 'it', 'again']\n",
      "After stop words removal: ['k', 'sent']\n",
      "After stemming with porters algorithm: ['sent']\n",
      "Tokenized sentence: ['p', 'per', 'min', 'to', 'call', 'germany', 'from', 'your', 'bt', 'line', 'just', 'p', 'per', 'min', 'check', 'planettalkinstant', 'com', 'for', 'info', 't', 's', 'c', 's', 'text', 'stop', 'to', 'opt', 'out']\n",
      "After stop words removal: ['p', 'per', 'min', 'call', 'germany', 'bt', 'line', 'p', 'per', 'min', 'check', 'planettalkinstant', 'com', 'info', 'c', 'text', 'stop', 'opt']\n",
      "After stemming with porters algorithm: ['per', 'min', 'call', 'germani', 'line', 'per', 'min', 'check', 'planettalkinst', 'com', 'info', 'text', 'stop', 'opt']\n",
      "Tokenized sentence: ['on', 'a', 'tuesday', 'night', 'r', 'u', 'real']\n",
      "After stop words removal: ['tuesday', 'night', 'r', 'u', 'real']\n",
      "After stemming with porters algorithm: ['tuesdai', 'night', 'real']\n",
      "Tokenized sentence: ['gr', 'new', 'service', 'live', 'sex', 'video', 'chat', 'on', 'your', 'mob', 'see', 'the', 'sexiest', 'dirtiest', 'girls', 'live', 'on', 'ur', 'phone', 'details', 'text', 'horny', 'to', 'to', 'cancel', 'send', 'stop', 'to']\n",
      "After stop words removal: ['gr', 'new', 'service', 'live', 'sex', 'video', 'chat', 'mob', 'see', 'sexiest', 'dirtiest', 'girls', 'live', 'ur', 'phone', 'details', 'text', 'horny', 'cancel', 'send', 'stop']\n",
      "After stemming with porters algorithm: ['new', 'servic', 'live', 'sex', 'video', 'chat', 'mob', 'see', 'sexiest', 'dirtiest', 'girl', 'live', 'phone', 'detail', 'text', 'horni', 'cancel', 'send', 'stop']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'tirupur', 'da', 'once', 'you', 'started', 'from', 'office', 'call', 'me']\n",
      "After stop words removal: ['tirupur', 'da', 'started', 'office', 'call']\n",
      "After stemming with porters algorithm: ['tirupur', 'star', 'offic', 'call']\n",
      "Tokenized sentence: ['hi', 'you', 'just', 'spoke', 'to', 'maneesha', 'v', 'we', 'd', 'like', 'to', 'know', 'if', 'you', 'were', 'satisfied', 'with', 'the', 'experience', 'reply', 'toll', 'free', 'with', 'yes', 'or', 'no']\n",
      "After stop words removal: ['hi', 'spoke', 'maneesha', 'v', 'like', 'know', 'satisfied', 'experience', 'reply', 'toll', 'free', 'yes']\n",
      "After stemming with porters algorithm: ['spoke', 'maneesha', 'like', 'know', 'satisfi', 'experi', 'repli', 'toll', 'free', 'ye']\n",
      "Tokenized sentence: ['yeah', 'i', 'll', 'try', 'to', 'scrounge', 'something', 'up']\n",
      "After stop words removal: ['yeah', 'try', 'scrounge', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['yeah', 'try', 'scroung', 'somet']\n",
      "Tokenized sentence: ['i', 'don', 't', 'want', 'you', 'to', 'leave', 'but', 'i', 'm', 'barely', 'doing', 'what', 'i', 'can', 'to', 'stay', 'sane', 'fighting', 'with', 'you', 'constantly', 'isn', 't', 'helping']\n",
      "After stop words removal: ['want', 'leave', 'barely', 'stay', 'sane', 'fighting', 'constantly', 'helping']\n",
      "fight\n",
      "help\n",
      "After stemming with porters algorithm: ['want', 'leav', 'bare', 'stai', 'sane', 'figh', 'constantli', 'hel']\n",
      "Tokenized sentence: ['sorry', 'i', 'guess', 'whenever', 'i', 'can', 'get', 'a', 'hold', 'of', 'my', 'connections', 'maybe', 'an', 'hour', 'or', 'two', 'i', 'll', 'text', 'you']\n",
      "After stop words removal: ['sorry', 'guess', 'whenever', 'get', 'hold', 'connections', 'maybe', 'hour', 'two', 'text']\n",
      "After stemming with porters algorithm: ['sorri', 'guess', 'whenev', 'get', 'hold', 'connect', 'mayb', 'hour', 'two', 'text']\n",
      "Tokenized sentence: ['shb', 'b', 'ok', 'lor', 'thanx']\n",
      "After stop words removal: ['shb', 'b', 'ok', 'lor', 'thanx']\n",
      "After stemming with porters algorithm: ['shb', 'lor', 'thanx']\n",
      "Tokenized sentence: ['i', 'm', 'not', 'coming', 'over', 'do', 'whatever', 'you', 'want']\n",
      "After stop words removal: ['coming', 'whatever', 'want']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'whatev', 'want']\n",
      "Tokenized sentence: ['erm', 'i', 'thought', 'the', 'contract', 'ran', 'out', 'the', 'th', 'of', 'october']\n",
      "After stop words removal: ['erm', 'thought', 'contract', 'ran', 'th', 'october']\n",
      "After stemming with porters algorithm: ['erm', 'thought', 'contract', 'ran', 'octob']\n",
      "Tokenized sentence: ['i', 'll', 'text', 'you', 'when', 'i', 'drop', 'x', 'off']\n",
      "After stop words removal: ['text', 'drop', 'x']\n",
      "After stemming with porters algorithm: ['text', 'drop']\n",
      "Tokenized sentence: ['oh', 'yes', 'i', 've', 'just', 'been', 'a', 'little', 'under', 'the', 'weather', 'so', 'i', 've', 'kind', 'of', 'been', 'coccooning', 'at', 'home']\n",
      "After stop words removal: ['oh', 'yes', 'little', 'weather', 'kind', 'coccooning', 'home']\n",
      "coccoon\n",
      "After stemming with porters algorithm: ['ye', 'littl', 'weather', 'kind', 'coccoon', 'home']\n",
      "Tokenized sentence: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'contact', 'u', 'u', 'have', 'won', 'the', 'pound', 'prize', 'claim', 'is', 'easy', 'call', 'now', 'only', 'p', 'per', 'min', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'min', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['time', 'tri', 'contact', 'pound', 'priz', 'claim', 'easi', 'call', 'per', 'min', 'nat', 'rate']\n",
      "Tokenized sentence: ['welp', 'apparently', 'he', 'retired']\n",
      "After stop words removal: ['welp', 'apparently', 'retired']\n",
      "After stemming with porters algorithm: ['welp', 'appar', 'retir']\n",
      "Tokenized sentence: ['i', 'thought', 'slide', 'is', 'enough']\n",
      "After stop words removal: ['thought', 'slide', 'enough']\n",
      "After stemming with porters algorithm: ['thought', 'slide', 'enough']\n",
      "Tokenized sentence: ['well', 'i', 'm', 'going', 'to', 'be', 'an', 'aunty']\n",
      "After stop words removal: ['well', 'going', 'aunty']\n",
      "go\n",
      "After stemming with porters algorithm: ['well', 'go', 'aunti']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['do', 'u', 'knw', 'dis', 'no', 'lt', 'gt']\n",
      "After stop words removal: ['u', 'knw', 'dis', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['knw', 'di']\n",
      "Tokenized sentence: ['no', 'i', 'm', 'good', 'for', 'the', 'movie', 'is', 'it', 'ok', 'if', 'i', 'leave', 'in', 'an', 'hourish']\n",
      "After stop words removal: ['good', 'movie', 'ok', 'leave', 'hourish']\n",
      "After stemming with porters algorithm: ['good', 'movi', 'leav', 'hourish']\n",
      "Tokenized sentence: ['what', 'can', 'i', 'do', 'might', 'accidant', 'tookplace', 'between', 'somewhere', 'ghodbandar', 'rd', 'traffic', 'moves', 'slovely', 'so', 'plz', 'slip', 'amp', 'don', 't', 'worry']\n",
      "After stop words removal: ['might', 'accidant', 'tookplace', 'somewhere', 'ghodbandar', 'rd', 'traffic', 'moves', 'slovely', 'plz', 'slip', 'amp', 'worry']\n",
      "After stemming with porters algorithm: ['might', 'accid', 'tookplac', 'somewher', 'ghodbandar', 'traffic', 'move', 'slove', 'plz', 'slip', 'amp', 'worri']\n",
      "Tokenized sentence: ['ok', 'u', 'can', 'take', 'me', 'shopping', 'when', 'u', 'get', 'paid', 'd']\n",
      "After stop words removal: ['ok', 'u', 'take', 'shopping', 'u', 'get', 'paid']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['take', 'shop', 'get', 'paid']\n",
      "Tokenized sentence: ['i', 'hate', 'when', 'she', 'does', 'this', 'she', 'turns', 'what', 'should', 'be', 'a', 'fun', 'shopping', 'trip', 'into', 'an', 'annoying', 'day', 'of', 'how', 'everything', 'would', 'look', 'in', 'her', 'house']\n",
      "After stop words removal: ['hate', 'turns', 'fun', 'shopping', 'trip', 'annoying', 'day', 'everything', 'would', 'look', 'house']\n",
      "shopp\n",
      "annoy\n",
      "everyth\n",
      "After stemming with porters algorithm: ['hate', 'turn', 'fun', 'shop', 'trip', 'annoi', 'dai', 'everyt', 'would', 'look', 'hous']\n",
      "Tokenized sentence: ['freemsg', 'why', 'haven', 't', 'you', 'replied', 'to', 'my', 'text', 'i', 'm', 'randy', 'sexy', 'female', 'and', 'live', 'local', 'luv', 'to', 'hear', 'from', 'u', 'netcollex', 'ltd', 'p', 'per', 'msg', 'reply', 'stop', 'to', 'end']\n",
      "After stop words removal: ['freemsg', 'replied', 'text', 'randy', 'sexy', 'female', 'live', 'local', 'luv', 'hear', 'u', 'netcollex', 'ltd', 'p', 'per', 'msg', 'reply', 'stop', 'end']\n",
      "After stemming with porters algorithm: ['freemsg', 'repli', 'text', 'randi', 'sexi', 'femal', 'live', 'local', 'luv', 'hear', 'netcollex', 'ltd', 'per', 'msg', 'repli', 'stop', 'end']\n",
      "Tokenized sentence: ['tacos', 'rajas', 'burrito', 'right']\n",
      "After stop words removal: ['tacos', 'rajas', 'burrito', 'right']\n",
      "After stemming with porters algorithm: ['taco', 'raja', 'burrito', 'right']\n",
      "Tokenized sentence: ['men', 'like', 'shorter', 'ladies', 'gaze', 'up', 'into', 'his', 'eyes']\n",
      "After stop words removal: ['men', 'like', 'shorter', 'ladies', 'gaze', 'eyes']\n",
      "After stemming with porters algorithm: ['men', 'like', 'shorter', 'ladi', 'gaz', 'ey']\n",
      "Tokenized sentence: ['i', 'can', 'ask', 'around', 'but', 'there', 's', 'not', 'a', 'lot', 'in', 'terms', 'of', 'mids', 'up', 'here']\n",
      "After stop words removal: ['ask', 'around', 'lot', 'terms', 'mids']\n",
      "After stemming with porters algorithm: ['ask', 'around', 'lot', 'term', 'mid']\n",
      "Tokenized sentence: ['our', 'ride', 'equally', 'uneventful', 'not', 'too', 'many', 'of', 'those', 'pesky', 'cyclists', 'around', 'at', 'that', 'time', 'of', 'night']\n",
      "After stop words removal: ['ride', 'equally', 'uneventful', 'many', 'pesky', 'cyclists', 'around', 'time', 'night']\n",
      "After stemming with porters algorithm: ['ride', 'equal', 'unev', 'mani', 'peski', 'cyclist', 'around', 'time', 'night']\n",
      "Tokenized sentence: ['congratulations', 'in', 'this', 'week', 's', 'competition', 'draw', 'u', 'have', 'won', 'the', 'prize', 'to', 'claim', 'just', 'call', 'b', 't', 'cs', 'stop', 'sms', 'over', 'only', 'ppm']\n",
      "After stop words removal: ['congratulations', 'week', 'competition', 'draw', 'u', 'prize', 'claim', 'call', 'b', 'cs', 'stop', 'sms', 'ppm']\n",
      "After stemming with porters algorithm: ['congratul', 'week', 'competit', 'draw', 'priz', 'claim', 'call', 'stop', 'sm', 'ppm']\n",
      "Tokenized sentence: ['thanx', 'today', 'cer', 'it', 'was', 'nice', 'catch', 'up', 'but', 'we', 'ave', 'find', 'more', 'time', 'more', 'often', 'oh', 'well', 'take', 'care', 'c', 'u', 'soon', 'c']\n",
      "After stop words removal: ['thanx', 'today', 'cer', 'nice', 'catch', 'ave', 'find', 'time', 'often', 'oh', 'well', 'take', 'care', 'c', 'u', 'soon', 'c']\n",
      "After stemming with porters algorithm: ['thanx', 'todai', 'cer', 'nice', 'catch', 'av', 'find', 'time', 'often', 'well', 'take', 'care', 'soon']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'contact', 'you', 're', 'your', 'response', 'to', 'our', 'offer', 'of', 'a', 'new', 'nokia', 'fone', 'and', 'camcorder', 'hit', 'reply', 'or', 'call', 'for', 'delivery']\n",
      "After stop words removal: ['tried', 'contact', 'response', 'offer', 'new', 'nokia', 'fone', 'camcorder', 'hit', 'reply', 'call', 'delivery']\n",
      "After stemming with porters algorithm: ['tri', 'contact', 'respons', 'offer', 'new', 'nokia', 'fone', 'camcord', 'hit', 'repli', 'call', 'deliveri']\n",
      "Tokenized sentence: ['hey', 'you', 'told', 'your', 'name', 'to', 'gautham', 'ah']\n",
      "After stop words removal: ['hey', 'told', 'name', 'gautham', 'ah']\n",
      "After stemming with porters algorithm: ['hei', 'told', 'name', 'gautham']\n",
      "Tokenized sentence: ['ya', 'just', 'telling', 'abt', 'tht', 'incident']\n",
      "After stop words removal: ['ya', 'telling', 'abt', 'tht', 'incident']\n",
      "tell\n",
      "After stemming with porters algorithm: ['tell', 'abt', 'tht', 'incid']\n",
      "Tokenized sentence: ['poor', 'girl', 'can', 't', 'go', 'one', 'day', 'lmao']\n",
      "After stop words removal: ['poor', 'girl', 'go', 'one', 'day', 'lmao']\n",
      "After stemming with porters algorithm: ['poor', 'girl', 'on', 'dai', 'lmao']\n",
      "Tokenized sentence: ['sir', 'i', 'am', 'waiting', 'for', 'your', 'call']\n",
      "After stop words removal: ['sir', 'waiting', 'call']\n",
      "wait\n",
      "After stemming with porters algorithm: ['sir', 'wait', 'call']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'be', 'there', 'so', 'i', 'can', 'kiss', 'you', 'and', 'feel', 'you', 'next', 'to', 'me']\n",
      "After stop words removal: ['want', 'kiss', 'feel', 'next']\n",
      "After stemming with porters algorithm: ['want', 'kiss', 'feel', 'next']\n",
      "Tokenized sentence: ['be', 'happy', 'there', 'i', 'will', 'come', 'after', 'noon']\n",
      "After stop words removal: ['happy', 'come', 'noon']\n",
      "After stemming with porters algorithm: ['happi', 'come', 'noon']\n",
      "Tokenized sentence: ['k', 'fyi', 'i', 'm', 'back', 'in', 'my', 'parents', 'place', 'in', 'south', 'tampa', 'so', 'i', 'might', 'need', 'to', 'do', 'the', 'deal', 'somewhere', 'else']\n",
      "After stop words removal: ['k', 'fyi', 'back', 'parents', 'place', 'south', 'tampa', 'might', 'need', 'deal', 'somewhere', 'else']\n",
      "After stemming with porters algorithm: ['fyi', 'back', 'parent', 'place', 'south', 'tampa', 'might', 'need', 'deal', 'somewher', 'els']\n",
      "Tokenized sentence: ['a', 'guy', 'who', 'gets', 'used', 'but', 'is', 'too', 'dumb', 'to', 'realize', 'it']\n",
      "After stop words removal: ['guy', 'gets', 'used', 'dumb', 'realize']\n",
      "After stemming with porters algorithm: ['gui', 'get', 'us', 'dumb', 'realiz']\n",
      "Tokenized sentence: ['dont', 'worry', 'day', 'very', 'big', 'lambu', 'ji', 'vl', 'come', 'til', 'then', 'enjoy', 'batchlor', 'party']\n",
      "After stop words removal: ['dont', 'worry', 'day', 'big', 'lambu', 'ji', 'vl', 'come', 'til', 'enjoy', 'batchlor', 'party']\n",
      "After stemming with porters algorithm: ['dont', 'worri', 'dai', 'big', 'lambu', 'come', 'til', 'enjoi', 'batchlor', 'parti']\n",
      "Tokenized sentence: ['can', 'a', 'not']\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['so', 'u', 'wan', 'come', 'for', 'our', 'dinner', 'tonight', 'a', 'not']\n",
      "After stop words removal: ['u', 'wan', 'come', 'dinner', 'tonight']\n",
      "After stemming with porters algorithm: ['wan', 'come', 'dinner', 'tonight']\n",
      "Tokenized sentence: ['lol', 'i', 'was', 'gonna', 'last', 'month', 'i', 'cashed', 'some', 'in', 'but', 'i', 'left', 'lt', 'gt', 'just', 'in', 'case', 'i', 'was', 'collecting', 'more', 'during', 'the', 'week', 'cause', 'they', 'announced', 'it', 'on', 'the', 'blog']\n",
      "After stop words removal: ['lol', 'gonna', 'last', 'month', 'cashed', 'left', 'lt', 'gt', 'case', 'collecting', 'week', 'cause', 'announced', 'blog']\n",
      "collect\n",
      "After stemming with porters algorithm: ['lol', 'gonna', 'last', 'month', 'cas', 'left', 'case', 'collec', 'week', 'caus', 'announ', 'blog']\n",
      "Tokenized sentence: ['i', 'm', 'wif', 'him', 'now', 'buying', 'tix', 'lar']\n",
      "After stop words removal: ['wif', 'buying', 'tix', 'lar']\n",
      "buy\n",
      "After stemming with porters algorithm: ['wif', 'bui', 'tix', 'lar']\n",
      "Tokenized sentence: ['nah', 'can', 't', 'help', 'you', 'there', 'i', 've', 'never', 'had', 'an', 'iphone']\n",
      "After stop words removal: ['nah', 'help', 'never', 'iphone']\n",
      "After stemming with porters algorithm: ['nah', 'help', 'never', 'iphon']\n",
      "Tokenized sentence: ['you', 'are', 'being', 'ripped', 'off', 'get', 'your', 'mobile', 'content', 'from', 'www', 'clubmoby', 'com', 'call', 'poly', 'true', 'pix', 'ringtones', 'games', 'six', 'downloads', 'for', 'only']\n",
      "After stop words removal: ['ripped', 'get', 'mobile', 'content', 'www', 'clubmoby', 'com', 'call', 'poly', 'true', 'pix', 'ringtones', 'games', 'six', 'downloads']\n",
      "After stemming with porters algorithm: ['rip', 'get', 'mobil', 'content', 'www', 'clubmobi', 'com', 'call', 'poli', 'true', 'pix', 'rington', 'game', 'six', 'download']\n",
      "Tokenized sentence: ['actually', 'i', 'm', 'waiting', 'for', 'weeks', 'when', 'they', 'start', 'putting', 'ad']\n",
      "After stop words removal: ['actually', 'waiting', 'weeks', 'start', 'putting', 'ad']\n",
      "wait\n",
      "putt\n",
      "After stemming with porters algorithm: ['actual', 'wait', 'week', 'start', 'put']\n",
      "Tokenized sentence: ['don', 'no', 'da', 'whats', 'you', 'plan']\n",
      "After stop words removal: ['da', 'whats', 'plan']\n",
      "After stemming with porters algorithm: ['what', 'plan']\n",
      "Tokenized sentence: ['nope', 'i', 'm', 'still', 'in', 'the', 'market']\n",
      "After stop words removal: ['nope', 'still', 'market']\n",
      "After stemming with porters algorithm: ['nope', 'still', 'market']\n",
      "Tokenized sentence: ['he', 's', 'just', 'gonna', 'worry', 'for', 'nothing', 'and', 'he', 'won', 't', 'give', 'you', 'money', 'its', 'no', 'use']\n",
      "After stop words removal: ['gonna', 'worry', 'nothing', 'give', 'money', 'use']\n",
      "noth\n",
      "After stemming with porters algorithm: ['gonna', 'worri', 'not', 'give', 'monei', 'us']\n",
      "Tokenized sentence: ['somebody', 'should', 'go', 'to', 'andros', 'and', 'steal', 'ice']\n",
      "After stop words removal: ['somebody', 'go', 'andros', 'steal', 'ice']\n",
      "After stemming with porters algorithm: ['somebodi', 'andro', 'steal', 'ic']\n",
      "Tokenized sentence: ['if', 'you', 'hear', 'a', 'loud', 'scream', 'in', 'about', 'lt', 'gt', 'minutes', 'its', 'cause', 'my', 'gyno', 'will', 'be', 'shoving', 'things', 'up', 'me', 'that', 'don', 't', 'belong']\n",
      "After stop words removal: ['hear', 'loud', 'scream', 'lt', 'gt', 'minutes', 'cause', 'gyno', 'shoving', 'things', 'belong']\n",
      "shov\n",
      "After stemming with porters algorithm: ['hear', 'loud', 'scream', 'minut', 'caus', 'gyno', 'shove', 'thing', 'belong']\n",
      "Tokenized sentence: ['are', 'you', 'going', 'to', 'write', 'ccna', 'exam', 'this', 'week']\n",
      "After stop words removal: ['going', 'write', 'ccna', 'exam', 'week']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'write', 'ccna', 'exam', 'week']\n",
      "Tokenized sentence: ['my', 'uncles', 'in', 'atlanta', 'wish', 'you', 'guys', 'a', 'great', 'semester']\n",
      "After stop words removal: ['uncles', 'atlanta', 'wish', 'guys', 'great', 'semester']\n",
      "After stemming with porters algorithm: ['uncl', 'atlanta', 'wish', 'gui', 'great', 'semest']\n",
      "Tokenized sentence: ['yup', 'ok', 'thanx']\n",
      "After stop words removal: ['yup', 'ok', 'thanx']\n",
      "After stemming with porters algorithm: ['yup', 'thanx']\n",
      "Tokenized sentence: ['k', 'makes', 'sense', 'btw', 'carlos', 'is', 'being', 'difficult', 'so', 'you', 'guys', 'are', 'gonna', 'smoke', 'while', 'i', 'go', 'pick', 'up', 'the', 'second', 'batch', 'and', 'get', 'gas']\n",
      "After stop words removal: ['k', 'makes', 'sense', 'btw', 'carlos', 'difficult', 'guys', 'gonna', 'smoke', 'go', 'pick', 'second', 'batch', 'get', 'gas']\n",
      "After stemming with porters algorithm: ['make', 'sens', 'btw', 'carlo', 'difficult', 'gui', 'gonna', 'smoke', 'pick', 'second', 'batch', 'get', 'ga']\n",
      "Tokenized sentence: ['k', 'k', 'are', 'you', 'in', 'college']\n",
      "After stop words removal: ['k', 'k', 'college']\n",
      "After stemming with porters algorithm: ['colleg']\n",
      "Tokenized sentence: ['kent', 'vale', 'lor', 'wait', 'me', 'there', 'ar']\n",
      "After stop words removal: ['kent', 'vale', 'lor', 'wait', 'ar']\n",
      "After stemming with porters algorithm: ['kent', 'vale', 'lor', 'wait']\n",
      "Tokenized sentence: ['what', 'today', 'sunday', 'sunday', 'is', 'holiday', 'so', 'no', 'work']\n",
      "After stop words removal: ['today', 'sunday', 'sunday', 'holiday', 'work']\n",
      "After stemming with porters algorithm: ['todai', 'sundai', 'sundai', 'holidai', 'work']\n",
      "Tokenized sentence: ['sos', 'any', 'amount', 'i', 'can', 'get', 'pls']\n",
      "After stop words removal: ['sos', 'amount', 'get', 'pls']\n",
      "After stemming with porters algorithm: ['so', 'amount', 'get', 'pl']\n",
      "Tokenized sentence: ['and', 'also', 'i', 've', 'sorta', 'blown', 'him', 'off', 'a', 'couple', 'times', 'recently', 'so', 'id', 'rather', 'not', 'text', 'him', 'out', 'of', 'the', 'blue', 'looking', 'for', 'weed']\n",
      "After stop words removal: ['also', 'sorta', 'blown', 'couple', 'times', 'recently', 'id', 'rather', 'text', 'blue', 'looking', 'weed']\n",
      "look\n",
      "After stemming with porters algorithm: ['also', 'sorta', 'blown', 'coupl', 'time', 'recent', 'rather', 'text', 'blue', 'look', 'weed']\n",
      "Tokenized sentence: ['yup', 'anything', 'lor', 'if', 'u', 'dun', 'wan', 'it', 's', 'ok']\n",
      "After stop words removal: ['yup', 'anything', 'lor', 'u', 'dun', 'wan', 'ok']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['yup', 'anyt', 'lor', 'dun', 'wan']\n",
      "Tokenized sentence: ['if', 'u', 'sending', 'her', 'home', 'first', 'it', 's', 'ok', 'lor', 'i', 'm', 'not', 'ready', 'yet']\n",
      "After stop words removal: ['u', 'sending', 'home', 'first', 'ok', 'lor', 'ready', 'yet']\n",
      "send\n",
      "After stemming with porters algorithm: ['sen', 'home', 'first', 'lor', 'readi', 'yet']\n",
      "Tokenized sentence: ['a', 'cute', 'thought', 'for', 'friendship', 'its', 'not', 'necessary', 'to', 'share', 'every', 'secret', 'with', 'ur', 'close', 'frnd']\n",
      "After stop words removal: ['cute', 'thought', 'friendship', 'necessary', 'share', 'every', 'secret', 'ur', 'close', 'frnd']\n",
      "After stemming with porters algorithm: ['cute', 'thought', 'friendship', 'necessari', 'share', 'everi', 'secret', 'close', 'frnd']\n",
      "Tokenized sentence: ['excellent', 'i', 'spent', 'lt', 'gt', 'years', 'in', 'the', 'air', 'force', 'iraq', 'and', 'afghanistan', 'i', 'am', 'stable', 'and', 'honest', 'do', 'you', 'like', 'traveling']\n",
      "After stop words removal: ['excellent', 'spent', 'lt', 'gt', 'years', 'air', 'force', 'iraq', 'afghanistan', 'stable', 'honest', 'like', 'traveling']\n",
      "travel\n",
      "After stemming with porters algorithm: ['excel', 'spent', 'year', 'air', 'forc', 'iraq', 'afghanistan', 'stabl', 'honest', 'like', 'travel']\n",
      "Tokenized sentence: ['where', 'is', 'that', 'one', 'day', 'training']\n",
      "After stop words removal: ['one', 'day', 'training']\n",
      "train\n",
      "After stemming with porters algorithm: ['on', 'dai', 'train']\n",
      "Tokenized sentence: ['hope', 'you', 'enjoyed', 'your', 'new', 'content', 'text', 'stop', 'to', 'to', 'unsubscribe', 'help', 'p', 'provided', 'by', 'tones', 'you', 'co', 'uk']\n",
      "After stop words removal: ['hope', 'enjoyed', 'new', 'content', 'text', 'stop', 'unsubscribe', 'help', 'p', 'provided', 'tones', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['hope', 'enjoi', 'new', 'content', 'text', 'stop', 'unsubscrib', 'help', 'provid', 'tone']\n",
      "Tokenized sentence: ['get', 'your', 'garden', 'ready', 'for', 'summer', 'with', 'a', 'free', 'selection', 'of', 'summer', 'bulbs', 'and', 'seeds', 'worth', 'only', 'with', 'the', 'scotsman', 'this', 'saturday', 'to', 'stop', 'go', 'notxt', 'co', 'uk']\n",
      "After stop words removal: ['get', 'garden', 'ready', 'summer', 'free', 'selection', 'summer', 'bulbs', 'seeds', 'worth', 'scotsman', 'saturday', 'stop', 'go', 'notxt', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['get', 'garden', 'readi', 'summer', 'free', 'select', 'summer', 'bulb', 'seed', 'worth', 'scotsman', 'saturdai', 'stop', 'notxt']\n",
      "Tokenized sentence: ['probably', 'want', 'to', 'pick', 'up', 'more']\n",
      "After stop words removal: ['probably', 'want', 'pick']\n",
      "After stemming with porters algorithm: ['probab', 'want', 'pick']\n",
      "Tokenized sentence: ['now', 'only', 'i', 'reached', 'home', 'i', 'am', 'very', 'tired', 'now', 'i', 'will', 'come', 'tomorro']\n",
      "After stop words removal: ['reached', 'home', 'tired', 'come', 'tomorro']\n",
      "After stemming with porters algorithm: ['reac', 'home', 'tire', 'come', 'tomorro']\n",
      "Tokenized sentence: ['a', 'few', 'people', 'are', 'at', 'the', 'game', 'i', 'm', 'at', 'the', 'mall', 'with', 'iouri', 'and', 'kaila']\n",
      "After stop words removal: ['people', 'game', 'mall', 'iouri', 'kaila']\n",
      "After stemming with porters algorithm: ['peopl', 'game', 'mall', 'iouri', 'kaila']\n",
      "Tokenized sentence: ['yes', 'please', 'leave', 'at', 'lt', 'gt', 'so', 'that', 'at', 'lt', 'gt', 'we', 'can', 'leave']\n",
      "After stop words removal: ['yes', 'please', 'leave', 'lt', 'gt', 'lt', 'gt', 'leave']\n",
      "After stemming with porters algorithm: ['ye', 'pleas', 'leav', 'leav']\n",
      "Tokenized sentence: ['sorry', 'dude', 'dont', 'know', 'how', 'i', 'forgot', 'even', 'after', 'dan', 'reminded', 'me', 'sorry', 'hope', 'you', 'guys', 'had', 'fun']\n",
      "After stop words removal: ['sorry', 'dude', 'dont', 'know', 'forgot', 'even', 'dan', 'reminded', 'sorry', 'hope', 'guys', 'fun']\n",
      "After stemming with porters algorithm: ['sorri', 'dude', 'dont', 'know', 'forgot', 'even', 'dan', 'remin', 'sorri', 'hope', 'gui', 'fun']\n",
      "Tokenized sentence: ['hi', 'missed', 'your', 'call', 'and', 'my', 'mumhas', 'beendropping', 'red', 'wine', 'all', 'over', 'theplace', 'what', 'is', 'your', 'adress']\n",
      "After stop words removal: ['hi', 'missed', 'call', 'mumhas', 'beendropping', 'red', 'wine', 'theplace', 'adress']\n",
      "beendropp\n",
      "After stemming with porters algorithm: ['miss', 'call', 'mumha', 'beendrop', 'red', 'wine', 'theplac', 'adress']\n",
      "Tokenized sentence: ['kothi', 'print', 'out', 'marandratha']\n",
      "After stop words removal: ['kothi', 'print', 'marandratha']\n",
      "After stemming with porters algorithm: ['kothi', 'print', 'marandratha']\n",
      "Tokenized sentence: ['beauty', 'sleep', 'can', 'help', 'ur', 'pimples', 'too']\n",
      "After stop words removal: ['beauty', 'sleep', 'help', 'ur', 'pimples']\n",
      "After stemming with porters algorithm: ['beauti', 'sleep', 'help', 'pimpl']\n",
      "Tokenized sentence: ['mm', 'so', 'you', 'asked', 'me', 'not', 'to', 'call', 'radio']\n",
      "After stop words removal: ['mm', 'asked', 'call', 'radio']\n",
      "After stemming with porters algorithm: ['as', 'call', 'radio']\n",
      "Tokenized sentence: ['u', 'coming', 'back', 'dinner', 'rite', 'dad', 'ask', 'me', 'so', 'i', 're', 'confirm', 'wif', 'u']\n",
      "After stop words removal: ['u', 'coming', 'back', 'dinner', 'rite', 'dad', 'ask', 'confirm', 'wif', 'u']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'back', 'dinner', 'rite', 'dad', 'ask', 'confirm', 'wif']\n",
      "Tokenized sentence: ['my', 'uncles', 'in', 'atlanta', 'wish', 'you', 'guys', 'a', 'great', 'semester']\n",
      "After stop words removal: ['uncles', 'atlanta', 'wish', 'guys', 'great', 'semester']\n",
      "After stemming with porters algorithm: ['uncl', 'atlanta', 'wish', 'gui', 'great', 'semest']\n",
      "Tokenized sentence: ['shhhhh', 'nobody', 'is', 'supposed', 'to', 'know']\n",
      "After stop words removal: ['shhhhh', 'nobody', 'supposed', 'know']\n",
      "After stemming with porters algorithm: ['shhhhh', 'nobodi', 'suppos', 'know']\n",
      "Tokenized sentence: ['jus', 'telling', 'u', 'dat', 'i', 'll', 'b', 'leaving', 'shanghai', 'on', 'st', 'instead', 'so', 'we', 'll', 'haf', 'more', 'time', 'meet', 'up', 'cya']\n",
      "After stop words removal: ['jus', 'telling', 'u', 'dat', 'b', 'leaving', 'shanghai', 'st', 'instead', 'haf', 'time', 'meet', 'cya']\n",
      "tell\n",
      "leav\n",
      "After stemming with porters algorithm: ['ju', 'tell', 'dat', 'leav', 'shanghai', 'instead', 'haf', 'time', 'meet', 'cya']\n",
      "Tokenized sentence: ['hi', 'dear', 'we', 'saw', 'dear', 'we', 'both', 'are', 'happy', 'where', 'you', 'my', 'battery', 'is', 'low']\n",
      "After stop words removal: ['hi', 'dear', 'saw', 'dear', 'happy', 'battery', 'low']\n",
      "After stemming with porters algorithm: ['dear', 'saw', 'dear', 'happi', 'batteri', 'low']\n",
      "Tokenized sentence: ['if', 'you', 're', 'not', 'in', 'my', 'car', 'in', 'an', 'hour', 'and', 'a', 'half', 'i', 'm', 'going', 'apeshit']\n",
      "After stop words removal: ['car', 'hour', 'half', 'going', 'apeshit']\n",
      "go\n",
      "After stemming with porters algorithm: ['car', 'hour', 'half', 'go', 'apeshit']\n",
      "Tokenized sentence: ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'freephone', 'between', 'am', 'pm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
      "After stop words removal: ['please', 'call', 'customer', 'service', 'representative', 'freephone', 'pm', 'guaranteed', 'cash', 'prize']\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'custom', 'servic', 'repres', 'freephon', 'guaranteed', 'cash', 'priz']\n",
      "Tokenized sentence: ['hi', 'cts', 'employee', 'how', 'are', 'you']\n",
      "After stop words removal: ['hi', 'cts', 'employee']\n",
      "After stemming with porters algorithm: ['ct', 'employe']\n",
      "Tokenized sentence: ['forwarded', 'from', 'please', 'call', 'immediately', 'as', 'there', 'is', 'an', 'urgent', 'message', 'waiting', 'for', 'you']\n",
      "After stop words removal: ['forwarded', 'please', 'call', 'immediately', 'urgent', 'message', 'waiting']\n",
      "wait\n",
      "After stemming with porters algorithm: ['forwar', 'pleas', 'call', 'immedi', 'urgent', 'messag', 'wait']\n",
      "Tokenized sentence: ['we', 'are', 'okay', 'going', 'to', 'sleep', 'now', 'later']\n",
      "After stop words removal: ['okay', 'going', 'sleep', 'later']\n",
      "go\n",
      "After stemming with porters algorithm: ['okai', 'go', 'sleep', 'later']\n",
      "Tokenized sentence: ['siva', 'is', 'in', 'hostel', 'aha']\n",
      "After stop words removal: ['siva', 'hostel', 'aha']\n",
      "After stemming with porters algorithm: ['siva', 'hostel', 'aha']\n",
      "Tokenized sentence: ['did', 'either', 'of', 'you', 'have', 'any', 'idea', 's', 'do', 'you', 'know', 'of', 'anyplaces', 'doing', 'something']\n",
      "After stop words removal: ['either', 'idea', 'know', 'anyplaces', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['either', 'idea', 'know', 'anyplac', 'somet']\n",
      "Tokenized sentence: ['check', 'mail', 'i', 'have', 'mailed', 'varma', 'and', 'kept', 'copy', 'to', 'you', 'regarding', 'membership', 'take', 'care', 'insha', 'allah']\n",
      "After stop words removal: ['check', 'mail', 'mailed', 'varma', 'kept', 'copy', 'regarding', 'membership', 'take', 'care', 'insha', 'allah']\n",
      "regard\n",
      "After stemming with porters algorithm: ['check', 'mail', 'mail', 'varma', 'kept', 'copi', 'regar', 'membership', 'take', 'care', 'insha', 'allah']\n",
      "Tokenized sentence: ['me', 'i', 'dont', 'know', 'again', 'oh']\n",
      "After stop words removal: ['dont', 'know', 'oh']\n",
      "After stemming with porters algorithm: ['dont', 'know']\n",
      "Tokenized sentence: ['horrible', 'gal', 'me', 'in', 'sch', 'doing', 'some', 'stuff', 'how', 'come', 'u', 'got', 'mc']\n",
      "After stop words removal: ['horrible', 'gal', 'sch', 'stuff', 'come', 'u', 'got', 'mc']\n",
      "After stemming with porters algorithm: ['horrib', 'gal', 'sch', 'stuff', 'come', 'got']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['it', 'is', 'only', 'yesterday', 'true', 'true']\n",
      "After stop words removal: ['yesterday', 'true', 'true']\n",
      "After stemming with porters algorithm: ['yesterdai', 'true', 'true']\n",
      "Tokenized sentence: ['what', 's', 'up', 'do', 'you', 'want', 'me', 'to', 'come', 'online']\n",
      "After stop words removal: ['want', 'come', 'online']\n",
      "After stemming with porters algorithm: ['want', 'come', 'onlin']\n",
      "Tokenized sentence: ['ok', 'i', 'feel', 'like', 'john', 'lennon']\n",
      "After stop words removal: ['ok', 'feel', 'like', 'john', 'lennon']\n",
      "After stemming with porters algorithm: ['feel', 'like', 'john', 'lennon']\n",
      "Tokenized sentence: ['o', 'we', 'cant', 'see', 'if', 'we', 'can', 'join', 'denis', 'and', 'mina', 'or', 'does', 'denis', 'want', 'alone', 'time']\n",
      "After stop words removal: ['cant', 'see', 'join', 'denis', 'mina', 'denis', 'want', 'alone', 'time']\n",
      "After stemming with porters algorithm: ['cant', 'see', 'join', 'deni', 'mina', 'deni', 'want', 'alon', 'time']\n",
      "Tokenized sentence: ['you', 'should', 'get', 'more', 'chicken', 'broth', 'if', 'you', 'want', 'ramen', 'unless', 'there', 's', 'some', 'i', 'don', 't', 'know', 'about']\n",
      "After stop words removal: ['get', 'chicken', 'broth', 'want', 'ramen', 'unless', 'know']\n",
      "After stemming with porters algorithm: ['get', 'chicken', 'broth', 'want', 'ramen', 'unless', 'know']\n",
      "Tokenized sentence: ['wishing', 'you', 'and', 'your', 'family', 'merry', 'x', 'mas', 'and', 'happy', 'new', 'year', 'in', 'advance']\n",
      "After stop words removal: ['wishing', 'family', 'merry', 'x', 'mas', 'happy', 'new', 'year', 'advance']\n",
      "wish\n",
      "After stemming with porters algorithm: ['wis', 'famili', 'merri', 'ma', 'happi', 'new', 'year', 'advanc']\n",
      "Tokenized sentence: ['k', 'k', 'pa', 'had', 'your', 'lunch', 'aha']\n",
      "After stop words removal: ['k', 'k', 'pa', 'lunch', 'aha']\n",
      "After stemming with porters algorithm: ['lunch', 'aha']\n",
      "Tokenized sentence: ['important', 'information', 'orange', 'user', 'xxxxxxx', 'today', 'is', 'your', 'lucky', 'day', 'find', 'out', 'why', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'there', 's', 'a', 'fantastic', 'surprise', 'awaiting', 'you']\n",
      "After stop words removal: ['important', 'information', 'orange', 'user', 'xxxxxxx', 'today', 'lucky', 'day', 'find', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'fantastic', 'surprise', 'awaiting']\n",
      "await\n",
      "After stemming with porters algorithm: ['import', 'inform', 'orang', 'user', 'xxxxxxx', 'todai', 'lucki', 'dai', 'find', 'log', 'onto', 'http', 'www', 'urawinn', 'com', 'fantast', 'surpris', 'await']\n",
      "Tokenized sentence: ['don', 'know', 'this', 'week', 'i', 'm', 'going', 'to', 'tirunelvai', 'da']\n",
      "After stop words removal: ['know', 'week', 'going', 'tirunelvai', 'da']\n",
      "go\n",
      "After stemming with porters algorithm: ['know', 'week', 'go', 'tirunelvai']\n",
      "Tokenized sentence: ['k', 'i', 'will', 'give', 'my', 'kvb', 'acc', 'details']\n",
      "After stop words removal: ['k', 'give', 'kvb', 'acc', 'details']\n",
      "After stemming with porters algorithm: ['give', 'kvb', 'acc', 'detail']\n",
      "Tokenized sentence: ['do', 'u', 'noe', 'wat', 'time', 'e', 'place', 'dat', 'sells', 'd', 'closes']\n",
      "After stop words removal: ['u', 'noe', 'wat', 'time', 'e', 'place', 'dat', 'sells', 'closes']\n",
      "After stemming with porters algorithm: ['noe', 'wat', 'time', 'place', 'dat', 'sell', 'close']\n",
      "Tokenized sentence: ['send', 'this', 'to', 'ur', 'friends', 'and', 'receive', 'something', 'about', 'ur', 'voice', 'how', 'is', 'my', 'speaking', 'expression', 'childish', 'naughty', 'sentiment', 'rowdy', 'ful', 'of', 'attitude', 'romantic', 'shy', 'attractive', 'funny', 'lt', 'gt', 'irritating', 'lt', 'gt', 'lovable', 'reply', 'me']\n",
      "After stop words removal: ['send', 'ur', 'friends', 'receive', 'something', 'ur', 'voice', 'speaking', 'expression', 'childish', 'naughty', 'sentiment', 'rowdy', 'ful', 'attitude', 'romantic', 'shy', 'attractive', 'funny', 'lt', 'gt', 'irritating', 'lt', 'gt', 'lovable', 'reply']\n",
      "someth\n",
      "speak\n",
      "irritat\n",
      "irritate\n",
      "After stemming with porters algorithm: ['send', 'friend', 'receiv', 'somet', 'voic', 'speak', 'express', 'childish', 'naughti', 'sentim', 'rowdi', 'ful', 'attitud', 'romant', 'shy', 'attract', 'funni', 'irrit', 'lovab', 'repli']\n",
      "Tokenized sentence: ['u', 'reach', 'orchard', 'already', 'u', 'wan', 'go', 'buy', 'tickets', 'first']\n",
      "After stop words removal: ['u', 'reach', 'orchard', 'already', 'u', 'wan', 'go', 'buy', 'tickets', 'first']\n",
      "After stemming with porters algorithm: ['reach', 'orchard', 'alreadi', 'wan', 'bui', 'ticket', 'first']\n",
      "Tokenized sentence: ['yeah', 'he', 'got', 'in', 'at', 'and', 'was', 'v', 'apologetic', 'n', 'had', 'fallen', 'out', 'and', 'she', 'was', 'actin', 'like', 'spoilt', 'child', 'and', 'he', 'got', 'caught', 'up', 'in', 'that', 'till', 'but', 'we', 'won', 't', 'go', 'there', 'not', 'doing', 'too', 'badly', 'cheers', 'you']\n",
      "After stop words removal: ['yeah', 'got', 'v', 'apologetic', 'n', 'fallen', 'actin', 'like', 'spoilt', 'child', 'got', 'caught', 'till', 'go', 'badly', 'cheers']\n",
      "After stemming with porters algorithm: ['yeah', 'got', 'apologet', 'fallen', 'actin', 'like', 'spoilt', 'child', 'got', 'caught', 'till', 'badli', 'cheer']\n",
      "Tokenized sentence: ['bloomberg', 'message', 'center', 'why', 'wait', 'apply', 'for', 'your', 'future', 'http', 'careers', 'bloomberg', 'com']\n",
      "After stop words removal: ['bloomberg', 'message', 'center', 'wait', 'apply', 'future', 'http', 'careers', 'bloomberg', 'com']\n",
      "After stemming with porters algorithm: ['bloomberg', 'messag', 'center', 'wait', 'appli', 'futur', 'http', 'career', 'bloomberg', 'com']\n",
      "Tokenized sentence: ['hmv', 'bonus', 'special', 'pounds', 'of', 'genuine', 'hmv', 'vouchers', 'to', 'be', 'won', 'just', 'answer', 'easy', 'questions', 'play', 'now', 'send', 'hmv', 'to', 'more', 'info', 'www', 'percent', 'real', 'com']\n",
      "After stop words removal: ['hmv', 'bonus', 'special', 'pounds', 'genuine', 'hmv', 'vouchers', 'answer', 'easy', 'questions', 'play', 'send', 'hmv', 'info', 'www', 'percent', 'real', 'com']\n",
      "After stemming with porters algorithm: ['hmv', 'bonu', 'special', 'pound', 'genuin', 'hmv', 'voucher', 'answer', 'easi', 'quest', 'plai', 'send', 'hmv', 'info', 'www', 'percent', 'real', 'com']\n",
      "Tokenized sentence: ['happy', 'valentines', 'day', 'i', 'know', 'its', 'early', 'but', 'i', 'have', 'hundreds', 'of', 'handsomes', 'and', 'beauties', 'to', 'wish', 'so', 'i', 'thought', 'to', 'finish', 'off', 'aunties', 'and', 'uncles', 'st']\n",
      "After stop words removal: ['happy', 'valentines', 'day', 'know', 'early', 'hundreds', 'handsomes', 'beauties', 'wish', 'thought', 'finish', 'aunties', 'uncles', 'st']\n",
      "After stemming with porters algorithm: ['happi', 'valentin', 'dai', 'know', 'earli', 'hund', 'handsom', 'beauti', 'wish', 'thought', 'finish', 'aunti', 'uncl']\n",
      "Tokenized sentence: ['yup', 'thk', 'of', 'u', 'oso', 'boring', 'wat']\n",
      "After stop words removal: ['yup', 'thk', 'u', 'oso', 'boring', 'wat']\n",
      "bor\n",
      "After stemming with porters algorithm: ['yup', 'thk', 'oso', 'bore', 'wat']\n",
      "Tokenized sentence: ['send', 'me', 'the', 'new', 'number']\n",
      "After stop words removal: ['send', 'new', 'number']\n",
      "After stemming with porters algorithm: ['send', 'new', 'number']\n",
      "Tokenized sentence: ['hey', 'something', 'came', 'up', 'last', 'min', 'think', 'i', 'wun', 'be', 'signing', 'up', 'tmr', 'hee']\n",
      "After stop words removal: ['hey', 'something', 'came', 'last', 'min', 'think', 'wun', 'signing', 'tmr', 'hee']\n",
      "someth\n",
      "sign\n",
      "After stemming with porters algorithm: ['hei', 'somet', 'came', 'last', 'min', 'think', 'wun', 'sig', 'tmr', 'hee']\n",
      "Tokenized sentence: ['i', 'will', 'come', 'tomorrow', 'di']\n",
      "After stop words removal: ['come', 'tomorrow', 'di']\n",
      "After stemming with porters algorithm: ['come', 'tomorrow']\n",
      "Tokenized sentence: ['surly', 'ill', 'give', 'it', 'to', 'you', 'while', 'coming', 'to', 'review']\n",
      "After stop words removal: ['surly', 'ill', 'give', 'coming', 'review']\n",
      "com\n",
      "After stemming with porters algorithm: ['surli', 'ill', 'give', 'come', 'review']\n",
      "Tokenized sentence: ['yeah', 'imma', 'come', 'over', 'cause', 'jay', 'wants', 'to', 'do', 'some', 'drugs']\n",
      "After stop words removal: ['yeah', 'imma', 'come', 'cause', 'jay', 'wants', 'drugs']\n",
      "After stemming with porters algorithm: ['yeah', 'imma', 'come', 'caus', 'jai', 'want', 'drug']\n",
      "Tokenized sentence: ['did', 'you', 'show', 'him', 'and', 'wot', 'did', 'he', 'say', 'or', 'could', 'u', 'not', 'c', 'him', 'dust']\n",
      "After stop words removal: ['show', 'wot', 'say', 'could', 'u', 'c', 'dust']\n",
      "After stemming with porters algorithm: ['show', 'wot', 'sai', 'could', 'dust']\n",
      "Tokenized sentence: ['just', 'arrived', 'see', 'you', 'in', 'a', 'couple', 'days', 'lt']\n",
      "After stop words removal: ['arrived', 'see', 'couple', 'days', 'lt']\n",
      "After stemming with porters algorithm: ['arriv', 'see', 'coupl', 'dai']\n",
      "Tokenized sentence: ['you', 'will', 'be', 'in', 'the', 'place', 'of', 'that', 'man']\n",
      "After stop words removal: ['place', 'man']\n",
      "After stemming with porters algorithm: ['place', 'man']\n",
      "Tokenized sentence: ['dear', 'good', 'morning', 'now', 'only', 'i', 'am', 'up']\n",
      "After stop words removal: ['dear', 'good', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['dear', 'good', 'mor']\n",
      "Tokenized sentence: ['my', 'darling', 'sister', 'how', 'are', 'you', 'doing', 'when', 's', 'school', 'resuming', 'is', 'there', 'a', 'minimum', 'wait', 'period', 'before', 'you', 'reapply', 'do', 'take', 'care']\n",
      "After stop words removal: ['darling', 'sister', 'school', 'resuming', 'minimum', 'wait', 'period', 'reapply', 'take', 'care']\n",
      "darl\n",
      "resum\n",
      "After stemming with porters algorithm: ['darl', 'sister', 'school', 'resum', 'minimum', 'wait', 'period', 'reappli', 'take', 'care']\n",
      "Tokenized sentence: ['keep', 'ur', 'problems', 'in', 'ur', 'heart']\n",
      "After stop words removal: ['keep', 'ur', 'problems', 'ur', 'heart']\n",
      "After stemming with porters algorithm: ['keep', 'problem', 'heart']\n",
      "Tokenized sentence: ['sun', 'ah', 'thk', 'mayb', 'can', 'if', 'dun', 'have', 'anythin', 'on', 'thk', 'have', 'to', 'book', 'e', 'lesson', 'e', 'pilates', 'is', 'at', 'orchard', 'mrt', 'u', 'noe', 'hor']\n",
      "After stop words removal: ['sun', 'ah', 'thk', 'mayb', 'dun', 'anythin', 'thk', 'book', 'e', 'lesson', 'e', 'pilates', 'orchard', 'mrt', 'u', 'noe', 'hor']\n",
      "After stemming with porters algorithm: ['sun', 'thk', 'mayb', 'dun', 'anythin', 'thk', 'book', 'lesson', 'pilat', 'orchard', 'mrt', 'noe', 'hor']\n",
      "Tokenized sentence: ['sir', 'i', 'am', 'waiting', 'for', 'your', 'call', 'once', 'free', 'please', 'call', 'me']\n",
      "After stop words removal: ['sir', 'waiting', 'call', 'free', 'please', 'call']\n",
      "wait\n",
      "After stemming with porters algorithm: ['sir', 'wait', 'call', 'free', 'pleas', 'call']\n",
      "Tokenized sentence: ['hey', 'girl', 'how', 'r', 'u', 'hope', 'u', 'r', 'well', 'me', 'an', 'del', 'r', 'bak', 'again', 'long', 'time', 'no', 'c', 'give', 'me', 'a', 'call', 'sum', 'time', 'from', 'lucyxx']\n",
      "After stop words removal: ['hey', 'girl', 'r', 'u', 'hope', 'u', 'r', 'well', 'del', 'r', 'bak', 'long', 'time', 'c', 'give', 'call', 'sum', 'time', 'lucyxx']\n",
      "After stemming with porters algorithm: ['hei', 'girl', 'hope', 'well', 'del', 'bak', 'long', 'time', 'give', 'call', 'sum', 'time', 'lucyxx']\n",
      "Tokenized sentence: ['hurry', 'home', 'u', 'big', 'butt', 'hang', 'up', 'on', 'your', 'last', 'caller', 'if', 'u', 'have', 'to', 'food', 'is', 'done', 'and', 'i', 'm', 'starving', 'don', 't', 'ask', 'what', 'i', 'cooked']\n",
      "After stop words removal: ['hurry', 'home', 'u', 'big', 'butt', 'hang', 'last', 'caller', 'u', 'food', 'done', 'starving', 'ask', 'cooked']\n",
      "starv\n",
      "After stemming with porters algorithm: ['hurri', 'home', 'big', 'butt', 'hang', 'last', 'caller', 'food', 'done', 'star', 'ask', 'cook']\n",
      "Tokenized sentence: ['solve', 'd', 'case', 'a', 'man', 'was', 'found', 'murdered', 'on', 'lt', 'decimal', 'gt', 'lt', 'gt', 'afternoon', 'his', 'wife', 'called', 'police', 'police', 'questioned', 'everyone', 'wife', 'sir', 'i', 'was', 'sleeping', 'when', 'the', 'murder', 'took', 'place', 'cook', 'i', 'was', 'cooking', 'gardener', 'i', 'was', 'picking', 'vegetables', 'house', 'maid', 'i', 'went', 'd', 'post', 'office', 'children', 'we', 'went', 'play', 'neighbour', 'we', 'went', 'a', 'marriage', 'police', 'arrested', 'd', 'murderer', 'immediately', 'who', 's', 'it', 'reply', 'with', 'reason', 'if', 'u', 'r', 'brilliant']\n",
      "After stop words removal: ['solve', 'case', 'man', 'found', 'murdered', 'lt', 'decimal', 'gt', 'lt', 'gt', 'afternoon', 'wife', 'called', 'police', 'police', 'questioned', 'everyone', 'wife', 'sir', 'sleeping', 'murder', 'took', 'place', 'cook', 'cooking', 'gardener', 'picking', 'vegetables', 'house', 'maid', 'went', 'post', 'office', 'children', 'went', 'play', 'neighbour', 'went', 'marriage', 'police', 'arrested', 'murderer', 'immediately', 'reply', 'reason', 'u', 'r', 'brilliant']\n",
      "sleep\n",
      "cook\n",
      "pick\n",
      "After stemming with porters algorithm: ['solv', 'case', 'man', 'found', 'murder', 'decim', 'afternoon', 'wife', 'call', 'polic', 'polic', 'quest', 'everyon', 'wife', 'sir', 'sleep', 'murder', 'took', 'place', 'cook', 'cook', 'garden', 'pic', 'veget', 'hous', 'maid', 'went', 'post', 'offic', 'children', 'went', 'plai', 'neighbour', 'went', 'marriag', 'polic', 'arres', 'murder', 'immedi', 'repli', 'reason', 'brilliant']\n",
      "Tokenized sentence: ['can', 'you', 'call', 'me', 'plz', 'your', 'number', 'shows', 'out', 'of', 'coveragd', 'area', 'i', 'have', 'urgnt', 'call', 'in', 'vasai', 'amp', 'have', 'to', 'reach', 'before', 'o', 'clock', 'so', 'call', 'me', 'plz']\n",
      "After stop words removal: ['call', 'plz', 'number', 'shows', 'coveragd', 'area', 'urgnt', 'call', 'vasai', 'amp', 'reach', 'clock', 'call', 'plz']\n",
      "After stemming with porters algorithm: ['call', 'plz', 'number', 'show', 'coveragd', 'area', 'urgnt', 'call', 'vasai', 'amp', 'reach', 'clock', 'call', 'plz']\n",
      "Tokenized sentence: ['are', 'you', 'ok', 'what', 'happen', 'to', 'behave', 'like', 'this']\n",
      "After stop words removal: ['ok', 'happen', 'behave', 'like']\n",
      "After stemming with porters algorithm: ['happen', 'behav', 'like']\n",
      "Tokenized sentence: ['wiskey', 'brandy', 'rum', 'gin', 'beer', 'vodka', 'scotch', 'shampain', 'wine', 'kudi', 'yarasu', 'dhina', 'vaazhthukkal']\n",
      "After stop words removal: ['wiskey', 'brandy', 'rum', 'gin', 'beer', 'vodka', 'scotch', 'shampain', 'wine', 'kudi', 'yarasu', 'dhina', 'vaazhthukkal']\n",
      "After stemming with porters algorithm: ['wiskei', 'brandi', 'rum', 'gin', 'beer', 'vodka', 'scotch', 'shampain', 'wine', 'kudi', 'yarasu', 'dhina', 'vaazhthukk']\n",
      "Tokenized sentence: ['fine', 'if', 'that', 's', 'the', 'way', 'u', 'feel', 'that', 's', 'the', 'way', 'its', 'gota', 'b']\n",
      "After stop words removal: ['fine', 'way', 'u', 'feel', 'way', 'gota', 'b']\n",
      "After stemming with porters algorithm: ['fine', 'wai', 'feel', 'wai', 'gota']\n",
      "Tokenized sentence: ['he', 'also', 'knows', 'about', 'lunch', 'menu', 'only', 'da', 'i', 'know']\n",
      "After stop words removal: ['also', 'knows', 'lunch', 'menu', 'da', 'know']\n",
      "After stemming with porters algorithm: ['also', 'know', 'lunch', 'menu', 'know']\n",
      "Tokenized sentence: ['i', 'wanna', 'watch', 'that', 'movie']\n",
      "After stop words removal: ['wanna', 'watch', 'movie']\n",
      "After stemming with porters algorithm: ['wanna', 'watch', 'movi']\n",
      "Tokenized sentence: ['you', 'are', 'right', 'meanwhile', 'how', 's', 'project', 'twins', 'comin', 'up']\n",
      "After stop words removal: ['right', 'meanwhile', 'project', 'twins', 'comin']\n",
      "After stemming with porters algorithm: ['right', 'meanwhil', 'project', 'twin', 'comin']\n",
      "Tokenized sentence: ['so', 'i', 'asked', 'how', 's', 'anthony', 'dad', 'and', 'your', 'bf']\n",
      "After stop words removal: ['asked', 'anthony', 'dad', 'bf']\n",
      "After stemming with porters algorithm: ['as', 'anthoni', 'dad']\n",
      "Tokenized sentence: ['ups', 'which', 'is', 'days', 'also', 'and', 'the', 'shipping', 'company', 'that', 'takes', 'wks', 'the', 'other', 'way', 'is', 'usps', 'which', 'takes', 'a', 'week', 'but', 'when', 'it', 'gets', 'to', 'lag', 'you', 'may', 'have', 'to', 'bribe', 'nipost', 'to', 'get', 'your', 'stuff']\n",
      "After stop words removal: ['ups', 'days', 'also', 'shipping', 'company', 'takes', 'wks', 'way', 'usps', 'takes', 'week', 'gets', 'lag', 'may', 'bribe', 'nipost', 'get', 'stuff']\n",
      "shipp\n",
      "After stemming with porters algorithm: ['up', 'dai', 'also', 'ship', 'compani', 'take', 'wk', 'wai', 'usp', 'take', 'week', 'get', 'lag', 'mai', 'bribe', 'nipost', 'get', 'stuff']\n",
      "Tokenized sentence: ['cant', 'think', 'of', 'anyone', 'with', 'spare', 'room', 'off', 'top', 'of', 'my', 'head']\n",
      "After stop words removal: ['cant', 'think', 'anyone', 'spare', 'room', 'top', 'head']\n",
      "After stemming with porters algorithm: ['cant', 'think', 'anyon', 'spare', 'room', 'top', 'head']\n",
      "Tokenized sentence: ['urgent', 'urgent', 'we', 'have', 'free', 'flights', 'to', 'europe', 'to', 'give', 'away', 'call', 'b', 'th', 'sept', 'take', 'a', 'friend', 'free', 'call', 'now', 'to', 'claim', 'on', 'ba', 'nnfwfly', 'ppm']\n",
      "After stop words removal: ['urgent', 'urgent', 'free', 'flights', 'europe', 'give', 'away', 'call', 'b', 'th', 'sept', 'take', 'friend', 'free', 'call', 'claim', 'ba', 'nnfwfly', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'urgent', 'free', 'flight', 'europ', 'give', 'awai', 'call', 'sept', 'take', 'friend', 'free', 'call', 'claim', 'nnfwfly', 'ppm']\n",
      "Tokenized sentence: ['great', 'hope', 'you', 'are', 'using', 'your', 'connections', 'from', 'mode', 'men', 'also', 'cos', 'you', 'can', 'never', 'know', 'why', 'old', 'friends', 'can', 'lead', 'you', 'to', 'today']\n",
      "After stop words removal: ['great', 'hope', 'using', 'connections', 'mode', 'men', 'also', 'cos', 'never', 'know', 'old', 'friends', 'lead', 'today']\n",
      "us\n",
      "After stemming with porters algorithm: ['great', 'hope', 'us', 'connect', 'mode', 'men', 'also', 'co', 'never', 'know', 'old', 'friend', 'lead', 'todai']\n",
      "Tokenized sentence: ['babe', 'what', 'are', 'you', 'doing', 'where', 'are', 'you', 'who', 'are', 'you', 'talking', 'to', 'do', 'you', 'think', 'of', 'me', 'are', 'you', 'being', 'a', 'good', 'boy', 'are', 'you', 'missing', 'me', 'do', 'you', 'love', 'me']\n",
      "After stop words removal: ['babe', 'talking', 'think', 'good', 'boy', 'missing', 'love']\n",
      "talk\n",
      "miss\n",
      "After stemming with porters algorithm: ['babe', 'tal', 'think', 'good', 'boi', 'miss', 'love']\n",
      "Tokenized sentence: ['i', 'm', 'putting', 'it', 'on', 'now', 'it', 'should', 'be', 'ready', 'for', 'lt', 'time', 'gt']\n",
      "After stop words removal: ['putting', 'ready', 'lt', 'time', 'gt']\n",
      "putt\n",
      "After stemming with porters algorithm: ['put', 'readi', 'time']\n",
      "Tokenized sentence: ['should', 'i', 'be', 'stalking', 'u']\n",
      "After stop words removal: ['stalking', 'u']\n",
      "stalk\n",
      "After stemming with porters algorithm: ['stal']\n",
      "Tokenized sentence: ['see', 'you', 'there']\n",
      "After stop words removal: ['see']\n",
      "After stemming with porters algorithm: ['see']\n",
      "Tokenized sentence: ['oh', 'thanks', 'a', 'lot', 'i', 'already', 'bought', 'eggs']\n",
      "After stop words removal: ['oh', 'thanks', 'lot', 'already', 'bought', 'eggs']\n",
      "After stemming with porters algorithm: ['thank', 'lot', 'alreadi', 'bought', 'egg']\n",
      "Tokenized sentence: ['sent', 'me', 'ur', 'email', 'id', 'soon']\n",
      "After stop words removal: ['sent', 'ur', 'email', 'id', 'soon']\n",
      "After stemming with porters algorithm: ['sent', 'email', 'soon']\n",
      "Tokenized sentence: ['cool', 'we', 'shall', 'go', 'and', 'see', 'have', 'to', 'go', 'to', 'tip', 'anyway', 'are', 'you', 'at', 'home', 'got', 'something', 'to', 'drop', 'in', 'later', 'so', 'lets', 'go', 'to', 'town', 'tonight', 'maybe', 'mum', 'can', 'take', 'us', 'in']\n",
      "After stop words removal: ['cool', 'shall', 'go', 'see', 'go', 'tip', 'anyway', 'home', 'got', 'something', 'drop', 'later', 'lets', 'go', 'town', 'tonight', 'maybe', 'mum', 'take', 'us']\n",
      "someth\n",
      "After stemming with porters algorithm: ['cool', 'shall', 'see', 'tip', 'anywai', 'home', 'got', 'somet', 'drop', 'later', 'let', 'town', 'tonight', 'mayb', 'mum', 'take']\n",
      "Tokenized sentence: ['wanna', 'have', 'a', 'laugh', 'try', 'chit', 'chat', 'on', 'your', 'mobile', 'now', 'logon', 'by', 'txting', 'the', 'word', 'chat', 'and', 'send', 'it', 'to', 'no', 'cm', 'po', 'box', 'london', 'w', 'a', 'zf', 'p', 'msg', 'rcvd']\n",
      "After stop words removal: ['wanna', 'laugh', 'try', 'chit', 'chat', 'mobile', 'logon', 'txting', 'word', 'chat', 'send', 'cm', 'po', 'box', 'london', 'w', 'zf', 'p', 'msg', 'rcvd']\n",
      "After stemming with porters algorithm: ['wanna', 'laugh', 'try', 'chit', 'chat', 'mobil', 'logon', 'txting', 'word', 'chat', 'send', 'box', 'london', 'msg', 'rcvd']\n",
      "Tokenized sentence: ['hey', 'i', 've', 'booked', 'the', 'pilates', 'and', 'yoga', 'lesson', 'already', 'haha']\n",
      "After stop words removal: ['hey', 'booked', 'pilates', 'yoga', 'lesson', 'already', 'haha']\n",
      "After stemming with porters algorithm: ['hei', 'book', 'pilat', 'yoga', 'lesson', 'alreadi', 'haha']\n",
      "Tokenized sentence: ['we', 're', 'on', 'the', 'opposite', 'side', 'from', 'where', 'we', 'dropped', 'you', 'off']\n",
      "After stop words removal: ['opposite', 'side', 'dropped']\n",
      "After stemming with porters algorithm: ['opposit', 'side', 'drop']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'a', 'ukp', 'prize', 'guaranteed', 'call', 'from', 'landline', 'claim', 'valid', 'hrs', 'only', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'awarded', 'ukp', 'prize', 'guaranteed', 'call', 'landline', 'claim', 'valid', 'hrs', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'awar', 'ukp', 'priz', 'guaranteed', 'call', 'landlin', 'claim', 'valid', 'hr', 'ppm']\n",
      "Tokenized sentence: ['now', 'that', 'you', 'have', 'started', 'dont', 'stop', 'just', 'pray', 'for', 'more', 'good', 'ideas', 'and', 'anything', 'i', 'see', 'that', 'can', 'help', 'you', 'guys', 'i', 'll', 'forward', 'you', 'a', 'link']\n",
      "After stop words removal: ['started', 'dont', 'stop', 'pray', 'good', 'ideas', 'anything', 'see', 'help', 'guys', 'forward', 'link']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['star', 'dont', 'stop', 'prai', 'good', 'idea', 'anyt', 'see', 'help', 'gui', 'forward', 'link']\n",
      "Tokenized sentence: ['staff', 'of', 'placement', 'training', 'in', 'amrita', 'college']\n",
      "After stop words removal: ['staff', 'placement', 'training', 'amrita', 'college']\n",
      "train\n",
      "After stemming with porters algorithm: ['staff', 'placem', 'train', 'amrita', 'colleg']\n",
      "Tokenized sentence: ['thanks', 'it', 'was', 'only', 'from', 'tescos', 'but', 'quite', 'nice', 'all', 'gone', 'now', 'speak', 'soon']\n",
      "After stop words removal: ['thanks', 'tescos', 'quite', 'nice', 'gone', 'speak', 'soon']\n",
      "After stemming with porters algorithm: ['thank', 'tesco', 'quit', 'nice', 'gone', 'speak', 'soon']\n",
      "Tokenized sentence: ['no', 'i', 'got', 'rumour', 'that', 'you', 'going', 'to', 'buy', 'apartment', 'in', 'chennai']\n",
      "After stop words removal: ['got', 'rumour', 'going', 'buy', 'apartment', 'chennai']\n",
      "go\n",
      "After stemming with porters algorithm: ['got', 'rumour', 'go', 'bui', 'apart', 'chennai']\n",
      "Tokenized sentence: ['sure', 'but', 'since', 'my', 'parents', 'will', 'be', 'working', 'on', 'tuesday', 'i', 'don', 't', 'really', 'need', 'a', 'cover', 'story']\n",
      "After stop words removal: ['sure', 'since', 'parents', 'working', 'tuesday', 'really', 'need', 'cover', 'story']\n",
      "work\n",
      "After stemming with porters algorithm: ['sure', 'sinc', 'parent', 'wor', 'tuesdai', 'realli', 'need', 'cover', 'stori']\n",
      "Tokenized sentence: ['well', 'then', 'you', 'have', 'a', 'great', 'weekend']\n",
      "After stop words removal: ['well', 'great', 'weekend']\n",
      "After stemming with porters algorithm: ['well', 'great', 'weekend']\n",
      "Tokenized sentence: ['i', 'felt', 'so', 'not', 'any', 'conveying', 'reason', 'ese', 'he', 'what', 'about', 'me']\n",
      "After stop words removal: ['felt', 'conveying', 'reason', 'ese']\n",
      "convey\n",
      "After stemming with porters algorithm: ['felt', 'convei', 'reason', 'es']\n",
      "Tokenized sentence: ['he', 'dint', 'tell', 'anything', 'he', 'is', 'angry', 'on', 'me', 'that', 'why', 'you', 'told', 'to', 'abi']\n",
      "After stop words removal: ['dint', 'tell', 'anything', 'angry', 'told', 'abi']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['dint', 'tell', 'anyt', 'angri', 'told', 'abi']\n",
      "Tokenized sentence: ['still', 'i', 'have', 'not', 'checked', 'it', 'da']\n",
      "After stop words removal: ['still', 'checked', 'da']\n",
      "After stemming with porters algorithm: ['still', 'chec']\n",
      "Tokenized sentence: ['goodmorning', 'today', 'i', 'am', 'late', 'for', 'lt', 'decimal', 'gt', 'min']\n",
      "After stop words removal: ['goodmorning', 'today', 'late', 'lt', 'decimal', 'gt', 'min']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['goodmor', 'todai', 'late', 'decim', 'min']\n",
      "Tokenized sentence: ['we', 'll', 'you', 'pay', 'over', 'like', 'lt', 'gt', 'yrs', 'so', 'its', 'not', 'too', 'difficult']\n",
      "After stop words removal: ['pay', 'like', 'lt', 'gt', 'yrs', 'difficult']\n",
      "After stemming with porters algorithm: ['pai', 'like', 'yr', 'difficult']\n",
      "Tokenized sentence: ['she', 'left', 'it', 'very', 'vague', 'she', 'just', 'said', 'she', 'would', 'inform', 'the', 'person', 'in', 'accounting', 'about', 'the', 'delayed', 'rent', 'and', 'that', 'i', 'should', 'discuss', 'with', 'the', 'housing', 'agency', 'about', 'my', 'renting', 'another', 'place', 'but', 'checking', 'online', 'now', 'and', 'all', 'places', 'around', 'usc', 'are', 'lt', 'gt', 'and', 'up']\n",
      "After stop words removal: ['left', 'vague', 'said', 'would', 'inform', 'person', 'accounting', 'delayed', 'rent', 'discuss', 'housing', 'agency', 'renting', 'another', 'place', 'checking', 'online', 'places', 'around', 'usc', 'lt', 'gt']\n",
      "account\n",
      "hous\n",
      "rent\n",
      "check\n",
      "After stemming with porters algorithm: ['left', 'vagu', 'said', 'would', 'inform', 'person', 'accoun', 'delai', 'rent', 'discuss', 'hous', 'agenc', 'ren', 'anoth', 'place', 'chec', 'onlin', 'place', 'around', 'usc']\n",
      "Tokenized sentence: ['dear', 'reached', 'railway', 'what', 'happen', 'to', 'you']\n",
      "After stop words removal: ['dear', 'reached', 'railway', 'happen']\n",
      "After stemming with porters algorithm: ['dear', 'reac', 'railwai', 'happen']\n",
      "Tokenized sentence: ['i', 've', 'not', 'called', 'you', 'in', 'a', 'while', 'this', 'is', 'hoping', 'it', 'was', 'l', 'r', 'malaria', 'and', 'that', 'you', 'know', 'that', 'we', 'miss', 'you', 'guys', 'i', 'miss', 'bani', 'big', 'so', 'pls', 'give', 'her', 'my', 'love', 'especially', 'have', 'a', 'great', 'day']\n",
      "After stop words removal: ['called', 'hoping', 'l', 'r', 'malaria', 'know', 'miss', 'guys', 'miss', 'bani', 'big', 'pls', 'give', 'love', 'especially', 'great', 'day']\n",
      "hop\n",
      "After stemming with porters algorithm: ['call', 'hope', 'malaria', 'know', 'miss', 'gui', 'miss', 'bani', 'big', 'pl', 'give', 'love', 'especi', 'great', 'dai']\n",
      "Tokenized sentence: ['i', 'wnt', 'to', 'buy', 'a', 'bmw', 'car', 'urgently', 'its', 'vry', 'urgent', 'but', 'hv', 'a', 'shortage', 'of', 'lt', 'gt', 'lacs', 'there', 'is', 'no', 'source', 'to', 'arng', 'dis', 'amt', 'lt', 'gt', 'lacs', 'thats', 'my', 'prob']\n",
      "After stop words removal: ['wnt', 'buy', 'bmw', 'car', 'urgently', 'vry', 'urgent', 'hv', 'shortage', 'lt', 'gt', 'lacs', 'source', 'arng', 'dis', 'amt', 'lt', 'gt', 'lacs', 'thats', 'prob']\n",
      "After stemming with porters algorithm: ['wnt', 'bui', 'bmw', 'car', 'urgent', 'vry', 'urgent', 'shortag', 'lac', 'sourc', 'arng', 'di', 'amt', 'lac', 'that', 'prob']\n",
      "Tokenized sentence: ['i', 'got', 'a', 'call', 'from', 'a', 'landline', 'number', 'i', 'am', 'asked', 'to', 'come', 'to', 'anna', 'nagar', 'i', 'will', 'go', 'in', 'the', 'afternoon']\n",
      "After stop words removal: ['got', 'call', 'landline', 'number', 'asked', 'come', 'anna', 'nagar', 'go', 'afternoon']\n",
      "After stemming with porters algorithm: ['got', 'call', 'landlin', 'number', 'as', 'come', 'anna', 'nagar', 'afternoon']\n",
      "Tokenized sentence: ['well', 'i', 'm', 'glad', 'you', 'didn', 't', 'find', 'it', 'totally', 'disagreeable', 'lol']\n",
      "After stop words removal: ['well', 'glad', 'find', 'totally', 'disagreeable', 'lol']\n",
      "After stemming with porters algorithm: ['well', 'glad', 'find', 'total', 'disagre', 'lol']\n",
      "Tokenized sentence: ['arun', 'can', 'u', 'transfr', 'me', 'd', 'amt']\n",
      "After stop words removal: ['arun', 'u', 'transfr', 'amt']\n",
      "After stemming with porters algorithm: ['arun', 'transfr', 'amt']\n",
      "Tokenized sentence: ['i', 'want', 'some', 'cock', 'my', 'hubby', 's', 'away', 'i', 'need', 'a', 'real', 'man', 'satisfy', 'me', 'txt', 'wife', 'to', 'for', 'no', 'strings', 'action', 'txt', 'stop', 'end', 'txt', 'rec', 'ea', 'otbox', 'la', 'ws']\n",
      "After stop words removal: ['want', 'cock', 'hubby', 'away', 'need', 'real', 'man', 'satisfy', 'txt', 'wife', 'strings', 'action', 'txt', 'stop', 'end', 'txt', 'rec', 'ea', 'otbox', 'la', 'ws']\n",
      "After stemming with porters algorithm: ['want', 'cock', 'hubbi', 'awai', 'need', 'real', 'man', 'satisfi', 'txt', 'wife', 'string', 'act', 'txt', 'stop', 'end', 'txt', 'rec', 'otbox']\n",
      "Tokenized sentence: ['hiya', 'sorry', 'didn', 't', 'hav', 'signal', 'i', 'haven', 't', 'seen', 'or', 'heard', 'from', 'and', 'neither', 'has', 'which', 'is', 'unusual', 'in', 'itself', 'i', 'll', 'put', 'on', 'the', 'case', 'and', 'get', 'him', 'to', 'sort', 'it', 'out', 'hugs', 'and', 'snogs']\n",
      "After stop words removal: ['hiya', 'sorry', 'hav', 'signal', 'seen', 'heard', 'neither', 'unusual', 'put', 'case', 'get', 'sort', 'hugs', 'snogs']\n",
      "After stemming with porters algorithm: ['hiya', 'sorri', 'hav', 'signal', 'seen', 'heard', 'neither', 'unusu', 'put', 'case', 'get', 'sort', 'hug', 'snog']\n",
      "Tokenized sentence: ['wan', 'win', 'a', 'meet', 'greet', 'with', 'westlife', 'u', 'or', 'a', 'm', 'they', 'are', 'currently', 'on', 'what', 'tour', 'unbreakable', 'untamed', 'unkempt', 'text', 'or', 'to', 'cost', 'p', 'std', 'text']\n",
      "After stop words removal: ['wan', 'win', 'meet', 'greet', 'westlife', 'u', 'currently', 'tour', 'unbreakable', 'untamed', 'unkempt', 'text', 'cost', 'p', 'std', 'text']\n",
      "After stemming with porters algorithm: ['wan', 'win', 'meet', 'greet', 'westlif', 'current', 'tour', 'unbreak', 'untam', 'unkempt', 'text', 'cost', 'std', 'text']\n",
      "Tokenized sentence: ['never', 'y', 'lei', 'i', 'v', 'lazy', 'got', 'wat', 'dat', 'day', 'send', 'me', 'da', 'url', 'cant', 'work', 'one']\n",
      "After stop words removal: ['never', 'lei', 'v', 'lazy', 'got', 'wat', 'dat', 'day', 'send', 'da', 'url', 'cant', 'work', 'one']\n",
      "After stemming with porters algorithm: ['never', 'lei', 'lazi', 'got', 'wat', 'dat', 'dai', 'send', 'url', 'cant', 'work', 'on']\n",
      "Tokenized sentence: ['phony', 'award', 'todays', 'voda', 'numbers', 'ending', 'xxxx', 'are', 'selected', 'to', 'receive', 'a', 'award', 'if', 'you', 'have', 'a', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "After stop words removal: ['phony', 'award', 'todays', 'voda', 'numbers', 'ending', 'xxxx', 'selected', 'receive', 'award', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "end\n",
      "quot\n",
      "After stemming with porters algorithm: ['phoni', 'award', 'todai', 'voda', 'number', 'en', 'xxxx', 'selec', 'receiv', 'award', 'match', 'pleas', 'call', 'quot', 'claim', 'code', 'standard', 'rate', 'app']\n",
      "Tokenized sentence: ['well', 'obviously', 'not', 'because', 'all', 'the', 'people', 'in', 'my', 'cool', 'college', 'life', 'went', 'home']\n",
      "After stop words removal: ['well', 'obviously', 'people', 'cool', 'college', 'life', 'went', 'home']\n",
      "After stemming with porters algorithm: ['well', 'obvious', 'peopl', 'cool', 'colleg', 'life', 'went', 'home']\n",
      "Tokenized sentence: ['midnight', 'at', 'the', 'earliest']\n",
      "After stop words removal: ['midnight', 'earliest']\n",
      "After stemming with porters algorithm: ['midnight', 'earliest']\n",
      "Tokenized sentence: ['lol', 'yep', 'did', 'that', 'yesterday', 'already', 'got', 'my', 'fireplace', 'now', 'its', 'just', 'another', 'icon', 'sitting', 'there', 'for', 'me']\n",
      "After stop words removal: ['lol', 'yep', 'yesterday', 'already', 'got', 'fireplace', 'another', 'icon', 'sitting']\n",
      "sitt\n",
      "After stemming with porters algorithm: ['lol', 'yep', 'yesterdai', 'alreadi', 'got', 'fireplac', 'anoth', 'icon', 'sit']\n",
      "Tokenized sentence: ['k', 'if', 'u', 'bored', 'up', 'just', 'come', 'to', 'my', 'home']\n",
      "After stop words removal: ['k', 'u', 'bored', 'come', 'home']\n",
      "After stemming with porters algorithm: ['bore', 'come', 'home']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'home', 'please', 'call']\n",
      "After stop words removal: ['home', 'please', 'call']\n",
      "After stemming with porters algorithm: ['home', 'pleas', 'call']\n",
      "Tokenized sentence: ['feb', 'lt', 'gt', 'is', 'i', 'love', 'u', 'day', 'send', 'dis', 'to', 'all', 'ur', 'valued', 'frnds', 'evn', 'me', 'if', 'comes', 'back', 'u', 'll', 'gt', 'married', 'd', 'person', 'u', 'luv', 'if', 'u', 'ignore', 'dis', 'u', 'will', 'lose', 'ur', 'luv', 'evr']\n",
      "After stop words removal: ['feb', 'lt', 'gt', 'love', 'u', 'day', 'send', 'dis', 'ur', 'valued', 'frnds', 'evn', 'comes', 'back', 'u', 'gt', 'married', 'person', 'u', 'luv', 'u', 'ignore', 'dis', 'u', 'lose', 'ur', 'luv', 'evr']\n",
      "After stemming with porters algorithm: ['feb', 'love', 'dai', 'send', 'di', 'valu', 'frnd', 'evn', 'come', 'back', 'marri', 'person', 'luv', 'ignor', 'di', 'lose', 'luv', 'evr']\n",
      "Tokenized sentence: ['i', 'dont', 'have', 'that', 'much', 'image', 'in', 'class']\n",
      "After stop words removal: ['dont', 'much', 'image', 'class']\n",
      "After stemming with porters algorithm: ['dont', 'much', 'imag', 'class']\n",
      "Tokenized sentence: ['i', 'dont', 'knw', 'pa', 'i', 'just', 'drink', 'milk']\n",
      "After stop words removal: ['dont', 'knw', 'pa', 'drink', 'milk']\n",
      "After stemming with porters algorithm: ['dont', 'knw', 'drink', 'milk']\n",
      "Tokenized sentence: ['awesome', 'that', 'gonna', 'be', 'soon', 'or', 'later', 'tonight']\n",
      "After stop words removal: ['awesome', 'gonna', 'soon', 'later', 'tonight']\n",
      "After stemming with porters algorithm: ['awesom', 'gonna', 'soon', 'later', 'tonight']\n",
      "Tokenized sentence: ['days', 'to', 'euro', 'kickoff', 'u', 'will', 'be', 'kept', 'informed', 'of', 'all', 'the', 'latest', 'news', 'and', 'results', 'daily', 'unsubscribe', 'send', 'get', 'euro', 'stop', 'to']\n",
      "After stop words removal: ['days', 'euro', 'kickoff', 'u', 'kept', 'informed', 'latest', 'news', 'results', 'daily', 'unsubscribe', 'send', 'get', 'euro', 'stop']\n",
      "After stemming with porters algorithm: ['dai', 'euro', 'kickoff', 'kept', 'infor', 'latest', 'new', 'result', 'daili', 'unsubscrib', 'send', 'get', 'euro', 'stop']\n",
      "Tokenized sentence: ['urgent', 'please', 'call', 'from', 'a', 'landline', 'cash', 'or', 'a', 'holiday', 'await', 'collection', 't', 'cs', 'sae', 'po', 'box', 'm', 'xy']\n",
      "After stop words removal: ['urgent', 'please', 'call', 'landline', 'cash', 'holiday', 'await', 'collection', 'cs', 'sae', 'po', 'box', 'xy']\n",
      "After stemming with porters algorithm: ['urgent', 'pleas', 'call', 'landlin', 'cash', 'holidai', 'await', 'collect', 'sae', 'box']\n",
      "Tokenized sentence: ['hcl', 'chennai', 'requires', 'freshers', 'for', 'voice', 'process', 'excellent', 'english', 'needed', 'salary', 'upto', 'lt', 'gt', 'call', 'ms', 'suman', 'lt', 'gt', 'for', 'telephonic', 'interview', 'via', 'indyarocks', 'com']\n",
      "After stop words removal: ['hcl', 'chennai', 'requires', 'freshers', 'voice', 'process', 'excellent', 'english', 'needed', 'salary', 'upto', 'lt', 'gt', 'call', 'ms', 'suman', 'lt', 'gt', 'telephonic', 'interview', 'via', 'indyarocks', 'com']\n",
      "After stemming with porters algorithm: ['hcl', 'chennai', 'requir', 'fresher', 'voic', 'process', 'excel', 'english', 'need', 'salari', 'upto', 'call', 'suman', 'telephon', 'interview', 'via', 'indyarock', 'com']\n",
      "Tokenized sentence: ['do', 'you', 'know', 'why', 'god', 'created', 'gap', 'between', 'your', 'fingers', 'so', 'that', 'one', 'who', 'is', 'made', 'for', 'you', 'comes', 'amp', 'fills', 'those', 'gaps', 'by', 'holding', 'your', 'hand', 'with', 'love']\n",
      "After stop words removal: ['know', 'god', 'created', 'gap', 'fingers', 'one', 'made', 'comes', 'amp', 'fills', 'gaps', 'holding', 'hand', 'love']\n",
      "create\n",
      "hold\n",
      "After stemming with porters algorithm: ['know', 'god', 'creat', 'gap', 'finger', 'on', 'made', 'come', 'amp', 'fill', 'gap', 'hol', 'hand', 'love']\n",
      "Tokenized sentence: ['neft', 'transaction', 'with', 'reference', 'number', 'lt', 'gt', 'for', 'rs', 'lt', 'decimal', 'gt', 'has', 'been', 'credited', 'to', 'the', 'beneficiary', 'account', 'on', 'lt', 'gt', 'at', 'lt', 'time', 'gt', 'lt', 'gt']\n",
      "After stop words removal: ['neft', 'transaction', 'reference', 'number', 'lt', 'gt', 'rs', 'lt', 'decimal', 'gt', 'credited', 'beneficiary', 'account', 'lt', 'gt', 'lt', 'time', 'gt', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['neft', 'transact', 'refer', 'number', 'decim', 'credit', 'beneficiari', 'account', 'time']\n",
      "Tokenized sentence: ['is', 'toshiba', 'portege', 'm', 'gd']\n",
      "After stop words removal: ['toshiba', 'portege', 'gd']\n",
      "After stemming with porters algorithm: ['toshiba', 'porteg']\n",
      "Tokenized sentence: ['hello', 'handsome', 'are', 'you', 'finding', 'that', 'job', 'not', 'being', 'lazy', 'working', 'towards', 'getting', 'back', 'that', 'net', 'for', 'mummy', 'where', 's', 'my', 'boytoy', 'now', 'does', 'he', 'miss', 'me']\n",
      "After stop words removal: ['hello', 'handsome', 'finding', 'job', 'lazy', 'working', 'towards', 'getting', 'back', 'net', 'mummy', 'boytoy', 'miss']\n",
      "find\n",
      "work\n",
      "gett\n",
      "After stemming with porters algorithm: ['hello', 'handsom', 'fin', 'job', 'lazi', 'wor', 'toward', 'get', 'back', 'net', 'mummi', 'boytoi', 'miss']\n",
      "Tokenized sentence: ['ditto', 'and', 'you', 'won', 't', 'have', 'to', 'worry', 'about', 'me', 'saying', 'anything', 'to', 'you', 'anymore', 'like', 'i', 'said', 'last', 'night', 'you', 'do', 'whatever', 'you', 'want', 'and', 'i', 'll', 'do', 'the', 'same', 'peace']\n",
      "After stop words removal: ['ditto', 'worry', 'saying', 'anything', 'anymore', 'like', 'said', 'last', 'night', 'whatever', 'want', 'peace']\n",
      "say\n",
      "anyth\n",
      "After stemming with porters algorithm: ['ditto', 'worri', 'sai', 'anyt', 'anymor', 'like', 'said', 'last', 'night', 'whatev', 'want', 'peac']\n",
      "Tokenized sentence: ['mathews', 'or', 'tait', 'or', 'edwards', 'or', 'anderson']\n",
      "After stop words removal: ['mathews', 'tait', 'edwards', 'anderson']\n",
      "After stemming with porters algorithm: ['mathew', 'tait', 'edward', 'anderson']\n",
      "Tokenized sentence: ['yeah', 'that', 's', 'the', 'impression', 'i', 'got']\n",
      "After stop words removal: ['yeah', 'impression', 'got']\n",
      "After stemming with porters algorithm: ['yeah', 'impress', 'got']\n",
      "Tokenized sentence: ['hello', 'no', 'news', 'on', 'job', 'they', 'are', 'making', 'me', 'wait', 'a', 'fifth', 'week', 'yeah', 'im', 'up', 'for', 'some', 'woozles', 'and', 'weasels', 'in', 'exeter', 'still', 'but', 'be', 'home', 'about']\n",
      "After stop words removal: ['hello', 'news', 'job', 'making', 'wait', 'fifth', 'week', 'yeah', 'im', 'woozles', 'weasels', 'exeter', 'still', 'home']\n",
      "mak\n",
      "After stemming with porters algorithm: ['hello', 'new', 'job', 'make', 'wait', 'fifth', 'week', 'yeah', 'woozl', 'weasel', 'exet', 'still', 'home']\n",
      "Tokenized sentence: ['ok', 'lor', 'wat', 'time', 'finish']\n",
      "After stop words removal: ['ok', 'lor', 'wat', 'time', 'finish']\n",
      "After stemming with porters algorithm: ['lor', 'wat', 'time', 'finish']\n",
      "Tokenized sentence: ['sez', 'hows', 'u', 'de', 'arab', 'boy', 'hope', 'u', 'r', 'all', 'good', 'give', 'my', 'love', 'evry', 'love', 'ya', 'eshxxxxxxxxxxx']\n",
      "After stop words removal: ['sez', 'hows', 'u', 'de', 'arab', 'boy', 'hope', 'u', 'r', 'good', 'give', 'love', 'evry', 'love', 'ya', 'eshxxxxxxxxxxx']\n",
      "After stemming with porters algorithm: ['sez', 'how', 'arab', 'boi', 'hope', 'good', 'give', 'love', 'evri', 'love', 'eshxxxxxxxxxxx']\n",
      "Tokenized sentence: ['hello', 'how', 's', 'you', 'and', 'how', 'did', 'saturday', 'go', 'i', 'was', 'just', 'texting', 'to', 'see', 'if', 'you', 'd', 'decided', 'to', 'do', 'anything', 'tomo', 'not', 'that', 'i', 'm', 'trying', 'to', 'invite', 'myself', 'or', 'anything']\n",
      "After stop words removal: ['hello', 'saturday', 'go', 'texting', 'see', 'decided', 'anything', 'tomo', 'trying', 'invite', 'anything']\n",
      "text\n",
      "anyth\n",
      "anyth\n",
      "After stemming with porters algorithm: ['hello', 'saturdai', 'tex', 'see', 'decid', 'anyt', 'tomo', 'trying', 'invit', 'anyt']\n",
      "Tokenized sentence: ['free', 'message', 'activate', 'your', 'free', 'text', 'messages', 'by', 'replying', 'to', 'this', 'message', 'with', 'the', 'word', 'free', 'for', 'terms', 'conditions', 'visit', 'www', 'com']\n",
      "After stop words removal: ['free', 'message', 'activate', 'free', 'text', 'messages', 'replying', 'message', 'word', 'free', 'terms', 'conditions', 'visit', 'www', 'com']\n",
      "reply\n",
      "After stemming with porters algorithm: ['free', 'messag', 'activ', 'free', 'text', 'messag', 'repl', 'messag', 'word', 'free', 'term', 'condit', 'visit', 'www', 'com']\n",
      "Tokenized sentence: ['text', 'meet', 'someone', 'sexy', 'today', 'u', 'can', 'find', 'a', 'date', 'or', 'even', 'flirt', 'its', 'up', 'to', 'u', 'join', 'just', 'p', 'reply', 'with', 'name', 'age', 'eg', 'sam', 'msg', 'recd', 'thirtyeight', 'pence']\n",
      "After stop words removal: ['text', 'meet', 'someone', 'sexy', 'today', 'u', 'find', 'date', 'even', 'flirt', 'u', 'join', 'p', 'reply', 'name', 'age', 'eg', 'sam', 'msg', 'recd', 'thirtyeight', 'pence']\n",
      "After stemming with porters algorithm: ['text', 'meet', 'someon', 'sexi', 'todai', 'find', 'date', 'even', 'flirt', 'join', 'repli', 'name', 'ag', 'sam', 'msg', 'recd', 'thirtyeight', 'penc']\n",
      "Tokenized sentence: ['congrats', 'mobile', 'g', 'videophones', 'r', 'yours', 'call', 'now', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyh', 'music', 'noline', 'rentl', 'bx', 'ip', 'we', 'pm']\n",
      "After stop words removal: ['congrats', 'mobile', 'g', 'videophones', 'r', 'call', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyh', 'music', 'noline', 'rentl', 'bx', 'ip', 'pm']\n",
      "After stemming with porters algorithm: ['congrat', 'mobil', 'videophon', 'call', 'videochat', 'wid', 'mate', 'plai', 'java', 'game', 'dload', 'polyh', 'music', 'nolin', 'rentl']\n",
      "Tokenized sentence: ['slept', 'i', 'thinkthis', 'time', 'lt', 'gt', 'pm', 'is', 'not', 'dangerous']\n",
      "After stop words removal: ['slept', 'thinkthis', 'time', 'lt', 'gt', 'pm', 'dangerous']\n",
      "After stemming with porters algorithm: ['slept', 'thinkthi', 'time', 'danger']\n",
      "Tokenized sentence: ['please', 'sen', 'my', 'kind', 'advice', 'please', 'come', 'here', 'and', 'try']\n",
      "After stop words removal: ['please', 'sen', 'kind', 'advice', 'please', 'come', 'try']\n",
      "After stemming with porters algorithm: ['pleas', 'sen', 'kind', 'advic', 'pleas', 'come', 'try']\n",
      "Tokenized sentence: ['sorry', 'i', 'missed', 'your', 'call', 'let', 's', 'talk', 'when', 'you', 'have', 'the', 'time', 'i', 'm', 'on']\n",
      "After stop words removal: ['sorry', 'missed', 'call', 'let', 'talk', 'time']\n",
      "After stemming with porters algorithm: ['sorri', 'miss', 'call', 'let', 'talk', 'time']\n",
      "Tokenized sentence: ['da', 'is', 'good', 'good', 'player', 'why', 'he', 'is', 'unsold']\n",
      "After stop words removal: ['da', 'good', 'good', 'player', 'unsold']\n",
      "After stemming with porters algorithm: ['good', 'good', 'player', 'unsold']\n",
      "Tokenized sentence: ['fuuuuck', 'i', 'need', 'to', 'stop', 'sleepin', 'sup']\n",
      "After stop words removal: ['fuuuuck', 'need', 'stop', 'sleepin', 'sup']\n",
      "After stemming with porters algorithm: ['fuuuuck', 'need', 'stop', 'sleepin', 'sup']\n",
      "Tokenized sentence: ['yup', 'izzit', 'still', 'raining', 'heavily', 'cos', 'i', 'm', 'in', 'e', 'mrt', 'i', 'can', 't', 'c', 'outside']\n",
      "After stop words removal: ['yup', 'izzit', 'still', 'raining', 'heavily', 'cos', 'e', 'mrt', 'c', 'outside']\n",
      "rain\n",
      "After stemming with porters algorithm: ['yup', 'izzit', 'still', 'rain', 'heavili', 'co', 'mrt', 'outsid']\n",
      "Tokenized sentence: ['is', 'ur', 'paper', 'today', 'in', 'e', 'morn', 'or', 'aft']\n",
      "After stop words removal: ['ur', 'paper', 'today', 'e', 'morn', 'aft']\n",
      "After stemming with porters algorithm: ['paper', 'todai', 'morn', 'aft']\n",
      "Tokenized sentence: ['i', 'wonder', 'how', 'you', 'got', 'online', 'my', 'love', 'had', 'you', 'gone', 'to', 'the', 'net', 'cafe', 'did', 'you', 'get', 'your', 'phone', 'recharged', 'were', 'you', 'on', 'a', 'friends', 'net', 'i', 'think', 'of', 'you', 'boytoy']\n",
      "After stop words removal: ['wonder', 'got', 'online', 'love', 'gone', 'net', 'cafe', 'get', 'phone', 'recharged', 'friends', 'net', 'think', 'boytoy']\n",
      "After stemming with porters algorithm: ['wonder', 'got', 'onlin', 'love', 'gone', 'net', 'cafe', 'get', 'phone', 'rechar', 'friend', 'net', 'think', 'boytoi']\n",
      "Tokenized sentence: ['i', 'walked', 'an', 'hour', 'c', 'u', 'doesn', 't', 'that', 'show', 'i', 'care', 'y', 'wont', 'u', 'believe', 'im', 'serious']\n",
      "After stop words removal: ['walked', 'hour', 'c', 'u', 'show', 'care', 'wont', 'u', 'believe', 'im', 'serious']\n",
      "After stemming with porters algorithm: ['wal', 'hour', 'show', 'care', 'wont', 'believ', 'seriou']\n",
      "Tokenized sentence: ['unless', 'it', 's', 'a', 'situation', 'where', 'you', 'go', 'gurl', 'would', 'be', 'more', 'appropriate']\n",
      "After stop words removal: ['unless', 'situation', 'go', 'gurl', 'would', 'appropriate']\n",
      "After stemming with porters algorithm: ['unless', 'situat', 'gurl', 'would', 'appropri']\n",
      "Tokenized sentence: ['customer', 'loyalty', 'offer', 'the', 'new', 'nokia', 'mobile', 'from', 'only', 'at', 'txtauction', 'txt', 'word', 'start', 'to', 'no', 'get', 'yours', 'now', 't', 'ctxt', 'tc', 'p', 'mtmsg']\n",
      "After stop words removal: ['customer', 'loyalty', 'offer', 'new', 'nokia', 'mobile', 'txtauction', 'txt', 'word', 'start', 'get', 'ctxt', 'tc', 'p', 'mtmsg']\n",
      "After stemming with porters algorithm: ['custom', 'loyalti', 'offer', 'new', 'nokia', 'mobil', 'txtauct', 'txt', 'word', 'start', 'get', 'ctxt', 'mtmsg']\n",
      "Tokenized sentence: ['howz', 'that', 'persons', 'story']\n",
      "After stop words removal: ['howz', 'persons', 'story']\n",
      "After stemming with porters algorithm: ['howz', 'person', 'stori']\n",
      "Tokenized sentence: ['appt', 'is', 'at', 'lt', 'time', 'gt', 'am', 'not', 'my', 'fault', 'u', 'don', 't', 'listen', 'i', 'told', 'u', 'twice']\n",
      "After stop words removal: ['appt', 'lt', 'time', 'gt', 'fault', 'u', 'listen', 'told', 'u', 'twice']\n",
      "After stemming with porters algorithm: ['appt', 'time', 'fault', 'listen', 'told', 'twice']\n",
      "Tokenized sentence: ['wot', 'is', 'u', 'up', 'then', 'bitch']\n",
      "After stop words removal: ['wot', 'u', 'bitch']\n",
      "After stemming with porters algorithm: ['wot', 'bitch']\n",
      "Tokenized sentence: ['no', 'da', 'i', 'am', 'happy', 'that', 'we', 'sit', 'together', 'na']\n",
      "After stop words removal: ['da', 'happy', 'sit', 'together', 'na']\n",
      "After stemming with porters algorithm: ['happi', 'sit', 'togeth']\n",
      "Tokenized sentence: ['so', 'what', 'do', 'you', 'guys', 'do']\n",
      "After stop words removal: ['guys']\n",
      "After stemming with porters algorithm: ['gui']\n",
      "Tokenized sentence: ['want', 'to', 'funk', 'up', 'ur', 'fone', 'with', 'a', 'weekly', 'new', 'tone', 'reply', 'tones', 'u', 'this', 'text', 'www', 'ringtones', 'co', 'uk', 'the', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stop words removal: ['want', 'funk', 'ur', 'fone', 'weekly', 'new', 'tone', 'reply', 'tones', 'u', 'text', 'www', 'ringtones', 'co', 'uk', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
      "After stemming with porters algorithm: ['want', 'funk', 'fone', 'weekli', 'new', 'tone', 'repli', 'tone', 'text', 'www', 'rington', 'origin', 'best', 'tone', 'gbp', 'network', 'oper', 'rate', 'appli']\n",
      "Tokenized sentence: ['aight', 'will', 'do', 'thanks', 'again', 'for', 'comin', 'out']\n",
      "After stop words removal: ['aight', 'thanks', 'comin']\n",
      "After stemming with porters algorithm: ['aight', 'thank', 'comin']\n",
      "Tokenized sentence: ['is', 'that', 'seriously', 'how', 'you', 'spell', 'his', 'name']\n",
      "After stop words removal: ['seriously', 'spell', 'name']\n",
      "After stemming with porters algorithm: ['serious', 'spell', 'name']\n",
      "Tokenized sentence: ['he', 'said', 'i', 'look', 'pretty', 'wif', 'long', 'hair', 'wat', 'but', 'i', 'thk', 'he', 's', 'cutting', 'quite', 'short', 'me', 'leh']\n",
      "After stop words removal: ['said', 'look', 'pretty', 'wif', 'long', 'hair', 'wat', 'thk', 'cutting', 'quite', 'short', 'leh']\n",
      "cutt\n",
      "After stemming with porters algorithm: ['said', 'look', 'pretti', 'wif', 'long', 'hair', 'wat', 'thk', 'cut', 'quit', 'short', 'leh']\n",
      "Tokenized sentence: ['no', 'its', 'not', 'specialisation', 'can', 'work', 'but', 'its', 'slave', 'labor', 'will', 'look', 'for', 'it', 'this', 'month', 'sha', 'cos', 'no', 'shakara', 'beggar']\n",
      "After stop words removal: ['specialisation', 'work', 'slave', 'labor', 'look', 'month', 'sha', 'cos', 'shakara', 'beggar']\n",
      "After stemming with porters algorithm: ['specialis', 'work', 'slave', 'labor', 'look', 'month', 'sha', 'co', 'shakara', 'beggar']\n",
      "Tokenized sentence: ['cool', 'i', 'll', 'text', 'you', 'when', 'i', 'm', 'on', 'the', 'way']\n",
      "After stop words removal: ['cool', 'text', 'way']\n",
      "After stemming with porters algorithm: ['cool', 'text', 'wai']\n",
      "Tokenized sentence: ['i', 'was', 'at', 'bugis', 'juz', 'now', 'wat', 'but', 'now', 'i', 'm', 'walking', 'home', 'oredi', 'so', 'late', 'then', 'reply', 'i', 'oso', 'saw', 'a', 'top', 'dat', 'i', 'like', 'but', 'din', 'buy', 'where', 'r', 'now']\n",
      "After stop words removal: ['bugis', 'juz', 'wat', 'walking', 'home', 'oredi', 'late', 'reply', 'oso', 'saw', 'top', 'dat', 'like', 'din', 'buy', 'r']\n",
      "walk\n",
      "After stemming with porters algorithm: ['bugi', 'juz', 'wat', 'wal', 'home', 'oredi', 'late', 'repli', 'oso', 'saw', 'top', 'dat', 'like', 'din', 'bui']\n",
      "Tokenized sentence: ['thought', 'we', 'could', 'go', 'out', 'for', 'dinner', 'i', 'll', 'treat', 'you', 'seem', 'ok']\n",
      "After stop words removal: ['thought', 'could', 'go', 'dinner', 'treat', 'seem', 'ok']\n",
      "After stemming with porters algorithm: ['thought', 'could', 'dinner', 'treat', 'seem']\n",
      "Tokenized sentence: ['what', 'time', 'you', 'coming', 'down', 'later']\n",
      "After stop words removal: ['time', 'coming', 'later']\n",
      "com\n",
      "After stemming with porters algorithm: ['time', 'come', 'later']\n",
      "Tokenized sentence: ['hey', 'elaine', 'is', 'today', 's', 'meeting', 'still', 'on']\n",
      "After stop words removal: ['hey', 'elaine', 'today', 'meeting', 'still']\n",
      "meet\n",
      "After stemming with porters algorithm: ['hei', 'elain', 'todai', 'meet', 'still']\n",
      "Tokenized sentence: ['ok', 'lar', 'i', 'double', 'check', 'wif', 'da', 'hair', 'dresser', 'already', 'he', 'said', 'wun', 'cut', 'v', 'short', 'he', 'said', 'will', 'cut', 'until', 'i', 'look', 'nice']\n",
      "After stop words removal: ['ok', 'lar', 'double', 'check', 'wif', 'da', 'hair', 'dresser', 'already', 'said', 'wun', 'cut', 'v', 'short', 'said', 'cut', 'look', 'nice']\n",
      "After stemming with porters algorithm: ['lar', 'doubl', 'check', 'wif', 'hair', 'dresser', 'alreadi', 'said', 'wun', 'cut', 'short', 'said', 'cut', 'look', 'nice']\n",
      "Tokenized sentence: ['sindu', 'got', 'job', 'in', 'birla', 'soft']\n",
      "After stop words removal: ['sindu', 'got', 'job', 'birla', 'soft']\n",
      "After stemming with porters algorithm: ['sindu', 'got', 'job', 'birla', 'soft']\n",
      "Tokenized sentence: ['the', 'greatest', 'test', 'of', 'courage', 'on', 'earth', 'is', 'to', 'bear', 'defeat', 'without', 'losing', 'heart', 'gn', 'tc']\n",
      "After stop words removal: ['greatest', 'test', 'courage', 'earth', 'bear', 'defeat', 'without', 'losing', 'heart', 'gn', 'tc']\n",
      "los\n",
      "After stemming with porters algorithm: ['greatest', 'test', 'courag', 'earth', 'bear', 'defeat', 'without', 'lose', 'heart']\n",
      "Tokenized sentence: ['can', 't', 'take', 'any', 'major', 'roles', 'in', 'community', 'outreach', 'you', 'rock', 'mel']\n",
      "After stop words removal: ['take', 'major', 'roles', 'community', 'outreach', 'rock', 'mel']\n",
      "After stemming with porters algorithm: ['take', 'major', 'role', 'commun', 'outreach', 'rock', 'mel']\n",
      "Tokenized sentence: ['that', 'one', 'week', 'leave', 'i', 'put', 'know', 'that', 'time', 'why']\n",
      "After stop words removal: ['one', 'week', 'leave', 'put', 'know', 'time']\n",
      "After stemming with porters algorithm: ['on', 'week', 'leav', 'put', 'know', 'time']\n",
      "Tokenized sentence: ['awesome', 'think', 'we', 'can', 'get', 'an', 'th', 'at', 'usf', 'some', 'time', 'tonight']\n",
      "After stop words removal: ['awesome', 'think', 'get', 'th', 'usf', 'time', 'tonight']\n",
      "After stemming with porters algorithm: ['awesom', 'think', 'get', 'usf', 'time', 'tonight']\n",
      "Tokenized sentence: ['natalja', 'f', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'www', 'sms', 'ac', 'u', 'nat', 'stop', 'send', 'stop', 'frnd', 'to']\n",
      "After stop words removal: ['natalja', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'www', 'sms', 'ac', 'u', 'nat', 'stop', 'send', 'stop', 'frnd']\n",
      "invit\n",
      "After stemming with porters algorithm: ['natalja', 'invit', 'friend', 'repli', 'ye', 'see', 'www', 'sm', 'nat', 'stop', 'send', 'stop', 'frnd']\n",
      "Tokenized sentence: ['win', 'a', 'year', 'supply', 'of', 'cds', 'a', 'store', 'of', 'ur', 'choice', 'worth', 'enter', 'our', 'weekly', 'draw', 'txt', 'music', 'to', 'ts', 'cs', 'www', 'ldew', 'com', 'subs', 'win', 'ppmx']\n",
      "After stop words removal: ['win', 'year', 'supply', 'cds', 'store', 'ur', 'choice', 'worth', 'enter', 'weekly', 'draw', 'txt', 'music', 'ts', 'cs', 'www', 'ldew', 'com', 'subs', 'win', 'ppmx']\n",
      "After stemming with porters algorithm: ['win', 'year', 'suppli', 'cd', 'store', 'choic', 'worth', 'enter', 'weekli', 'draw', 'txt', 'music', 'www', 'ldew', 'com', 'sub', 'win', 'ppmx']\n",
      "Tokenized sentence: ['what', 'happened', 'to', 'our', 'yo', 'date']\n",
      "After stop words removal: ['happened', 'yo', 'date']\n",
      "After stemming with porters algorithm: ['happen', 'date']\n",
      "Tokenized sentence: ['i', 'attended', 'but', 'nothing', 'is', 'there']\n",
      "After stop words removal: ['attended', 'nothing']\n",
      "noth\n",
      "After stemming with porters algorithm: ['atten', 'not']\n",
      "Tokenized sentence: ['seriously', 'tell', 'her', 'those', 'exact', 'words', 'right', 'now']\n",
      "After stop words removal: ['seriously', 'tell', 'exact', 'words', 'right']\n",
      "After stemming with porters algorithm: ['serious', 'tell', 'exact', 'word', 'right']\n",
      "Tokenized sentence: ['now', 'i', 'm', 'going', 'for', 'lunch']\n",
      "After stop words removal: ['going', 'lunch']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'lunch']\n",
      "Tokenized sentence: ['double', 'mins', 'and', 'txts', 'months', 'free', 'bluetooth', 'on', 'orange', 'available', 'on', 'sony', 'nokia', 'motorola', 'phones', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'n', 'dx']\n",
      "After stop words removal: ['double', 'mins', 'txts', 'months', 'free', 'bluetooth', 'orange', 'available', 'sony', 'nokia', 'motorola', 'phones', 'call', 'mobileupd', 'call', 'optout', 'n', 'dx']\n",
      "After stemming with porters algorithm: ['doubl', 'min', 'txt', 'month', 'free', 'bluetooth', 'orang', 'avail', 'soni', 'nokia', 'motorola', 'phone', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['hi', 'did', 'you', 'asked', 'to', 'waheeda', 'fathima', 'about', 'leave']\n",
      "After stop words removal: ['hi', 'asked', 'waheeda', 'fathima', 'leave']\n",
      "After stemming with porters algorithm: ['as', 'waheeda', 'fathima', 'leav']\n",
      "Tokenized sentence: ['hot', 'live', 'fantasies', 'call', 'now', 'just', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'k']\n",
      "After stop words removal: ['hot', 'live', 'fantasies', 'call', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'k']\n",
      "After stemming with porters algorithm: ['hot', 'live', 'fantasi', 'call', 'per', 'min', 'ntt', 'ltd', 'box', 'croydon']\n",
      "Tokenized sentence: ['ill', 'call', 'you', 'evening', 'ill', 'some', 'ideas']\n",
      "After stop words removal: ['ill', 'call', 'evening', 'ill', 'ideas']\n",
      "even\n",
      "After stemming with porters algorithm: ['ill', 'call', 'even', 'ill', 'idea']\n",
      "Tokenized sentence: ['this', 'pay', 'is', 'lt', 'decimal', 'gt', 'lakhs']\n",
      "After stop words removal: ['pay', 'lt', 'decimal', 'gt', 'lakhs']\n",
      "After stemming with porters algorithm: ['pai', 'decim', 'lakh']\n",
      "Tokenized sentence: ['i', 'not', 'busy', 'juz', 'dun', 'wan', 'go', 'so', 'early', 'hee']\n",
      "After stop words removal: ['busy', 'juz', 'dun', 'wan', 'go', 'early', 'hee']\n",
      "After stemming with porters algorithm: ['busi', 'juz', 'dun', 'wan', 'earli', 'hee']\n",
      "Tokenized sentence: ['you', 'are', 'chosen', 'to', 'receive', 'a', 'award', 'pls', 'call', 'claim', 'number', 'to', 'collect', 'your', 'award', 'which', 'you', 'are', 'selected', 'to', 'receive', 'as', 'a', 'valued', 'mobile', 'customer']\n",
      "After stop words removal: ['chosen', 'receive', 'award', 'pls', 'call', 'claim', 'number', 'collect', 'award', 'selected', 'receive', 'valued', 'mobile', 'customer']\n",
      "After stemming with porters algorithm: ['chosen', 'receiv', 'award', 'pl', 'call', 'claim', 'number', 'collect', 'award', 'selec', 'receiv', 'valu', 'mobil', 'custom']\n",
      "Tokenized sentence: ['hope', 'you', 'are', 'not', 'scared']\n",
      "After stop words removal: ['hope', 'scared']\n",
      "After stemming with porters algorithm: ['hope', 'scare']\n",
      "Tokenized sentence: ['adult', 'content', 'your', 'video', 'will', 'be', 'with', 'you', 'shortly']\n",
      "After stop words removal: ['adult', 'content', 'video', 'shortly']\n",
      "After stemming with porters algorithm: ['adult', 'content', 'video', 'shortli']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'won', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'reach', 'you', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'reach', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'bonu', 'caller', 'priz', 'attempt', 'reach', 'call', 'asap', 'box', 'ppm']\n",
      "Tokenized sentence: ['urgent', 'ur', 'awarded', 'a', 'complimentary', 'trip', 'to', 'eurodisinc', 'trav', 'aco', 'entry', 'or', 'to', 'claim', 'txt', 'dis', 'to', 'morefrmmob', 'shracomorsglsuplt', 'ls', 'aj']\n",
      "After stop words removal: ['urgent', 'ur', 'awarded', 'complimentary', 'trip', 'eurodisinc', 'trav', 'aco', 'entry', 'claim', 'txt', 'dis', 'morefrmmob', 'shracomorsglsuplt', 'ls', 'aj']\n",
      "After stemming with porters algorithm: ['urgent', 'awar', 'complimentari', 'trip', 'eurodisinc', 'trav', 'aco', 'entri', 'claim', 'txt', 'di', 'morefrmmob', 'shracomorsglsuplt']\n",
      "Tokenized sentence: ['yetunde', 'i', 'm', 'sorry', 'but', 'moji', 'and', 'i', 'seem', 'too', 'busy', 'to', 'be', 'able', 'to', 'go', 'shopping', 'can', 'you', 'just', 'please', 'find', 'some', 'other', 'way', 'to', 'get', 'what', 'you', 'wanted', 'us', 'to', 'get', 'please', 'forgive', 'me', 'you', 'can', 'reply', 'free', 'via', 'yahoo', 'messenger']\n",
      "After stop words removal: ['yetunde', 'sorry', 'moji', 'seem', 'busy', 'able', 'go', 'shopping', 'please', 'find', 'way', 'get', 'wanted', 'us', 'get', 'please', 'forgive', 'reply', 'free', 'via', 'yahoo', 'messenger']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['yetund', 'sorri', 'moji', 'seem', 'busi', 'abl', 'shop', 'pleas', 'find', 'wai', 'get', 'wan', 'get', 'pleas', 'forgiv', 'repli', 'free', 'via', 'yahoo', 'messeng']\n",
      "Tokenized sentence: ['hmm', 'ok', 'i', 'll', 'stay', 'for', 'like', 'an', 'hour', 'cos', 'my', 'eye', 'is', 'really', 'sore']\n",
      "After stop words removal: ['hmm', 'ok', 'stay', 'like', 'hour', 'cos', 'eye', 'really', 'sore']\n",
      "After stemming with porters algorithm: ['hmm', 'stai', 'like', 'hour', 'co', 'ey', 'realli', 'sore']\n",
      "Tokenized sentence: ['cab', 'is', 'available', 'they', 'pick', 'up', 'and', 'drop', 'at', 'door', 'steps']\n",
      "After stop words removal: ['cab', 'available', 'pick', 'drop', 'door', 'steps']\n",
      "After stemming with porters algorithm: ['cab', 'avail', 'pick', 'drop', 'door', 'step']\n",
      "Tokenized sentence: ['did', 'u', 'download', 'the', 'fring', 'app']\n",
      "After stop words removal: ['u', 'download', 'fring', 'app']\n",
      "After stemming with porters algorithm: ['download', 'fring', 'app']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'office', 'whats', 'the', 'matter', 'msg', 'me', 'now', 'i', 'will', 'call', 'you', 'at', 'break']\n",
      "After stop words removal: ['office', 'whats', 'matter', 'msg', 'call', 'break']\n",
      "After stemming with porters algorithm: ['offic', 'what', 'matter', 'msg', 'call', 'break']\n",
      "Tokenized sentence: ['all', 'done', 'all', 'handed', 'in', 'don', 't', 'know', 'if', 'mega', 'shop', 'in', 'asda', 'counts', 'as', 'celebration', 'but', 'thats', 'what', 'i', 'm', 'doing']\n",
      "After stop words removal: ['done', 'handed', 'know', 'mega', 'shop', 'asda', 'counts', 'celebration', 'thats']\n",
      "After stemming with porters algorithm: ['done', 'han', 'know', 'mega', 'shop', 'asda', 'count', 'celebr', 'that']\n",
      "Tokenized sentence: ['sday', 'only', 'joined', 'so', 'training', 'we', 'started', 'today']\n",
      "After stop words removal: ['sday', 'joined', 'training', 'started', 'today']\n",
      "train\n",
      "After stemming with porters algorithm: ['sdai', 'join', 'train', 'star', 'todai']\n",
      "Tokenized sentence: ['small', 'problem', 'in', 'auction', 'punj', 'now', 'asking', 'tiwary']\n",
      "After stop words removal: ['small', 'problem', 'auction', 'punj', 'asking', 'tiwary']\n",
      "ask\n",
      "After stemming with porters algorithm: ['small', 'problem', 'auct', 'punj', 'as', 'tiwari']\n",
      "Tokenized sentence: ['dont', 'think', 'you', 'need', 'yellow', 'card', 'for', 'uk', 'travel', 'ask', 'someone', 'that', 'has', 'gone', 'before', 'if', 'you', 'do', 'its', 'just', 'lt', 'gt', 'bucks']\n",
      "After stop words removal: ['dont', 'think', 'need', 'yellow', 'card', 'uk', 'travel', 'ask', 'someone', 'gone', 'lt', 'gt', 'bucks']\n",
      "After stemming with porters algorithm: ['dont', 'think', 'need', 'yellow', 'card', 'travel', 'ask', 'someon', 'gone', 'buck']\n",
      "Tokenized sentence: ['ok', 'ok', 'ok', 'then', 'whats', 'ur', 'todays', 'plan']\n",
      "After stop words removal: ['ok', 'ok', 'ok', 'whats', 'ur', 'todays', 'plan']\n",
      "After stemming with porters algorithm: ['what', 'todai', 'plan']\n",
      "Tokenized sentence: ['wat', 'r', 'u', 'doing']\n",
      "After stop words removal: ['wat', 'r', 'u']\n",
      "After stemming with porters algorithm: ['wat']\n",
      "Tokenized sentence: ['hey', 'das', 'cool', 'iknow', 'all', 'wellda', 'peril', 'of', 'studentfinancial', 'crisis', 'spk', 'u', 'l', 'r']\n",
      "After stop words removal: ['hey', 'das', 'cool', 'iknow', 'wellda', 'peril', 'studentfinancial', 'crisis', 'spk', 'u', 'l', 'r']\n",
      "After stemming with porters algorithm: ['hei', 'da', 'cool', 'iknow', 'wellda', 'peril', 'studentfinanci', 'crisi', 'spk']\n",
      "Tokenized sentence: ['its', 'a', 'part', 'of', 'checking', 'iq']\n",
      "After stop words removal: ['part', 'checking', 'iq']\n",
      "check\n",
      "After stemming with porters algorithm: ['part', 'chec']\n",
      "Tokenized sentence: ['aight', 'ill', 'get', 'on', 'fb', 'in', 'a', 'couple', 'minutes']\n",
      "After stop words removal: ['aight', 'ill', 'get', 'fb', 'couple', 'minutes']\n",
      "After stemming with porters algorithm: ['aight', 'ill', 'get', 'coupl', 'minut']\n",
      "Tokenized sentence: ['hi', 'babe', 'its', 'me', 'thanks', 'for', 'coming', 'even', 'though', 'it', 'didnt', 'go', 'that', 'well', 'i', 'just', 'wanted', 'my', 'bed', 'hope', 'to', 'see', 'you', 'soon', 'love', 'and', 'kisses', 'xxx']\n",
      "After stop words removal: ['hi', 'babe', 'thanks', 'coming', 'even', 'though', 'didnt', 'go', 'well', 'wanted', 'bed', 'hope', 'see', 'soon', 'love', 'kisses', 'xxx']\n",
      "com\n",
      "After stemming with porters algorithm: ['babe', 'thank', 'come', 'even', 'though', 'didnt', 'well', 'wan', 'bed', 'hope', 'see', 'soon', 'love', 'kiss', 'xxx']\n",
      "Tokenized sentence: ['at', 'what', 'time', 'are', 'you', 'coming']\n",
      "After stop words removal: ['time', 'coming']\n",
      "com\n",
      "After stemming with porters algorithm: ['time', 'come']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'big', 'chic', 'common', 'declare']\n",
      "After stop words removal: ['big', 'chic', 'common', 'declare']\n",
      "After stemming with porters algorithm: ['big', 'chic', 'common', 'declar']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call']\n",
      "Tokenized sentence: ['she', 'was', 'supposed', 'to', 'be', 'but', 'couldn', 't', 'make', 'it', 'she', 's', 'still', 'in', 'town', 'though']\n",
      "After stop words removal: ['supposed', 'make', 'still', 'town', 'though']\n",
      "After stemming with porters algorithm: ['suppos', 'make', 'still', 'town', 'though']\n",
      "Tokenized sentence: ['aight', 'i', 'll', 'grab', 'something', 'to', 'eat', 'too', 'text', 'me', 'when', 'you', 're', 'back', 'at', 'mu']\n",
      "After stop words removal: ['aight', 'grab', 'something', 'eat', 'text', 'back', 'mu']\n",
      "someth\n",
      "After stemming with porters algorithm: ['aight', 'grab', 'somet', 'eat', 'text', 'back']\n",
      "Tokenized sentence: ['are', 'you', 'coming', 'to', 'day', 'for', 'class']\n",
      "After stop words removal: ['coming', 'day', 'class']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'dai', 'class']\n",
      "Tokenized sentence: ['hey', 'boys', 'want', 'hot', 'xxx', 'pics', 'sent', 'direct', 'ur', 'phone', 'txt', 'porn', 'to', 'hrs', 'free', 'and', 'then', 'just', 'p', 'per', 'day', 'to', 'stop', 'text', 'stopbcm', 'sf', 'wc', 'n', 'xx']\n",
      "After stop words removal: ['hey', 'boys', 'want', 'hot', 'xxx', 'pics', 'sent', 'direct', 'ur', 'phone', 'txt', 'porn', 'hrs', 'free', 'p', 'per', 'day', 'stop', 'text', 'stopbcm', 'sf', 'wc', 'n', 'xx']\n",
      "After stemming with porters algorithm: ['hei', 'boi', 'want', 'hot', 'xxx', 'pic', 'sent', 'direct', 'phone', 'txt', 'porn', 'hr', 'free', 'per', 'dai', 'stop', 'text', 'stopbcm']\n",
      "Tokenized sentence: ['if', 'you', 'mean', 'the', 'website', 'yes']\n",
      "After stop words removal: ['mean', 'website', 'yes']\n",
      "After stemming with porters algorithm: ['mean', 'websit', 'ye']\n",
      "Tokenized sentence: ['might', 'ax', 'well', 'im', 'there']\n",
      "After stop words removal: ['might', 'ax', 'well', 'im']\n",
      "After stemming with porters algorithm: ['might', 'well']\n",
      "Tokenized sentence: ['tddnewsletter', 'emc', 'co', 'uk', 'more', 'games', 'from', 'thedailydraw', 'dear', 'helen', 'dozens', 'of', 'free', 'games', 'with', 'great', 'prizeswith']\n",
      "After stop words removal: ['tddnewsletter', 'emc', 'co', 'uk', 'games', 'thedailydraw', 'dear', 'helen', 'dozens', 'free', 'games', 'great', 'prizeswith']\n",
      "After stemming with porters algorithm: ['tddnewslett', 'emc', 'game', 'thedailydraw', 'dear', 'helen', 'dozen', 'free', 'game', 'great', 'prizeswith']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'just', 'checking', 'up', 'on', 'you']\n",
      "After stop words removal: ['checking']\n",
      "check\n",
      "After stemming with porters algorithm: ['chec']\n",
      "Tokenized sentence: ['congrats', 'nokia', 'video', 'camera', 'phone', 'is', 'your', 'call', 'calls', 'cost', 'ppm', 'ave', 'call', 'mins', 'vary', 'from', 'mobiles', 'close', 'post', 'bcm', 'ldn', 'wc', 'n', 'xx']\n",
      "After stop words removal: ['congrats', 'nokia', 'video', 'camera', 'phone', 'call', 'calls', 'cost', 'ppm', 'ave', 'call', 'mins', 'vary', 'mobiles', 'close', 'post', 'bcm', 'ldn', 'wc', 'n', 'xx']\n",
      "After stemming with porters algorithm: ['congrat', 'nokia', 'video', 'camera', 'phone', 'call', 'call', 'cost', 'ppm', 'av', 'call', 'min', 'vari', 'mobil', 'close', 'post', 'bcm', 'ldn']\n",
      "Tokenized sentence: ['i', 'need', 'coz', 'i', 'never', 'go', 'before']\n",
      "After stop words removal: ['need', 'coz', 'never', 'go']\n",
      "After stemming with porters algorithm: ['need', 'coz', 'never']\n",
      "Tokenized sentence: ['still', 'in', 'customer', 'place']\n",
      "After stop words removal: ['still', 'customer', 'place']\n",
      "After stemming with porters algorithm: ['still', 'custom', 'place']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['otherwise', 'had', 'part', 'time', 'job', 'na', 'tuition']\n",
      "After stop words removal: ['otherwise', 'part', 'time', 'job', 'na', 'tuition']\n",
      "After stemming with porters algorithm: ['otherwis', 'part', 'time', 'job', 'tuit']\n",
      "Tokenized sentence: ['good', 'afternoon', 'my', 'love', 'how', 'goes', 'your', 'day', 'what', 'are', 'you', 'up', 'to', 'i', 'woke', 'early', 'and', 'am', 'online', 'waiting', 'for', 'you', 'hmmm', 'italian', 'boy', 'is', 'online', 'i', 'see', 'grins']\n",
      "After stop words removal: ['good', 'afternoon', 'love', 'goes', 'day', 'woke', 'early', 'online', 'waiting', 'hmmm', 'italian', 'boy', 'online', 'see', 'grins']\n",
      "wait\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'love', 'goe', 'dai', 'woke', 'earli', 'onlin', 'wait', 'hmmm', 'italian', 'boi', 'onlin', 'see', 'grin']\n",
      "Tokenized sentence: ['v', 'aluable', 'a', 'ffectionate', 'l', 'oveable', 'e', 'ternal', 'n', 'oble', 't', 'ruthful', 'i', 'ntimate', 'n', 'atural', 'e', 'namous', 'happy', 'valentines', 'day', 'in', 'advance']\n",
      "After stop words removal: ['v', 'aluable', 'ffectionate', 'l', 'oveable', 'e', 'ternal', 'n', 'oble', 'ruthful', 'ntimate', 'n', 'atural', 'e', 'namous', 'happy', 'valentines', 'day', 'advance']\n",
      "After stemming with porters algorithm: ['aluab', 'ffect', 'oveab', 'ternal', 'obl', 'ruth', 'ntimat', 'atur', 'namou', 'happi', 'valentin', 'dai', 'advanc']\n",
      "Tokenized sentence: ['how', 'much', 'u', 'trying', 'to', 'get']\n",
      "After stop words removal: ['much', 'u', 'trying', 'get']\n",
      "After stemming with porters algorithm: ['much', 'trying', 'get']\n",
      "Tokenized sentence: ['if', 'you', 'aren', 't', 'here', 'in', 'the', 'next', 'lt', 'gt', 'hours', 'imma', 'flip', 'my', 'shit']\n",
      "After stop words removal: ['next', 'lt', 'gt', 'hours', 'imma', 'flip', 'shit']\n",
      "After stemming with porters algorithm: ['next', 'hour', 'imma', 'flip', 'shit']\n",
      "Tokenized sentence: ['i', 'like', 'you', 'peoples', 'very', 'much', 'but', 'am', 'very', 'shy', 'pa']\n",
      "After stop words removal: ['like', 'peoples', 'much', 'shy', 'pa']\n",
      "After stemming with porters algorithm: ['like', 'peopl', 'much', 'shy']\n",
      "Tokenized sentence: ['so', 'how', 'are', 'you', 'really', 'what', 'are', 'you', 'up', 'to', 'how', 's', 'the', 'masters', 'and', 'so', 'on']\n",
      "After stop words removal: ['really', 'masters']\n",
      "After stemming with porters algorithm: ['realli', 'master']\n",
      "Tokenized sentence: ['i', 'can', 'do', 'that', 'i', 'want', 'to', 'please', 'you', 'both', 'inside', 'and', 'outside', 'the', 'bedroom']\n",
      "After stop words removal: ['want', 'please', 'inside', 'outside', 'bedroom']\n",
      "After stemming with porters algorithm: ['want', 'pleas', 'insid', 'outsid', 'bedroom']\n",
      "Tokenized sentence: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'it', 'seriously', 'coz', 'being', 'angry', 'is', 'd', 'most', 'childish', 'n', 'true', 'way', 'of', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'have', 'nice', 'day', 'da']\n",
      "After stop words removal: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'seriously', 'coz', 'angry', 'childish', 'n', 'true', 'way', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'nice', 'day', 'da']\n",
      "show\n",
      "After stemming with porters algorithm: ['wen', 'lovab', 'bcum', 'angri', 'wid', 'dnt', 'take', 'serious', 'coz', 'angri', 'childish', 'true', 'wai', 'showe', 'deep', 'affect', 'care', 'luv', 'kettoda', 'manda', 'nice', 'dai']\n",
      "Tokenized sentence: ['keep', 'yourself', 'safe', 'for', 'me', 'because', 'i', 'need', 'you', 'and', 'i', 'miss', 'you', 'already', 'and', 'i', 'envy', 'everyone', 'that', 'see', 's', 'you', 'in', 'real', 'life']\n",
      "After stop words removal: ['keep', 'safe', 'need', 'miss', 'already', 'envy', 'everyone', 'see', 'real', 'life']\n",
      "After stemming with porters algorithm: ['keep', 'safe', 'need', 'miss', 'alreadi', 'envi', 'everyon', 'see', 'real', 'life']\n",
      "Tokenized sentence: ['i', 'take', 'it', 'we', 'didn', 't', 'have', 'the', 'phone', 'callon', 'friday', 'can', 'we', 'assume', 'we', 'won', 't', 'have', 'it', 'this', 'year', 'now']\n",
      "After stop words removal: ['take', 'phone', 'callon', 'friday', 'assume', 'year']\n",
      "After stemming with porters algorithm: ['take', 'phone', 'callon', 'fridai', 'assum', 'year']\n",
      "Tokenized sentence: ['wonders', 'in', 'my', 'world', 'th', 'you', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'and', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "After stop words removal: ['wonders', 'world', 'th', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "morn\n",
      "After stemming with porters algorithm: ['wonder', 'world', 'style', 'smile', 'person', 'natur', 'sm', 'love', 'friendship', 'good', 'mor', 'dear']\n",
      "Tokenized sentence: ['sir', 'hope', 'your', 'day', 'is', 'going', 'smoothly', 'i', 'really', 'hoped', 'i', 'wont', 'have', 'to', 'bother', 'you', 'about', 'this', 'i', 'have', 'some', 'bills', 'that', 'i', 'can', 't', 'settle', 'this', 'month', 'i', 'am', 'out', 'of', 'all', 'extra', 'cash', 'i', 'know', 'this', 'is', 'a', 'challenging', 'time', 'for', 'you', 'also', 'but', 'i', 'have', 'to', 'let', 'you', 'know']\n",
      "After stop words removal: ['sir', 'hope', 'day', 'going', 'smoothly', 'really', 'hoped', 'wont', 'bother', 'bills', 'settle', 'month', 'extra', 'cash', 'know', 'challenging', 'time', 'also', 'let', 'know']\n",
      "go\n",
      "challeng\n",
      "After stemming with porters algorithm: ['sir', 'hope', 'dai', 'go', 'smoothli', 'realli', 'hope', 'wont', 'bother', 'bill', 'settl', 'month', 'extra', 'cash', 'know', 'challen', 'time', 'also', 'let', 'know']\n",
      "Tokenized sentence: ['look', 'at', 'the', 'fuckin', 'time', 'what', 'the', 'fuck', 'you', 'think', 'is', 'up']\n",
      "After stop words removal: ['look', 'fuckin', 'time', 'fuck', 'think']\n",
      "After stemming with porters algorithm: ['look', 'fuckin', 'time', 'fuck', 'think']\n",
      "Tokenized sentence: ['glad', 'it', 'went', 'well', 'come', 'over', 'at', 'then', 'we', 'll', 'have', 'plenty', 'of', 'time', 'before', 'claire', 'goes', 'to', 'work']\n",
      "After stop words removal: ['glad', 'went', 'well', 'come', 'plenty', 'time', 'claire', 'goes', 'work']\n",
      "After stemming with porters algorithm: ['glad', 'went', 'well', 'come', 'plenti', 'time', 'clair', 'goe', 'work']\n",
      "Tokenized sentence: ['is', 'ur', 'lecture', 'over']\n",
      "After stop words removal: ['ur', 'lecture']\n",
      "After stemming with porters algorithm: ['lectur']\n",
      "Tokenized sentence: ['dunno', 'i', 'juz', 'askin', 'cos', 'i', 'got', 'a', 'card', 'got', 'off', 'a', 'salon', 'called', 'hair', 'sense', 'so', 'i', 'tot', 'it', 's', 'da', 'one', 'cut', 'ur', 'hair']\n",
      "After stop words removal: ['dunno', 'juz', 'askin', 'cos', 'got', 'card', 'got', 'salon', 'called', 'hair', 'sense', 'tot', 'da', 'one', 'cut', 'ur', 'hair']\n",
      "After stemming with porters algorithm: ['dunno', 'juz', 'askin', 'co', 'got', 'card', 'got', 'salon', 'call', 'hair', 'sens', 'tot', 'on', 'cut', 'hair']\n",
      "Tokenized sentence: ['yun', 'buying', 'but', 'school', 'got', 'offer', 'plus', 'only']\n",
      "After stop words removal: ['yun', 'buying', 'school', 'got', 'offer', 'plus']\n",
      "buy\n",
      "After stemming with porters algorithm: ['yun', 'bui', 'school', 'got', 'offer', 'plu']\n",
      "Tokenized sentence: ['i', 'just', 'cooked', 'a', 'rather', 'nice', 'salmon', 'a', 'la', 'you']\n",
      "After stop words removal: ['cooked', 'rather', 'nice', 'salmon', 'la']\n",
      "After stemming with porters algorithm: ['cook', 'rather', 'nice', 'salmon']\n",
      "Tokenized sentence: ['you', 've', 'already', 'got', 'a', 'flaky', 'parent', 'it', 'snot', 'supposed', 'to', 'be', 'the', 'child', 's', 'job', 'to', 'support', 'the', 'parent', 'not', 'until', 'they', 're', 'the', 'ride', 'age', 'anyway', 'i', 'm', 'supposed', 'to', 'be', 'there', 'to', 'support', 'you', 'and', 'now', 'i', 've', 'hurt', 'you', 'unintentional', 'but', 'hurt', 'nonetheless']\n",
      "After stop words removal: ['already', 'got', 'flaky', 'parent', 'snot', 'supposed', 'child', 'job', 'support', 'parent', 'ride', 'age', 'anyway', 'supposed', 'support', 'hurt', 'unintentional', 'hurt', 'nonetheless']\n",
      "After stemming with porters algorithm: ['alreadi', 'got', 'flaki', 'parent', 'snot', 'suppos', 'child', 'job', 'support', 'parent', 'ride', 'ag', 'anywai', 'suppos', 'support', 'hurt', 'unintent', 'hurt', 'nonetheless']\n",
      "Tokenized sentence: ['how', 'tall', 'are', 'you', 'princess']\n",
      "After stop words removal: ['tall', 'princess']\n",
      "After stemming with porters algorithm: ['tall', 'princess']\n",
      "Tokenized sentence: ['thanx', 'yup', 'we', 'coming', 'back', 'on', 'sun', 'finish', 'dinner', 'going', 'back', 'hotel', 'now', 'time', 'flies', 'we', 're', 'tog', 'exactly', 'a', 'mth', 'today', 'hope', 'we', 'll', 'haf', 'many', 'more', 'mths', 'to', 'come']\n",
      "After stop words removal: ['thanx', 'yup', 'coming', 'back', 'sun', 'finish', 'dinner', 'going', 'back', 'hotel', 'time', 'flies', 'tog', 'exactly', 'mth', 'today', 'hope', 'haf', 'many', 'mths', 'come']\n",
      "com\n",
      "go\n",
      "After stemming with porters algorithm: ['thanx', 'yup', 'come', 'back', 'sun', 'finish', 'dinner', 'go', 'back', 'hotel', 'time', 'fli', 'tog', 'exactli', 'mth', 'todai', 'hope', 'haf', 'mani', 'mth', 'come']\n",
      "Tokenized sentence: ['should', 'i', 'buy', 'him', 'a', 'blackberry', 'bold', 'or', 'torch', 'should', 'i', 'buy', 'him', 'new', 'or', 'used', 'let', 'me', 'know', 'plus', 'are', 'you', 'saying', 'i', 'should', 'buy', 'the', 'lt', 'gt', 'g', 'wifi', 'ipad', 'and', 'what', 'are', 'you', 'saying', 'about', 'the', 'about', 'the', 'lt', 'gt', 'g']\n",
      "After stop words removal: ['buy', 'blackberry', 'bold', 'torch', 'buy', 'new', 'used', 'let', 'know', 'plus', 'saying', 'buy', 'lt', 'gt', 'g', 'wifi', 'ipad', 'saying', 'lt', 'gt', 'g']\n",
      "say\n",
      "say\n",
      "After stemming with porters algorithm: ['bui', 'blackberri', 'bold', 'torch', 'bui', 'new', 'us', 'let', 'know', 'plu', 'sai', 'bui', 'wifi', 'ipad', 'sai']\n",
      "Tokenized sentence: ['if', 'we', 'win', 'its', 'really', 'no', 'side', 'for', 'long', 'time']\n",
      "After stop words removal: ['win', 'really', 'side', 'long', 'time']\n",
      "After stemming with porters algorithm: ['win', 'realli', 'side', 'long', 'time']\n",
      "Tokenized sentence: ['boo', 'what', 'time', 'u', 'get', 'out', 'u', 'were', 'supposed', 'to', 'take', 'me', 'shopping', 'today']\n",
      "After stop words removal: ['boo', 'time', 'u', 'get', 'u', 'supposed', 'take', 'shopping', 'today']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['boo', 'time', 'get', 'suppos', 'take', 'shop', 'todai']\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 'bonus', 'points', 'to', 'claim', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'bonus', 'points', 'claim', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'unredeem', 'bonu', 'point', 'claim', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['my', 'birthday', 'is', 'on', 'feb', 'lt', 'gt', 'da']\n",
      "After stop words removal: ['birthday', 'feb', 'lt', 'gt', 'da']\n",
      "After stemming with porters algorithm: ['birthdai', 'feb']\n",
      "Tokenized sentence: ['do', 'you', 'know', 'when', 'dad', 'will', 'be', 'back']\n",
      "After stop words removal: ['know', 'dad', 'back']\n",
      "After stemming with porters algorithm: ['know', 'dad', 'back']\n",
      "Tokenized sentence: ['hi', 'its', 'lucy', 'hubby', 'at', 'meetins', 'all', 'day', 'fri', 'i', 'will', 'b', 'alone', 'at', 'hotel', 'u', 'fancy', 'cumin', 'over', 'pls', 'leave', 'msg', 'day', 'lucy', 'x', 'calls', 'minmobsmorelkpobox', 'hp', 'fl']\n",
      "After stop words removal: ['hi', 'lucy', 'hubby', 'meetins', 'day', 'fri', 'b', 'alone', 'hotel', 'u', 'fancy', 'cumin', 'pls', 'leave', 'msg', 'day', 'lucy', 'x', 'calls', 'minmobsmorelkpobox', 'hp', 'fl']\n",
      "After stemming with porters algorithm: ['luci', 'hubbi', 'meetin', 'dai', 'fri', 'alon', 'hotel', 'fanci', 'cumin', 'pl', 'leav', 'msg', 'dai', 'luci', 'call', 'minmobsmorelkpobox']\n",
      "Tokenized sentence: ['dont', 'worry', 'i', 'guess', 'he', 's', 'busy']\n",
      "After stop words removal: ['dont', 'worry', 'guess', 'busy']\n",
      "After stemming with porters algorithm: ['dont', 'worri', 'guess', 'busi']\n",
      "Tokenized sentence: ['aight', 'i', 'll', 'hit', 'you', 'up', 'when', 'i', 'get', 'some', 'cash']\n",
      "After stop words removal: ['aight', 'hit', 'get', 'cash']\n",
      "After stemming with porters algorithm: ['aight', 'hit', 'get', 'cash']\n",
      "Tokenized sentence: ['i', 'sent', 'lanre', 'fakeye', 's', 'eckankar', 'details', 'to', 'the', 'mail', 'box']\n",
      "After stop words removal: ['sent', 'lanre', 'fakeye', 'eckankar', 'details', 'mail', 'box']\n",
      "After stemming with porters algorithm: ['sent', 'lanr', 'fakey', 'eckankar', 'detail', 'mail', 'box']\n",
      "Tokenized sentence: ['had', 'your', 'contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'all', 'free', 'double', 'mins', 'text', 'on', 'orange', 'tariffs', 'text', 'yes', 'for', 'callback', 'no', 'to', 'remove', 'from', 'records']\n",
      "After stop words removal: ['contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'free', 'double', 'mins', 'text', 'orange', 'tariffs', 'text', 'yes', 'callback', 'remove', 'records']\n",
      "After stemming with porters algorithm: ['contract', 'mobil', 'mnth', 'latest', 'motorola', 'nokia', 'etc', 'free', 'doubl', 'min', 'text', 'orang', 'tariff', 'text', 'ye', 'callback', 'remov', 'record']\n",
      "Tokenized sentence: ['hot', 'live', 'fantasies', 'call', 'now', 'just', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'is', 'a', 'national', 'rate', 'call']\n",
      "After stop words removal: ['hot', 'live', 'fantasies', 'call', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'national', 'rate', 'call']\n",
      "After stemming with porters algorithm: ['hot', 'live', 'fantasi', 'call', 'per', 'min', 'ntt', 'ltd', 'box', 'croydon', 'nat', 'rate', 'call']\n",
      "Tokenized sentence: ['i', 'wasn', 't', 'well', 'babe', 'i', 'have', 'swollen', 'glands', 'at', 'my', 'throat', 'what', 'did', 'you', 'end', 'up', 'doing']\n",
      "After stop words removal: ['well', 'babe', 'swollen', 'glands', 'throat', 'end']\n",
      "After stemming with porters algorithm: ['well', 'babe', 'swollen', 'gland', 'throat', 'end']\n",
      "Tokenized sentence: ['thanks', 'for', 'being', 'there', 'for', 'me', 'just', 'to', 'talk', 'to', 'on', 'saturday', 'you', 'are', 'very', 'dear', 'to', 'me', 'i', 'cherish', 'having', 'you', 'as', 'a', 'brother', 'and', 'role', 'model']\n",
      "After stop words removal: ['thanks', 'talk', 'saturday', 'dear', 'cherish', 'brother', 'role', 'model']\n",
      "After stemming with porters algorithm: ['thank', 'talk', 'saturdai', 'dear', 'cherish', 'brother', 'role', 'model']\n",
      "Tokenized sentence: ['it', 'll', 'be', 'tough', 'but', 'i', 'll', 'do', 'what', 'i', 'have', 'to']\n",
      "After stop words removal: ['tough']\n",
      "After stemming with porters algorithm: ['tough']\n",
      "Tokenized sentence: ['i', 'm', 'working', 'technical', 'support', 'voice', 'process', 'networking', 'field']\n",
      "After stop words removal: ['working', 'technical', 'support', 'voice', 'process', 'networking', 'field']\n",
      "work\n",
      "network\n",
      "After stemming with porters algorithm: ['wor', 'technic', 'support', 'voic', 'process', 'networ', 'field']\n",
      "Tokenized sentence: ['good', 'good', 'billy', 'mates', 'all', 'gone', 'just', 'been', 'jogging', 'again', 'did', 'enjoy', 'concert']\n",
      "After stop words removal: ['good', 'good', 'billy', 'mates', 'gone', 'jogging', 'enjoy', 'concert']\n",
      "jogg\n",
      "After stemming with porters algorithm: ['good', 'good', 'billi', 'mate', 'gone', 'jog', 'enjoi', 'concert']\n",
      "Tokenized sentence: ['i', 'think', 'u', 'have', 'the', 'wrong', 'number']\n",
      "After stop words removal: ['think', 'u', 'wrong', 'number']\n",
      "After stemming with porters algorithm: ['think', 'wrong', 'number']\n",
      "Tokenized sentence: ['match', 'started', 'india', 'lt', 'gt', 'for']\n",
      "After stop words removal: ['match', 'started', 'india', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['match', 'star', 'india']\n",
      "Tokenized sentence: ['forgot', 'you', 'were', 'working', 'today', 'wanna', 'chat', 'but', 'things', 'are', 'ok', 'so', 'drop', 'me', 'a', 'text', 'when', 'you', 're', 'free', 'bored', 'etc', 'and', 'i', 'll', 'ring', 'hope', 'all', 'is', 'well', 'nose', 'essay', 'and', 'all', 'xx']\n",
      "After stop words removal: ['forgot', 'working', 'today', 'wanna', 'chat', 'things', 'ok', 'drop', 'text', 'free', 'bored', 'etc', 'ring', 'hope', 'well', 'nose', 'essay', 'xx']\n",
      "work\n",
      "After stemming with porters algorithm: ['forgot', 'wor', 'todai', 'wanna', 'chat', 'thing', 'drop', 'text', 'free', 'bore', 'etc', 'ring', 'hope', 'well', 'nose', 'essai']\n",
      "Tokenized sentence: ['short', 'but', 'cute', 'be', 'a', 'good', 'person']\n",
      "After stop words removal: ['short', 'cute', 'good', 'person']\n",
      "After stemming with porters algorithm: ['short', 'cute', 'good', 'person']\n",
      "Tokenized sentence: ['then', 'its', 'most', 'likely', 'called', 'mittelschmertz', 'google', 'it', 'if', 'you', 'dont', 'have', 'paracetamol', 'dont', 'worry', 'it', 'will', 'go']\n",
      "After stop words removal: ['likely', 'called', 'mittelschmertz', 'google', 'dont', 'paracetamol', 'dont', 'worry', 'go']\n",
      "After stemming with porters algorithm: ['like', 'call', 'mittelschmertz', 'googl', 'dont', 'paracetamol', 'dont', 'worri']\n",
      "Tokenized sentence: ['beautiful', 'truth', 'against', 'gravity', 'read', 'carefully', 'our', 'heart', 'feels', 'light', 'when', 'someone', 'is', 'in', 'it', 'but', 'it', 'feels', 'very', 'heavy', 'when', 'someone', 'leaves', 'it', 'good', 'night']\n",
      "After stop words removal: ['beautiful', 'truth', 'gravity', 'read', 'carefully', 'heart', 'feels', 'light', 'someone', 'feels', 'heavy', 'someone', 'leaves', 'good', 'night']\n",
      "After stemming with porters algorithm: ['beauti', 'truth', 'graviti', 'read', 'carefulli', 'heart', 'feel', 'light', 'someon', 'feel', 'heavi', 'someon', 'leav', 'good', 'night']\n",
      "Tokenized sentence: ['please', 'call', 'immediately', 'as', 'there', 'is', 'an', 'urgent', 'message', 'waiting', 'for', 'you']\n",
      "After stop words removal: ['please', 'call', 'immediately', 'urgent', 'message', 'waiting']\n",
      "wait\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'immedi', 'urgent', 'messag', 'wait']\n",
      "Tokenized sentence: ['double', 'mins', 'double', 'txt', 'price', 'linerental', 'on', 'latest', 'orange', 'bluetooth', 'mobiles', 'call', 'mobileupd', 'for', 'the', 'very', 'latest', 'offers', 'or', 'call', 'optout', 'lf']\n",
      "After stop words removal: ['double', 'mins', 'double', 'txt', 'price', 'linerental', 'latest', 'orange', 'bluetooth', 'mobiles', 'call', 'mobileupd', 'latest', 'offers', 'call', 'optout', 'lf']\n",
      "After stemming with porters algorithm: ['doubl', 'min', 'doubl', 'txt', 'price', 'liner', 'latest', 'orang', 'bluetooth', 'mobil', 'call', 'mobileupd', 'latest', 'offer', 'call', 'optout']\n",
      "Tokenized sentence: ['lt', 'gt', 'w', 'jetton', 'ave', 'if', 'you', 'forgot']\n",
      "After stop words removal: ['lt', 'gt', 'w', 'jetton', 'ave', 'forgot']\n",
      "After stemming with porters algorithm: ['jetton', 'av', 'forgot']\n",
      "Tokenized sentence: ['no', 'de', 'am', 'seeing', 'in', 'online', 'shop', 'so', 'that', 'i', 'asked']\n",
      "After stop words removal: ['de', 'seeing', 'online', 'shop', 'asked']\n",
      "see\n",
      "After stemming with porters algorithm: ['see', 'onlin', 'shop', 'as']\n",
      "Tokenized sentence: ['ok', 'can', 'be', 'later', 'showing', 'around', 'if', 'you', 'want', 'cld', 'have', 'drink', 'before', 'wld', 'prefer', 'not', 'to', 'spend', 'money', 'on', 'nosh', 'if', 'you', 'don', 't', 'mind', 'as', 'doing', 'that', 'nxt', 'wk']\n",
      "After stop words removal: ['ok', 'later', 'showing', 'around', 'want', 'cld', 'drink', 'wld', 'prefer', 'spend', 'money', 'nosh', 'mind', 'nxt', 'wk']\n",
      "show\n",
      "After stemming with porters algorithm: ['later', 'showe', 'around', 'want', 'cld', 'drink', 'wld', 'prefer', 'spend', 'monei', 'nosh', 'mind', 'nxt']\n",
      "Tokenized sentence: ['what', 'he', 'said', 'is', 'not', 'the', 'matter', 'my', 'mind', 'saying', 'some', 'other', 'matter', 'is', 'there']\n",
      "After stop words removal: ['said', 'matter', 'mind', 'saying', 'matter']\n",
      "say\n",
      "After stemming with porters algorithm: ['said', 'matter', 'mind', 'sai', 'matter']\n",
      "Tokenized sentence: ['convey', 'my', 'regards', 'to', 'him']\n",
      "After stop words removal: ['convey', 'regards']\n",
      "After stemming with porters algorithm: ['convei', 'regard']\n",
      "Tokenized sentence: ['someone', 'has', 'conacted', 'our', 'dating', 'service', 'and', 'entered', 'your', 'phone', 'because', 'they', 'fancy', 'you', 'to', 'find', 'out', 'who', 'it', 'is', 'call', 'from', 'landline', 'pobox', 'n', 'tf']\n",
      "After stop words removal: ['someone', 'conacted', 'dating', 'service', 'entered', 'phone', 'fancy', 'find', 'call', 'landline', 'pobox', 'n', 'tf']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someon', 'conac', 'date', 'servic', 'enter', 'phone', 'fanci', 'find', 'call', 'landlin', 'pobox']\n",
      "Tokenized sentence: ['you', 'dont', 'know', 'you', 'jabo', 'me', 'abi']\n",
      "After stop words removal: ['dont', 'know', 'jabo', 'abi']\n",
      "After stemming with porters algorithm: ['dont', 'know', 'jabo', 'abi']\n",
      "Tokenized sentence: ['as', 'usual', 'u', 'can', 'call', 'me', 'ard', 'smth']\n",
      "After stop words removal: ['usual', 'u', 'call', 'ard', 'smth']\n",
      "After stemming with porters algorithm: ['usual', 'call', 'ard', 'smth']\n",
      "Tokenized sentence: ['back', 'work', 'morro', 'half', 'term', 'over', 'can', 'u', 'c', 'me', 'nite', 'some', 'sexy', 'passion', 'b', 'i', 'have', 'go', 'back', 'chat', 'now', 'luv', 'dena', 'calls', 'minmobsmorelkpobox', 'hp', 'fl']\n",
      "After stop words removal: ['back', 'work', 'morro', 'half', 'term', 'u', 'c', 'nite', 'sexy', 'passion', 'b', 'go', 'back', 'chat', 'luv', 'dena', 'calls', 'minmobsmorelkpobox', 'hp', 'fl']\n",
      "After stemming with porters algorithm: ['back', 'work', 'morro', 'half', 'term', 'nite', 'sexi', 'passion', 'back', 'chat', 'luv', 'dena', 'call', 'minmobsmorelkpobox']\n",
      "Tokenized sentence: ['pandy', 'joined', 'w', 'technologies', 'today', 'he', 'got', 'job']\n",
      "After stop words removal: ['pandy', 'joined', 'w', 'technologies', 'today', 'got', 'job']\n",
      "After stemming with porters algorithm: ['pandi', 'join', 'technologi', 'todai', 'got', 'job']\n",
      "Tokenized sentence: ['uh', 'heads', 'up', 'we', 'don', 't', 'have', 'that', 'much', 'left']\n",
      "After stop words removal: ['uh', 'heads', 'much', 'left']\n",
      "After stemming with porters algorithm: ['head', 'much', 'left']\n",
      "Tokenized sentence: ['sexy', 'singles', 'are', 'waiting', 'for', 'you', 'text', 'your', 'age', 'followed', 'by', 'your', 'gender', 'as', 'wither', 'm', 'or', 'f', 'e', 'g', 'f', 'for', 'gay', 'men', 'text', 'your', 'age', 'followed', 'by', 'a', 'g', 'e', 'g', 'g']\n",
      "After stop words removal: ['sexy', 'singles', 'waiting', 'text', 'age', 'followed', 'gender', 'wither', 'f', 'e', 'g', 'f', 'gay', 'men', 'text', 'age', 'followed', 'g', 'e', 'g', 'g']\n",
      "wait\n",
      "After stemming with porters algorithm: ['sexi', 'singl', 'wait', 'text', 'ag', 'follow', 'gender', 'wither', 'gai', 'men', 'text', 'ag', 'follow']\n",
      "Tokenized sentence: ['am', 'also', 'doing', 'in', 'cbe', 'only', 'but', 'have', 'to', 'pay']\n",
      "After stop words removal: ['also', 'cbe', 'pay']\n",
      "After stemming with porters algorithm: ['also', 'cbe', 'pai']\n",
      "Tokenized sentence: ['what', 'should', 'i', 'eat', 'fo', 'lunch', 'senor']\n",
      "After stop words removal: ['eat', 'fo', 'lunch', 'senor']\n",
      "After stemming with porters algorithm: ['eat', 'lunch', 'senor']\n",
      "Tokenized sentence: ['there', 'are', 'no', 'other', 'charges', 'after', 'transfer', 'charges', 'and', 'you', 'can', 'withdraw', 'anyhow', 'you', 'like']\n",
      "After stop words removal: ['charges', 'transfer', 'charges', 'withdraw', 'anyhow', 'like']\n",
      "After stemming with porters algorithm: ['charg', 'transfer', 'charg', 'withdraw', 'anyhow', 'like']\n",
      "Tokenized sentence: ['what', 'are', 'your', 'new', 'years', 'plans']\n",
      "After stop words removal: ['new', 'years', 'plans']\n",
      "After stemming with porters algorithm: ['new', 'year', 'plan']\n",
      "Tokenized sentence: ['am', 'not', 'interested', 'to', 'do', 'like', 'that']\n",
      "After stop words removal: ['interested', 'like']\n",
      "After stemming with porters algorithm: ['interes', 'like']\n",
      "Tokenized sentence: ['ha', 'ha', 'had', 'popped', 'down', 'to', 'the', 'loo', 'when', 'you', 'hello', 'ed', 'me', 'hello']\n",
      "After stop words removal: ['ha', 'ha', 'popped', 'loo', 'hello', 'ed', 'hello']\n",
      "After stemming with porters algorithm: ['pop', 'loo', 'hello', 'hello']\n",
      "Tokenized sentence: ['i', 'shall', 'book', 'chez', 'jules', 'for', 'half', 'eight', 'if', 'that', 's', 'ok', 'with', 'you']\n",
      "After stop words removal: ['shall', 'book', 'chez', 'jules', 'half', 'eight', 'ok']\n",
      "After stemming with porters algorithm: ['shall', 'book', 'chez', 'jule', 'half', 'eight']\n",
      "Tokenized sentence: ['going', 'thru', 'a', 'very', 'different', 'feeling', 'wavering', 'decisions', 'and', 'coping', 'up', 'with', 'the', 'same', 'is', 'the', 'same', 'individual', 'time', 'will', 'heal', 'everything', 'i', 'believe']\n",
      "After stop words removal: ['going', 'thru', 'different', 'feeling', 'wavering', 'decisions', 'coping', 'individual', 'time', 'heal', 'everything', 'believe']\n",
      "go\n",
      "feel\n",
      "waver\n",
      "cop\n",
      "everyth\n",
      "After stemming with porters algorithm: ['go', 'thru', 'differ', 'feel', 'waver', 'decis', 'cope', 'individu', 'time', 'heal', 'everyt', 'believ']\n",
      "Tokenized sentence: ['you', 're', 'right', 'i', 'have', 'now', 'that', 'i', 'think', 'about', 'it']\n",
      "After stop words removal: ['right', 'think']\n",
      "After stemming with porters algorithm: ['right', 'think']\n",
      "Tokenized sentence: ['lol', 'where', 'do', 'u', 'come', 'up', 'with', 'these', 'ideas']\n",
      "After stop words removal: ['lol', 'u', 'come', 'ideas']\n",
      "After stemming with porters algorithm: ['lol', 'come', 'idea']\n",
      "Tokenized sentence: ['hi', 'my', 'love', 'how', 'goes', 'that', 'day', 'fuck', 'this', 'morning', 'i', 'woke', 'and', 'dropped', 'my', 'cell', 'on', 'the', 'way', 'down', 'the', 'stairs', 'but', 'it', 'seems', 'alright', 'phews', 'i', 'miss', 'you']\n",
      "After stop words removal: ['hi', 'love', 'goes', 'day', 'fuck', 'morning', 'woke', 'dropped', 'cell', 'way', 'stairs', 'seems', 'alright', 'phews', 'miss']\n",
      "morn\n",
      "After stemming with porters algorithm: ['love', 'goe', 'dai', 'fuck', 'mor', 'woke', 'drop', 'cell', 'wai', 'stair', 'seem', 'alright', 'phew', 'miss']\n",
      "Tokenized sentence: ['you', 'll', 'not', 'rcv', 'any', 'more', 'msgs', 'from', 'the', 'chat', 'svc', 'for', 'free', 'hardcore', 'services', 'text', 'go', 'to', 'if', 'u', 'get', 'nothing', 'u', 'must', 'age', 'verify', 'with', 'yr', 'network', 'try', 'again']\n",
      "After stop words removal: ['rcv', 'msgs', 'chat', 'svc', 'free', 'hardcore', 'services', 'text', 'go', 'u', 'get', 'nothing', 'u', 'must', 'age', 'verify', 'yr', 'network', 'try']\n",
      "noth\n",
      "After stemming with porters algorithm: ['rcv', 'msg', 'chat', 'svc', 'free', 'hardcor', 'servic', 'text', 'get', 'not', 'must', 'ag', 'verifi', 'network', 'try']\n",
      "Tokenized sentence: ['true', 'its', 'easier', 'with', 'her', 'here']\n",
      "After stop words removal: ['true', 'easier']\n",
      "After stemming with porters algorithm: ['true', 'easier']\n",
      "Tokenized sentence: ['back', 'in', 'brum', 'thanks', 'for', 'putting', 'us', 'up', 'and', 'keeping', 'us', 'all', 'and', 'happy', 'see', 'you', 'soon']\n",
      "After stop words removal: ['back', 'brum', 'thanks', 'putting', 'us', 'keeping', 'us', 'happy', 'see', 'soon']\n",
      "putt\n",
      "keep\n",
      "After stemming with porters algorithm: ['back', 'brum', 'thank', 'put', 'keep', 'happi', 'see', 'soon']\n",
      "Tokenized sentence: ['feel', 'like', 'trying', 'kadeem', 'again', 'v']\n",
      "After stop words removal: ['feel', 'like', 'trying', 'kadeem', 'v']\n",
      "After stemming with porters algorithm: ['feel', 'like', 'trying', 'kadeem']\n",
      "Tokenized sentence: ['i', 'am', 'not', 'having', 'her', 'number', 'sir']\n",
      "After stop words removal: ['number', 'sir']\n",
      "After stemming with porters algorithm: ['number', 'sir']\n",
      "Tokenized sentence: ['congrats', 'mobile', 'g', 'videophones', 'r', 'yours', 'call', 'now', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyh', 'music', 'noline', 'rentl', 'bx', 'ip', 'we', 'pm']\n",
      "After stop words removal: ['congrats', 'mobile', 'g', 'videophones', 'r', 'call', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyh', 'music', 'noline', 'rentl', 'bx', 'ip', 'pm']\n",
      "After stemming with porters algorithm: ['congrat', 'mobil', 'videophon', 'call', 'videochat', 'wid', 'mate', 'plai', 'java', 'game', 'dload', 'polyh', 'music', 'nolin', 'rentl']\n",
      "Tokenized sentence: ['ok', 'i', 'shall', 'talk', 'to', 'him']\n",
      "After stop words removal: ['ok', 'shall', 'talk']\n",
      "After stemming with porters algorithm: ['shall', 'talk']\n",
      "Tokenized sentence: ['i', 'll', 'talk', 'to', 'the', 'others', 'and', 'probably', 'just', 'come', 'early', 'tomorrow', 'then']\n",
      "After stop words removal: ['talk', 'others', 'probably', 'come', 'early', 'tomorrow']\n",
      "After stemming with porters algorithm: ['talk', 'other', 'probab', 'come', 'earli', 'tomorrow']\n",
      "Tokenized sentence: ['congratulations', 'thanks', 'to', 'a', 'good', 'friend', 'u', 'have', 'won', 'the', 'xmas', 'prize', 'claim', 'is', 'easy', 'just', 'call', 'now', 'only', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['congratulations', 'thanks', 'good', 'friend', 'u', 'xmas', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['congratul', 'thank', 'good', 'friend', 'xma', 'priz', 'claim', 'easi', 'call', 'per', 'minut', 'nat', 'rate']\n",
      "Tokenized sentence: ['no', 'i', 'll', 'meet', 'you', 'in', 'the', 'library']\n",
      "After stop words removal: ['meet', 'library']\n",
      "After stemming with porters algorithm: ['meet', 'librari']\n",
      "Tokenized sentence: ['rose', 'needs', 'water', 'season', 'needs', 'change', 'poet', 'needs', 'imagination', 'my', 'phone', 'needs', 'ur', 'sms', 'and', 'i', 'need', 'ur', 'lovely', 'frndship', 'forever']\n",
      "After stop words removal: ['rose', 'needs', 'water', 'season', 'needs', 'change', 'poet', 'needs', 'imagination', 'phone', 'needs', 'ur', 'sms', 'need', 'ur', 'lovely', 'frndship', 'forever']\n",
      "After stemming with porters algorithm: ['rose', 'need', 'water', 'season', 'need', 'chang', 'poet', 'need', 'imagin', 'phone', 'need', 'sm', 'need', 'love', 'frndship', 'forev']\n",
      "Tokenized sentence: ['sms', 'services', 'for', 'your', 'inclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'with', 'stop', 'no', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stop words removal: ['sms', 'services', 'inclusive', 'text', 'credits', 'pls', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscribe', 'stop', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
      "After stemming with porters algorithm: ['sm', 'servic', 'inclus', 'text', 'credit', 'pl', 'goto', 'www', 'comuk', 'net', 'login', 'qxj', 'unsubscrib', 'stop', 'extra', 'charg', 'help', 'comuk']\n",
      "Tokenized sentence: ['sitting', 'ard', 'nothing', 'to', 'do', 'lor', 'u', 'leh', 'busy', 'w', 'work']\n",
      "After stop words removal: ['sitting', 'ard', 'nothing', 'lor', 'u', 'leh', 'busy', 'w', 'work']\n",
      "sitt\n",
      "noth\n",
      "After stemming with porters algorithm: ['sit', 'ard', 'not', 'lor', 'leh', 'busi', 'work']\n",
      "Tokenized sentence: ['let', 'me', 'know', 'how', 'it', 'changes', 'in', 'the', 'next', 'hrs', 'it', 'can', 'even', 'be', 'appendix', 'but', 'you', 'are', 'out', 'of', 'that', 'age', 'range', 'however', 'its', 'not', 'impossible', 'so', 'just', 'chill', 'and', 'let', 'me', 'know', 'in', 'hrs']\n",
      "After stop words removal: ['let', 'know', 'changes', 'next', 'hrs', 'even', 'appendix', 'age', 'range', 'however', 'impossible', 'chill', 'let', 'know', 'hrs']\n",
      "After stemming with porters algorithm: ['let', 'know', 'chang', 'next', 'hr', 'even', 'appendix', 'ag', 'rang', 'howev', 'imposs', 'chill', 'let', 'know', 'hr']\n",
      "Tokenized sentence: ['hi', 'its', 'kate', 'it', 'was', 'lovely', 'to', 'see', 'you', 'tonight', 'and', 'ill', 'phone', 'you', 'tomorrow', 'i', 'got', 'to', 'sing', 'and', 'a', 'guy', 'gave', 'me', 'his', 'card', 'xxx']\n",
      "After stop words removal: ['hi', 'kate', 'lovely', 'see', 'tonight', 'ill', 'phone', 'tomorrow', 'got', 'sing', 'guy', 'gave', 'card', 'xxx']\n",
      "After stemming with porters algorithm: ['kate', 'love', 'see', 'tonight', 'ill', 'phone', 'tomorrow', 'got', 'sing', 'gui', 'gave', 'card', 'xxx']\n",
      "Tokenized sentence: ['hello', 'hello', 'hi', 'lou', 'sorry', 'it', 'took', 'so', 'long', 'reply', 'i', 'left', 'mobile', 'at', 'friends', 'in', 'lancaster', 'just', 'got', 'it', 'bak', 'neway', 'im', 'sorry', 'i', 'couldn', 't', 'make', 'ur', 'b', 'day', 'hun']\n",
      "After stop words removal: ['hello', 'hello', 'hi', 'lou', 'sorry', 'took', 'long', 'reply', 'left', 'mobile', 'friends', 'lancaster', 'got', 'bak', 'neway', 'im', 'sorry', 'make', 'ur', 'b', 'day', 'hun']\n",
      "After stemming with porters algorithm: ['hello', 'hello', 'lou', 'sorri', 'took', 'long', 'repli', 'left', 'mobil', 'friend', 'lancast', 'got', 'bak', 'newai', 'sorri', 'make', 'dai', 'hun']\n",
      "Tokenized sentence: ['your', 'gonna', 'have', 'to', 'pick', 'up', 'a', 'burger', 'for', 'yourself', 'on', 'your', 'way', 'home', 'i', 'can', 't', 'even', 'move', 'pain', 'is', 'killing', 'me']\n",
      "After stop words removal: ['gonna', 'pick', 'burger', 'way', 'home', 'even', 'move', 'pain', 'killing']\n",
      "kill\n",
      "After stemming with porters algorithm: ['gonna', 'pick', 'burger', 'wai', 'home', 'even', 'move', 'pain', 'kill']\n",
      "Tokenized sentence: ['meeting', 'u', 'is', 'my', 'work', 'tel', 'me', 'when', 'shall', 'i', 'do', 'my', 'work', 'tomorrow']\n",
      "After stop words removal: ['meeting', 'u', 'work', 'tel', 'shall', 'work', 'tomorrow']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'work', 'tel', 'shall', 'work', 'tomorrow']\n",
      "Tokenized sentence: ['dude', 'sux', 'for', 'snake', 'he', 'got', 'old', 'and', 'raiden', 'got', 'buff']\n",
      "After stop words removal: ['dude', 'sux', 'snake', 'got', 'old', 'raiden', 'got', 'buff']\n",
      "After stemming with porters algorithm: ['dude', 'sux', 'snake', 'got', 'old', 'raiden', 'got', 'buff']\n",
      "Tokenized sentence: ['dunno', 'my', 'dad', 'said', 'he', 'coming', 'home', 'bring', 'us', 'out', 'lunch', 'yup', 'i', 'go', 'w', 'u', 'lor', 'i', 'call', 'u', 'when', 'i', 'reach', 'school', 'lor']\n",
      "After stop words removal: ['dunno', 'dad', 'said', 'coming', 'home', 'bring', 'us', 'lunch', 'yup', 'go', 'w', 'u', 'lor', 'call', 'u', 'reach', 'school', 'lor']\n",
      "com\n",
      "After stemming with porters algorithm: ['dunno', 'dad', 'said', 'come', 'home', 'bring', 'lunch', 'yup', 'lor', 'call', 'reach', 'school', 'lor']\n",
      "Tokenized sentence: ['yes', 'fine']\n",
      "After stop words removal: ['yes', 'fine']\n",
      "After stemming with porters algorithm: ['ye', 'fine']\n",
      "Tokenized sentence: ['sry', 'can', 't', 'talk', 'on', 'phone', 'with', 'parents']\n",
      "After stop words removal: ['sry', 'talk', 'phone', 'parents']\n",
      "After stemming with porters algorithm: ['sry', 'talk', 'phone', 'parent']\n",
      "Tokenized sentence: ['nvm', 'take', 'ur', 'time']\n",
      "After stop words removal: ['nvm', 'take', 'ur', 'time']\n",
      "After stemming with porters algorithm: ['nvm', 'take', 'time']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['thanx', 'but', 'my', 'birthday', 'is', 'over', 'already']\n",
      "After stop words removal: ['thanx', 'birthday', 'already']\n",
      "After stemming with porters algorithm: ['thanx', 'birthdai', 'alreadi']\n",
      "Tokenized sentence: ['i', 'know', 'i', 'm', 'lacking', 'on', 'most', 'of', 'this', 'particular', 'dramastorm', 's', 'details', 'but', 'for', 'the', 'most', 'part', 'i', 'm', 'not', 'worried', 'about', 'that']\n",
      "After stop words removal: ['know', 'lacking', 'particular', 'dramastorm', 'details', 'part', 'worried']\n",
      "lack\n",
      "After stemming with porters algorithm: ['know', 'lac', 'particular', 'dramastorm', 'detail', 'part', 'worri']\n",
      "Tokenized sentence: ['i', 'm', 'hungry', 'buy', 'smth', 'home']\n",
      "After stop words removal: ['hungry', 'buy', 'smth', 'home']\n",
      "After stemming with porters algorithm: ['hungri', 'bui', 'smth', 'home']\n",
      "Tokenized sentence: ['dunno', 'da', 'next', 'show', 'aft', 'is', 'toa', 'payoh', 'got']\n",
      "After stop words removal: ['dunno', 'da', 'next', 'show', 'aft', 'toa', 'payoh', 'got']\n",
      "After stemming with porters algorithm: ['dunno', 'next', 'show', 'aft', 'toa', 'payoh', 'got']\n",
      "Tokenized sentence: ['kit', 'strip', 'you', 'have', 'been', 'billed', 'p', 'netcollex', 'ltd', 'po', 'box', 'ig', 'oja']\n",
      "After stop words removal: ['kit', 'strip', 'billed', 'p', 'netcollex', 'ltd', 'po', 'box', 'ig', 'oja']\n",
      "After stemming with porters algorithm: ['kit', 'strip', 'bill', 'netcollex', 'ltd', 'box', 'oja']\n",
      "Tokenized sentence: ['i', 'm', 'home']\n",
      "After stop words removal: ['home']\n",
      "After stemming with porters algorithm: ['home']\n",
      "Tokenized sentence: ['ok', 'lor', 'but', 'buy', 'wat']\n",
      "After stop words removal: ['ok', 'lor', 'buy', 'wat']\n",
      "After stemming with porters algorithm: ['lor', 'bui', 'wat']\n",
      "Tokenized sentence: ['mmm', 'fuck', 'merry', 'christmas', 'to', 'me']\n",
      "After stop words removal: ['mmm', 'fuck', 'merry', 'christmas']\n",
      "After stemming with porters algorithm: ['mmm', 'fuck', 'merri', 'christma']\n",
      "Tokenized sentence: ['money', 'you', 'r', 'a', 'lucky', 'winner', 'claim', 'your', 'prize', 'text', 'money', 'over', 'million', 'to', 'give', 'away', 'ppt', 'x', 'normal', 'text', 'rate', 'box', 'w', 't', 'jy']\n",
      "After stop words removal: ['money', 'r', 'lucky', 'winner', 'claim', 'prize', 'text', 'money', 'million', 'give', 'away', 'ppt', 'x', 'normal', 'text', 'rate', 'box', 'w', 'jy']\n",
      "After stemming with porters algorithm: ['monei', 'lucki', 'winner', 'claim', 'priz', 'text', 'monei', 'million', 'give', 'awai', 'ppt', 'normal', 'text', 'rate', 'box']\n",
      "Tokenized sentence: ['sorry', 'sir', 'i', 'will', 'call', 'you', 'tomorrow', 'senthil', 'hsbc']\n",
      "After stop words removal: ['sorry', 'sir', 'call', 'tomorrow', 'senthil', 'hsbc']\n",
      "After stemming with porters algorithm: ['sorri', 'sir', 'call', 'tomorrow', 'senthil', 'hsbc']\n",
      "Tokenized sentence: ['are', 'you', 'available', 'for', 'soiree', 'on', 'june', 'rd']\n",
      "After stop words removal: ['available', 'soiree', 'june', 'rd']\n",
      "After stemming with porters algorithm: ['avail', 'soire', 'june']\n",
      "Tokenized sentence: ['urgent', 'please', 'call', 'from', 'landline', 'cash', 'or', 'a', 'luxury', 'canary', 'islands', 'holiday', 'await', 'collection', 't', 'cs', 'sae', 'award', 'm', 'aq', 'ppm']\n",
      "After stop words removal: ['urgent', 'please', 'call', 'landline', 'cash', 'luxury', 'canary', 'islands', 'holiday', 'await', 'collection', 'cs', 'sae', 'award', 'aq', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'pleas', 'call', 'landlin', 'cash', 'luxuri', 'canari', 'island', 'holidai', 'await', 'collect', 'sae', 'award', 'ppm']\n",
      "Tokenized sentence: ['dont', 'you', 'have', 'message', 'offer']\n",
      "After stop words removal: ['dont', 'message', 'offer']\n",
      "After stemming with porters algorithm: ['dont', 'messag', 'offer']\n",
      "Tokenized sentence: ['i', 'll', 'probably', 'be', 'around', 'mu', 'a', 'lot']\n",
      "After stop words removal: ['probably', 'around', 'mu', 'lot']\n",
      "After stemming with porters algorithm: ['probab', 'around', 'lot']\n",
      "Tokenized sentence: ['u', 'are', 'subscribed', 'to', 'the', 'best', 'mobile', 'content', 'service', 'in', 'the', 'uk', 'for', 'per', 'days', 'until', 'you', 'send', 'stop', 'to', 'helpline']\n",
      "After stop words removal: ['u', 'subscribed', 'best', 'mobile', 'content', 'service', 'uk', 'per', 'days', 'send', 'stop', 'helpline']\n",
      "After stemming with porters algorithm: ['subscrib', 'best', 'mobil', 'content', 'servic', 'per', 'dai', 'send', 'stop', 'helplin']\n",
      "Tokenized sentence: ['waiting', 'for', 'your', 'call']\n",
      "After stop words removal: ['waiting', 'call']\n",
      "wait\n",
      "After stemming with porters algorithm: ['wait', 'call']\n",
      "Tokenized sentence: ['er']\n",
      "After stop words removal: ['er']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['fyi', 'i', 'm', 'taking', 'a', 'quick', 'shower', 'be', 'at', 'epsilon', 'in', 'like', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['fyi', 'taking', 'quick', 'shower', 'epsilon', 'like', 'lt', 'gt', 'min']\n",
      "tak\n",
      "After stemming with porters algorithm: ['fyi', 'take', 'quick', 'shower', 'epsilon', 'like', 'min']\n",
      "Tokenized sentence: ['go', 'to', 'write', 'msg', 'put', 'on', 'dictionary', 'mode', 'cover', 'the', 'screen', 'with', 'hand', 'press', 'lt', 'gt', 'gently', 'remove', 'ur', 'hand', 'its', 'interesting']\n",
      "After stop words removal: ['go', 'write', 'msg', 'put', 'dictionary', 'mode', 'cover', 'screen', 'hand', 'press', 'lt', 'gt', 'gently', 'remove', 'ur', 'hand', 'interesting']\n",
      "interest\n",
      "After stemming with porters algorithm: ['write', 'msg', 'put', 'dictionari', 'mode', 'cover', 'screen', 'hand', 'press', 'gentli', 'remov', 'hand', 'interes']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'escape', 'theatre', 'now', 'going', 'to', 'watch', 'kavalan', 'in', 'a', 'few', 'minutes']\n",
      "After stop words removal: ['escape', 'theatre', 'going', 'watch', 'kavalan', 'minutes']\n",
      "go\n",
      "After stemming with porters algorithm: ['escap', 'theatr', 'go', 'watch', 'kavalan', 'minut']\n",
      "Tokenized sentence: ['should', 'i', 'tell', 'my', 'friend', 'not', 'to', 'come', 'round', 'til', 'like', 'lt', 'gt', 'ish']\n",
      "After stop words removal: ['tell', 'friend', 'come', 'round', 'til', 'like', 'lt', 'gt', 'ish']\n",
      "After stemming with porters algorithm: ['tell', 'friend', 'come', 'round', 'til', 'like', 'ish']\n",
      "Tokenized sentence: ['u', 'sure', 'u', 'can', 't', 'take', 'any', 'sick', 'time']\n",
      "After stop words removal: ['u', 'sure', 'u', 'take', 'sick', 'time']\n",
      "After stemming with porters algorithm: ['sure', 'take', 'sick', 'time']\n",
      "Tokenized sentence: ['yeah', 'i', 'am', 'so', 'i', 'll', 'leave', 'maybe', 'ish']\n",
      "After stop words removal: ['yeah', 'leave', 'maybe', 'ish']\n",
      "After stemming with porters algorithm: ['yeah', 'leav', 'mayb', 'ish']\n",
      "Tokenized sentence: ['eerie', 'nokia', 'tones', 'u', 'rply', 'tone', 'title', 'to', 'eg', 'tone', 'dracula', 'to', 'titles', 'ghost', 'addamsfa', 'munsters', 'exorcist', 'twilight', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'p']\n",
      "After stop words removal: ['eerie', 'nokia', 'tones', 'u', 'rply', 'tone', 'title', 'eg', 'tone', 'dracula', 'titles', 'ghost', 'addamsfa', 'munsters', 'exorcist', 'twilight', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'p']\n",
      "After stemming with porters algorithm: ['eeri', 'nokia', 'tone', 'rply', 'tone', 'titl', 'tone', 'dracula', 'titl', 'ghost', 'addamsfa', 'munster', 'exorcist', 'twilight', 'www', 'getz', 'pobox']\n",
      "Tokenized sentence: ['this', 'is', 'ur', 'face', 'test', 'lt', 'gt', 'select', 'any', 'number', 'i', 'will', 'tell', 'ur', 'face', 'astrology', 'am', 'waiting', 'quick', 'reply']\n",
      "After stop words removal: ['ur', 'face', 'test', 'lt', 'gt', 'select', 'number', 'tell', 'ur', 'face', 'astrology', 'waiting', 'quick', 'reply']\n",
      "wait\n",
      "After stemming with porters algorithm: ['face', 'test', 'select', 'number', 'tell', 'face', 'astrologi', 'wait', 'quick', 'repli']\n",
      "Tokenized sentence: ['goodmorning', 'today', 'i', 'am', 'late', 'for', 'hrs', 'because', 'of', 'back', 'pain']\n",
      "After stop words removal: ['goodmorning', 'today', 'late', 'hrs', 'back', 'pain']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['goodmor', 'todai', 'late', 'hr', 'back', 'pain']\n",
      "Tokenized sentence: ['please', 'protect', 'yourself', 'from', 'e', 'threats', 'sib', 'never', 'asks', 'for', 'sensitive', 'information', 'like', 'passwords', 'atm', 'sms', 'pin', 'thru', 'email', 'never', 'share', 'your', 'password', 'with', 'anybody']\n",
      "After stop words removal: ['please', 'protect', 'e', 'threats', 'sib', 'never', 'asks', 'sensitive', 'information', 'like', 'passwords', 'atm', 'sms', 'pin', 'thru', 'email', 'never', 'share', 'password', 'anybody']\n",
      "After stemming with porters algorithm: ['pleas', 'protect', 'threat', 'sib', 'never', 'ask', 'sensit', 'inform', 'like', 'password', 'atm', 'sm', 'pin', 'thru', 'email', 'never', 'share', 'password', 'anybodi']\n",
      "Tokenized sentence: ['its', 'ok', 'called', 'mom', 'instead', 'have', 'fun']\n",
      "After stop words removal: ['ok', 'called', 'mom', 'instead', 'fun']\n",
      "After stemming with porters algorithm: ['call', 'mom', 'instead', 'fun']\n",
      "Tokenized sentence: ['life', 'has', 'never', 'been', 'this', 'much', 'fun', 'and', 'great', 'until', 'you', 'came', 'in', 'you', 'made', 'it', 'truly', 'special', 'for', 'me', 'i', 'won', 't', 'forget', 'you', 'enjoy', 'one', 'gbp', 'sms']\n",
      "After stop words removal: ['life', 'never', 'much', 'fun', 'great', 'came', 'made', 'truly', 'special', 'forget', 'enjoy', 'one', 'gbp', 'sms']\n",
      "After stemming with porters algorithm: ['life', 'never', 'much', 'fun', 'great', 'came', 'made', 'truli', 'special', 'forget', 'enjoi', 'on', 'gbp', 'sm']\n",
      "Tokenized sentence: ['yep', 'at', 'derek', 's', 'house', 'now', 'see', 'you', 'sunday', 'lt']\n",
      "After stop words removal: ['yep', 'derek', 'house', 'see', 'sunday', 'lt']\n",
      "After stemming with porters algorithm: ['yep', 'derek', 'hous', 'see', 'sundai']\n",
      "Tokenized sentence: ['tbs', 'persolvo', 'been', 'chasing', 'us', 'since', 'sept', 'for', 'definitely', 'not', 'paying', 'now', 'thanks', 'to', 'your', 'information', 'we', 'will', 'ignore', 'them', 'kath', 'manchester']\n",
      "After stop words removal: ['tbs', 'persolvo', 'chasing', 'us', 'since', 'sept', 'definitely', 'paying', 'thanks', 'information', 'ignore', 'kath', 'manchester']\n",
      "chas\n",
      "pay\n",
      "After stemming with porters algorithm: ['tb', 'persolvo', 'chase', 'sinc', 'sept', 'definit', 'pai', 'thank', 'inform', 'ignor', 'kath', 'manchest']\n",
      "Tokenized sentence: ['are', 'you', 'being', 'good', 'baby']\n",
      "After stop words removal: ['good', 'baby']\n",
      "After stemming with porters algorithm: ['good', 'babi']\n",
      "Tokenized sentence: ['ok', 'i', 'am', 'on', 'the', 'way', 'to', 'railway']\n",
      "After stop words removal: ['ok', 'way', 'railway']\n",
      "After stemming with porters algorithm: ['wai', 'railwai']\n",
      "Tokenized sentence: ['r', 'we', 'still', 'meeting', 'dinner', 'tonight']\n",
      "After stop words removal: ['r', 'still', 'meeting', 'dinner', 'tonight']\n",
      "meet\n",
      "After stemming with porters algorithm: ['still', 'meet', 'dinner', 'tonight']\n",
      "Tokenized sentence: ['ok', 'thats', 'cool', 'its', 'just', 'off', 'either', 'raglan', 'rd', 'or', 'edward', 'rd', 'behind', 'the', 'cricket', 'ground', 'gimme', 'ring', 'when', 'ur', 'closeby', 'see', 'you', 'tuesday']\n",
      "After stop words removal: ['ok', 'thats', 'cool', 'either', 'raglan', 'rd', 'edward', 'rd', 'behind', 'cricket', 'ground', 'gimme', 'ring', 'ur', 'closeby', 'see', 'tuesday']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'either', 'raglan', 'edward', 'behind', 'cricket', 'ground', 'gimm', 'ring', 'closebi', 'see', 'tuesdai']\n",
      "Tokenized sentence: ['stupid', 'auto', 'correct', 'on', 'my', 'phone']\n",
      "After stop words removal: ['stupid', 'auto', 'correct', 'phone']\n",
      "After stemming with porters algorithm: ['stupid', 'auto', 'correct', 'phone']\n",
      "Tokenized sentence: ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'freephone', 'between', 'am', 'pm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
      "After stop words removal: ['please', 'call', 'customer', 'service', 'representative', 'freephone', 'pm', 'guaranteed', 'cash', 'prize']\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'custom', 'servic', 'repres', 'freephon', 'guaranteed', 'cash', 'priz']\n",
      "Tokenized sentence: ['is', 'your', 'hamster', 'dead', 'hey', 'so', 'tmr', 'i', 'meet', 'you', 'at', 'pm', 'orchard', 'mrt']\n",
      "After stop words removal: ['hamster', 'dead', 'hey', 'tmr', 'meet', 'pm', 'orchard', 'mrt']\n",
      "After stemming with porters algorithm: ['hamster', 'dead', 'hei', 'tmr', 'meet', 'orchard', 'mrt']\n",
      "Tokenized sentence: ['oh', 'i', 'was', 'thkin', 'of', 'goin', 'yogasana', 'at', 'den', 'no', 'nd', 'to', 'go', 'at', 'den', 'can', 'rush', 'to', 'parco', 'nb', 'okie', 'lor', 'u', 'call', 'me', 'when', 'ready']\n",
      "After stop words removal: ['oh', 'thkin', 'goin', 'yogasana', 'den', 'nd', 'go', 'den', 'rush', 'parco', 'nb', 'okie', 'lor', 'u', 'call', 'ready']\n",
      "After stemming with porters algorithm: ['thkin', 'goin', 'yogasana', 'den', 'den', 'rush', 'parco', 'oki', 'lor', 'call', 'readi']\n",
      "Tokenized sentence: ['if', 'i', 'said', 'anything', 'wrong', 'sorry', 'de']\n",
      "After stop words removal: ['said', 'anything', 'wrong', 'sorry', 'de']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['said', 'anyt', 'wrong', 'sorri']\n",
      "Tokenized sentence: ['that', 'was', 'random', 'saw', 'my', 'old', 'roomate', 'on', 'campus', 'he', 'graduated']\n",
      "After stop words removal: ['random', 'saw', 'old', 'roomate', 'campus', 'graduated']\n",
      "graduate\n",
      "After stemming with porters algorithm: ['random', 'saw', 'old', 'roomat', 'campu', 'graduat']\n",
      "Tokenized sentence: ['maybe', 'westshore', 'or', 'hyde', 'park', 'village', 'the', 'place', 'near', 'my', 'house']\n",
      "After stop words removal: ['maybe', 'westshore', 'hyde', 'park', 'village', 'place', 'near', 'house']\n",
      "After stemming with porters algorithm: ['mayb', 'westshor', 'hyde', 'park', 'villag', 'place', 'near', 'hous']\n",
      "Tokenized sentence: ['let', 'ur', 'heart', 'be', 'ur', 'compass', 'ur', 'mind', 'ur', 'map', 'ur', 'soul', 'ur', 'guide', 'and', 'u', 'will', 'never', 'loose', 'in', 'world', 'gnun', 'sent', 'via', 'way', 'sms', 'com']\n",
      "After stop words removal: ['let', 'ur', 'heart', 'ur', 'compass', 'ur', 'mind', 'ur', 'map', 'ur', 'soul', 'ur', 'guide', 'u', 'never', 'loose', 'world', 'gnun', 'sent', 'via', 'way', 'sms', 'com']\n",
      "After stemming with porters algorithm: ['let', 'heart', 'compass', 'mind', 'map', 'soul', 'guid', 'never', 'loos', 'world', 'gnun', 'sent', 'via', 'wai', 'sm', 'com']\n",
      "Tokenized sentence: ['oh', 'ok', 'wat', 's', 'ur', 'email']\n",
      "After stop words removal: ['oh', 'ok', 'wat', 'ur', 'email']\n",
      "After stemming with porters algorithm: ['wat', 'email']\n",
      "Tokenized sentence: ['you', 'know', 'wot', 'people', 'wear', 't', 'shirts', 'jumpers', 'hat', 'belt', 'is', 'all', 'we', 'know', 'we', 'r', 'at', 'cribbs']\n",
      "After stop words removal: ['know', 'wot', 'people', 'wear', 'shirts', 'jumpers', 'hat', 'belt', 'know', 'r', 'cribbs']\n",
      "After stemming with porters algorithm: ['know', 'wot', 'peopl', 'wear', 'shirt', 'jumper', 'hat', 'belt', 'know', 'cribb']\n",
      "Tokenized sentence: ['have', 'you', 'finished', 'work', 'yet']\n",
      "After stop words removal: ['finished', 'work', 'yet']\n",
      "After stemming with porters algorithm: ['finis', 'work', 'yet']\n",
      "Tokenized sentence: ['well', 'done', 'blimey', 'exercise', 'yeah', 'i', 'kinda', 'remember', 'wot', 'that', 'is', 'hmm']\n",
      "After stop words removal: ['well', 'done', 'blimey', 'exercise', 'yeah', 'kinda', 'remember', 'wot', 'hmm']\n",
      "After stemming with porters algorithm: ['well', 'done', 'blimei', 'exercis', 'yeah', 'kinda', 'rememb', 'wot', 'hmm']\n",
      "Tokenized sentence: ['ok', 'very', 'good', 'its', 'all', 'about', 'making', 'that', 'money']\n",
      "After stop words removal: ['ok', 'good', 'making', 'money']\n",
      "mak\n",
      "After stemming with porters algorithm: ['good', 'make', 'monei']\n",
      "Tokenized sentence: ['that', 'call', 'cost', 'which', 'i', 'guess', 'isnt', 'bad', 'miss', 'ya', 'need', 'ya', 'want', 'ya', 'love', 'ya']\n",
      "After stop words removal: ['call', 'cost', 'guess', 'isnt', 'bad', 'miss', 'ya', 'need', 'ya', 'want', 'ya', 'love', 'ya']\n",
      "After stemming with porters algorithm: ['call', 'cost', 'guess', 'isnt', 'bad', 'miss', 'need', 'want', 'love']\n",
      "Tokenized sentence: ['speak', 'only', 'when', 'you', 'feel', 'your', 'words', 'are', 'better', 'than', 'the', 'silence', 'gud', 'mrng']\n",
      "After stop words removal: ['speak', 'feel', 'words', 'better', 'silence', 'gud', 'mrng']\n",
      "After stemming with porters algorithm: ['speak', 'feel', 'word', 'better', 'silenc', 'gud', 'mrng']\n",
      "Tokenized sentence: ['nah', 'dub', 'but', 'je', 'still', 'buff']\n",
      "After stop words removal: ['nah', 'dub', 'je', 'still', 'buff']\n",
      "After stemming with porters algorithm: ['nah', 'dub', 'still', 'buff']\n",
      "Tokenized sentence: ['and', 'he', 's', 'apparently', 'bffs', 'with', 'carly', 'quick', 'now']\n",
      "After stop words removal: ['apparently', 'bffs', 'carly', 'quick']\n",
      "After stemming with porters algorithm: ['appar', 'bff', 'carli', 'quick']\n",
      "Tokenized sentence: ['depends', 'on', 'individual', 'lor', 'e', 'hair', 'dresser', 'say', 'pretty', 'but', 'my', 'parents', 'say', 'look', 'gong', 'u', 'kaypoh', 'i', 'also', 'dunno', 'wat', 'she', 'collecting']\n",
      "After stop words removal: ['depends', 'individual', 'lor', 'e', 'hair', 'dresser', 'say', 'pretty', 'parents', 'say', 'look', 'gong', 'u', 'kaypoh', 'also', 'dunno', 'wat', 'collecting']\n",
      "collect\n",
      "After stemming with porters algorithm: ['depend', 'individu', 'lor', 'hair', 'dresser', 'sai', 'pretti', 'parent', 'sai', 'look', 'gong', 'kaypoh', 'also', 'dunno', 'wat', 'collec']\n",
      "Tokenized sentence: ['hey', 'i', 'am', 'really', 'horny', 'want', 'to', 'chat', 'or', 'see', 'me', 'naked', 'text', 'hot', 'to', 'text', 'charged', 'at', 'pm', 'to', 'unsubscribe', 'text', 'stop']\n",
      "After stop words removal: ['hey', 'really', 'horny', 'want', 'chat', 'see', 'naked', 'text', 'hot', 'text', 'charged', 'pm', 'unsubscribe', 'text', 'stop']\n",
      "After stemming with porters algorithm: ['hei', 'realli', 'horni', 'want', 'chat', 'see', 'nake', 'text', 'hot', 'text', 'char', 'unsubscrib', 'text', 'stop']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'video', 'phone', 'anytime', 'any', 'network', 'mins', 'inclusive', 'video', 'calls', 'and', 'downloads', 'per', 'week', 'free', 'deltomorrow', 'call', 'or', 'reply', 'now']\n",
      "After stop words removal: ['want', 'new', 'video', 'phone', 'anytime', 'network', 'mins', 'inclusive', 'video', 'calls', 'downloads', 'per', 'week', 'free', 'deltomorrow', 'call', 'reply']\n",
      "After stemming with porters algorithm: ['want', 'new', 'video', 'phone', 'anytim', 'network', 'min', 'inclus', 'video', 'call', 'download', 'per', 'week', 'free', 'deltomorrow', 'call', 'repli']\n",
      "Tokenized sentence: ['que', 'pases', 'un', 'buen', 'tiempo', 'or', 'something', 'like', 'that']\n",
      "After stop words removal: ['que', 'pases', 'un', 'buen', 'tiempo', 'something', 'like']\n",
      "someth\n",
      "After stemming with porters algorithm: ['que', 'pase', 'buen', 'tiempo', 'somet', 'like']\n",
      "Tokenized sentence: ['hey', 'kate']\n",
      "After stop words removal: ['hey', 'kate']\n",
      "After stemming with porters algorithm: ['hei', 'kate']\n",
      "Tokenized sentence: ['text', 'me', 'when', 'you', 'get', 'off', 'don', 't', 'call', 'my', 'phones', 'having', 'problems']\n",
      "After stop words removal: ['text', 'get', 'call', 'phones', 'problems']\n",
      "After stemming with porters algorithm: ['text', 'get', 'call', 'phone', 'problem']\n",
      "Tokenized sentence: ['really', 'good', 'dhanush', 'rocks', 'once', 'again']\n",
      "After stop words removal: ['really', 'good', 'dhanush', 'rocks']\n",
      "After stemming with porters algorithm: ['realli', 'good', 'dhanush', 'rock']\n",
      "Tokenized sentence: ['then', 'u', 'better', 'go', 'sleep', 'dun', 'disturb', 'u', 'liao', 'u', 'wake', 'up', 'then', 'msg', 'me', 'lor']\n",
      "After stop words removal: ['u', 'better', 'go', 'sleep', 'dun', 'disturb', 'u', 'liao', 'u', 'wake', 'msg', 'lor']\n",
      "After stemming with porters algorithm: ['better', 'sleep', 'dun', 'disturb', 'liao', 'wake', 'msg', 'lor']\n",
      "Tokenized sentence: ['friendship', 'poem', 'dear', 'o', 'dear', 'u', 'r', 'not', 'near', 'but', 'i', 'can', 'hear', 'dont', 'get', 'fear', 'live', 'with', 'cheer', 'no', 'more', 'tear', 'u', 'r', 'always', 'my', 'dear', 'gud', 'ni']\n",
      "After stop words removal: ['friendship', 'poem', 'dear', 'dear', 'u', 'r', 'near', 'hear', 'dont', 'get', 'fear', 'live', 'cheer', 'tear', 'u', 'r', 'always', 'dear', 'gud', 'ni']\n",
      "After stemming with porters algorithm: ['friendship', 'poem', 'dear', 'dear', 'near', 'hear', 'dont', 'get', 'fear', 'live', 'cheer', 'tear', 'alwai', 'dear', 'gud']\n",
      "Tokenized sentence: ['pls', 'she', 'needs', 'to', 'dat', 'slowly', 'or', 'she', 'will', 'vomit', 'more']\n",
      "After stop words removal: ['pls', 'needs', 'dat', 'slowly', 'vomit']\n",
      "After stemming with porters algorithm: ['pl', 'need', 'dat', 'slowli', 'vomit']\n",
      "Tokenized sentence: ['no', 'my', 'mum', 'went', 'dentist']\n",
      "After stop words removal: ['mum', 'went', 'dentist']\n",
      "After stemming with porters algorithm: ['mum', 'went', 'dentist']\n",
      "Tokenized sentence: ['okay', 'good', 'no', 'problem', 'and', 'thanx']\n",
      "After stop words removal: ['okay', 'good', 'problem', 'thanx']\n",
      "After stemming with porters algorithm: ['okai', 'good', 'problem', 'thanx']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['all', 'sounds', 'good', 'fingers', 'makes', 'it', 'difficult', 'to', 'type']\n",
      "After stop words removal: ['sounds', 'good', 'fingers', 'makes', 'difficult', 'type']\n",
      "After stemming with porters algorithm: ['sound', 'good', 'finger', 'make', 'difficult', 'type']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'awar', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr', 'ppm']\n",
      "Tokenized sentence: ['oclock', 'at', 'mine', 'just', 'to', 'bash', 'out', 'a', 'flat', 'plan']\n",
      "After stop words removal: ['oclock', 'mine', 'bash', 'flat', 'plan']\n",
      "After stemming with porters algorithm: ['oclock', 'mine', 'bash', 'flat', 'plan']\n",
      "Tokenized sentence: ['am', 'watching', 'house', 'very', 'entertaining', 'am', 'getting', 'the', 'whole', 'hugh', 'laurie', 'thing', 'even', 'with', 'the', 'stick', 'indeed', 'especially', 'with', 'the', 'stick']\n",
      "After stop words removal: ['watching', 'house', 'entertaining', 'getting', 'whole', 'hugh', 'laurie', 'thing', 'even', 'stick', 'indeed', 'especially', 'stick']\n",
      "watch\n",
      "entertain\n",
      "gett\n",
      "After stemming with porters algorithm: ['watc', 'hous', 'entertain', 'get', 'whole', 'hugh', 'lauri', 'thing', 'even', 'stick', 'inde', 'especi', 'stick']\n",
      "Tokenized sentence: ['i', 'm', 'at', 'work', 'please', 'call']\n",
      "After stop words removal: ['work', 'please', 'call']\n",
      "After stemming with porters algorithm: ['work', 'pleas', 'call']\n",
      "Tokenized sentence: ['i', 'got', 'a', 'call', 'from', 'a', 'landline', 'number', 'i', 'am', 'asked', 'to', 'come', 'to', 'anna', 'nagar', 'i', 'will', 'go', 'in', 'the', 'afternoon']\n",
      "After stop words removal: ['got', 'call', 'landline', 'number', 'asked', 'come', 'anna', 'nagar', 'go', 'afternoon']\n",
      "After stemming with porters algorithm: ['got', 'call', 'landlin', 'number', 'as', 'come', 'anna', 'nagar', 'afternoon']\n",
      "Tokenized sentence: ['are', 'u', 'awake', 'is', 'there', 'snow', 'there']\n",
      "After stop words removal: ['u', 'awake', 'snow']\n",
      "After stemming with porters algorithm: ['awak', 'snow']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'guaranteed', 'award', 'or', 'maybe', 'even', 'cash', 'to', 'claim', 'ur', 'award', 'call', 'free', 'on', 'its', 'a', 'legitimat', 'efreefone', 'number', 'wat', 'do', 'u', 'think']\n",
      "After stop words removal: ['guaranteed', 'award', 'maybe', 'even', 'cash', 'claim', 'ur', 'award', 'call', 'free', 'legitimat', 'efreefone', 'number', 'wat', 'u', 'think']\n",
      "After stemming with porters algorithm: ['guaranteed', 'award', 'mayb', 'even', 'cash', 'claim', 'award', 'call', 'free', 'legitimat', 'efreefon', 'number', 'wat', 'think']\n",
      "Tokenized sentence: ['free', 'top', 'ringtone', 'sub', 'to', 'weekly', 'ringtone', 'get', 'st', 'week', 'free', 'send', 'subpoly', 'to', 'per', 'week', 'stop', 'sms']\n",
      "After stop words removal: ['free', 'top', 'ringtone', 'sub', 'weekly', 'ringtone', 'get', 'st', 'week', 'free', 'send', 'subpoly', 'per', 'week', 'stop', 'sms']\n",
      "After stemming with porters algorithm: ['free', 'top', 'rington', 'sub', 'weekli', 'rington', 'get', 'week', 'free', 'send', 'subpoli', 'per', 'week', 'stop', 'sm']\n",
      "Tokenized sentence: ['camera', 'you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stop words removal: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stemming with porters algorithm: ['camera', 'awar', 'sipix', 'digit', 'camera', 'call', 'fromm', 'landlin', 'deliveri', 'within', 'dai']\n",
      "Tokenized sentence: ['no', 'you', 'll', 'just', 'get', 'a', 'headache', 'trying', 'to', 'figure', 'it', 'out', 'u', 'can', 'trust', 'me', 'to', 'do', 'the', 'math', 'i', 'promise', 'o']\n",
      "After stop words removal: ['get', 'headache', 'trying', 'figure', 'u', 'trust', 'math', 'promise']\n",
      "After stemming with porters algorithm: ['get', 'headach', 'trying', 'figur', 'trust', 'math', 'promis']\n",
      "Tokenized sentence: ['there', 's', 'no', 'point', 'hangin', 'on', 'to', 'mr', 'not', 'right', 'if', 'he', 's', 'not', 'makin', 'u', 'happy']\n",
      "After stop words removal: ['point', 'hangin', 'mr', 'right', 'makin', 'u', 'happy']\n",
      "After stemming with porters algorithm: ['point', 'hangin', 'right', 'makin', 'happi']\n",
      "Tokenized sentence: ['omg', 'you', 'can', 'make', 'a', 'wedding', 'chapel', 'in', 'frontierville', 'why', 'do', 'they', 'get', 'all', 'the', 'good', 'stuff']\n",
      "After stop words removal: ['omg', 'make', 'wedding', 'chapel', 'frontierville', 'get', 'good', 'stuff']\n",
      "wedd\n",
      "After stemming with porters algorithm: ['omg', 'make', 'wed', 'chapel', 'frontiervil', 'get', 'good', 'stuff']\n",
      "Tokenized sentence: ['thing', 'r', 'good', 'thanx', 'got', 'exams', 'in', 'march', 'ive', 'done', 'no', 'revision', 'is', 'fran', 'still', 'with', 'boyf', 'ive', 'gotta', 'interviw', 'exeter', 'bit', 'worried', 'x']\n",
      "After stop words removal: ['thing', 'r', 'good', 'thanx', 'got', 'exams', 'march', 'ive', 'done', 'revision', 'fran', 'still', 'boyf', 'ive', 'gotta', 'interviw', 'exeter', 'bit', 'worried', 'x']\n",
      "After stemming with porters algorithm: ['thing', 'good', 'thanx', 'got', 'exam', 'march', 'iv', 'done', 'revis', 'fran', 'still', 'boyf', 'iv', 'gotta', 'interviw', 'exet', 'bit', 'worri']\n",
      "Tokenized sentence: ['ok', 'me', 'watching', 'tv', 'too']\n",
      "After stop words removal: ['ok', 'watching', 'tv']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc']\n",
      "Tokenized sentence: ['hot', 'live', 'fantasies', 'call', 'now', 'just', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'is', 'a', 'national', 'rate', 'call']\n",
      "After stop words removal: ['hot', 'live', 'fantasies', 'call', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'national', 'rate', 'call']\n",
      "After stemming with porters algorithm: ['hot', 'live', 'fantasi', 'call', 'per', 'min', 'ntt', 'ltd', 'box', 'croydon', 'nat', 'rate', 'call']\n",
      "Tokenized sentence: ['my', 'sort', 'code', 'is', 'and', 'acc', 'no', 'is', 'the', 'bank', 'is', 'natwest', 'can', 'you', 'reply', 'to', 'confirm', 'i', 've', 'sent', 'this', 'to', 'the', 'right', 'person']\n",
      "After stop words removal: ['sort', 'code', 'acc', 'bank', 'natwest', 'reply', 'confirm', 'sent', 'right', 'person']\n",
      "After stemming with porters algorithm: ['sort', 'code', 'acc', 'bank', 'natwest', 'repli', 'confirm', 'sent', 'right', 'person']\n",
      "Tokenized sentence: ['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's']\n",
      "After stop words removal: ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'wkly', 'comp', 'win', 'cup', 'final', 'tkt', 'mai', 'text', 'receiv', 'entri', 'quest', 'std', 'txt', 'rate', 'appli']\n",
      "Tokenized sentence: ['shall', 'i', 'start', 'from', 'hear']\n",
      "After stop words removal: ['shall', 'start', 'hear']\n",
      "After stemming with porters algorithm: ['shall', 'start', 'hear']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later', 'in', 'meeting', 'any', 'thing', 'related', 'to', 'trade', 'please', 'call', 'arul', 'lt', 'gt']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'meeting', 'thing', 'related', 'trade', 'please', 'call', 'arul', 'lt', 'gt']\n",
      "meet\n",
      "relate\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'meet', 'thing', 'relat', 'trade', 'pleas', 'call', 'arul']\n",
      "Tokenized sentence: ['morning', 'only', 'i', 'can', 'ok']\n",
      "After stop words removal: ['morning', 'ok']\n",
      "morn\n",
      "After stemming with porters algorithm: ['mor']\n",
      "Tokenized sentence: ['we', 'r', 'outside', 'already']\n",
      "After stop words removal: ['r', 'outside', 'already']\n",
      "After stemming with porters algorithm: ['outsid', 'alreadi']\n",
      "Tokenized sentence: ['a', 'boy', 'loved', 'a', 'gal', 'he', 'propsd', 'bt', 'she', 'didnt', 'mind', 'he', 'gv', 'lv', 'lttrs', 'bt', 'her', 'frnds', 'threw', 'thm', 'again', 'd', 'boy', 'decided', 'aproach', 'd', 'gal', 'dt', 'time', 'a', 'truck', 'was', 'speeding', 'towards', 'd', 'gal', 'wn', 'it', 'was', 'about', 'hit', 'd', 'girl', 'd', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'her', 'she', 'asked', 'hw', 'cn', 'u', 'run', 'so', 'fast', 'd', 'boy', 'replied', 'boost', 'is', 'd', 'secret', 'of', 'my', 'energy', 'n', 'instantly', 'd', 'girl', 'shouted', 'our', 'energy', 'n', 'thy', 'lived', 'happily', 'gthr', 'drinking', 'boost', 'evrydy', 'moral', 'of', 'd', 'story', 'i', 'hv', 'free', 'msgs', 'd', 'gud', 'ni']\n",
      "After stop words removal: ['boy', 'loved', 'gal', 'propsd', 'bt', 'didnt', 'mind', 'gv', 'lv', 'lttrs', 'bt', 'frnds', 'threw', 'thm', 'boy', 'decided', 'aproach', 'gal', 'dt', 'time', 'truck', 'speeding', 'towards', 'gal', 'wn', 'hit', 'girl', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'asked', 'hw', 'cn', 'u', 'run', 'fast', 'boy', 'replied', 'boost', 'secret', 'energy', 'n', 'instantly', 'girl', 'shouted', 'energy', 'n', 'thy', 'lived', 'happily', 'gthr', 'drinking', 'boost', 'evrydy', 'moral', 'story', 'hv', 'free', 'msgs', 'gud', 'ni']\n",
      "speed\n",
      "drink\n",
      "After stemming with porters algorithm: ['boi', 'love', 'gal', 'propsd', 'didnt', 'mind', 'lttr', 'frnd', 'threw', 'thm', 'boi', 'decid', 'aproach', 'gal', 'time', 'truck', 'speed', 'toward', 'gal', 'hit', 'girl', 'boi', 'ran', 'like', 'hell', 'save', 'as', 'run', 'fast', 'boi', 'repli', 'boost', 'secret', 'energi', 'instantli', 'girl', 'shout', 'energi', 'thy', 'live', 'happili', 'gthr', 'drin', 'boost', 'evrydi', 'moral', 'stori', 'free', 'msg', 'gud']\n",
      "Tokenized sentence: ['how', 'would', 'my', 'ip', 'address', 'test', 'that', 'considering', 'my', 'computer', 'isn', 't', 'a', 'minecraft', 'server']\n",
      "After stop words removal: ['would', 'ip', 'address', 'test', 'considering', 'computer', 'minecraft', 'server']\n",
      "consider\n",
      "After stemming with porters algorithm: ['would', 'address', 'test', 'consid', 'comput', 'minecraft', 'server']\n",
      "Tokenized sentence: ['oops', 'i', 'll', 'let', 'you', 'know', 'when', 'my', 'roommate', 's', 'done']\n",
      "After stop words removal: ['oops', 'let', 'know', 'roommate', 'done']\n",
      "After stemming with porters algorithm: ['oop', 'let', 'know', 'roommat', 'done']\n",
      "Tokenized sentence: ['under', 'the', 'sea', 'there', 'lays', 'a', 'rock', 'in', 'the', 'rock', 'there', 'is', 'an', 'envelope', 'in', 'the', 'envelope', 'there', 'is', 'a', 'paper', 'on', 'the', 'paper', 'there', 'are', 'words']\n",
      "After stop words removal: ['sea', 'lays', 'rock', 'rock', 'envelope', 'envelope', 'paper', 'paper', 'words']\n",
      "After stemming with porters algorithm: ['sea', 'lai', 'rock', 'rock', 'envelop', 'envelop', 'paper', 'paper', 'word']\n",
      "Tokenized sentence: ['my', 'planning', 'usually', 'stops', 'at', 'find', 'hella', 'weed']\n",
      "After stop words removal: ['planning', 'usually', 'stops', 'find', 'hella', 'weed']\n",
      "plann\n",
      "After stemming with porters algorithm: ['plan', 'usual', 'stop', 'find', 'hella', 'weed']\n",
      "Tokenized sentence: ['no', 'i', 'didn', 't', 'mean', 'to', 'post', 'it', 'i', 'wrote', 'it', 'and', 'like', 'so', 'many', 'other', 'times', 'i', 've', 'ritten', 'stuff', 'to', 'you', 'i', 'let', 'it', 'sit', 'there', 'it', 'was', 'what', 'i', 'was', 'feeling', 'at', 'the', 'time', 'i', 'was', 'angry', 'before', 'i', 'left', 'i', 'hit', 'send', 'then', 'stop', 'it', 'wasn', 't', 'there', 'i', 'checked', 'on', 'my', 'phone', 'when', 'i', 'got', 'to', 'my', 'car', 'it', 'wasn', 't', 'there', 'you', 'said', 'you', 'didn', 't', 'sleep', 'you', 'were', 'bored', 'so', 'why', 'wouldn', 't', 'that', 'be', 'the', 'time', 'to', 'clean', 'fold', 'laundry', 'etc', 'at', 'least', 'make', 'the', 'bed']\n",
      "After stop words removal: ['mean', 'post', 'wrote', 'like', 'many', 'times', 'ritten', 'stuff', 'let', 'sit', 'feeling', 'time', 'angry', 'left', 'hit', 'send', 'stop', 'checked', 'phone', 'got', 'car', 'said', 'sleep', 'bored', 'time', 'clean', 'fold', 'laundry', 'etc', 'least', 'make', 'bed']\n",
      "feel\n",
      "After stemming with porters algorithm: ['mean', 'post', 'wrote', 'like', 'mani', 'time', 'ritten', 'stuff', 'let', 'sit', 'feel', 'time', 'angri', 'left', 'hit', 'send', 'stop', 'chec', 'phone', 'got', 'car', 'said', 'sleep', 'bore', 'time', 'clean', 'fold', 'laundri', 'etc', 'least', 'make', 'bed']\n",
      "Tokenized sentence: ['wife', 'how', 'she', 'knew', 'the', 'time', 'of', 'murder', 'exactly']\n",
      "After stop words removal: ['wife', 'knew', 'time', 'murder', 'exactly']\n",
      "After stemming with porters algorithm: ['wife', 'knew', 'time', 'murder', 'exactli']\n",
      "Tokenized sentence: ['how', 'i', 'noe', 'she', 's', 'in', 'da', 'car', 'now', 'later', 'then', 'c', 'lar', 'i', 'm', 'wearing', 'shorts']\n",
      "After stop words removal: ['noe', 'da', 'car', 'later', 'c', 'lar', 'wearing', 'shorts']\n",
      "wear\n",
      "After stemming with porters algorithm: ['noe', 'car', 'later', 'lar', 'wear', 'short']\n",
      "Tokenized sentence: ['pls', 'i', 'wont', 'belive', 'god', 'not', 'only', 'jesus']\n",
      "After stop words removal: ['pls', 'wont', 'belive', 'god', 'jesus']\n",
      "After stemming with porters algorithm: ['pl', 'wont', 'beliv', 'god', 'jesu']\n",
      "Tokenized sentence: ['ok', 'but', 'tell', 'me', 'half', 'an', 'hr', 'b', 'u', 'come', 'i', 'need', 'prepare']\n",
      "After stop words removal: ['ok', 'tell', 'half', 'hr', 'b', 'u', 'come', 'need', 'prepare']\n",
      "After stemming with porters algorithm: ['tell', 'half', 'come', 'need', 'prepar']\n",
      "Tokenized sentence: ['haf', 'u', 'found', 'him', 'i', 'feel', 'so', 'stupid', 'da', 'v', 'cam', 'was', 'working']\n",
      "After stop words removal: ['haf', 'u', 'found', 'feel', 'stupid', 'da', 'v', 'cam', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['haf', 'found', 'feel', 'stupid', 'cam', 'wor']\n",
      "Tokenized sentence: ['all', 'the', 'lastest', 'from', 'stereophonics', 'marley', 'dizzee', 'racal', 'libertines', 'and', 'the', 'strokes', 'win', 'nookii', 'games', 'with', 'flirt', 'click', 'themob', 'wap', 'bookmark', 'or', 'text', 'wap', 'to']\n",
      "After stop words removal: ['lastest', 'stereophonics', 'marley', 'dizzee', 'racal', 'libertines', 'strokes', 'win', 'nookii', 'games', 'flirt', 'click', 'themob', 'wap', 'bookmark', 'text', 'wap']\n",
      "After stemming with porters algorithm: ['lastest', 'stereophon', 'marlei', 'dizze', 'racal', 'libertin', 'stroke', 'win', 'nookii', 'game', 'flirt', 'click', 'themob', 'wap', 'bookmark', 'text', 'wap']\n",
      "Tokenized sentence: ['come', 'lt', 'n', 'pass', 'to', 'me', 'lar']\n",
      "After stop words removal: ['come', 'lt', 'n', 'pass', 'lar']\n",
      "After stemming with porters algorithm: ['come', 'pass', 'lar']\n",
      "Tokenized sentence: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mobile', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stop words removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mobile', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'www', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'norm', 'p', 'tone']\n",
      "After stemming with porters algorithm: ['free', 'week', 'nokia', 'tone', 'mobil', 'everi', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'mate', 'www', 'getz', 'pobox', 'norm', 'tone']\n",
      "Tokenized sentence: ['not', 'planned', 'yet', 'going', 'to', 'join', 'company', 'on', 'jan', 'only', 'don', 'know', 'what', 'will', 'happen', 'after', 'that']\n",
      "After stop words removal: ['planned', 'yet', 'going', 'join', 'company', 'jan', 'know', 'happen']\n",
      "go\n",
      "After stemming with porters algorithm: ['plan', 'yet', 'go', 'join', 'compani', 'jan', 'know', 'happen']\n",
      "Tokenized sentence: ['babe', 'i', 'miiiiiiissssssssss', 'you', 'i', 'need', 'you', 'i', 'crave', 'you', 'geeee', 'i', 'm', 'so', 'sad', 'without', 'you', 'babe', 'i', 'love', 'you']\n",
      "After stop words removal: ['babe', 'miiiiiiissssssssss', 'need', 'crave', 'geeee', 'sad', 'without', 'babe', 'love']\n",
      "After stemming with porters algorithm: ['babe', 'miiiiiiissssssssss', 'need', 'crave', 'geeee', 'sad', 'without', 'babe', 'love']\n",
      "Tokenized sentence: ['i', 'll', 'reach', 'in', 'ard', 'mins', 'ok']\n",
      "After stop words removal: ['reach', 'ard', 'mins', 'ok']\n",
      "After stemming with porters algorithm: ['reach', 'ard', 'min']\n",
      "Tokenized sentence: ['congrats', 'mobile', 'g', 'videophones', 'r', 'yours', 'call', 'now', 'videochat', 'wid', 'your', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl']\n",
      "After stop words removal: ['congrats', 'mobile', 'g', 'videophones', 'r', 'call', 'videochat', 'wid', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl']\n",
      "After stemming with porters algorithm: ['congrat', 'mobil', 'videophon', 'call', 'videochat', 'wid', 'mate', 'plai', 'java', 'game', 'dload', 'polyph', 'music', 'nolin', 'rentl']\n",
      "Tokenized sentence: ['nah', 'man', 'my', 'car', 'is', 'meant', 'to', 'be', 'crammed', 'full', 'of', 'people']\n",
      "After stop words removal: ['nah', 'man', 'car', 'meant', 'crammed', 'full', 'people']\n",
      "After stemming with porters algorithm: ['nah', 'man', 'car', 'meant', 'cram', 'full', 'peopl']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'nokia', 'i', 'colour', 'phone', 'deliveredtomorrow', 'with', 'free', 'minutes', 'to', 'any', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'or', 'call']\n",
      "After stop words removal: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minutes', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minut', 'mobil', 'free', 'text', 'free', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['alright', 'i', 'll', 'make', 'sure', 'the', 'car', 'is', 'back', 'tonight']\n",
      "After stop words removal: ['alright', 'make', 'sure', 'car', 'back', 'tonight']\n",
      "After stemming with porters algorithm: ['alright', 'make', 'sure', 'car', 'back', 'tonight']\n",
      "Tokenized sentence: ['no', 'it', 's', 'waiting', 'in', 'e', 'car', 'dat', 's', 'bored', 'wat', 'cos', 'wait', 'outside', 'got', 'nothing', 'do', 'at', 'home', 'can', 'do', 'my', 'stuff', 'or', 'watch', 'tv', 'wat']\n",
      "After stop words removal: ['waiting', 'e', 'car', 'dat', 'bored', 'wat', 'cos', 'wait', 'outside', 'got', 'nothing', 'home', 'stuff', 'watch', 'tv', 'wat']\n",
      "wait\n",
      "noth\n",
      "After stemming with porters algorithm: ['wait', 'car', 'dat', 'bore', 'wat', 'co', 'wait', 'outsid', 'got', 'not', 'home', 'stuff', 'watch', 'wat']\n",
      "Tokenized sentence: ['okey', 'dokey', 'i', 'll', 'be', 'over', 'in', 'a', 'bit', 'just', 'sorting', 'some', 'stuff', 'out']\n",
      "After stop words removal: ['okey', 'dokey', 'bit', 'sorting', 'stuff']\n",
      "sort\n",
      "After stemming with porters algorithm: ['okei', 'dokei', 'bit', 'sor', 'stuff']\n",
      "Tokenized sentence: ['they', 'said', 'dun', 'haf', 'passport', 'or', 'smth', 'like', 'dat', 'or', 'juz', 'send', 'to', 'my', 'email', 'account']\n",
      "After stop words removal: ['said', 'dun', 'haf', 'passport', 'smth', 'like', 'dat', 'juz', 'send', 'email', 'account']\n",
      "After stemming with porters algorithm: ['said', 'dun', 'haf', 'passport', 'smth', 'like', 'dat', 'juz', 'send', 'email', 'account']\n",
      "Tokenized sentence: ['many', 'times', 'we', 'lose', 'our', 'best', 'ones', 'bcoz', 'we', 'are']\n",
      "After stop words removal: ['many', 'times', 'lose', 'best', 'ones', 'bcoz']\n",
      "After stemming with porters algorithm: ['mani', 'time', 'lose', 'best', 'on', 'bcoz']\n",
      "Tokenized sentence: ['ok', 'leave', 'no', 'need', 'to', 'ask']\n",
      "After stop words removal: ['ok', 'leave', 'need', 'ask']\n",
      "After stemming with porters algorithm: ['leav', 'need', 'ask']\n",
      "Tokenized sentence: ['i', 'm', 'going', 'lunch', 'now', 'wif', 'my', 'family', 'then', 'aft', 'dat', 'i', 'go', 'str', 'orchard', 'lor']\n",
      "After stop words removal: ['going', 'lunch', 'wif', 'family', 'aft', 'dat', 'go', 'str', 'orchard', 'lor']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'lunch', 'wif', 'famili', 'aft', 'dat', 'str', 'orchard', 'lor']\n",
      "Tokenized sentence: ['download', 'as', 'many', 'ringtones', 'as', 'u', 'like', 'no', 'restrictions', 's', 'choose', 'u', 'can', 'even', 'send', 'yr', 'buddys', 'txt', 'sir', 'to']\n",
      "After stop words removal: ['download', 'many', 'ringtones', 'u', 'like', 'restrictions', 'choose', 'u', 'even', 'send', 'yr', 'buddys', 'txt', 'sir']\n",
      "After stemming with porters algorithm: ['download', 'mani', 'rington', 'like', 'restrict', 'choos', 'even', 'send', 'buddi', 'txt', 'sir']\n",
      "Tokenized sentence: ['freemsg', 'today', 's', 'the', 'day', 'if', 'you', 'are', 'ready', 'i', 'm', 'horny', 'live', 'in', 'your', 'town', 'i', 'love', 'sex', 'fun', 'games', 'netcollex', 'ltd', 'p', 'per', 'msg', 'reply', 'stop', 'to', 'end']\n",
      "After stop words removal: ['freemsg', 'today', 'day', 'ready', 'horny', 'live', 'town', 'love', 'sex', 'fun', 'games', 'netcollex', 'ltd', 'p', 'per', 'msg', 'reply', 'stop', 'end']\n",
      "After stemming with porters algorithm: ['freemsg', 'todai', 'dai', 'readi', 'horni', 'live', 'town', 'love', 'sex', 'fun', 'game', 'netcollex', 'ltd', 'per', 'msg', 'repli', 'stop', 'end']\n",
      "Tokenized sentence: ['your', 'opinion', 'about', 'me', 'over', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'not', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stop words removal: ['opinion', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
      "After stemming with porters algorithm: ['opinion', 'jada', 'kusruthi', 'lovab', 'silent', 'spl', 'charact', 'matur', 'stylish', 'simpl', 'pl', 'repli']\n",
      "Tokenized sentence: ['hey', 'i', 'will', 'be', 'late', 'i', 'm', 'at', 'amk', 'need', 'to', 'drink', 'tea', 'or', 'coffee']\n",
      "After stop words removal: ['hey', 'late', 'amk', 'need', 'drink', 'tea', 'coffee']\n",
      "After stemming with porters algorithm: ['hei', 'late', 'amk', 'need', 'drink', 'tea', 'coffe']\n",
      "Tokenized sentence: ['lol', 'boo', 'i', 'was', 'hoping', 'for', 'a', 'laugh']\n",
      "After stop words removal: ['lol', 'boo', 'hoping', 'laugh']\n",
      "hop\n",
      "After stemming with porters algorithm: ['lol', 'boo', 'hope', 'laugh']\n",
      "Tokenized sentence: ['wait', 'i', 'will', 'come', 'out', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['wait', 'come', 'lt', 'gt', 'min']\n",
      "After stemming with porters algorithm: ['wait', 'come', 'min']\n",
      "Tokenized sentence: ['good', 'no', 'swimsuit', 'allowed']\n",
      "After stop words removal: ['good', 'swimsuit', 'allowed']\n",
      "After stemming with porters algorithm: ['good', 'swimsuit', 'allow']\n",
      "Tokenized sentence: ['jay', 'told', 'me', 'already', 'will', 'do']\n",
      "After stop words removal: ['jay', 'told', 'already']\n",
      "After stemming with porters algorithm: ['jai', 'told', 'alreadi']\n",
      "Tokenized sentence: ['do', 'you', 'like', 'italian', 'food']\n",
      "After stop words removal: ['like', 'italian', 'food']\n",
      "After stemming with porters algorithm: ['like', 'italian', 'food']\n",
      "Tokenized sentence: ['doing', 'nothing', 'then', 'u', 'not', 'having', 'dinner', 'w', 'us']\n",
      "After stop words removal: ['nothing', 'u', 'dinner', 'w', 'us']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not', 'dinner']\n",
      "Tokenized sentence: ['hi', 'chachi', 'tried', 'calling', 'u', 'now', 'unable', 'to', 'reach', 'u', 'pl', 'give', 'me', 'a', 'missed', 'cal', 'once', 'u', 'c', 'tiz', 'msg', 'kanagu']\n",
      "After stop words removal: ['hi', 'chachi', 'tried', 'calling', 'u', 'unable', 'reach', 'u', 'pl', 'give', 'missed', 'cal', 'u', 'c', 'tiz', 'msg', 'kanagu']\n",
      "call\n",
      "After stemming with porters algorithm: ['chachi', 'tri', 'call', 'unab', 'reach', 'give', 'miss', 'cal', 'tiz', 'msg', 'kanagu']\n",
      "Tokenized sentence: ['how', 'come', 'guoyang', 'go', 'n', 'tell', 'her', 'then', 'u', 'told', 'her']\n",
      "After stop words removal: ['come', 'guoyang', 'go', 'n', 'tell', 'u', 'told']\n",
      "After stemming with porters algorithm: ['come', 'guoyang', 'tell', 'told']\n",
      "Tokenized sentence: ['have', 'you', 'been', 'practising', 'your', 'curtsey']\n",
      "After stop words removal: ['practising', 'curtsey']\n",
      "practis\n",
      "After stemming with porters algorithm: ['practis', 'curtsei']\n",
      "Tokenized sentence: ['alright', 'we', 'll', 'bring', 'it', 'to', 'you', 'see', 'you', 'in', 'like', 'lt', 'gt', 'mins']\n",
      "After stop words removal: ['alright', 'bring', 'see', 'like', 'lt', 'gt', 'mins']\n",
      "After stemming with porters algorithm: ['alright', 'bring', 'see', 'like', 'min']\n",
      "Tokenized sentence: ['thank', 'god', 'they', 'are', 'in', 'bed']\n",
      "After stop words removal: ['thank', 'god', 'bed']\n",
      "After stemming with porters algorithm: ['thank', 'god', 'bed']\n",
      "Tokenized sentence: ['yeah', 'jay', 's', 'sort', 'of', 'a', 'fucking', 'retard']\n",
      "After stop words removal: ['yeah', 'jay', 'sort', 'fucking', 'retard']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['yeah', 'jai', 'sort', 'fuc', 'retard']\n",
      "Tokenized sentence: ['why', 'is', 'that', 'princess', 'i', 'bet', 'the', 'brothas', 'are', 'all', 'chasing', 'you']\n",
      "After stop words removal: ['princess', 'bet', 'brothas', 'chasing']\n",
      "chas\n",
      "After stemming with porters algorithm: ['princess', 'bet', 'brotha', 'chase']\n",
      "Tokenized sentence: ['yeah', 'that', 's', 'fine', 'it', 's', 'to', 'get', 'in', 'is', 'that', 'ok']\n",
      "After stop words removal: ['yeah', 'fine', 'get', 'ok']\n",
      "After stemming with porters algorithm: ['yeah', 'fine', 'get']\n",
      "Tokenized sentence: ['sorry', 'went', 'to', 'bed', 'early', 'nightnight']\n",
      "After stop words removal: ['sorry', 'went', 'bed', 'early', 'nightnight']\n",
      "After stemming with porters algorithm: ['sorri', 'went', 'bed', 'earli', 'nightnight']\n",
      "Tokenized sentence: ['busy', 'here', 'trying', 'to', 'finish', 'for', 'new', 'year', 'i', 'am', 'looking', 'forward', 'to', 'finally', 'meeting', 'you']\n",
      "After stop words removal: ['busy', 'trying', 'finish', 'new', 'year', 'looking', 'forward', 'finally', 'meeting']\n",
      "look\n",
      "meet\n",
      "After stemming with porters algorithm: ['busi', 'trying', 'finish', 'new', 'year', 'look', 'forward', 'final', 'meet']\n",
      "Tokenized sentence: ['cuz', 'ibored', 'and', 'don', 'wanna', 'study']\n",
      "After stop words removal: ['cuz', 'ibored', 'wanna', 'study']\n",
      "After stemming with porters algorithm: ['cuz', 'ibor', 'wanna', 'studi']\n",
      "Tokenized sentence: ['miserable', 'they', 'don', 't', 'tell', 'u', 'that', 'the', 'side', 'effects', 'of', 'birth', 'control', 'are', 'massive', 'gut', 'wrenching', 'cramps', 'for', 'the', 'first', 'months', 'i', 'didn', 't', 'sleep', 'at', 'all', 'last', 'night']\n",
      "After stop words removal: ['miserable', 'tell', 'u', 'side', 'effects', 'birth', 'control', 'massive', 'gut', 'wrenching', 'cramps', 'first', 'months', 'sleep', 'last', 'night']\n",
      "wrench\n",
      "After stemming with porters algorithm: ['miser', 'tell', 'side', 'effect', 'birth', 'control', 'massiv', 'gut', 'wrenc', 'cramp', 'first', 'month', 'sleep', 'last', 'night']\n",
      "Tokenized sentence: ['havent', 'still', 'waitin', 'as', 'usual', 'come', 'back', 'sch', 'oredi']\n",
      "After stop words removal: ['havent', 'still', 'waitin', 'usual', 'come', 'back', 'sch', 'oredi']\n",
      "After stemming with porters algorithm: ['havent', 'still', 'waitin', 'usual', 'come', 'back', 'sch', 'oredi']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'office', 'now', 'da', 'where', 'are', 'you']\n",
      "After stop words removal: ['office', 'da']\n",
      "After stemming with porters algorithm: ['offic']\n",
      "Tokenized sentence: ['freemsg', 'hi', 'baby', 'wow', 'just', 'got', 'a', 'new', 'cam', 'moby', 'wanna', 'c', 'a', 'hot', 'pic', 'or', 'fancy', 'a', 'chat', 'im', 'w', 'in', 'utxt', 'rply', 'chat', 'to', 'hlp', 'msg', 'p', 'rcv']\n",
      "After stop words removal: ['freemsg', 'hi', 'baby', 'wow', 'got', 'new', 'cam', 'moby', 'wanna', 'c', 'hot', 'pic', 'fancy', 'chat', 'im', 'w', 'utxt', 'rply', 'chat', 'hlp', 'msg', 'p', 'rcv']\n",
      "After stemming with porters algorithm: ['freemsg', 'babi', 'wow', 'got', 'new', 'cam', 'mobi', 'wanna', 'hot', 'pic', 'fanci', 'chat', 'utxt', 'rply', 'chat', 'hlp', 'msg', 'rcv']\n",
      "Tokenized sentence: ['sunshine', 'quiz', 'win', 'a', 'super', 'sony', 'dvd', 'recorder', 'if', 'you', 'canname', 'the', 'capital', 'of', 'australia', 'text', 'mquiz', 'to', 'b']\n",
      "After stop words removal: ['sunshine', 'quiz', 'win', 'super', 'sony', 'dvd', 'recorder', 'canname', 'capital', 'australia', 'text', 'mquiz', 'b']\n",
      "After stemming with porters algorithm: ['sunshin', 'quiz', 'win', 'super', 'soni', 'dvd', 'record', 'cannam', 'capit', 'australia', 'text', 'mquiz']\n",
      "Tokenized sentence: ['mine', 'here', 'like', 'all', 'fr', 'china', 'then', 'so', 'noisy']\n",
      "After stop words removal: ['mine', 'like', 'fr', 'china', 'noisy']\n",
      "After stemming with porters algorithm: ['mine', 'like', 'china', 'noisi']\n",
      "Tokenized sentence: ['hurt', 'me', 'tease', 'me', 'make', 'me', 'cry', 'but', 'in', 'the', 'end', 'of', 'my', 'life', 'when', 'i', 'die', 'plz', 'keep', 'one', 'rose', 'on', 'my', 'grave', 'and', 'say', 'stupid', 'i', 'miss', 'u', 'have', 'a', 'nice', 'day', 'bslvyl']\n",
      "After stop words removal: ['hurt', 'tease', 'make', 'cry', 'end', 'life', 'die', 'plz', 'keep', 'one', 'rose', 'grave', 'say', 'stupid', 'miss', 'u', 'nice', 'day', 'bslvyl']\n",
      "After stemming with porters algorithm: ['hurt', 'teas', 'make', 'cry', 'end', 'life', 'die', 'plz', 'keep', 'on', 'rose', 'grave', 'sai', 'stupid', 'miss', 'nice', 'dai', 'bslvyl']\n",
      "Tokenized sentence: ['then', 'we', 'gotta', 'do', 'it', 'after', 'that']\n",
      "After stop words removal: ['gotta']\n",
      "After stemming with porters algorithm: ['gotta']\n",
      "Tokenized sentence: ['we', 'made', 'it', 'eta', 'at', 'taunton', 'is', 'as', 'planned', 'hope', 'that', 's', 'still', 'okday', 'good', 'to', 'see', 'you', 'xx']\n",
      "After stop words removal: ['made', 'eta', 'taunton', 'planned', 'hope', 'still', 'okday', 'good', 'see', 'xx']\n",
      "After stemming with porters algorithm: ['made', 'eta', 'taunton', 'plan', 'hope', 'still', 'okdai', 'good', 'see']\n",
      "Tokenized sentence: ['pass', 'dis', 'to', 'all', 'ur', 'contacts', 'n', 'see', 'wat', 'u', 'get', 'red', 'i', 'm', 'in', 'luv', 'wid', 'u', 'blue', 'u', 'put', 'a', 'smile', 'on', 'my', 'face', 'purple', 'u', 'r', 'realy', 'hot', 'pink', 'u', 'r', 'so', 'swt', 'orange', 'i', 'thnk', 'i', 'lyk', 'u', 'green', 'i', 'realy', 'wana', 'go', 'out', 'wid', 'u', 'yelow', 'i', 'wnt', 'u', 'bck', 'black', 'i', 'm', 'jealous', 'of', 'u', 'brown', 'i', 'miss', 'you', 'nw', 'plz', 'giv', 'me', 'one', 'color']\n",
      "After stop words removal: ['pass', 'dis', 'ur', 'contacts', 'n', 'see', 'wat', 'u', 'get', 'red', 'luv', 'wid', 'u', 'blue', 'u', 'put', 'smile', 'face', 'purple', 'u', 'r', 'realy', 'hot', 'pink', 'u', 'r', 'swt', 'orange', 'thnk', 'lyk', 'u', 'green', 'realy', 'wana', 'go', 'wid', 'u', 'yelow', 'wnt', 'u', 'bck', 'black', 'jealous', 'u', 'brown', 'miss', 'nw', 'plz', 'giv', 'one', 'color']\n",
      "After stemming with porters algorithm: ['pass', 'di', 'contact', 'see', 'wat', 'get', 'red', 'luv', 'wid', 'blue', 'put', 'smile', 'face', 'purpl', 'reali', 'hot', 'pink', 'swt', 'orang', 'thnk', 'lyk', 'green', 'reali', 'wana', 'wid', 'yelow', 'wnt', 'bck', 'black', 'jealou', 'brown', 'miss', 'plz', 'giv', 'on', 'color']\n",
      "Tokenized sentence: ['lmao', 'ok', 'i', 'wont', 'be', 'needing', 'u', 'to', 'do', 'my', 'hair', 'anymore']\n",
      "After stop words removal: ['lmao', 'ok', 'wont', 'needing', 'u', 'hair', 'anymore']\n",
      "need\n",
      "After stemming with porters algorithm: ['lmao', 'wont', 'need', 'hair', 'anymor']\n",
      "Tokenized sentence: ['doesn', 't', 'g', 'have', 'class', 'early', 'tomorrow', 'and', 'thus', 'shouldn', 't', 'be', 'trying', 'to', 'smoke', 'at', 'lt', 'gt']\n",
      "After stop words removal: ['g', 'class', 'early', 'tomorrow', 'thus', 'trying', 'smoke', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['class', 'earli', 'tomorrow', 'thu', 'trying', 'smoke']\n",
      "Tokenized sentence: ['rock', 'yr', 'chik', 'get', 's', 'of', 'filthy', 'films', 'xxx', 'pics', 'on', 'yr', 'phone', 'now', 'rply', 'filth', 'to', 'saristar', 'ltd', 'e', 'yt', 'p', 'per', 'days', 'stop', 'cancel']\n",
      "After stop words removal: ['rock', 'yr', 'chik', 'get', 'filthy', 'films', 'xxx', 'pics', 'yr', 'phone', 'rply', 'filth', 'saristar', 'ltd', 'e', 'yt', 'p', 'per', 'days', 'stop', 'cancel']\n",
      "After stemming with porters algorithm: ['rock', 'chik', 'get', 'filthi', 'film', 'xxx', 'pic', 'phone', 'rply', 'filth', 'saristar', 'ltd', 'per', 'dai', 'stop', 'cancel']\n",
      "Tokenized sentence: ['sms', 'ac', 'jsco', 'energy', 'is', 'high', 'but', 'u', 'may', 'not', 'know', 'where', 'channel', 'it', 'day', 'ur', 'leadership', 'skills', 'r', 'strong', 'psychic', 'reply', 'ans', 'w', 'question', 'end', 'reply', 'end', 'jsco']\n",
      "After stop words removal: ['sms', 'ac', 'jsco', 'energy', 'high', 'u', 'may', 'know', 'channel', 'day', 'ur', 'leadership', 'skills', 'r', 'strong', 'psychic', 'reply', 'ans', 'w', 'question', 'end', 'reply', 'end', 'jsco']\n",
      "After stemming with porters algorithm: ['sm', 'jsco', 'energi', 'high', 'mai', 'know', 'channel', 'dai', 'leadership', 'skill', 'strong', 'psychic', 'repli', 'an', 'quest', 'end', 'repli', 'end', 'jsco']\n",
      "Tokenized sentence: ['it', 'only', 'does', 'simple', 'arithmetic', 'not', 'percentages']\n",
      "After stop words removal: ['simple', 'arithmetic', 'percentages']\n",
      "After stemming with porters algorithm: ['simpl', 'arithmet', 'percentag']\n",
      "Tokenized sentence: ['ahhh', 'work', 'i', 'vaguely', 'remember', 'that', 'what', 'does', 'it', 'feel', 'like', 'lol']\n",
      "After stop words removal: ['ahhh', 'work', 'vaguely', 'remember', 'feel', 'like', 'lol']\n",
      "After stemming with porters algorithm: ['ahhh', 'work', 'vagu', 'rememb', 'feel', 'like', 'lol']\n",
      "Tokenized sentence: ['sad', 'puppy', 'noise']\n",
      "After stop words removal: ['sad', 'puppy', 'noise']\n",
      "After stemming with porters algorithm: ['sad', 'puppi', 'nois']\n",
      "Tokenized sentence: ['wow', 'you', 're', 'right', 'i', 'didn', 't', 'mean', 'to', 'do', 'that', 'i', 'guess', 'once', 'i', 'gave', 'up', 'on', 'boston', 'men', 'and', 'changed', 'my', 'search', 'location', 'to', 'nyc', 'something', 'changed', 'cuz', 'on', 'my', 'signin', 'page', 'it', 'still', 'says', 'boston']\n",
      "After stop words removal: ['wow', 'right', 'mean', 'guess', 'gave', 'boston', 'men', 'changed', 'search', 'location', 'nyc', 'something', 'changed', 'cuz', 'signin', 'page', 'still', 'says', 'boston']\n",
      "someth\n",
      "After stemming with porters algorithm: ['wow', 'right', 'mean', 'guess', 'gave', 'boston', 'men', 'chan', 'search', 'locat', 'nyc', 'somet', 'chan', 'cuz', 'signin', 'page', 'still', 'sai', 'boston']\n",
      "Tokenized sentence: ['you', 're', 'gonna', 'have', 'to', 'be', 'way', 'more', 'specific', 'than', 'that']\n",
      "After stop words removal: ['gonna', 'way', 'specific']\n",
      "After stemming with porters algorithm: ['gonna', 'wai', 'specif']\n",
      "Tokenized sentence: ['i', 'have', 'to', 'take', 'exam', 'with', 'march']\n",
      "After stop words removal: ['take', 'exam', 'march']\n",
      "After stemming with porters algorithm: ['take', 'exam', 'march']\n",
      "Tokenized sentence: ['win', 'a', 'shopping', 'spree', 'every', 'week', 'starting', 'now', 'play', 'text', 'store', 'to', 'skilgme', 'tscs', 'winawk', 'age', 'perweeksub']\n",
      "After stop words removal: ['win', 'shopping', 'spree', 'every', 'week', 'starting', 'play', 'text', 'store', 'skilgme', 'tscs', 'winawk', 'age', 'perweeksub']\n",
      "shopp\n",
      "start\n",
      "After stemming with porters algorithm: ['win', 'shop', 'spree', 'everi', 'week', 'star', 'plai', 'text', 'store', 'skilgm', 'tsc', 'winawk', 'ag', 'perweeksub']\n",
      "Tokenized sentence: ['ok', 'i', 'will', 'tell', 'her', 'to', 'stay', 'out', 'yeah', 'its', 'been', 'tough', 'but', 'we', 'are', 'optimistic', 'things', 'will', 'improve', 'this', 'month']\n",
      "After stop words removal: ['ok', 'tell', 'stay', 'yeah', 'tough', 'optimistic', 'things', 'improve', 'month']\n",
      "After stemming with porters algorithm: ['tell', 'stai', 'yeah', 'tough', 'optimist', 'thing', 'improv', 'month']\n",
      "Tokenized sentence: ['wish', 'i', 'were', 'with', 'you', 'now']\n",
      "After stop words removal: ['wish']\n",
      "After stemming with porters algorithm: ['wish']\n",
      "Tokenized sentence: ['u', 'r', 'subscribed', 'textcomp', 'wkly', 'comp', 'st', 'wk', 's', 'free', 'question', 'follows', 'subsequent', 'wks', 'charged', 'p', 'msg', 'unsubscribe', 'txt', 'stop', 'custcare']\n",
      "After stop words removal: ['u', 'r', 'subscribed', 'textcomp', 'wkly', 'comp', 'st', 'wk', 'free', 'question', 'follows', 'subsequent', 'wks', 'charged', 'p', 'msg', 'unsubscribe', 'txt', 'stop', 'custcare']\n",
      "After stemming with porters algorithm: ['subscrib', 'textcomp', 'wkly', 'comp', 'free', 'quest', 'follow', 'subsequ', 'wk', 'char', 'msg', 'unsubscrib', 'txt', 'stop', 'custcar']\n",
      "Tokenized sentence: ['ur', 'balance', 'is', 'now', 'next', 'question', 'complete', 'the', 'landmark', 'big', 'a', 'bob', 'b', 'barry', 'or', 'c', 'ben', 'text', 'a', 'b', 'or', 'c', 'to', 'good', 'luck']\n",
      "After stop words removal: ['ur', 'balance', 'next', 'question', 'complete', 'landmark', 'big', 'bob', 'b', 'barry', 'c', 'ben', 'text', 'b', 'c', 'good', 'luck']\n",
      "After stemming with porters algorithm: ['balanc', 'next', 'quest', 'complet', 'landmark', 'big', 'bob', 'barri', 'ben', 'text', 'good', 'luck']\n",
      "Tokenized sentence: ['yes', 'princess', 'i', 'want', 'to', 'please', 'you', 'every', 'night', 'your', 'wish', 'is', 'my', 'command']\n",
      "After stop words removal: ['yes', 'princess', 'want', 'please', 'every', 'night', 'wish', 'command']\n",
      "After stemming with porters algorithm: ['ye', 'princess', 'want', 'pleas', 'everi', 'night', 'wish', 'command']\n",
      "Tokenized sentence: ['that', 's', 'ok', 'i', 'popped', 'in', 'to', 'ask', 'bout', 'something', 'and', 'she', 'said', 'you', 'd', 'been', 'in', 'are', 'you', 'around', 'tonght', 'wen', 'this', 'girl', 'comes']\n",
      "After stop words removal: ['ok', 'popped', 'ask', 'bout', 'something', 'said', 'around', 'tonght', 'wen', 'girl', 'comes']\n",
      "someth\n",
      "After stemming with porters algorithm: ['pop', 'ask', 'bout', 'somet', 'said', 'around', 'tonght', 'wen', 'girl', 'come']\n",
      "Tokenized sentence: ['prabha', 'i', 'm', 'soryda', 'realy', 'frm', 'heart', 'i', 'm', 'sory']\n",
      "After stop words removal: ['prabha', 'soryda', 'realy', 'frm', 'heart', 'sory']\n",
      "After stemming with porters algorithm: ['prabha', 'soryda', 'reali', 'frm', 'heart', 'sori']\n",
      "Tokenized sentence: ['damn', 'can', 'you', 'make', 'it', 'tonight', 'or', 'do', 'you', 'want', 'to', 'just', 'wait', 'til', 'tomorrow']\n",
      "After stop words removal: ['damn', 'make', 'tonight', 'want', 'wait', 'til', 'tomorrow']\n",
      "After stemming with porters algorithm: ['damn', 'make', 'tonight', 'want', 'wait', 'til', 'tomorrow']\n",
      "Tokenized sentence: ['can', 'you', 'pls', 'send', 'me', 'that', 'company', 'name', 'in', 'saibaba', 'colany']\n",
      "After stop words removal: ['pls', 'send', 'company', 'name', 'saibaba', 'colany']\n",
      "After stemming with porters algorithm: ['pl', 'send', 'compani', 'name', 'saibaba', 'colani']\n",
      "Tokenized sentence: ['love', 'it', 'the', 'girls', 'at', 'the', 'office', 'may', 'wonder', 'why', 'you', 'are', 'smiling', 'but', 'sore']\n",
      "After stop words removal: ['love', 'girls', 'office', 'may', 'wonder', 'smiling', 'sore']\n",
      "smil\n",
      "After stemming with porters algorithm: ['love', 'girl', 'offic', 'mai', 'wonder', 'smile', 'sore']\n",
      "Tokenized sentence: ['wa', 'ur', 'openin', 'sentence', 'very', 'formal', 'anyway', 'i', 'm', 'fine', 'too', 'juz', 'tt', 'i', 'm', 'eatin', 'too', 'much', 'n', 'puttin', 'on', 'weight', 'haha', 'so', 'anythin', 'special', 'happened']\n",
      "After stop words removal: ['wa', 'ur', 'openin', 'sentence', 'formal', 'anyway', 'fine', 'juz', 'tt', 'eatin', 'much', 'n', 'puttin', 'weight', 'haha', 'anythin', 'special', 'happened']\n",
      "After stemming with porters algorithm: ['openin', 'sentenc', 'formal', 'anywai', 'fine', 'juz', 'eatin', 'much', 'puttin', 'weight', 'haha', 'anythin', 'special', 'happen']\n",
      "Tokenized sentence: ['actually', 'i', 'decided', 'i', 'was', 'too', 'hungry', 'so', 'i', 'haven', 't', 'left', 'yet', 'v']\n",
      "After stop words removal: ['actually', 'decided', 'hungry', 'left', 'yet', 'v']\n",
      "After stemming with porters algorithm: ['actual', 'decid', 'hungri', 'left', 'yet']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['when', 'can', 'come', 'out']\n",
      "After stop words removal: ['come']\n",
      "After stemming with porters algorithm: ['come']\n",
      "Tokenized sentence: ['well', 'i', 'know', 'z', 'will', 'take', 'care', 'of', 'me', 'so', 'no', 'worries']\n",
      "After stop words removal: ['well', 'know', 'z', 'take', 'care', 'worries']\n",
      "After stemming with porters algorithm: ['well', 'know', 'take', 'care', 'worri']\n",
      "Tokenized sentence: ['tell', 'rob', 'to', 'mack', 'his', 'gf', 'in', 'the', 'theater']\n",
      "After stop words removal: ['tell', 'rob', 'mack', 'gf', 'theater']\n",
      "After stemming with porters algorithm: ['tell', 'rob', 'mack', 'theater']\n",
      "Tokenized sentence: ['will', 'u', 'meet', 'ur', 'dream', 'partner', 'soon', 'is', 'ur', 'career', 'off', 'a', 'flyng', 'start', 'find', 'out', 'free', 'txt', 'horo', 'followed', 'by', 'ur', 'star', 'sign', 'e', 'g', 'horo', 'aries']\n",
      "After stop words removal: ['u', 'meet', 'ur', 'dream', 'partner', 'soon', 'ur', 'career', 'flyng', 'start', 'find', 'free', 'txt', 'horo', 'followed', 'ur', 'star', 'sign', 'e', 'g', 'horo', 'aries']\n",
      "After stemming with porters algorithm: ['meet', 'dream', 'partner', 'soon', 'career', 'flyng', 'start', 'find', 'free', 'txt', 'horo', 'follow', 'star', 'sign', 'horo', 'ari']\n",
      "Tokenized sentence: ['uncle', 'g', 'just', 'checking', 'up', 'on', 'you', 'do', 'have', 'a', 'rewarding', 'month']\n",
      "After stop words removal: ['uncle', 'g', 'checking', 'rewarding', 'month']\n",
      "check\n",
      "reward\n",
      "After stemming with porters algorithm: ['uncl', 'chec', 'rewar', 'month']\n",
      "Tokenized sentence: ['aiyar', 'hard', 'type', 'u', 'later', 'free', 'then', 'tell', 'me', 'then', 'i', 'call', 'n', 'scold', 'n', 'tell', 'u']\n",
      "After stop words removal: ['aiyar', 'hard', 'type', 'u', 'later', 'free', 'tell', 'call', 'n', 'scold', 'n', 'tell', 'u']\n",
      "After stemming with porters algorithm: ['aiyar', 'hard', 'type', 'later', 'free', 'tell', 'call', 'scold', 'tell']\n",
      "Tokenized sentence: ['good', 'morning', 'princess', 'happy', 'new', 'year']\n",
      "After stop words removal: ['good', 'morning', 'princess', 'happy', 'new', 'year']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'princess', 'happi', 'new', 'year']\n",
      "Tokenized sentence: ['honeybee', 'said', 'i', 'm', 'd', 'sweetest', 'in', 'd', 'world', 'god', 'laughed', 'amp', 'said', 'wait', 'u', 'havnt', 'met', 'd', 'person', 'reading', 'this', 'msg', 'moral', 'even', 'god', 'can', 'crack', 'jokes', 'gm', 'gn', 'ge', 'gn']\n",
      "After stop words removal: ['honeybee', 'said', 'sweetest', 'world', 'god', 'laughed', 'amp', 'said', 'wait', 'u', 'havnt', 'met', 'person', 'reading', 'msg', 'moral', 'even', 'god', 'crack', 'jokes', 'gm', 'gn', 'ge', 'gn']\n",
      "read\n",
      "After stemming with porters algorithm: ['honeybe', 'said', 'sweetest', 'world', 'god', 'laug', 'amp', 'said', 'wait', 'havnt', 'met', 'person', 'read', 'msg', 'moral', 'even', 'god', 'crack', 'joke']\n",
      "Tokenized sentence: ['what', 'u', 'talking', 'bout', 'early', 'morning', 'it', 's', 'almost', 'noon', 'where', 'your', 'at']\n",
      "After stop words removal: ['u', 'talking', 'bout', 'early', 'morning', 'almost', 'noon']\n",
      "talk\n",
      "morn\n",
      "After stemming with porters algorithm: ['tal', 'bout', 'earli', 'mor', 'almost', 'noon']\n",
      "Tokenized sentence: ['can', 'i', 'm', 'free']\n",
      "After stop words removal: ['free']\n",
      "After stemming with porters algorithm: ['free']\n",
      "Tokenized sentence: ['i', 'sent', 'them', 'do', 'you', 'like']\n",
      "After stop words removal: ['sent', 'like']\n",
      "After stemming with porters algorithm: ['sent', 'like']\n",
      "Tokenized sentence: ['probably', 'money', 'worries', 'things', 'are', 'coming', 'due', 'and', 'i', 'have', 'several', 'outstanding', 'invoices', 'for', 'work', 'i', 'did', 'two', 'and', 'three', 'months', 'ago']\n",
      "After stop words removal: ['probably', 'money', 'worries', 'things', 'coming', 'due', 'several', 'outstanding', 'invoices', 'work', 'two', 'three', 'months', 'ago']\n",
      "com\n",
      "outstand\n",
      "After stemming with porters algorithm: ['probab', 'monei', 'worri', 'thing', 'come', 'due', 'sever', 'outstan', 'invoic', 'work', 'two', 'three', 'month', 'ago']\n",
      "Tokenized sentence: ['can', 'you', 'talk', 'with', 'me']\n",
      "After stop words removal: ['talk']\n",
      "After stemming with porters algorithm: ['talk']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'call', 'you', 're', 'your', 'reply', 'to', 'our', 'sms', 'for', 'a', 'video', 'mobile', 'mins', 'unlimited', 'text', 'free', 'camcorder', 'reply', 'of', 'call', 'now']\n",
      "After stop words removal: ['tried', 'call', 'reply', 'sms', 'video', 'mobile', 'mins', 'unlimited', 'text', 'free', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['tri', 'call', 'repli', 'sm', 'video', 'mobil', 'min', 'unlimit', 'text', 'free', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['im', 'done', 'just', 'studyn', 'in', 'library']\n",
      "After stop words removal: ['im', 'done', 'studyn', 'library']\n",
      "After stemming with porters algorithm: ['done', 'studyn', 'librari']\n",
      "Tokenized sentence: ['hey', 'you', 'gave', 'them', 'your', 'photo', 'when', 'you', 'registered', 'for', 'driving', 'ah', 'tmr', 'wanna', 'meet', 'at', 'yck']\n",
      "After stop words removal: ['hey', 'gave', 'photo', 'registered', 'driving', 'ah', 'tmr', 'wanna', 'meet', 'yck']\n",
      "driv\n",
      "After stemming with porters algorithm: ['hei', 'gave', 'photo', 'regist', 'drive', 'tmr', 'wanna', 'meet', 'yck']\n",
      "Tokenized sentence: ['yeah', 'no', 'probs', 'last', 'night', 'is', 'obviously', 'catching', 'up', 'with', 'you', 'speak', 'soon']\n",
      "After stop words removal: ['yeah', 'probs', 'last', 'night', 'obviously', 'catching', 'speak', 'soon']\n",
      "catch\n",
      "After stemming with porters algorithm: ['yeah', 'prob', 'last', 'night', 'obvious', 'catc', 'speak', 'soon']\n",
      "Tokenized sentence: ['no', 'to', 'be', 'nosy', 'i', 'guess', 'idk', 'am', 'i', 'over', 'reacting', 'if', 'i', 'm', 'freaked']\n",
      "After stop words removal: ['nosy', 'guess', 'idk', 'reacting', 'freaked']\n",
      "react\n",
      "After stemming with porters algorithm: ['nosi', 'guess', 'idk', 'reac', 'freak']\n",
      "Tokenized sentence: ['yar', 'lor', 'how', 'u', 'noe', 'u', 'used', 'dat', 'route', 'too']\n",
      "After stop words removal: ['yar', 'lor', 'u', 'noe', 'u', 'used', 'dat', 'route']\n",
      "After stemming with porters algorithm: ['yar', 'lor', 'noe', 'us', 'dat', 'rout']\n",
      "Tokenized sentence: ['are', 'you', 'driving', 'or', 'training']\n",
      "After stop words removal: ['driving', 'training']\n",
      "driv\n",
      "train\n",
      "After stemming with porters algorithm: ['drive', 'train']\n",
      "Tokenized sentence: ['nice', 'wait', 'should', 'you', 'be', 'texting', 'right', 'now', 'i', 'm', 'not', 'gonna', 'pay', 'your', 'ticket', 'ya', 'know']\n",
      "After stop words removal: ['nice', 'wait', 'texting', 'right', 'gonna', 'pay', 'ticket', 'ya', 'know']\n",
      "text\n",
      "After stemming with porters algorithm: ['nice', 'wait', 'tex', 'right', 'gonna', 'pai', 'ticket', 'know']\n",
      "Tokenized sentence: ['i', 'agree', 'so', 'i', 'can', 'stop', 'thinkin', 'about', 'ipad', 'can', 'you', 'please', 'ask', 'macho', 'the', 'same', 'question']\n",
      "After stop words removal: ['agree', 'stop', 'thinkin', 'ipad', 'please', 'ask', 'macho', 'question']\n",
      "After stemming with porters algorithm: ['agre', 'stop', 'thinkin', 'ipad', 'pleas', 'ask', 'macho', 'quest']\n",
      "Tokenized sentence: ['should', 'i', 'have', 'picked', 'up', 'a', 'receipt', 'or', 'something', 'earlier']\n",
      "After stop words removal: ['picked', 'receipt', 'something', 'earlier']\n",
      "someth\n",
      "After stemming with porters algorithm: ['pic', 'receipt', 'somet', 'earlier']\n",
      "Tokenized sentence: ['macha', 'dont', 'feel', 'upset', 'i', 'can', 'assume', 'your', 'mindset', 'believe', 'me', 'one', 'evening', 'with', 'me', 'and', 'i', 'have', 'some', 'wonderful', 'plans', 'for', 'both', 'of', 'us', 'let', 'life', 'begin', 'again', 'call', 'me', 'anytime']\n",
      "After stop words removal: ['macha', 'dont', 'feel', 'upset', 'assume', 'mindset', 'believe', 'one', 'evening', 'wonderful', 'plans', 'us', 'let', 'life', 'begin', 'call', 'anytime']\n",
      "even\n",
      "After stemming with porters algorithm: ['macha', 'dont', 'feel', 'upset', 'assum', 'mindset', 'believ', 'on', 'even', 'wonder', 'plan', 'let', 'life', 'begin', 'call', 'anytim']\n",
      "Tokenized sentence: ['call', 'germany', 'for', 'only', 'pence', 'per', 'minute', 'call', 'from', 'a', 'fixed', 'line', 'via', 'access', 'number', 'no', 'prepayment', 'direct', 'access']\n",
      "After stop words removal: ['call', 'germany', 'pence', 'per', 'minute', 'call', 'fixed', 'line', 'via', 'access', 'number', 'prepayment', 'direct', 'access']\n",
      "After stemming with porters algorithm: ['call', 'germani', 'penc', 'per', 'minut', 'call', 'fix', 'line', 'via', 'access', 'number', 'prepay', 'direct', 'access']\n",
      "Tokenized sentence: ['are', 'you', 'comingdown', 'later']\n",
      "After stop words removal: ['comingdown', 'later']\n",
      "After stemming with porters algorithm: ['comingdown', 'later']\n",
      "Tokenized sentence: ['hows', 'the', 'pain', 'dear', 'y', 'r', 'u', 'smiling']\n",
      "After stop words removal: ['hows', 'pain', 'dear', 'r', 'u', 'smiling']\n",
      "smil\n",
      "After stemming with porters algorithm: ['how', 'pain', 'dear', 'smile']\n",
      "Tokenized sentence: ['there', 'is', 'a', 'first', 'time', 'for', 'everything']\n",
      "After stop words removal: ['first', 'time', 'everything']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['first', 'time', 'everyt']\n",
      "Tokenized sentence: ['k', 'text', 'me', 'when', 'you', 're', 'on', 'the', 'way']\n",
      "After stop words removal: ['k', 'text', 'way']\n",
      "After stemming with porters algorithm: ['text', 'wai']\n",
      "Tokenized sentence: ['double', 'eviction', 'this', 'week', 'spiral', 'and', 'michael', 'and', 'good', 'riddance', 'to', 'them']\n",
      "After stop words removal: ['double', 'eviction', 'week', 'spiral', 'michael', 'good', 'riddance']\n",
      "After stemming with porters algorithm: ['doubl', 'evict', 'week', 'spiral', 'michael', 'good', 'riddanc']\n",
      "Tokenized sentence: ['called', 'dad', 'oredi']\n",
      "After stop words removal: ['called', 'dad', 'oredi']\n",
      "After stemming with porters algorithm: ['call', 'dad', 'oredi']\n",
      "Tokenized sentence: ['ur', 'ringtone', 'service', 'has', 'changed', 'free', 'credits', 'go', 'to', 'club', 'mobiles', 'com', 'to', 'choose', 'content', 'now', 'stop', 'txt', 'club', 'stop', 'to', 'p', 'wk', 'club', 'po', 'box', 'mk', 'wt']\n",
      "After stop words removal: ['ur', 'ringtone', 'service', 'changed', 'free', 'credits', 'go', 'club', 'mobiles', 'com', 'choose', 'content', 'stop', 'txt', 'club', 'stop', 'p', 'wk', 'club', 'po', 'box', 'mk', 'wt']\n",
      "After stemming with porters algorithm: ['rington', 'servic', 'chan', 'free', 'credit', 'club', 'mobil', 'com', 'choos', 'content', 'stop', 'txt', 'club', 'stop', 'club', 'box']\n",
      "Tokenized sentence: ['hi', 'jon', 'pete', 'here', 'ive', 'bin', 'spain', 'recently', 'hav', 'sum', 'dinero', 'left', 'bill', 'said', 'u', 'or', 'ur', 'rents', 'mayb', 'interested', 'in', 'it', 'i', 'hav', 'pes', 'so', 'around', 'tb', 'james']\n",
      "After stop words removal: ['hi', 'jon', 'pete', 'ive', 'bin', 'spain', 'recently', 'hav', 'sum', 'dinero', 'left', 'bill', 'said', 'u', 'ur', 'rents', 'mayb', 'interested', 'hav', 'pes', 'around', 'tb', 'james']\n",
      "After stemming with porters algorithm: ['jon', 'pete', 'iv', 'bin', 'spain', 'recent', 'hav', 'sum', 'dinero', 'left', 'bill', 'said', 'rent', 'mayb', 'interes', 'hav', 'pe', 'around', 'jame']\n",
      "Tokenized sentence: ['hi', 'frnd', 'which', 'is', 'best', 'way', 'to', 'avoid', 'missunderstding', 'wit', 'our', 'beloved', 'one', 's']\n",
      "After stop words removal: ['hi', 'frnd', 'best', 'way', 'avoid', 'missunderstding', 'wit', 'beloved', 'one']\n",
      "missunderstd\n",
      "After stemming with porters algorithm: ['frnd', 'best', 'wai', 'avoid', 'missunderst', 'wit', 'belov', 'on']\n",
      "Tokenized sentence: ['helloooo', 'wake', 'up', 'sweet', 'morning', 'welcomes', 'you', 'enjoy', 'this', 'day', 'with', 'full', 'of', 'joy', 'gud', 'mrng']\n",
      "After stop words removal: ['helloooo', 'wake', 'sweet', 'morning', 'welcomes', 'enjoy', 'day', 'full', 'joy', 'gud', 'mrng']\n",
      "morn\n",
      "After stemming with porters algorithm: ['helloooo', 'wake', 'sweet', 'mor', 'welcom', 'enjoi', 'dai', 'full', 'joi', 'gud', 'mrng']\n",
      "Tokenized sentence: ['sorry', 'da', 'i', 'gone', 'mad', 'so', 'many', 'pending', 'works', 'what', 'to', 'do']\n",
      "After stop words removal: ['sorry', 'da', 'gone', 'mad', 'many', 'pending', 'works']\n",
      "pend\n",
      "After stemming with porters algorithm: ['sorri', 'gone', 'mad', 'mani', 'pen', 'work']\n",
      "Tokenized sentence: ['u', 'need', 'my', 'presnts', 'always', 'bcz', 'u', 'cant', 'mis', 'love', 'jeevithathile', 'irulinae', 'neekunna', 'prakasamanu', 'sneham', 'prakasam', 'ennal', 'prabha', 'that', 'mns', 'prabha', 'is', 'love', 'got', 'it', 'dont', 'mis', 'me']\n",
      "After stop words removal: ['u', 'need', 'presnts', 'always', 'bcz', 'u', 'cant', 'mis', 'love', 'jeevithathile', 'irulinae', 'neekunna', 'prakasamanu', 'sneham', 'prakasam', 'ennal', 'prabha', 'mns', 'prabha', 'love', 'got', 'dont', 'mis']\n",
      "After stemming with porters algorithm: ['need', 'presnt', 'alwai', 'bcz', 'cant', 'mi', 'love', 'jeevithathil', 'irulina', 'neekunna', 'prakasamanu', 'sneham', 'prakasam', 'ennal', 'prabha', 'mn', 'prabha', 'love', 'got', 'dont', 'mi']\n",
      "Tokenized sentence: ['he', 'will', 'you', 'guys', 'close']\n",
      "After stop words removal: ['guys', 'close']\n",
      "After stemming with porters algorithm: ['gui', 'close']\n",
      "Tokenized sentence: ['congrats', 'kano', 'whr', 's', 'the', 'treat', 'maga']\n",
      "After stop words removal: ['congrats', 'kano', 'whr', 'treat', 'maga']\n",
      "After stemming with porters algorithm: ['congrat', 'kano', 'whr', 'treat', 'maga']\n",
      "Tokenized sentence: ['he', 'like', 'not', 'v', 'shock', 'leh', 'cos', 'telling', 'shuhui', 'is', 'like', 'telling', 'leona', 'also', 'like', 'dat', 'almost', 'all', 'know', 'liao', 'he', 'got', 'ask', 'me', 'abt', 'ur', 'reaction', 'lor']\n",
      "After stop words removal: ['like', 'v', 'shock', 'leh', 'cos', 'telling', 'shuhui', 'like', 'telling', 'leona', 'also', 'like', 'dat', 'almost', 'know', 'liao', 'got', 'ask', 'abt', 'ur', 'reaction', 'lor']\n",
      "tell\n",
      "tell\n",
      "After stemming with porters algorithm: ['like', 'shock', 'leh', 'co', 'tell', 'shuhui', 'like', 'tell', 'leona', 'also', 'like', 'dat', 'almost', 'know', 'liao', 'got', 'ask', 'abt', 'react', 'lor']\n",
      "Tokenized sentence: ['that', 'would', 'be', 'great', 'we', 'll', 'be', 'at', 'the', 'guild', 'could', 'meet', 'on', 'bristol', 'road', 'or', 'somewhere', 'will', 'get', 'in', 'touch', 'over', 'weekend', 'our', 'plans', 'take', 'flight', 'have', 'a', 'good', 'week']\n",
      "After stop words removal: ['would', 'great', 'guild', 'could', 'meet', 'bristol', 'road', 'somewhere', 'get', 'touch', 'weekend', 'plans', 'take', 'flight', 'good', 'week']\n",
      "After stemming with porters algorithm: ['would', 'great', 'guild', 'could', 'meet', 'bristol', 'road', 'somewher', 'get', 'touch', 'weekend', 'plan', 'take', 'flight', 'good', 'week']\n",
      "Tokenized sentence: ['how', 'come', 'u', 'got', 'nothing', 'to', 'do']\n",
      "After stop words removal: ['come', 'u', 'got', 'nothing']\n",
      "noth\n",
      "After stemming with porters algorithm: ['come', 'got', 'not']\n",
      "Tokenized sentence: ['sounds', 'good', 'keep', 'me', 'posted']\n",
      "After stop words removal: ['sounds', 'good', 'keep', 'posted']\n",
      "After stemming with porters algorithm: ['sound', 'good', 'keep', 'pos']\n",
      "Tokenized sentence: ['never', 'try', 'alone', 'to', 'take', 'the', 'weight', 'of', 'a', 'tear', 'that', 'comes', 'out', 'of', 'ur', 'heart', 'and', 'falls', 'through', 'ur', 'eyes', 'always', 'remember', 'a', 'stupid', 'friend', 'is', 'here', 'to', 'share', 'bslvyl']\n",
      "After stop words removal: ['never', 'try', 'alone', 'take', 'weight', 'tear', 'comes', 'ur', 'heart', 'falls', 'ur', 'eyes', 'always', 'remember', 'stupid', 'friend', 'share', 'bslvyl']\n",
      "After stemming with porters algorithm: ['never', 'try', 'alon', 'take', 'weight', 'tear', 'come', 'heart', 'fall', 'ey', 'alwai', 'rememb', 'stupid', 'friend', 'share', 'bslvyl']\n",
      "Tokenized sentence: ['no', 'polyphonic', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'pt', 'to', 'st', 'tone', 'free', 'so', 'get', 'txtin', 'now', 'and', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stop words removal: ['polyphonic', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'pt', 'st', 'tone', 'free', 'get', 'txtin', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stemming with porters algorithm: ['polyphon', 'tone', 'mob', 'everi', 'week', 'txt', 'tone', 'free', 'get', 'txtin', 'tell', 'friend', 'tone', 'repli', 'info']\n",
      "Tokenized sentence: ['hi', 'im', 'having', 'the', 'most', 'relaxing', 'time', 'ever', 'we', 'have', 'to', 'get', 'up', 'at', 'am', 'every', 'day', 'was', 'the', 'party', 'good', 'the', 'other', 'night', 'i', 'get', 'home', 'tomorrow', 'at', 'ish']\n",
      "After stop words removal: ['hi', 'im', 'relaxing', 'time', 'ever', 'get', 'every', 'day', 'party', 'good', 'night', 'get', 'home', 'tomorrow', 'ish']\n",
      "relax\n",
      "After stemming with porters algorithm: ['relax', 'time', 'ever', 'get', 'everi', 'dai', 'parti', 'good', 'night', 'get', 'home', 'tomorrow', 'ish']\n",
      "Tokenized sentence: ['arngd', 'marriage', 'is', 'while', 'u', 'r', 'walkin', 'unfortuntly', 'a', 'snake', 'bites', 'u', 'bt', 'love', 'marriage', 'is', 'dancing', 'in', 'frnt', 'of', 'd', 'snake', 'amp', 'sayin', 'bite', 'me', 'bite', 'me']\n",
      "After stop words removal: ['arngd', 'marriage', 'u', 'r', 'walkin', 'unfortuntly', 'snake', 'bites', 'u', 'bt', 'love', 'marriage', 'dancing', 'frnt', 'snake', 'amp', 'sayin', 'bite', 'bite']\n",
      "danc\n",
      "After stemming with porters algorithm: ['arngd', 'marriag', 'walkin', 'unfortuntli', 'snake', 'bite', 'love', 'marriag', 'dan', 'frnt', 'snake', 'amp', 'sayin', 'bite', 'bite']\n",
      "Tokenized sentence: ['you', 'intrepid', 'duo', 'you', 'have', 'a', 'great', 'time', 'and', 'see', 'you', 'both', 'soon']\n",
      "After stop words removal: ['intrepid', 'duo', 'great', 'time', 'see', 'soon']\n",
      "After stemming with porters algorithm: ['intrepid', 'duo', 'great', 'time', 'see', 'soon']\n",
      "Tokenized sentence: ['reply', 'with', 'your', 'name', 'and', 'address', 'and', 'you', 'will', 'receive', 'by', 'post', 'a', 'weeks', 'completely', 'free', 'accommodation', 'at', 'various', 'global', 'locations', 'www', 'phb', 'com', 'ph', 'p']\n",
      "After stop words removal: ['reply', 'name', 'address', 'receive', 'post', 'weeks', 'completely', 'free', 'accommodation', 'various', 'global', 'locations', 'www', 'phb', 'com', 'ph', 'p']\n",
      "After stemming with porters algorithm: ['repli', 'name', 'address', 'receiv', 'post', 'week', 'complet', 'free', 'accommod', 'variou', 'global', 'locat', 'www', 'phb', 'com']\n",
      "Tokenized sentence: ['tone', 'club', 'your', 'subs', 'has', 'now', 'expired', 're', 'sub', 'reply', 'monoc', 'monos', 'or', 'polyc', 'polys', 'weekly', 'p', 'per', 'week', 'txt', 'stop', 'stop', 'this', 'msg', 'free', 'stream']\n",
      "After stop words removal: ['tone', 'club', 'subs', 'expired', 'sub', 'reply', 'monoc', 'monos', 'polyc', 'polys', 'weekly', 'p', 'per', 'week', 'txt', 'stop', 'stop', 'msg', 'free', 'stream']\n",
      "After stemming with porters algorithm: ['tone', 'club', 'sub', 'expir', 'sub', 'repli', 'monoc', 'mono', 'polyc', 'poli', 'weekli', 'per', 'week', 'txt', 'stop', 'stop', 'msg', 'free', 'stream']\n",
      "Tokenized sentence: ['hi', 'this', 'is', 'roger', 'from', 'cl', 'how', 'are', 'you']\n",
      "After stop words removal: ['hi', 'roger', 'cl']\n",
      "After stemming with porters algorithm: ['roger']\n",
      "Tokenized sentence: ['hey', 'thk', 'we', 'juz', 'go', 'accordin', 'to', 'wat', 'we', 'discussed', 'yest', 'lor', 'except', 'no', 'kb', 'on', 'sun', 'cos', 'there', 's', 'nt', 'much', 'lesson', 'to', 'go', 'if', 'we', 'attend', 'kb', 'on', 'sat']\n",
      "After stop words removal: ['hey', 'thk', 'juz', 'go', 'accordin', 'wat', 'discussed', 'yest', 'lor', 'except', 'kb', 'sun', 'cos', 'nt', 'much', 'lesson', 'go', 'attend', 'kb', 'sat']\n",
      "After stemming with porters algorithm: ['hei', 'thk', 'juz', 'accordin', 'wat', 'discuss', 'yest', 'lor', 'except', 'sun', 'co', 'much', 'lesson', 'attend', 'sat']\n",
      "Tokenized sentence: ['sleeping', 'nt', 'feeling', 'well']\n",
      "After stop words removal: ['sleeping', 'nt', 'feeling', 'well']\n",
      "sleep\n",
      "feel\n",
      "After stemming with porters algorithm: ['sleep', 'feel', 'well']\n",
      "Tokenized sentence: ['why', 'did', 'i', 'wake', 'up', 'on', 'my', 'own', 'gt']\n",
      "After stop words removal: ['wake', 'gt']\n",
      "After stemming with porters algorithm: ['wake']\n",
      "Tokenized sentence: ['i', 'see', 'the', 'letter', 'b', 'on', 'my', 'car']\n",
      "After stop words removal: ['see', 'letter', 'b', 'car']\n",
      "After stemming with porters algorithm: ['see', 'letter', 'car']\n",
      "Tokenized sentence: ['sorry', 'about', 'that', 'this', 'is', 'my', 'mates', 'phone', 'and', 'i', 'didnt', 'write', 'it', 'love', 'kate']\n",
      "After stop words removal: ['sorry', 'mates', 'phone', 'didnt', 'write', 'love', 'kate']\n",
      "After stemming with porters algorithm: ['sorri', 'mate', 'phone', 'didnt', 'write', 'love', 'kate']\n",
      "Tokenized sentence: ['otherwise', 'had', 'part', 'time', 'job', 'na', 'tuition']\n",
      "After stop words removal: ['otherwise', 'part', 'time', 'job', 'na', 'tuition']\n",
      "After stemming with porters algorithm: ['otherwis', 'part', 'time', 'job', 'tuit']\n",
      "Tokenized sentence: ['u', 'can', 'call', 'me', 'now']\n",
      "After stop words removal: ['u', 'call']\n",
      "After stemming with porters algorithm: ['call']\n",
      "Tokenized sentence: ['lol', 'your', 'always', 'so', 'convincing']\n",
      "After stop words removal: ['lol', 'always', 'convincing']\n",
      "convinc\n",
      "After stemming with porters algorithm: ['lol', 'alwai', 'convin']\n",
      "Tokenized sentence: ['leave', 'it', 'de', 'start', 'prepare', 'for', 'next']\n",
      "After stop words removal: ['leave', 'de', 'start', 'prepare', 'next']\n",
      "After stemming with porters algorithm: ['leav', 'start', 'prepar', 'next']\n",
      "Tokenized sentence: ['buy', 'one', 'egg', 'for', 'me', 'da', 'please']\n",
      "After stop words removal: ['buy', 'one', 'egg', 'da', 'please']\n",
      "After stemming with porters algorithm: ['bui', 'on', 'egg', 'pleas']\n",
      "Tokenized sentence: ['alex', 'says', 'he', 's', 'not', 'ok', 'with', 'you', 'not', 'being', 'ok', 'with', 'it']\n",
      "After stop words removal: ['alex', 'says', 'ok', 'ok']\n",
      "After stemming with porters algorithm: ['alex', 'sai']\n",
      "Tokenized sentence: ['hey', 'are', 'we', 'going', 'for', 'the', 'lo', 'lesson', 'or', 'gym']\n",
      "After stop words removal: ['hey', 'going', 'lo', 'lesson', 'gym']\n",
      "go\n",
      "After stemming with porters algorithm: ['hei', 'go', 'lesson', 'gym']\n",
      "Tokenized sentence: ['what', 'to', 'think', 'no', 'one', 'saying', 'clearly', 'ok', 'leave', 'no', 'need', 'to', 'ask', 'her', 'i', 'will', 'go', 'if', 'she', 'come', 'or', 'not']\n",
      "After stop words removal: ['think', 'one', 'saying', 'clearly', 'ok', 'leave', 'need', 'ask', 'go', 'come']\n",
      "say\n",
      "After stemming with porters algorithm: ['think', 'on', 'sai', 'clearli', 'leav', 'need', 'ask', 'come']\n",
      "Tokenized sentence: ['do', 'you', 'ever', 'notice', 'that', 'when', 'you', 're', 'driving', 'anyone', 'going', 'slower', 'than', 'you', 'is', 'an', 'idiot', 'and', 'everyone', 'driving', 'faster', 'than', 'you', 'is', 'a', 'maniac']\n",
      "After stop words removal: ['ever', 'notice', 'driving', 'anyone', 'going', 'slower', 'idiot', 'everyone', 'driving', 'faster', 'maniac']\n",
      "driv\n",
      "go\n",
      "driv\n",
      "After stemming with porters algorithm: ['ever', 'notic', 'drive', 'anyon', 'go', 'slower', 'idiot', 'everyon', 'drive', 'faster', 'maniac']\n",
      "Tokenized sentence: ['hi', 'i', 'got', 'the', 'money', 'da']\n",
      "After stop words removal: ['hi', 'got', 'money', 'da']\n",
      "After stemming with porters algorithm: ['got', 'monei']\n",
      "Tokenized sentence: ['you', 'call', 'him', 'and', 'tell', 'now', 'infront', 'of', 'them', 'call', 'him', 'now']\n",
      "After stop words removal: ['call', 'tell', 'infront', 'call']\n",
      "After stemming with porters algorithm: ['call', 'tell', 'infront', 'call']\n",
      "Tokenized sentence: ['me', 'hungry', 'buy', 'some', 'food', 'good', 'lei', 'but', 'mum', 'n', 'yun', 'dun', 'wan', 'juz', 'buy', 'a', 'little', 'bit']\n",
      "After stop words removal: ['hungry', 'buy', 'food', 'good', 'lei', 'mum', 'n', 'yun', 'dun', 'wan', 'juz', 'buy', 'little', 'bit']\n",
      "After stemming with porters algorithm: ['hungri', 'bui', 'food', 'good', 'lei', 'mum', 'yun', 'dun', 'wan', 'juz', 'bui', 'littl', 'bit']\n",
      "Tokenized sentence: ['i', 'luv', 'u', 'soo', 'much', 'u', 'don', 't', 'understand', 'how', 'special', 'u', 'r', 'me', 'ring', 'u', 'morrow', 'luv', 'u', 'xxx']\n",
      "After stop words removal: ['luv', 'u', 'soo', 'much', 'u', 'understand', 'special', 'u', 'r', 'ring', 'u', 'morrow', 'luv', 'u', 'xxx']\n",
      "After stemming with porters algorithm: ['luv', 'soo', 'much', 'understand', 'special', 'ring', 'morrow', 'luv', 'xxx']\n",
      "Tokenized sentence: ['a', 'swt', 'thought', 'nver', 'get', 'tired', 'of', 'doing', 'little', 'things', 'lovable', 'persons', 'coz', 'somtimes', 'those', 'little', 'things', 'occupy', 'd', 'biggest', 'part', 'in', 'their', 'hearts', 'gud', 'ni']\n",
      "After stop words removal: ['swt', 'thought', 'nver', 'get', 'tired', 'little', 'things', 'lovable', 'persons', 'coz', 'somtimes', 'little', 'things', 'occupy', 'biggest', 'part', 'hearts', 'gud', 'ni']\n",
      "After stemming with porters algorithm: ['swt', 'thought', 'nver', 'get', 'tire', 'littl', 'thing', 'lovab', 'person', 'coz', 'somtim', 'littl', 'thing', 'occupi', 'biggest', 'part', 'heart', 'gud']\n",
      "Tokenized sentence: ['big', 'brother', 's', 'really', 'scraped', 'the', 'barrel', 'with', 'this', 'shower', 'of', 'social', 'misfits']\n",
      "After stop words removal: ['big', 'brother', 'really', 'scraped', 'barrel', 'shower', 'social', 'misfits']\n",
      "After stemming with porters algorithm: ['big', 'brother', 'realli', 'scrape', 'barrel', 'shower', 'social', 'misfit']\n",
      "Tokenized sentence: ['ok', 'i', 'll', 'do', 'you', 'right', 'later']\n",
      "After stop words removal: ['ok', 'right', 'later']\n",
      "After stemming with porters algorithm: ['right', 'later']\n",
      "Tokenized sentence: ['yeah', 'probably', 'i', 'still', 'gotta', 'check', 'out', 'with', 'leo']\n",
      "After stop words removal: ['yeah', 'probably', 'still', 'gotta', 'check', 'leo']\n",
      "After stemming with porters algorithm: ['yeah', 'probab', 'still', 'gotta', 'check', 'leo']\n",
      "Tokenized sentence: ['i', 'm', 'not', 'driving', 'raining', 'then', 'i', 'll', 'get', 'caught', 'at', 'e', 'mrt', 'station', 'lor']\n",
      "After stop words removal: ['driving', 'raining', 'get', 'caught', 'e', 'mrt', 'station', 'lor']\n",
      "driv\n",
      "rain\n",
      "After stemming with porters algorithm: ['drive', 'rain', 'get', 'caught', 'mrt', 'stat', 'lor']\n",
      "Tokenized sentence: ['what', 'part', 'of', 'don', 't', 'initiate', 'don', 't', 'you', 'understand']\n",
      "After stop words removal: ['part', 'initiate', 'understand']\n",
      "After stemming with porters algorithm: ['part', 'initi', 'understand']\n",
      "Tokenized sentence: ['thanx', 'e', 'brownie', 'it', 's', 'v', 'nice']\n",
      "After stop words removal: ['thanx', 'e', 'brownie', 'v', 'nice']\n",
      "After stemming with porters algorithm: ['thanx', 'browni', 'nice']\n",
      "Tokenized sentence: ['u', 'wake', 'up', 'already', 'thanx', 'e', 'tau', 'sar', 'piah', 'it', 's', 'quite', 'nice']\n",
      "After stop words removal: ['u', 'wake', 'already', 'thanx', 'e', 'tau', 'sar', 'piah', 'quite', 'nice']\n",
      "After stemming with porters algorithm: ['wake', 'alreadi', 'thanx', 'tau', 'sar', 'piah', 'quit', 'nice']\n",
      "Tokenized sentence: ['i', 'can', 'send', 'you', 'a', 'pic', 'if', 'you', 'like']\n",
      "After stop words removal: ['send', 'pic', 'like']\n",
      "After stemming with porters algorithm: ['send', 'pic', 'like']\n",
      "Tokenized sentence: ['i', 'need', 'details', 'about', 'that', 'online', 'job']\n",
      "After stop words removal: ['need', 'details', 'online', 'job']\n",
      "After stemming with porters algorithm: ['need', 'detail', 'onlin', 'job']\n",
      "Tokenized sentence: ['ok', 'i', 'juz', 'receive']\n",
      "After stop words removal: ['ok', 'juz', 'receive']\n",
      "After stemming with porters algorithm: ['juz', 'receiv']\n",
      "Tokenized sentence: ['sending', 'you', 'greetings', 'of', 'joy', 'and', 'happiness', 'do', 'have', 'a', 'gr', 'evening']\n",
      "After stop words removal: ['sending', 'greetings', 'joy', 'happiness', 'gr', 'evening']\n",
      "send\n",
      "greet\n",
      "even\n",
      "After stemming with porters algorithm: ['sen', 'greet', 'joi', 'happi', 'even']\n",
      "Tokenized sentence: ['hi', 'i', 'm', 'sorry', 'i', 'missed', 'your', 'call', 'can', 'you', 'pls', 'call', 'back']\n",
      "After stop words removal: ['hi', 'sorry', 'missed', 'call', 'pls', 'call', 'back']\n",
      "After stemming with porters algorithm: ['sorri', 'miss', 'call', 'pl', 'call', 'back']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'hospital', 'da', 'i', 'will', 'return', 'home', 'in', 'evening']\n",
      "After stop words removal: ['hospital', 'da', 'return', 'home', 'evening']\n",
      "even\n",
      "After stemming with porters algorithm: ['hospit', 'return', 'home', 'even']\n",
      "Tokenized sentence: ['free', 'entry', 'into', 'our', 'weekly', 'comp', 'just', 'send', 'the', 'word', 'win', 'to', 'now', 't', 'c', 'www', 'txttowin', 'co', 'uk']\n",
      "After stop words removal: ['free', 'entry', 'weekly', 'comp', 'send', 'word', 'win', 'c', 'www', 'txttowin', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'weekli', 'comp', 'send', 'word', 'win', 'www', 'txttowin']\n",
      "Tokenized sentence: ['fyi', 'i', 'm', 'at', 'usf', 'now', 'swing', 'by', 'the', 'room', 'whenever']\n",
      "After stop words removal: ['fyi', 'usf', 'swing', 'room', 'whenever']\n",
      "After stemming with porters algorithm: ['fyi', 'usf', 'swing', 'room', 'whenev']\n",
      "Tokenized sentence: ['have', 'you', 'started', 'in', 'skye']\n",
      "After stop words removal: ['started', 'skye']\n",
      "After stemming with porters algorithm: ['star', 'skye']\n",
      "Tokenized sentence: ['oh', 'howda', 'gud', 'gud', 'mathe', 'en', 'samachara', 'chikku']\n",
      "After stop words removal: ['oh', 'howda', 'gud', 'gud', 'mathe', 'en', 'samachara', 'chikku']\n",
      "After stemming with porters algorithm: ['howda', 'gud', 'gud', 'math', 'samachara', 'chikku']\n",
      "Tokenized sentence: ['nope', 'i', 'll', 'come', 'online', 'now']\n",
      "After stop words removal: ['nope', 'come', 'online']\n",
      "After stemming with porters algorithm: ['nope', 'come', 'onlin']\n",
      "Tokenized sentence: ['especially', 'since', 'i', 'talk', 'about', 'boston', 'all', 'up', 'in', 'my', 'personal', 'statement', 'lol', 'i', 'woulda', 'changed', 'that', 'if', 'i', 'had', 'realized', 'it', 'said', 'nyc', 'it', 'says', 'boston', 'now']\n",
      "After stop words removal: ['especially', 'since', 'talk', 'boston', 'personal', 'statement', 'lol', 'woulda', 'changed', 'realized', 'said', 'nyc', 'says', 'boston']\n",
      "realize\n",
      "After stemming with porters algorithm: ['especi', 'sinc', 'talk', 'boston', 'person', 'statem', 'lol', 'woulda', 'chan', 'realiz', 'said', 'nyc', 'sai', 'boston']\n",
      "Tokenized sentence: ['good', 'morning', 'my', 'boytoy', 'how', 's', 'those', 'yummy', 'lips', 'where', 's', 'my', 'sexy', 'buns', 'now', 'what', 'do', 'you', 'do', 'do', 'you', 'think', 'of', 'me', 'do', 'you', 'crave', 'me', 'do', 'you', 'need', 'me']\n",
      "After stop words removal: ['good', 'morning', 'boytoy', 'yummy', 'lips', 'sexy', 'buns', 'think', 'crave', 'need']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'boytoi', 'yummi', 'lip', 'sexi', 'bun', 'think', 'crave', 'need']\n",
      "Tokenized sentence: ['those', 'cocksuckers', 'if', 'it', 'makes', 'you', 'feel', 'better', 'ipads', 'are', 'worthless', 'garbage', 'novelty', 'items', 'and', 'you', 'should', 'feel', 'bad', 'for', 'even', 'wanting', 'one']\n",
      "After stop words removal: ['cocksuckers', 'makes', 'feel', 'better', 'ipads', 'worthless', 'garbage', 'novelty', 'items', 'feel', 'bad', 'even', 'wanting', 'one']\n",
      "want\n",
      "After stemming with porters algorithm: ['cocksuck', 'make', 'feel', 'better', 'ipad', 'worthless', 'garbag', 'novelti', 'item', 'feel', 'bad', 'even', 'wan', 'on']\n",
      "Tokenized sentence: ['this', 'msg', 'is', 'for', 'your', 'mobile', 'content', 'order', 'it', 'has', 'been', 'resent', 'as', 'previous', 'attempt', 'failed', 'due', 'to', 'network', 'error', 'queries', 'to', 'customersqueries', 'netvision', 'uk', 'com']\n",
      "After stop words removal: ['msg', 'mobile', 'content', 'order', 'resent', 'previous', 'attempt', 'failed', 'due', 'network', 'error', 'queries', 'customersqueries', 'netvision', 'uk', 'com']\n",
      "After stemming with porters algorithm: ['msg', 'mobil', 'content', 'order', 'resent', 'previou', 'attempt', 'fail', 'due', 'network', 'error', 'queri', 'customersqueri', 'netvis', 'com']\n",
      "Tokenized sentence: ['welcome', 'please', 'reply', 'with', 'your', 'age', 'and', 'gender', 'to', 'begin', 'e', 'g', 'm']\n",
      "After stop words removal: ['welcome', 'please', 'reply', 'age', 'gender', 'begin', 'e', 'g']\n",
      "After stemming with porters algorithm: ['welcom', 'pleas', 'repli', 'ag', 'gender', 'begin']\n",
      "Tokenized sentence: ['pls', 'confirm', 'the', 'time', 'to', 'collect', 'the', 'cheque']\n",
      "After stop words removal: ['pls', 'confirm', 'time', 'collect', 'cheque']\n",
      "After stemming with porters algorithm: ['pl', 'confirm', 'time', 'collect', 'chequ']\n",
      "Tokenized sentence: ['is', 'there', 'coming', 'friday', 'is', 'leave', 'for', 'pongal', 'do', 'you', 'get', 'any', 'news', 'from', 'your', 'work', 'place']\n",
      "After stop words removal: ['coming', 'friday', 'leave', 'pongal', 'get', 'news', 'work', 'place']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'fridai', 'leav', 'pongal', 'get', 'new', 'work', 'place']\n",
      "Tokenized sentence: ['ringtoneking']\n",
      "After stop words removal: ['ringtoneking']\n",
      "ringtonek\n",
      "After stemming with porters algorithm: ['ringtonek']\n",
      "Tokenized sentence: ['did', 'you', 'see', 'that', 'film']\n",
      "After stop words removal: ['see', 'film']\n",
      "After stemming with porters algorithm: ['see', 'film']\n",
      "Tokenized sentence: ['thanks', 'and', 'or', 'bomb', 'and', 'date', 'as', 'my', 'phone', 'wanted', 'to', 'say']\n",
      "After stop words removal: ['thanks', 'bomb', 'date', 'phone', 'wanted', 'say']\n",
      "After stemming with porters algorithm: ['thank', 'bomb', 'date', 'phone', 'wan', 'sai']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'lick', 'your', 'pussy', 'now']\n",
      "After stop words removal: ['want', 'lick', 'pussy']\n",
      "After stemming with porters algorithm: ['want', 'lick', 'pussi']\n",
      "Tokenized sentence: ['yup']\n",
      "After stop words removal: ['yup']\n",
      "After stemming with porters algorithm: ['yup']\n",
      "Tokenized sentence: ['think', 'i', 'could', 'stop', 'by', 'in', 'like', 'an', 'hour', 'or', 'so', 'my', 'roommate', 's', 'looking', 'to', 'stock', 'up', 'for', 'a', 'trip']\n",
      "After stop words removal: ['think', 'could', 'stop', 'like', 'hour', 'roommate', 'looking', 'stock', 'trip']\n",
      "look\n",
      "After stemming with porters algorithm: ['think', 'could', 'stop', 'like', 'hour', 'roommat', 'look', 'stock', 'trip']\n",
      "Tokenized sentence: ['thank', 'you', 'meet', 'you', 'monday']\n",
      "After stop words removal: ['thank', 'meet', 'monday']\n",
      "After stemming with porters algorithm: ['thank', 'meet', 'mondai']\n",
      "Tokenized sentence: ['update', 'now', 'mths', 'half', 'price', 'orange', 'line', 'rental', 'mins', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'j', 'q']\n",
      "After stop words removal: ['update', 'mths', 'half', 'price', 'orange', 'line', 'rental', 'mins', 'call', 'mobileupd', 'call', 'optout', 'j', 'q']\n",
      "After stemming with porters algorithm: ['updat', 'mth', 'half', 'price', 'orang', 'line', 'rental', 'min', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['this', 'is', 'wishing', 'you', 'a', 'great', 'day', 'moji', 'told', 'me', 'about', 'your', 'offer', 'and', 'as', 'always', 'i', 'was', 'speechless', 'you', 'offer', 'so', 'easily', 'to', 'go', 'to', 'great', 'lengths', 'on', 'my', 'behalf', 'and', 'its', 'stunning', 'my', 'exam', 'is', 'next', 'friday', 'after', 'that', 'i', 'will', 'keep', 'in', 'touch', 'more', 'sorry']\n",
      "After stop words removal: ['wishing', 'great', 'day', 'moji', 'told', 'offer', 'always', 'speechless', 'offer', 'easily', 'go', 'great', 'lengths', 'behalf', 'stunning', 'exam', 'next', 'friday', 'keep', 'touch', 'sorry']\n",
      "wish\n",
      "stunn\n",
      "After stemming with porters algorithm: ['wis', 'great', 'dai', 'moji', 'told', 'offer', 'alwai', 'speechless', 'offer', 'easili', 'great', 'length', 'behalf', 'stun', 'exam', 'next', 'fridai', 'keep', 'touch', 'sorri']\n",
      "Tokenized sentence: ['s', 'now', 'only', 'i', 'took', 'tablets', 'reaction', 'morning', 'only']\n",
      "After stop words removal: ['took', 'tablets', 'reaction', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['took', 'tablet', 'react', 'mor']\n",
      "Tokenized sentence: ['get', 'ready', 'to', 'moan', 'and', 'scream']\n",
      "After stop words removal: ['get', 'ready', 'moan', 'scream']\n",
      "After stemming with porters algorithm: ['get', 'readi', 'moan', 'scream']\n",
      "Tokenized sentence: ['convey', 'my', 'regards', 'to', 'him']\n",
      "After stop words removal: ['convey', 'regards']\n",
      "After stemming with porters algorithm: ['convei', 'regard']\n",
      "Tokenized sentence: ['so', 'do', 'you', 'have', 'samus', 'shoulders', 'yet']\n",
      "After stop words removal: ['samus', 'shoulders', 'yet']\n",
      "After stemming with porters algorithm: ['samu', 'shoulder', 'yet']\n",
      "Tokenized sentence: ['i', 'think', 'steyn', 'surely', 'get', 'one', 'wicket']\n",
      "After stop words removal: ['think', 'steyn', 'surely', 'get', 'one', 'wicket']\n",
      "After stemming with porters algorithm: ['think', 'steyn', 'sure', 'get', 'on', 'wicket']\n",
      "Tokenized sentence: ['double', 'mins', 'double', 'txt', 'price', 'linerental', 'on', 'latest', 'orange', 'bluetooth', 'mobiles', 'call', 'mobileupd', 'for', 'the', 'very', 'latest', 'offers', 'or', 'call', 'optout', 'lf']\n",
      "After stop words removal: ['double', 'mins', 'double', 'txt', 'price', 'linerental', 'latest', 'orange', 'bluetooth', 'mobiles', 'call', 'mobileupd', 'latest', 'offers', 'call', 'optout', 'lf']\n",
      "After stemming with porters algorithm: ['doubl', 'min', 'doubl', 'txt', 'price', 'liner', 'latest', 'orang', 'bluetooth', 'mobil', 'call', 'mobileupd', 'latest', 'offer', 'call', 'optout']\n",
      "Tokenized sentence: ['nope', 'but', 'i', 'll', 'b', 'going', 'sch', 'on', 'fri', 'quite', 'early', 'lor', 'cos', 'mys', 'sis', 'got', 'paper', 'in', 'da', 'morn']\n",
      "After stop words removal: ['nope', 'b', 'going', 'sch', 'fri', 'quite', 'early', 'lor', 'cos', 'mys', 'sis', 'got', 'paper', 'da', 'morn']\n",
      "go\n",
      "After stemming with porters algorithm: ['nope', 'go', 'sch', 'fri', 'quit', 'earli', 'lor', 'co', 'my', 'si', 'got', 'paper', 'morn']\n",
      "Tokenized sentence: ['good', 'afternoon', 'on', 'this', 'glorious', 'anniversary', 'day', 'my', 'sweet', 'j', 'i', 'hope', 'this', 'finds', 'you', 'happy', 'and', 'content', 'my', 'prey', 'i', 'think', 'of', 'you', 'and', 'send', 'a', 'teasing', 'kiss', 'from', 'across', 'the', 'sea', 'coaxing', 'images', 'of', 'fond', 'souveniers', 'you', 'cougar', 'pen']\n",
      "After stop words removal: ['good', 'afternoon', 'glorious', 'anniversary', 'day', 'sweet', 'j', 'hope', 'finds', 'happy', 'content', 'prey', 'think', 'send', 'teasing', 'kiss', 'across', 'sea', 'coaxing', 'images', 'fond', 'souveniers', 'cougar', 'pen']\n",
      "teas\n",
      "coax\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'gloriou', 'anniversari', 'dai', 'sweet', 'hope', 'find', 'happi', 'content', 'prei', 'think', 'send', 'teas', 'kiss', 'across', 'sea', 'coax', 'imag', 'fond', 'souveni', 'cougar', 'pen']\n",
      "Tokenized sentence: ['cud', 'u', 'tell', 'ppl', 'im', 'gona', 'b', 'a', 'bit', 'l', 'cos', 'buses', 'hav', 'gon', 'past', 'cos', 'they', 'were', 'full', 'im', 'still', 'waitin', 'pete', 'x']\n",
      "After stop words removal: ['cud', 'u', 'tell', 'ppl', 'im', 'gona', 'b', 'bit', 'l', 'cos', 'buses', 'hav', 'gon', 'past', 'cos', 'full', 'im', 'still', 'waitin', 'pete', 'x']\n",
      "After stemming with porters algorithm: ['cud', 'tell', 'ppl', 'gona', 'bit', 'co', 'buse', 'hav', 'gon', 'past', 'co', 'full', 'still', 'waitin', 'pete']\n",
      "Tokenized sentence: ['anything', 'lor', 'if', 'they', 'all', 'go', 'then', 'i', 'go', 'lor']\n",
      "After stop words removal: ['anything', 'lor', 'go', 'go', 'lor']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'lor', 'lor']\n",
      "Tokenized sentence: ['oh', 'dang', 'i', 'didn', 't', 'mean', 'o', 'send', 'that', 'to', 'you', 'lol']\n",
      "After stop words removal: ['oh', 'dang', 'mean', 'send', 'lol']\n",
      "After stemming with porters algorithm: ['dang', 'mean', 'send', 'lol']\n",
      "Tokenized sentence: ['black', 'shirt', 'n', 'blue', 'jeans', 'i', 'thk', 'i', 'c']\n",
      "After stop words removal: ['black', 'shirt', 'n', 'blue', 'jeans', 'thk', 'c']\n",
      "After stemming with porters algorithm: ['black', 'shirt', 'blue', 'jean', 'thk']\n",
      "Tokenized sentence: ['oh', 'then', 'your', 'phone', 'phoned', 'me', 'but', 'it', 'disconnected']\n",
      "After stop words removal: ['oh', 'phone', 'phoned', 'disconnected']\n",
      "After stemming with porters algorithm: ['phone', 'phone', 'disconnec']\n",
      "Tokenized sentence: ['howz', 'that', 'persons', 'story']\n",
      "After stop words removal: ['howz', 'persons', 'story']\n",
      "After stemming with porters algorithm: ['howz', 'person', 'stori']\n",
      "Tokenized sentence: ['there', 'bold', 'lt', 'gt', 'is', 'that', 'yours']\n",
      "After stop words removal: ['bold', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['bold']\n",
      "Tokenized sentence: ['no', 'probs', 'hon', 'how', 'u', 'doinat', 'the', 'mo']\n",
      "After stop words removal: ['probs', 'hon', 'u', 'doinat', 'mo']\n",
      "After stemming with porters algorithm: ['prob', 'hon', 'doinat']\n",
      "Tokenized sentence: ['they', 'said', 'if', 'its', 'gonna', 'snow', 'it', 'will', 'start', 'around', 'or', 'pm', 'tonite', 'they', 'are', 'predicting', 'an', 'inch', 'of', 'accumulation']\n",
      "After stop words removal: ['said', 'gonna', 'snow', 'start', 'around', 'pm', 'tonite', 'predicting', 'inch', 'accumulation']\n",
      "predict\n",
      "After stemming with porters algorithm: ['said', 'gonna', 'snow', 'start', 'around', 'tonit', 'predic', 'inch', 'accumul']\n",
      "Tokenized sentence: ['thank', 'you', 'so', 'much', 'when', 'we', 'skyped', 'wit', 'kz', 'and', 'sura', 'we', 'didnt', 'get', 'the', 'pleasure', 'of', 'your', 'company', 'hope', 'you', 'are', 'good', 'we', 've', 'given', 'you', 'ultimatum', 'oh', 'we', 'are', 'countin', 'down', 'to', 'aburo', 'enjoy']\n",
      "After stop words removal: ['thank', 'much', 'skyped', 'wit', 'kz', 'sura', 'didnt', 'get', 'pleasure', 'company', 'hope', 'good', 'given', 'ultimatum', 'oh', 'countin', 'aburo', 'enjoy']\n",
      "After stemming with porters algorithm: ['thank', 'much', 'skyped', 'wit', 'sura', 'didnt', 'get', 'pleasur', 'compani', 'hope', 'good', 'given', 'ultimatum', 'countin', 'aburo', 'enjoi']\n",
      "Tokenized sentence: ['want', 'to', 'finally', 'have', 'lunch', 'today']\n",
      "After stop words removal: ['want', 'finally', 'lunch', 'today']\n",
      "After stemming with porters algorithm: ['want', 'final', 'lunch', 'todai']\n",
      "Tokenized sentence: ['send', 'to', 'someone', 'else']\n",
      "After stop words removal: ['send', 'someone', 'else']\n",
      "After stemming with porters algorithm: ['send', 'someon', 'els']\n",
      "Tokenized sentence: ['congrats', 'year', 'special', 'cinema', 'pass', 'for', 'is', 'yours', 'call', 'now', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'all', 'free', 'bx', 'ip', 'we', 'pm', 'dont', 'miss', 'out']\n",
      "After stop words removal: ['congrats', 'year', 'special', 'cinema', 'pass', 'call', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'free', 'bx', 'ip', 'pm', 'dont', 'miss']\n",
      "After stemming with porters algorithm: ['congrat', 'year', 'special', 'cinema', 'pass', 'call', 'suprman', 'matrix', 'starwar', 'etc', 'free', 'dont', 'miss']\n",
      "Tokenized sentence: ['she', 'doesnt', 'need', 'any', 'test']\n",
      "After stop words removal: ['doesnt', 'need', 'test']\n",
      "After stemming with porters algorithm: ['doesnt', 'need', 'test']\n",
      "Tokenized sentence: ['huh', 'hyde', 'park', 'not', 'in', 'mel', 'ah', 'opps', 'got', 'confused', 'anyway', 'if', 'tt', 's', 'e', 'best', 'choice', 'den', 'we', 'juz', 'have', 'to', 'take', 'it']\n",
      "After stop words removal: ['huh', 'hyde', 'park', 'mel', 'ah', 'opps', 'got', 'confused', 'anyway', 'tt', 'e', 'best', 'choice', 'den', 'juz', 'take']\n",
      "After stemming with porters algorithm: ['huh', 'hyde', 'park', 'mel', 'opp', 'got', 'confus', 'anywai', 'best', 'choic', 'den', 'juz', 'take']\n",
      "Tokenized sentence: ['you', 'give', 'us', 'back', 'my', 'id', 'proof', 'and', 'lt', 'gt', 'rs', 'we', 'wont', 'allow', 'you', 'to', 'work', 'we', 'will', 'come', 'to', 'your', 'home', 'within', 'days']\n",
      "After stop words removal: ['give', 'us', 'back', 'id', 'proof', 'lt', 'gt', 'rs', 'wont', 'allow', 'work', 'come', 'home', 'within', 'days']\n",
      "After stemming with porters algorithm: ['give', 'back', 'proof', 'wont', 'allow', 'work', 'come', 'home', 'within', 'dai']\n",
      "Tokenized sentence: ['yay', 'finally', 'lol', 'i', 'missed', 'our', 'cinema', 'trip', 'last', 'week']\n",
      "After stop words removal: ['yay', 'finally', 'lol', 'missed', 'cinema', 'trip', 'last', 'week']\n",
      "After stemming with porters algorithm: ['yai', 'final', 'lol', 'miss', 'cinema', 'trip', 'last', 'week']\n",
      "Tokenized sentence: ['hey', 'you', 'still', 'want', 'to', 'go', 'for', 'yogasana', 'coz', 'if', 'we', 'end', 'at', 'cine', 'then', 'can', 'go', 'bathe', 'and', 'hav', 'the', 'steam', 'bath']\n",
      "After stop words removal: ['hey', 'still', 'want', 'go', 'yogasana', 'coz', 'end', 'cine', 'go', 'bathe', 'hav', 'steam', 'bath']\n",
      "After stemming with porters algorithm: ['hei', 'still', 'want', 'yogasana', 'coz', 'end', 'cine', 'bath', 'hav', 'steam', 'bath']\n",
      "Tokenized sentence: ['oh', 'lk', 'tt', 'den', 'we', 'take', 'e', 'one', 'tt', 'ends', 'at', 'cine', 'lor', 'dun', 'wan', 'yogasana', 'oso', 'can']\n",
      "After stop words removal: ['oh', 'lk', 'tt', 'den', 'take', 'e', 'one', 'tt', 'ends', 'cine', 'lor', 'dun', 'wan', 'yogasana', 'oso']\n",
      "After stemming with porters algorithm: ['den', 'take', 'on', 'end', 'cine', 'lor', 'dun', 'wan', 'yogasana', 'oso']\n",
      "Tokenized sentence: ['yes', 'that', 'will', 'be', 'fine', 'love', 'you', 'be', 'safe']\n",
      "After stop words removal: ['yes', 'fine', 'love', 'safe']\n",
      "After stemming with porters algorithm: ['ye', 'fine', 'love', 'safe']\n",
      "Tokenized sentence: ['ey', 'calm', 'downon', 'theacusations', 'itxt', 'u', 'cos', 'iwana', 'know', 'wotu', 'r', 'doin', 'at', 'thew', 'end', 'haventcn', 'u', 'in', 'ages', 'ring', 'me', 'if', 'ur', 'up', 'nething', 'sat', 'love', 'j', 'xxx']\n",
      "After stop words removal: ['ey', 'calm', 'downon', 'theacusations', 'itxt', 'u', 'cos', 'iwana', 'know', 'wotu', 'r', 'doin', 'thew', 'end', 'haventcn', 'u', 'ages', 'ring', 'ur', 'nething', 'sat', 'love', 'j', 'xxx']\n",
      "neth\n",
      "After stemming with porters algorithm: ['calm', 'downon', 'theacus', 'itxt', 'co', 'iwana', 'know', 'wotu', 'doin', 'thew', 'end', 'haventcn', 'ag', 'ring', 'net', 'sat', 'love', 'xxx']\n",
      "Tokenized sentence: ['thanks', 'for', 'the', 'vote', 'now', 'sing', 'along', 'with', 'the', 'stars', 'with', 'karaoke', 'on', 'your', 'mobile', 'for', 'a', 'free', 'link', 'just', 'reply', 'with', 'sing', 'now']\n",
      "After stop words removal: ['thanks', 'vote', 'sing', 'along', 'stars', 'karaoke', 'mobile', 'free', 'link', 'reply', 'sing']\n",
      "After stemming with porters algorithm: ['thank', 'vote', 'sing', 'along', 'star', 'karaok', 'mobil', 'free', 'link', 'repli', 'sing']\n",
      "Tokenized sentence: ['ur', 'going', 'bahamas', 'callfreefone', 'and', 'speak', 'to', 'a', 'live', 'operator', 'to', 'claim', 'either', 'bahamas', 'cruise', 'of', 'cash', 'only', 'to', 'opt', 'out', 'txt', 'x', 'to']\n",
      "After stop words removal: ['ur', 'going', 'bahamas', 'callfreefone', 'speak', 'live', 'operator', 'claim', 'either', 'bahamas', 'cruise', 'cash', 'opt', 'txt', 'x']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'bahama', 'callfreefon', 'speak', 'live', 'oper', 'claim', 'either', 'bahama', 'cruis', 'cash', 'opt', 'txt']\n",
      "Tokenized sentence: ['aiyo', 'u', 'always', 'c', 'our', 'ex', 'one', 'i', 'dunno', 'abt', 'mei', 'she', 'haven', 'reply', 'first', 'time', 'u', 'reply', 'so', 'fast', 'y', 'so', 'lucky', 'not', 'workin', 'huh', 'got', 'bao', 'by', 'ur', 'sugardad', 'ah', 'gee']\n",
      "After stop words removal: ['aiyo', 'u', 'always', 'c', 'ex', 'one', 'dunno', 'abt', 'mei', 'reply', 'first', 'time', 'u', 'reply', 'fast', 'lucky', 'workin', 'huh', 'got', 'bao', 'ur', 'sugardad', 'ah', 'gee']\n",
      "After stemming with porters algorithm: ['aiyo', 'alwai', 'on', 'dunno', 'abt', 'mei', 'repli', 'first', 'time', 'repli', 'fast', 'lucki', 'workin', 'huh', 'got', 'bao', 'sugardad', 'gee']\n",
      "Tokenized sentence: ['hope', 'you', 'are', 'having', 'a', 'good', 'week', 'just', 'checking', 'in']\n",
      "After stop words removal: ['hope', 'good', 'week', 'checking']\n",
      "check\n",
      "After stemming with porters algorithm: ['hope', 'good', 'week', 'chec']\n",
      "Tokenized sentence: ['like', 'i', 'made', 'him', 'throw', 'up', 'when', 'we', 'were', 'smoking', 'in', 'our', 'friend', 's', 'car', 'one', 'time', 'it', 'was', 'awesome']\n",
      "After stop words removal: ['like', 'made', 'throw', 'smoking', 'friend', 'car', 'one', 'time', 'awesome']\n",
      "smok\n",
      "After stemming with porters algorithm: ['like', 'made', 'throw', 'smoke', 'friend', 'car', 'on', 'time', 'awesom']\n",
      "Tokenized sentence: ['twinks', 'bears', 'scallies', 'skins', 'and', 'jocks', 'are', 'calling', 'now', 'don', 't', 'miss', 'the', 'weekend', 's', 'fun', 'call', 'at', 'p', 'min', 'stop', 'texts', 'call', 'nat', 'rate']\n",
      "After stop words removal: ['twinks', 'bears', 'scallies', 'skins', 'jocks', 'calling', 'miss', 'weekend', 'fun', 'call', 'p', 'min', 'stop', 'texts', 'call', 'nat', 'rate']\n",
      "call\n",
      "After stemming with porters algorithm: ['twink', 'bear', 'scalli', 'skin', 'jock', 'call', 'miss', 'weekend', 'fun', 'call', 'min', 'stop', 'text', 'call', 'nat', 'rate']\n",
      "Tokenized sentence: ['get', 'your', 'garden', 'ready', 'for', 'summer', 'with', 'a', 'free', 'selection', 'of', 'summer', 'bulbs', 'and', 'seeds', 'worth', 'only', 'with', 'the', 'scotsman', 'this', 'saturday', 'to', 'stop', 'go', 'notxt', 'co', 'uk']\n",
      "After stop words removal: ['get', 'garden', 'ready', 'summer', 'free', 'selection', 'summer', 'bulbs', 'seeds', 'worth', 'scotsman', 'saturday', 'stop', 'go', 'notxt', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['get', 'garden', 'readi', 'summer', 'free', 'select', 'summer', 'bulb', 'seed', 'worth', 'scotsman', 'saturdai', 'stop', 'notxt']\n",
      "Tokenized sentence: ['even', 'u', 'dont', 'get', 'in', 'trouble', 'while', 'convincing', 'just', 'tel', 'him', 'once', 'or', 'twice', 'and', 'just', 'tel', 'neglect', 'his', 'msgs', 'dont', 'c', 'and', 'read', 'it', 'just', 'dont', 'reply']\n",
      "After stop words removal: ['even', 'u', 'dont', 'get', 'trouble', 'convincing', 'tel', 'twice', 'tel', 'neglect', 'msgs', 'dont', 'c', 'read', 'dont', 'reply']\n",
      "convinc\n",
      "After stemming with porters algorithm: ['even', 'dont', 'get', 'troubl', 'convin', 'tel', 'twice', 'tel', 'neglect', 'msg', 'dont', 'read', 'dont', 'repli']\n",
      "Tokenized sentence: ['thanks', 'honey', 'have', 'a', 'great', 'day']\n",
      "After stop words removal: ['thanks', 'honey', 'great', 'day']\n",
      "After stemming with porters algorithm: ['thank', 'honei', 'great', 'dai']\n",
      "Tokenized sentence: ['sry', 'da', 'jst', 'nw', 'only', 'i', 'came', 'to', 'home']\n",
      "After stop words removal: ['sry', 'da', 'jst', 'nw', 'came', 'home']\n",
      "After stemming with porters algorithm: ['sry', 'jst', 'came', 'home']\n",
      "Tokenized sentence: ['aiyo', 'a', 'bit', 'pai', 'seh', 'noe', 'scared', 'he', 'dun', 'rem', 'who', 'i', 'am', 'then', 'die', 'hee', 'but', 'he', 'become', 'better', 'lookin', 'oredi', 'leh']\n",
      "After stop words removal: ['aiyo', 'bit', 'pai', 'seh', 'noe', 'scared', 'dun', 'rem', 'die', 'hee', 'become', 'better', 'lookin', 'oredi', 'leh']\n",
      "After stemming with porters algorithm: ['aiyo', 'bit', 'pai', 'seh', 'noe', 'scare', 'dun', 'rem', 'die', 'hee', 'becom', 'better', 'lookin', 'oredi', 'leh']\n",
      "Tokenized sentence: ['good', 'words', 'but', 'words', 'may', 'leave', 'u', 'in', 'dismay', 'many', 'times']\n",
      "After stop words removal: ['good', 'words', 'words', 'may', 'leave', 'u', 'dismay', 'many', 'times']\n",
      "After stemming with porters algorithm: ['good', 'word', 'word', 'mai', 'leav', 'dismai', 'mani', 'time']\n",
      "Tokenized sentence: ['oh', 'k', 'k', 'where', 'did', 'you', 'take', 'test']\n",
      "After stop words removal: ['oh', 'k', 'k', 'take', 'test']\n",
      "After stemming with porters algorithm: ['take', 'test']\n",
      "Tokenized sentence: ['free', 'message', 'activate', 'your', 'free', 'text', 'messages', 'by', 'replying', 'to', 'this', 'message', 'with', 'the', 'word', 'free', 'for', 'terms', 'conditions', 'visit', 'www', 'com']\n",
      "After stop words removal: ['free', 'message', 'activate', 'free', 'text', 'messages', 'replying', 'message', 'word', 'free', 'terms', 'conditions', 'visit', 'www', 'com']\n",
      "reply\n",
      "After stemming with porters algorithm: ['free', 'messag', 'activ', 'free', 'text', 'messag', 'repl', 'messag', 'word', 'free', 'term', 'condit', 'visit', 'www', 'com']\n",
      "Tokenized sentence: ['is', 'xy', 'going', 'e', 'lunch']\n",
      "After stop words removal: ['xy', 'going', 'e', 'lunch']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'lunch']\n",
      "Tokenized sentence: ['love', 'that', 'holiday', 'monday', 'feeling', 'even', 'if', 'i', 'have', 'to', 'go', 'to', 'the', 'dentists', 'in', 'an', 'hour']\n",
      "After stop words removal: ['love', 'holiday', 'monday', 'feeling', 'even', 'go', 'dentists', 'hour']\n",
      "feel\n",
      "After stemming with porters algorithm: ['love', 'holidai', 'mondai', 'feel', 'even', 'dentist', 'hour']\n",
      "Tokenized sentence: ['hi', 'darlin', 'i', 'hope', 'you', 'had', 'a', 'nice', 'night', 'i', 'wish', 'i', 'had', 'come', 'cant', 'wait', 'to', 'see', 'you', 'love', 'fran', 'ps', 'i', 'want', 'dirty', 'anal', 'sex', 'and', 'i', 'want', 'a', 'man', 'gang', 'bang']\n",
      "After stop words removal: ['hi', 'darlin', 'hope', 'nice', 'night', 'wish', 'come', 'cant', 'wait', 'see', 'love', 'fran', 'ps', 'want', 'dirty', 'anal', 'sex', 'want', 'man', 'gang', 'bang']\n",
      "After stemming with porters algorithm: ['darlin', 'hope', 'nice', 'night', 'wish', 'come', 'cant', 'wait', 'see', 'love', 'fran', 'want', 'dirti', 'anal', 'sex', 'want', 'man', 'gang', 'bang']\n",
      "Tokenized sentence: ['yes', 'here', 'tv', 'is', 'always', 'available', 'in', 'work', 'place']\n",
      "After stop words removal: ['yes', 'tv', 'always', 'available', 'work', 'place']\n",
      "After stemming with porters algorithm: ['ye', 'alwai', 'avail', 'work', 'place']\n",
      "Tokenized sentence: ['r', 'u', 'sam', 'p', 'in', 'eachother', 'if', 'we', 'meet', 'we', 'can', 'go', 'my', 'house']\n",
      "After stop words removal: ['r', 'u', 'sam', 'p', 'eachother', 'meet', 'go', 'house']\n",
      "After stemming with porters algorithm: ['sam', 'eachoth', 'meet', 'hous']\n",
      "Tokenized sentence: ['yes', 'i', 'started', 'to', 'send', 'requests', 'to', 'make', 'it', 'but', 'pain', 'came', 'back', 'so', 'i', 'm', 'back', 'in', 'bed', 'double', 'coins', 'at', 'the', 'factory', 'too', 'i', 'gotta', 'cash', 'in', 'all', 'my', 'nitros']\n",
      "After stop words removal: ['yes', 'started', 'send', 'requests', 'make', 'pain', 'came', 'back', 'back', 'bed', 'double', 'coins', 'factory', 'gotta', 'cash', 'nitros']\n",
      "After stemming with porters algorithm: ['ye', 'star', 'send', 'request', 'make', 'pain', 'came', 'back', 'back', 'bed', 'doubl', 'coin', 'factori', 'gotta', 'cash', 'nitro']\n",
      "Tokenized sentence: ['oh', 'ok']\n",
      "After stop words removal: ['oh', 'ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['ya', 'had', 'just', 'now', 'onion', 'roast']\n",
      "After stop words removal: ['ya', 'onion', 'roast']\n",
      "After stemming with porters algorithm: ['onion', 'roast']\n",
      "Tokenized sentence: ['yes', 'baby', 'i', 'need', 'to', 'stretch', 'open', 'your', 'pussy']\n",
      "After stop words removal: ['yes', 'baby', 'need', 'stretch', 'open', 'pussy']\n",
      "After stemming with porters algorithm: ['ye', 'babi', 'need', 'stretch', 'open', 'pussi']\n",
      "Tokenized sentence: ['she', 'said', 'do', 'u', 'mind', 'if', 'i', 'go', 'into', 'the', 'bedroom', 'for', 'a', 'minute', 'ok', 'i', 'sed', 'in', 'a', 'sexy', 'mood', 'she', 'came', 'out', 'minuts', 'latr', 'wid', 'a', 'cake', 'n', 'my', 'wife']\n",
      "After stop words removal: ['said', 'u', 'mind', 'go', 'bedroom', 'minute', 'ok', 'sed', 'sexy', 'mood', 'came', 'minuts', 'latr', 'wid', 'cake', 'n', 'wife']\n",
      "After stemming with porters algorithm: ['said', 'mind', 'bedroom', 'minut', 'sed', 'sexi', 'mood', 'came', 'minut', 'latr', 'wid', 'cake', 'wife']\n",
      "Tokenized sentence: ['sorry', 'i', 'will', 'be', 'able', 'to', 'get', 'to', 'you', 'see', 'you', 'in', 'the', 'morning']\n",
      "After stop words removal: ['sorry', 'able', 'get', 'see', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['sorri', 'abl', 'get', 'see', 'mor']\n",
      "Tokenized sentence: ['en', 'chikku', 'nange', 'bakra', 'msg', 'kalstiya', 'then', 'had', 'tea', 'coffee']\n",
      "After stop words removal: ['en', 'chikku', 'nange', 'bakra', 'msg', 'kalstiya', 'tea', 'coffee']\n",
      "After stemming with porters algorithm: ['chikku', 'nang', 'bakra', 'msg', 'kalstiya', 'tea', 'coffe']\n",
      "Tokenized sentence: ['lt', 'gt', 'that', 's', 'all', 'guess', 'that', 's', 'easy', 'enough']\n",
      "After stop words removal: ['lt', 'gt', 'guess', 'easy', 'enough']\n",
      "After stemming with porters algorithm: ['guess', 'easi', 'enough']\n",
      "Tokenized sentence: ['goodmorning', 'sleeping', 'ga']\n",
      "After stop words removal: ['goodmorning', 'sleeping', 'ga']\n",
      "goodmorn\n",
      "sleep\n",
      "After stemming with porters algorithm: ['goodmor', 'sleep']\n",
      "Tokenized sentence: ['thank', 'you', 'do', 'you', 'generally', 'date', 'the', 'brothas']\n",
      "After stop words removal: ['thank', 'generally', 'date', 'brothas']\n",
      "After stemming with porters algorithm: ['thank', 'gener', 'date', 'brotha']\n",
      "Tokenized sentence: ['u', 'definitely', 'need', 'a', 'module', 'from', 'e', 'humanities', 'dis', 'sem', 'izzit', 'u', 'wan', 'take', 'other', 'modules', 'st']\n",
      "After stop words removal: ['u', 'definitely', 'need', 'module', 'e', 'humanities', 'dis', 'sem', 'izzit', 'u', 'wan', 'take', 'modules', 'st']\n",
      "After stemming with porters algorithm: ['definit', 'need', 'modul', 'human', 'di', 'sem', 'izzit', 'wan', 'take', 'modul']\n",
      "Tokenized sentence: ['natalie', 'f', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'www', 'sms', 'ac', 'u', 'natalie', 'k', 'stop', 'send', 'stop', 'frnd', 'to']\n",
      "After stop words removal: ['natalie', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'www', 'sms', 'ac', 'u', 'natalie', 'k', 'stop', 'send', 'stop', 'frnd']\n",
      "invit\n",
      "After stemming with porters algorithm: ['natali', 'invit', 'friend', 'repli', 'ye', 'see', 'www', 'sm', 'natali', 'stop', 'send', 'stop', 'frnd']\n",
      "Tokenized sentence: ['why', 'don', 't', 'you', 'go', 'tell', 'your', 'friend', 'you', 're', 'not', 'sure', 'you', 'want', 'to', 'live', 'with', 'him', 'because', 'he', 'smokes', 'too', 'much', 'then', 'spend', 'hours', 'begging', 'him', 'to', 'come', 'smoke']\n",
      "After stop words removal: ['go', 'tell', 'friend', 'sure', 'want', 'live', 'smokes', 'much', 'spend', 'hours', 'begging', 'come', 'smoke']\n",
      "begg\n",
      "After stemming with porters algorithm: ['tell', 'friend', 'sure', 'want', 'live', 'smoke', 'much', 'spend', 'hour', 'beg', 'come', 'smoke']\n",
      "Tokenized sentence: ['free', 'day', 'sexy', 'st', 'george', 's', 'day', 'pic', 'of', 'jordan', 'txt', 'pic', 'to', 'dont', 'miss', 'out', 'then', 'every', 'wk', 'a', 'saucy', 'celeb', 'more', 'pics', 'c', 'pocketbabe', 'co', 'uk', 'wk']\n",
      "After stop words removal: ['free', 'day', 'sexy', 'st', 'george', 'day', 'pic', 'jordan', 'txt', 'pic', 'dont', 'miss', 'every', 'wk', 'saucy', 'celeb', 'pics', 'c', 'pocketbabe', 'co', 'uk', 'wk']\n",
      "After stemming with porters algorithm: ['free', 'dai', 'sexi', 'georg', 'dai', 'pic', 'jordan', 'txt', 'pic', 'dont', 'miss', 'everi', 'sauci', 'celeb', 'pic', 'pocketbab']\n",
      "Tokenized sentence: ['i', 'm', 'really', 'sorry', 'i', 'won', 't', 'b', 'able', 'do', 'this', 'friday', 'hope', 'u', 'can', 'find', 'an', 'alternative', 'hope', 'yr', 'term', 's', 'going', 'ok']\n",
      "After stop words removal: ['really', 'sorry', 'b', 'able', 'friday', 'hope', 'u', 'find', 'alternative', 'hope', 'yr', 'term', 'going', 'ok']\n",
      "go\n",
      "After stemming with porters algorithm: ['realli', 'sorri', 'abl', 'fridai', 'hope', 'find', 'altern', 'hope', 'term', 'go']\n",
      "Tokenized sentence: ['how', 'its', 'a', 'little', 'difficult', 'but', 'its', 'a', 'simple', 'way', 'to', 'enter', 'this', 'place']\n",
      "After stop words removal: ['little', 'difficult', 'simple', 'way', 'enter', 'place']\n",
      "After stemming with porters algorithm: ['littl', 'difficult', 'simpl', 'wai', 'enter', 'place']\n",
      "Tokenized sentence: ['i', 'am', 'getting', 'threats', 'from', 'your', 'sales', 'executive', 'shifad', 'as', 'i', 'raised', 'complaint', 'against', 'him', 'its', 'an', 'official', 'message']\n",
      "After stop words removal: ['getting', 'threats', 'sales', 'executive', 'shifad', 'raised', 'complaint', 'official', 'message']\n",
      "gett\n",
      "After stemming with porters algorithm: ['get', 'threat', 'sale', 'execut', 'shifad', 'rais', 'complaint', 'offici', 'messag']\n",
      "Tokenized sentence: ['say', 'until', 'like', 'dat', 'i', 'dun', 'buy', 'ericsson', 'oso', 'cannot', 'oredi', 'lar']\n",
      "After stop words removal: ['say', 'like', 'dat', 'dun', 'buy', 'ericsson', 'oso', 'cannot', 'oredi', 'lar']\n",
      "After stemming with porters algorithm: ['sai', 'like', 'dat', 'dun', 'bui', 'ericsson', 'oso', 'cannot', 'oredi', 'lar']\n",
      "Tokenized sentence: ['have', 'a', 'good', 'trip', 'watch', 'out', 'for', 'remember', 'when', 'you', 'get', 'back', 'we', 'must', 'decide', 'about', 'easter']\n",
      "After stop words removal: ['good', 'trip', 'watch', 'remember', 'get', 'back', 'must', 'decide', 'easter']\n",
      "After stemming with porters algorithm: ['good', 'trip', 'watch', 'rememb', 'get', 'back', 'must', 'decid', 'easter']\n",
      "Tokenized sentence: ['nice', 'nice', 'how', 'is', 'it', 'working']\n",
      "After stop words removal: ['nice', 'nice', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['nice', 'nice', 'wor']\n",
      "Tokenized sentence: ['camera', 'quite', 'good', 'mega', 'pixels', 'optical', 'and', 'digital', 'dooms', 'have', 'a', 'lovely', 'holiday', 'be', 'safe', 'and', 'i', 'hope', 'you', 'hav', 'a', 'good', 'journey', 'happy', 'new', 'year', 'to', 'you', 'both', 'see', 'you', 'in', 'a', 'couple', 'of', 'weeks']\n",
      "After stop words removal: ['camera', 'quite', 'good', 'mega', 'pixels', 'optical', 'digital', 'dooms', 'lovely', 'holiday', 'safe', 'hope', 'hav', 'good', 'journey', 'happy', 'new', 'year', 'see', 'couple', 'weeks']\n",
      "After stemming with porters algorithm: ['camera', 'quit', 'good', 'mega', 'pixel', 'optic', 'digit', 'doom', 'love', 'holidai', 'safe', 'hope', 'hav', 'good', 'journei', 'happi', 'new', 'year', 'see', 'coupl', 'week']\n",
      "Tokenized sentence: ['and', 'picking', 'them', 'up', 'from', 'various', 'points']\n",
      "After stop words removal: ['picking', 'various', 'points']\n",
      "pick\n",
      "After stemming with porters algorithm: ['pic', 'variou', 'point']\n",
      "Tokenized sentence: ['come', 'back', 'to', 'tampa', 'ffffuuuuuuu']\n",
      "After stop words removal: ['come', 'back', 'tampa', 'ffffuuuuuuu']\n",
      "After stemming with porters algorithm: ['come', 'back', 'tampa', 'ffffuuuuuuu']\n",
      "Tokenized sentence: ['compliments', 'to', 'you', 'was', 'away', 'from', 'the', 'system', 'how', 'your', 'side']\n",
      "After stop words removal: ['compliments', 'away', 'system', 'side']\n",
      "After stemming with porters algorithm: ['complim', 'awai', 'system', 'side']\n",
      "Tokenized sentence: ['hi', 'wlcome', 'back', 'did', 'wonder', 'if', 'you', 'got', 'eaten', 'by', 'a', 'lion', 'or', 'something', 'nothing', 'much']\n",
      "After stop words removal: ['hi', 'wlcome', 'back', 'wonder', 'got', 'eaten', 'lion', 'something', 'nothing', 'much']\n",
      "someth\n",
      "noth\n",
      "After stemming with porters algorithm: ['wlcome', 'back', 'wonder', 'got', 'eaten', 'lion', 'somet', 'not', 'much']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['today', 's', 'offer', 'claim', 'ur', 'worth', 'of', 'discount', 'vouchers', 'text', 'yes', 'to', 'now', 'savamob', 'member', 'offers', 'mobile', 't', 'cs', 'sub', 'unsub', 'reply', 'x']\n",
      "After stop words removal: ['today', 'offer', 'claim', 'ur', 'worth', 'discount', 'vouchers', 'text', 'yes', 'savamob', 'member', 'offers', 'mobile', 'cs', 'sub', 'unsub', 'reply', 'x']\n",
      "After stemming with porters algorithm: ['todai', 'offer', 'claim', 'worth', 'discount', 'voucher', 'text', 'ye', 'savamob', 'member', 'offer', 'mobil', 'sub', 'unsub', 'repli']\n",
      "Tokenized sentence: ['urgent', 'call', 'from', 'landline', 'your', 'complementary', 'tenerife', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'po', 'box', 'wa', 'px', 'ppm', 'sender', 'hol', 'offer']\n",
      "After stop words removal: ['urgent', 'call', 'landline', 'complementary', 'tenerife', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'po', 'box', 'wa', 'px', 'ppm', 'sender', 'hol', 'offer']\n",
      "After stemming with porters algorithm: ['urgent', 'call', 'landlin', 'complementari', 'tenerif', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm', 'sender', 'hol', 'offer']\n",
      "Tokenized sentence: ['buy', 'space', 'invaders', 'a', 'chance', 'win', 'orig', 'arcade', 'game', 'console', 'press', 'for', 'games', 'arcade', 'std', 'wap', 'charge', 'see', 'o', 'co', 'uk', 'games', 'terms', 'settings', 'no', 'purchase']\n",
      "After stop words removal: ['buy', 'space', 'invaders', 'chance', 'win', 'orig', 'arcade', 'game', 'console', 'press', 'games', 'arcade', 'std', 'wap', 'charge', 'see', 'co', 'uk', 'games', 'terms', 'settings', 'purchase']\n",
      "sett\n",
      "After stemming with porters algorithm: ['bui', 'space', 'invad', 'chanc', 'win', 'orig', 'arcad', 'game', 'consol', 'press', 'game', 'arcad', 'std', 'wap', 'charg', 'see', 'game', 'term', 'set', 'purchas']\n",
      "Tokenized sentence: ['are', 'you', 'in', 'town', 'this', 'is', 'v', 'important']\n",
      "After stop words removal: ['town', 'v', 'important']\n",
      "After stemming with porters algorithm: ['town', 'import']\n",
      "Tokenized sentence: ['hi', 'kindly', 'give', 'us', 'back', 'our', 'documents', 'which', 'we', 'submitted', 'for', 'loan', 'from', 'stapati']\n",
      "After stop words removal: ['hi', 'kindly', 'give', 'us', 'back', 'documents', 'submitted', 'loan', 'stapati']\n",
      "After stemming with porters algorithm: ['kindli', 'give', 'back', 'docum', 'submit', 'loan', 'stapati']\n",
      "Tokenized sentence: ['pls', 'send', 'me', 'the', 'correct', 'name', 'da']\n",
      "After stop words removal: ['pls', 'send', 'correct', 'name', 'da']\n",
      "After stemming with porters algorithm: ['pl', 'send', 'correct', 'name']\n",
      "Tokenized sentence: ['aight', 'what', 'time', 'you', 'want', 'me', 'to', 'come', 'up']\n",
      "After stop words removal: ['aight', 'time', 'want', 'come']\n",
      "After stemming with porters algorithm: ['aight', 'time', 'want', 'come']\n",
      "Tokenized sentence: ['well', 'you', 'told', 'others', 'you', 'd', 'marry', 'them']\n",
      "After stop words removal: ['well', 'told', 'others', 'marry']\n",
      "After stemming with porters algorithm: ['well', 'told', 'other', 'marri']\n",
      "Tokenized sentence: ['baaaaaaaabe', 'wake', 'up', 'i', 'miss', 'you', 'i', 'crave', 'you', 'i', 'need', 'you']\n",
      "After stop words removal: ['baaaaaaaabe', 'wake', 'miss', 'crave', 'need']\n",
      "After stemming with porters algorithm: ['baaaaaaaab', 'wake', 'miss', 'crave', 'need']\n",
      "Tokenized sentence: ['well', 'the', 'general', 'price', 'is', 'lt', 'gt', 'oz', 'let', 'me', 'know', 'if', 'when', 'how', 'much', 'you', 'want']\n",
      "After stop words removal: ['well', 'general', 'price', 'lt', 'gt', 'oz', 'let', 'know', 'much', 'want']\n",
      "After stemming with porters algorithm: ['well', 'gener', 'price', 'let', 'know', 'much', 'want']\n",
      "Tokenized sentence: ['sorry', 'pa', 'i', 'dont', 'knw', 'who', 'ru', 'pa']\n",
      "After stop words removal: ['sorry', 'pa', 'dont', 'knw', 'ru', 'pa']\n",
      "After stemming with porters algorithm: ['sorri', 'dont', 'knw']\n",
      "Tokenized sentence: ['lol', 'you', 'won', 't', 'feel', 'bad', 'when', 'i', 'use', 'her', 'money', 'to', 'take', 'you', 'out', 'to', 'a', 'steak', 'dinner', 'd']\n",
      "After stop words removal: ['lol', 'feel', 'bad', 'use', 'money', 'take', 'steak', 'dinner']\n",
      "After stemming with porters algorithm: ['lol', 'feel', 'bad', 'us', 'monei', 'take', 'steak', 'dinner']\n",
      "Tokenized sentence: ['you', 'see', 'the', 'requirements', 'please']\n",
      "After stop words removal: ['see', 'requirements', 'please']\n",
      "After stemming with porters algorithm: ['see', 'requir', 'pleas']\n",
      "Tokenized sentence: ['hey', 'there', 's', 'veggie', 'pizza']\n",
      "After stop words removal: ['hey', 'veggie', 'pizza']\n",
      "After stemming with porters algorithm: ['hei', 'veggi', 'pizza']\n",
      "Tokenized sentence: ['nvm', 'i', 'm', 'going', 'to', 'wear', 'my', 'sport', 'shoes', 'anyway', 'i', 'm', 'going', 'to', 'be', 'late', 'leh']\n",
      "After stop words removal: ['nvm', 'going', 'wear', 'sport', 'shoes', 'anyway', 'going', 'late', 'leh']\n",
      "go\n",
      "go\n",
      "After stemming with porters algorithm: ['nvm', 'go', 'wear', 'sport', 'shoe', 'anywai', 'go', 'late', 'leh']\n",
      "Tokenized sentence: ['dad', 'went', 'out', 'oredi']\n",
      "After stop words removal: ['dad', 'went', 'oredi']\n",
      "After stemming with porters algorithm: ['dad', 'went', 'oredi']\n",
      "Tokenized sentence: ['i', 'dont', 'have', 'i', 'shall', 'buy', 'one', 'dear']\n",
      "After stop words removal: ['dont', 'shall', 'buy', 'one', 'dear']\n",
      "After stemming with porters algorithm: ['dont', 'shall', 'bui', 'on', 'dear']\n",
      "Tokenized sentence: ['make', 'sure', 'alex', 'knows', 'his', 'birthday', 'is', 'over', 'in', 'fifteen', 'minutes', 'as', 'far', 'as', 'you', 're', 'concerned']\n",
      "After stop words removal: ['make', 'sure', 'alex', 'knows', 'birthday', 'fifteen', 'minutes', 'far', 'concerned']\n",
      "After stemming with porters algorithm: ['make', 'sure', 'alex', 'know', 'birthdai', 'fifteen', 'minut', 'far', 'concer']\n",
      "Tokenized sentence: ['i', 'was', 'about', 'to', 'do', 'it', 'when', 'i', 'texted', 'i', 'finished', 'a', 'long', 'time', 'ago', 'and', 'showered', 'and', 'er', 'ything']\n",
      "After stop words removal: ['texted', 'finished', 'long', 'time', 'ago', 'showered', 'er', 'ything']\n",
      "After stemming with porters algorithm: ['tex', 'finis', 'long', 'time', 'ago', 'shower', 'ything']\n",
      "Tokenized sentence: ['k', 'did', 'you', 'call', 'me', 'just', 'now', 'ah']\n",
      "After stop words removal: ['k', 'call', 'ah']\n",
      "After stemming with porters algorithm: ['call']\n",
      "Tokenized sentence: ['i', 'like', 'to', 'think', 'there', 's', 'always', 'the', 'possibility', 'of', 'being', 'in', 'a', 'pub', 'later']\n",
      "After stop words removal: ['like', 'think', 'always', 'possibility', 'pub', 'later']\n",
      "After stemming with porters algorithm: ['like', 'think', 'alwai', 'possib', 'pub', 'later']\n",
      "Tokenized sentence: ['hmmm', 'k', 'but', 'i', 'want', 'to', 'change', 'the', 'field', 'quickly', 'da', 'i', 'wanna', 'get', 'system', 'administrator', 'or', 'network', 'administrator']\n",
      "After stop words removal: ['hmmm', 'k', 'want', 'change', 'field', 'quickly', 'da', 'wanna', 'get', 'system', 'administrator', 'network', 'administrator']\n",
      "After stemming with porters algorithm: ['hmmm', 'want', 'chang', 'field', 'quickli', 'wanna', 'get', 'system', 'administr', 'network', 'administr']\n",
      "Tokenized sentence: ['true', 'lov', 'n', 'care', 'wil', 'nevr', 'go', 'unrecognized', 'though', 'somone', 'often', 'makes', 'mistakes', 'when', 'valuing', 'it', 'but', 'they', 'will', 'definitly', 'undrstnd', 'once', 'when', 'they', 'start', 'missing', 'it']\n",
      "After stop words removal: ['true', 'lov', 'n', 'care', 'wil', 'nevr', 'go', 'unrecognized', 'though', 'somone', 'often', 'makes', 'mistakes', 'valuing', 'definitly', 'undrstnd', 'start', 'missing']\n",
      "unrecognize\n",
      "valu\n",
      "miss\n",
      "After stemming with porters algorithm: ['true', 'lov', 'care', 'wil', 'nevr', 'unrecogn', 'though', 'somon', 'often', 'make', 'mistak', 'valu', 'definitli', 'undrstnd', 'start', 'miss']\n",
      "Tokenized sentence: ['you', 'are', 'not', 'bothering', 'me', 'but', 'you', 'have', 'to', 'trust', 'my', 'answers', 'pls']\n",
      "After stop words removal: ['bothering', 'trust', 'answers', 'pls']\n",
      "bother\n",
      "After stemming with porters algorithm: ['bother', 'trust', 'answer', 'pl']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'nokia', 'i', 'colour', 'phone', 'delivered', 'tomorrow', 'with', 'free', 'minutes', 'to', 'any', 'mobile', 'free', 'text', 'free', 'camcorder', 'reply', 'or', 'call']\n",
      "After stop words removal: ['want', 'new', 'nokia', 'colour', 'phone', 'delivered', 'tomorrow', 'free', 'minutes', 'mobile', 'free', 'text', 'free', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['want', 'new', 'nokia', 'colour', 'phone', 'deliv', 'tomorrow', 'free', 'minut', 'mobil', 'free', 'text', 'free', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['id', 'onluy', 'matters', 'when', 'getting', 'on', 'from', 'offcampus']\n",
      "After stop words removal: ['id', 'onluy', 'matters', 'getting', 'offcampus']\n",
      "gett\n",
      "After stemming with porters algorithm: ['onlui', 'matter', 'get', 'offcampu']\n",
      "Tokenized sentence: ['ok', 'can']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['dear', 'all', 'as', 'we', 'know', 'lt', 'gt', 'th', 'is', 'the', 'lt', 'gt', 'th', 'birthday', 'of', 'our', 'loving', 'gopalettan', 'we', 'are', 'planning', 'to', 'give', 'a', 'small', 'gift', 'on', 'that', 'day', 'those', 'who', 'like', 'to', 'participate', 'in', 'that', 'you', 'are', 'welcome', 'please', 'contact', 'our', 'admin', 'team', 'for', 'more', 'details']\n",
      "After stop words removal: ['dear', 'know', 'lt', 'gt', 'th', 'lt', 'gt', 'th', 'birthday', 'loving', 'gopalettan', 'planning', 'give', 'small', 'gift', 'day', 'like', 'participate', 'welcome', 'please', 'contact', 'admin', 'team', 'details']\n",
      "lov\n",
      "plann\n",
      "After stemming with porters algorithm: ['dear', 'know', 'birthdai', 'love', 'gopalettan', 'plan', 'give', 'small', 'gift', 'dai', 'like', 'particip', 'welcom', 'pleas', 'contact', 'admin', 'team', 'detail']\n",
      "Tokenized sentence: ['at', 'home', 'watching', 'tv', 'lor']\n",
      "After stop words removal: ['home', 'watching', 'tv', 'lor']\n",
      "watch\n",
      "After stemming with porters algorithm: ['home', 'watc', 'lor']\n",
      "Tokenized sentence: ['just', 'dropped', 'em', 'off', 'omw', 'back', 'now']\n",
      "After stop words removal: ['dropped', 'em', 'omw', 'back']\n",
      "After stemming with porters algorithm: ['drop', 'omw', 'back']\n",
      "Tokenized sentence: ['you', 'are', 'right', 'though', 'i', 'can', 't', 'give', 'you', 'the', 'space', 'you', 'want', 'and', 'need', 'this', 'is', 'really', 'starting', 'to', 'become', 'an', 'issue', 'i', 'was', 'going', 'to', 'suggest', 'setting', 'a', 'definite', 'move', 'out', 'if', 'i', 'm', 'still', 'there', 'after', 'greece', 'but', 'maybe', 'you', 'are', 'ready', 'and', 'should', 'do', 'it', 'now']\n",
      "After stop words removal: ['right', 'though', 'give', 'space', 'want', 'need', 'really', 'starting', 'become', 'issue', 'going', 'suggest', 'setting', 'definite', 'move', 'still', 'greece', 'maybe', 'ready']\n",
      "start\n",
      "go\n",
      "sett\n",
      "After stemming with porters algorithm: ['right', 'though', 'give', 'space', 'want', 'need', 'realli', 'star', 'becom', 'issu', 'go', 'suggest', 'set', 'definit', 'move', 'still', 'greec', 'mayb', 'readi']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'its', 'been', 'ages', 'how', 's', 'abj']\n",
      "After stop words removal: ['ages', 'abj']\n",
      "After stemming with porters algorithm: ['ag', 'abj']\n",
      "Tokenized sentence: ['sms', 'ac', 'sun', 'posts', 'hello', 'you', 'seem', 'cool']\n",
      "After stop words removal: ['sms', 'ac', 'sun', 'posts', 'hello', 'seem', 'cool']\n",
      "After stemming with porters algorithm: ['sm', 'sun', 'post', 'hello', 'seem', 'cool']\n",
      "Tokenized sentence: ['is', 'that', 'what', 'time', 'you', 'want', 'me', 'to', 'come']\n",
      "After stop words removal: ['time', 'want', 'come']\n",
      "After stemming with porters algorithm: ['time', 'want', 'come']\n",
      "Tokenized sentence: ['well', 'good', 'morning', 'mr', 'hows', 'london', 'treatin', 'ya', 'treacle']\n",
      "After stop words removal: ['well', 'good', 'morning', 'mr', 'hows', 'london', 'treatin', 'ya', 'treacle']\n",
      "morn\n",
      "After stemming with porters algorithm: ['well', 'good', 'mor', 'how', 'london', 'treatin', 'treacl']\n",
      "Tokenized sentence: ['y', 'de', 'asking', 'like', 'this']\n",
      "After stop words removal: ['de', 'asking', 'like']\n",
      "ask\n",
      "After stemming with porters algorithm: ['as', 'like']\n",
      "Tokenized sentence: ['imagine', 'life', 'without', 'me', 'see', 'how', 'fast', 'u', 'are', 'searching', 'me', 'don', 't', 'worry', 'l', 'm', 'always', 'there', 'to', 'disturb', 'u', 'goodnoon']\n",
      "After stop words removal: ['imagine', 'life', 'without', 'see', 'fast', 'u', 'searching', 'worry', 'l', 'always', 'disturb', 'u', 'goodnoon']\n",
      "search\n",
      "After stemming with porters algorithm: ['imagin', 'life', 'without', 'see', 'fast', 'searc', 'worri', 'alwai', 'disturb', 'goodnoon']\n",
      "Tokenized sentence: ['i', 'know', 'you', 'mood', 'off', 'today']\n",
      "After stop words removal: ['know', 'mood', 'today']\n",
      "After stemming with porters algorithm: ['know', 'mood', 'todai']\n",
      "Tokenized sentence: ['u', 'haven', 't', 'lost', 'me', 'ill', 'always', 'b', 'here', 'u', 'i', 'didn', 't', 'intend', 'hurt', 'u', 'but', 'i', 'never', 'knew', 'how', 'u', 'felt', 'about', 'me', 'when', 'iwas', 'marine', 'that', 's', 'what', 'itried', 'tell', 'urmom', 'i', 'careabout', 'u']\n",
      "After stop words removal: ['u', 'lost', 'ill', 'always', 'b', 'u', 'intend', 'hurt', 'u', 'never', 'knew', 'u', 'felt', 'iwas', 'marine', 'itried', 'tell', 'urmom', 'careabout', 'u']\n",
      "After stemming with porters algorithm: ['lost', 'ill', 'alwai', 'intend', 'hurt', 'never', 'knew', 'felt', 'iwa', 'marin', 'itri', 'tell', 'urmom', 'careabout']\n",
      "Tokenized sentence: ['so', 'how', 's', 'scotland', 'hope', 'you', 'are', 'not', 'over', 'showing', 'your', 'jjc', 'tendencies', 'take', 'care', 'live', 'the', 'dream']\n",
      "After stop words removal: ['scotland', 'hope', 'showing', 'jjc', 'tendencies', 'take', 'care', 'live', 'dream']\n",
      "show\n",
      "After stemming with porters algorithm: ['scotland', 'hope', 'showe', 'jjc', 'tendenc', 'take', 'care', 'live', 'dream']\n",
      "Tokenized sentence: ['only', 'if', 'you', 'promise', 'your', 'getting', 'out', 'as', 'soon', 'as', 'you', 'can', 'and', 'you', 'll', 'text', 'me', 'in', 'the', 'morning', 'to', 'let', 'me', 'know', 'you', 'made', 'it', 'in', 'ok']\n",
      "After stop words removal: ['promise', 'getting', 'soon', 'text', 'morning', 'let', 'know', 'made', 'ok']\n",
      "gett\n",
      "morn\n",
      "After stemming with porters algorithm: ['promis', 'get', 'soon', 'text', 'mor', 'let', 'know', 'made']\n",
      "Tokenized sentence: ['yeah', 'i', 'imagine', 'he', 'would', 'be', 'really', 'gentle', 'unlike', 'the', 'other', 'docs', 'who', 'treat', 'their', 'patients', 'like', 'turkeys']\n",
      "After stop words removal: ['yeah', 'imagine', 'would', 'really', 'gentle', 'unlike', 'docs', 'treat', 'patients', 'like', 'turkeys']\n",
      "After stemming with porters algorithm: ['yeah', 'imagin', 'would', 'realli', 'gentl', 'unlik', 'doc', 'treat', 'patient', 'like', 'turkei']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'sent', 'lt', 'gt', 'mesages', 'today', 'thats', 'y', 'sorry', 'if', 'i', 'hurts']\n",
      "After stop words removal: ['want', 'sent', 'lt', 'gt', 'mesages', 'today', 'thats', 'sorry', 'hurts']\n",
      "After stemming with porters algorithm: ['want', 'sent', 'mesag', 'todai', 'that', 'sorri', 'hurt']\n",
      "Tokenized sentence: ['we', 'have', 'to', 'pick', 'rayan', 'macleran', 'there']\n",
      "After stop words removal: ['pick', 'rayan', 'macleran']\n",
      "After stemming with porters algorithm: ['pick', 'rayan', 'macleran']\n",
      "Tokenized sentence: ['dear', 'me', 'at', 'cherthala', 'in', 'case', 'u', 'r', 'coming', 'cochin', 'pls', 'call', 'bfore', 'u', 'start', 'i', 'shall', 'also', 'reach', 'accordingly', 'or', 'tell', 'me', 'which', 'day', 'u', 'r', 'coming', 'tmorow', 'i', 'am', 'engaged', 'ans', 'its', 'holiday']\n",
      "After stop words removal: ['dear', 'cherthala', 'case', 'u', 'r', 'coming', 'cochin', 'pls', 'call', 'bfore', 'u', 'start', 'shall', 'also', 'reach', 'accordingly', 'tell', 'day', 'u', 'r', 'coming', 'tmorow', 'engaged', 'ans', 'holiday']\n",
      "com\n",
      "com\n",
      "After stemming with porters algorithm: ['dear', 'cherthala', 'case', 'come', 'cochin', 'pl', 'call', 'bfore', 'start', 'shall', 'also', 'reach', 'accordingli', 'tell', 'dai', 'come', 'tmorow', 'engag', 'an', 'holidai']\n",
      "Tokenized sentence: ['hot', 'live', 'fantasies', 'call', 'now', 'just', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'is', 'a', 'national', 'rate', 'call']\n",
      "After stop words removal: ['hot', 'live', 'fantasies', 'call', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'national', 'rate', 'call']\n",
      "After stemming with porters algorithm: ['hot', 'live', 'fantasi', 'call', 'per', 'min', 'ntt', 'ltd', 'box', 'croydon', 'nat', 'rate', 'call']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later', 'in', 'meeting']\n",
      "After stop words removal: ['sorry', 'call', 'later', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later', 'meet']\n",
      "Tokenized sentence: ['u', 'don', 't', 'remember', 'that', 'old', 'commercial']\n",
      "After stop words removal: ['u', 'remember', 'old', 'commercial']\n",
      "After stemming with porters algorithm: ['rememb', 'old', 'commerci']\n",
      "Tokenized sentence: ['free', 'any', 'day', 'but', 'i', 'finish', 'at', 'on', 'mon', 'n', 'thurs']\n",
      "After stop words removal: ['free', 'day', 'finish', 'mon', 'n', 'thurs']\n",
      "After stemming with porters algorithm: ['free', 'dai', 'finish', 'mon', 'thur']\n",
      "Tokenized sentence: ['i', 'miss', 'you', 'so', 'much', 'i', 'm', 'so', 'desparate', 'i', 'have', 'recorded', 'the', 'message', 'you', 'left', 'for', 'me', 'the', 'other', 'day', 'and', 'listen', 'to', 'it', 'just', 'to', 'hear', 'the', 'sound', 'of', 'your', 'voice', 'i', 'love', 'you']\n",
      "After stop words removal: ['miss', 'much', 'desparate', 'recorded', 'message', 'left', 'day', 'listen', 'hear', 'sound', 'voice', 'love']\n",
      "After stemming with porters algorithm: ['miss', 'much', 'despar', 'recor', 'messag', 'left', 'dai', 'listen', 'hear', 'sound', 'voic', 'love']\n",
      "Tokenized sentence: ['i', 'am', 'seeking', 'a', 'lady', 'in', 'the', 'street', 'and', 'a', 'freak', 'in', 'the', 'sheets', 'is', 'that', 'you']\n",
      "After stop words removal: ['seeking', 'lady', 'street', 'freak', 'sheets']\n",
      "seek\n",
      "After stemming with porters algorithm: ['seek', 'ladi', 'street', 'freak', 'sheet']\n",
      "Tokenized sentence: ['you', 'got', 'job', 'in', 'wipro', 'you', 'will', 'get', 'every', 'thing', 'in', 'life', 'in', 'or', 'years']\n",
      "After stop words removal: ['got', 'job', 'wipro', 'get', 'every', 'thing', 'life', 'years']\n",
      "After stemming with porters algorithm: ['got', 'job', 'wipro', 'get', 'everi', 'thing', 'life', 'year']\n",
      "Tokenized sentence: ['cheers', 'u', 'tex', 'mecause', 'u', 'werebored', 'yeah', 'okden', 'hunny', 'r', 'uin', 'wk', 'sat', 'sound', 's', 'likeyour', 'havin', 'gr', 'fun', 'j', 'keep', 'updat', 'countinlots', 'of', 'loveme', 'xxxxx']\n",
      "After stop words removal: ['cheers', 'u', 'tex', 'mecause', 'u', 'werebored', 'yeah', 'okden', 'hunny', 'r', 'uin', 'wk', 'sat', 'sound', 'likeyour', 'havin', 'gr', 'fun', 'j', 'keep', 'updat', 'countinlots', 'loveme', 'xxxxx']\n",
      "After stemming with porters algorithm: ['cheer', 'tex', 'mecaus', 'werebor', 'yeah', 'okden', 'hunni', 'uin', 'sat', 'sound', 'likeyour', 'havin', 'fun', 'keep', 'updat', 'countinlot', 'lovem', 'xxxxx']\n",
      "Tokenized sentence: ['prof', 'you', 'have', 'passed', 'in', 'all', 'the', 'papers', 'in', 'this', 'sem', 'congrats', 'student', 'enna', 'kalaachutaarama', 'prof', 'gud', 'mrng']\n",
      "After stop words removal: ['prof', 'passed', 'papers', 'sem', 'congrats', 'student', 'enna', 'kalaachutaarama', 'prof', 'gud', 'mrng']\n",
      "After stemming with porters algorithm: ['prof', 'pass', 'paper', 'sem', 'congrat', 'student', 'enna', 'kalaachutaarama', 'prof', 'gud', 'mrng']\n",
      "Tokenized sentence: ['are', 'you', 'angry', 'with', 'me', 'what', 'happen', 'dear']\n",
      "After stop words removal: ['angry', 'happen', 'dear']\n",
      "After stemming with porters algorithm: ['angri', 'happen', 'dear']\n",
      "Tokenized sentence: ['i', 'have', 'no', 'money', 'steve', 'mate']\n",
      "After stop words removal: ['money', 'steve', 'mate']\n",
      "After stemming with porters algorithm: ['monei', 'steve', 'mate']\n",
      "Tokenized sentence: ['cool', 'text', 'me', 'when', 'you', 'head', 'out']\n",
      "After stop words removal: ['cool', 'text', 'head']\n",
      "After stemming with porters algorithm: ['cool', 'text', 'head']\n",
      "Tokenized sentence: ['s', 's', 'india', 'going', 'to', 'draw', 'the', 'series', 'after', 'many', 'years', 'in', 'south', 'african', 'soil']\n",
      "After stop words removal: ['india', 'going', 'draw', 'series', 'many', 'years', 'south', 'african', 'soil']\n",
      "go\n",
      "After stemming with porters algorithm: ['india', 'go', 'draw', 'seri', 'mani', 'year', 'south', 'african', 'soil']\n",
      "Tokenized sentence: ['yar', 'lor', 'keep', 'raining', 'non', 'stop', 'or', 'u', 'wan', 'go', 'elsewhere']\n",
      "After stop words removal: ['yar', 'lor', 'keep', 'raining', 'non', 'stop', 'u', 'wan', 'go', 'elsewhere']\n",
      "rain\n",
      "After stemming with porters algorithm: ['yar', 'lor', 'keep', 'rain', 'non', 'stop', 'wan', 'elsewher']\n",
      "Tokenized sentence: ['ok', 'c', 'u', 'then']\n",
      "After stop words removal: ['ok', 'c', 'u']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['i', 'm', 'not', 'smoking', 'while', 'people', 'use', 'wylie', 'smokes', 'too', 'much', 'to', 'justify', 'ruining', 'my', 'shit']\n",
      "After stop words removal: ['smoking', 'people', 'use', 'wylie', 'smokes', 'much', 'justify', 'ruining', 'shit']\n",
      "smok\n",
      "ruin\n",
      "After stemming with porters algorithm: ['smoke', 'peopl', 'us', 'wylie', 'smoke', 'much', 'justifi', 'ruin', 'shit']\n",
      "Tokenized sentence: ['i', 'thk', 'gotta', 'go', 'home', 'by', 'urself', 'cos', 'i', 'll', 'b', 'going', 'out', 'shopping', 'my', 'frens', 'present']\n",
      "After stop words removal: ['thk', 'gotta', 'go', 'home', 'urself', 'cos', 'b', 'going', 'shopping', 'frens', 'present']\n",
      "go\n",
      "shopp\n",
      "After stemming with porters algorithm: ['thk', 'gotta', 'home', 'urself', 'co', 'go', 'shop', 'fren', 'present']\n",
      "Tokenized sentence: ['i', 'm', 'already', 'back', 'home', 'so', 'no', 'probably', 'not']\n",
      "After stop words removal: ['already', 'back', 'home', 'probably']\n",
      "After stemming with porters algorithm: ['alreadi', 'back', 'home', 'probab']\n",
      "Tokenized sentence: ['i', 'll', 'see', 'if', 'i', 'can', 'swing', 'by', 'in', 'a', 'bit', 'got', 'some', 'things', 'to', 'take', 'care', 'of', 'here', 'firsg']\n",
      "After stop words removal: ['see', 'swing', 'bit', 'got', 'things', 'take', 'care', 'firsg']\n",
      "After stemming with porters algorithm: ['see', 'swing', 'bit', 'got', 'thing', 'take', 'care', 'firsg']\n",
      "Tokenized sentence: ['how', 'is', 'it', 'possible', 'to', 'teach', 'you', 'and', 'where']\n",
      "After stop words removal: ['possible', 'teach']\n",
      "After stemming with porters algorithm: ['possib', 'teach']\n",
      "Tokenized sentence: ['sir', 'goodmorning', 'once', 'free', 'call', 'me']\n",
      "After stop words removal: ['sir', 'goodmorning', 'free', 'call']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['sir', 'goodmor', 'free', 'call']\n",
      "Tokenized sentence: ['im', 'just', 'wondering', 'what', 'your', 'doing', 'right', 'now']\n",
      "After stop words removal: ['im', 'wondering', 'right']\n",
      "wonder\n",
      "After stemming with porters algorithm: ['wonder', 'right']\n",
      "Tokenized sentence: ['what', 'time', 'you', 'thinkin', 'of', 'goin']\n",
      "After stop words removal: ['time', 'thinkin', 'goin']\n",
      "After stemming with porters algorithm: ['time', 'thinkin', 'goin']\n",
      "Tokenized sentence: ['yes', 'the', 'only', 'place', 'in', 'town', 'to', 'meet', 'exciting', 'adult', 'singles', 'is', 'now', 'in', 'the', 'uk', 'txt', 'chat', 'to', 'now', 'p', 'msg']\n",
      "After stop words removal: ['yes', 'place', 'town', 'meet', 'exciting', 'adult', 'singles', 'uk', 'txt', 'chat', 'p', 'msg']\n",
      "excit\n",
      "After stemming with porters algorithm: ['ye', 'place', 'town', 'meet', 'excit', 'adult', 'singl', 'txt', 'chat', 'msg']\n",
      "Tokenized sentence: ['thanks', 'for', 'loving', 'me', 'so', 'you', 'rock']\n",
      "After stop words removal: ['thanks', 'loving', 'rock']\n",
      "lov\n",
      "After stemming with porters algorithm: ['thank', 'love', 'rock']\n",
      "Tokenized sentence: ['hope', 'you', 'are', 'having', 'a', 'great', 'day']\n",
      "After stop words removal: ['hope', 'great', 'day']\n",
      "After stemming with porters algorithm: ['hope', 'great', 'dai']\n",
      "Tokenized sentence: ['good', 'sleep', 'is', 'about', 'rhythm', 'the', 'person', 'has', 'to', 'establish', 'a', 'rhythm', 'that', 'the', 'body', 'will', 'learn', 'and', 'use', 'if', 'you', 'want', 'to', 'know', 'more']\n",
      "After stop words removal: ['good', 'sleep', 'rhythm', 'person', 'establish', 'rhythm', 'body', 'learn', 'use', 'want', 'know']\n",
      "After stemming with porters algorithm: ['good', 'sleep', 'rhythm', 'person', 'establish', 'rhythm', 'bodi', 'learn', 'us', 'want', 'know']\n",
      "Tokenized sentence: ['there', 'generally', 'isn', 't', 'one', 'it', 's', 'an', 'uncountable', 'noun', 'u', 'in', 'the', 'dictionary', 'pieces', 'of', 'research']\n",
      "After stop words removal: ['generally', 'one', 'uncountable', 'noun', 'u', 'dictionary', 'pieces', 'research']\n",
      "After stemming with porters algorithm: ['gener', 'on', 'uncount', 'noun', 'dictionari', 'piec', 'research']\n",
      "Tokenized sentence: ['u', 'studying', 'in', 'sch', 'or', 'going', 'home', 'anyway', 'i', 'll', 'b', 'going', 'sch', 'later']\n",
      "After stop words removal: ['u', 'studying', 'sch', 'going', 'home', 'anyway', 'b', 'going', 'sch', 'later']\n",
      "study\n",
      "go\n",
      "go\n",
      "After stemming with porters algorithm: ['stud', 'sch', 'go', 'home', 'anywai', 'go', 'sch', 'later']\n",
      "Tokenized sentence: ['please', 'call', 'immediately', 'as', 'there', 'is', 'an', 'urgent', 'message', 'waiting', 'for', 'you']\n",
      "After stop words removal: ['please', 'call', 'immediately', 'urgent', 'message', 'waiting']\n",
      "wait\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'immedi', 'urgent', 'messag', 'wait']\n",
      "Tokenized sentence: ['hello', 'boytoy', 'geeee', 'i', 'm', 'missing', 'you', 'today', 'i', 'like', 'to', 'send', 'you', 'a', 'tm', 'and', 'remind', 'you', 'i', 'm', 'thinking', 'of', 'you', 'and', 'you', 'are', 'loved', 'loving', 'kiss']\n",
      "After stop words removal: ['hello', 'boytoy', 'geeee', 'missing', 'today', 'like', 'send', 'tm', 'remind', 'thinking', 'loved', 'loving', 'kiss']\n",
      "miss\n",
      "think\n",
      "lov\n",
      "After stemming with porters algorithm: ['hello', 'boytoi', 'geeee', 'miss', 'todai', 'like', 'send', 'remind', 'thin', 'love', 'love', 'kiss']\n",
      "Tokenized sentence: ['i', 'want', 'snow', 'it', 's', 'just', 'freezing', 'and', 'windy']\n",
      "After stop words removal: ['want', 'snow', 'freezing', 'windy']\n",
      "freez\n",
      "After stemming with porters algorithm: ['want', 'snow', 'freez', 'windi']\n",
      "Tokenized sentence: ['storming', 'msg', 'wen', 'u', 'lift', 'd', 'phne', 'u', 'say', 'hello', 'do', 'u', 'knw', 'wt', 'is', 'd', 'real', 'meaning', 'of', 'hello', 'it', 's', 'd', 'name', 'of', 'a', 'girl', 'yes', 'and', 'u', 'knw', 'who', 'is', 'dat', 'girl', 'margaret', 'hello', 'she', 'is', 'd', 'girlfrnd', 'f', 'grahmbell', 'who', 'invnted', 'telphone', 'moral', 'one', 'can', 'get', 'd', 'name', 'of', 'a', 'person']\n",
      "After stop words removal: ['storming', 'msg', 'wen', 'u', 'lift', 'phne', 'u', 'say', 'hello', 'u', 'knw', 'wt', 'real', 'meaning', 'hello', 'name', 'girl', 'yes', 'u', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'f', 'grahmbell', 'invnted', 'telphone', 'moral', 'one', 'get', 'name', 'person']\n",
      "storm\n",
      "mean\n",
      "After stemming with porters algorithm: ['stor', 'msg', 'wen', 'lift', 'phne', 'sai', 'hello', 'knw', 'real', 'mean', 'hello', 'name', 'girl', 'ye', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'grahmbel', 'invn', 'telphon', 'moral', 'on', 'get', 'name', 'person']\n",
      "Tokenized sentence: ['k', 'i', 'll', 'take', 'care', 'of', 'it']\n",
      "After stop words removal: ['k', 'take', 'care']\n",
      "After stemming with porters algorithm: ['take', 'care']\n",
      "Tokenized sentence: ['we', 'stopped', 'to', 'get', 'ice', 'cream', 'and', 'will', 'go', 'back', 'after']\n",
      "After stop words removal: ['stopped', 'get', 'ice', 'cream', 'go', 'back']\n",
      "After stemming with porters algorithm: ['stop', 'get', 'ic', 'cream', 'back']\n",
      "Tokenized sentence: ['aight', 'call', 'me', 'once', 'you', 're', 'close']\n",
      "After stop words removal: ['aight', 'call', 'close']\n",
      "After stemming with porters algorithm: ['aight', 'call', 'close']\n",
      "Tokenized sentence: ['no', 'let', 'me', 'do', 'the', 'math', 'your', 'not', 'good', 'at', 'it']\n",
      "After stop words removal: ['let', 'math', 'good']\n",
      "After stemming with porters algorithm: ['let', 'math', 'good']\n",
      "Tokenized sentence: ['you', 'all', 'ready', 'for', 'big', 'day', 'tomorrow']\n",
      "After stop words removal: ['ready', 'big', 'day', 'tomorrow']\n",
      "After stemming with porters algorithm: ['readi', 'big', 'dai', 'tomorrow']\n",
      "Tokenized sentence: ['st', 'wk', 'free', 'gr', 'tones', 'str', 'u', 'each', 'wk', 'txt', 'nokia', 'on', 'to', 'for', 'classic', 'nokia', 'tones', 'or', 'hit', 'on', 'to', 'for', 'polys', 'nokia', 'p', 'poly', 'p']\n",
      "After stop words removal: ['st', 'wk', 'free', 'gr', 'tones', 'str', 'u', 'wk', 'txt', 'nokia', 'classic', 'nokia', 'tones', 'hit', 'polys', 'nokia', 'p', 'poly', 'p']\n",
      "After stemming with porters algorithm: ['free', 'tone', 'str', 'txt', 'nokia', 'classic', 'nokia', 'tone', 'hit', 'poli', 'nokia', 'poli']\n",
      "Tokenized sentence: ['i', 'm', 'coming', 'home', 'dinner']\n",
      "After stop words removal: ['coming', 'home', 'dinner']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'home', 'dinner']\n",
      "Tokenized sentence: ['ever', 'green', 'quote', 'ever', 'told', 'by', 'jerry', 'in', 'cartoon', 'a', 'person', 'who', 'irritates', 'u', 'always', 'is', 'the', 'one', 'who', 'loves', 'u', 'vry', 'much', 'but', 'fails', 'to', 'express', 'it', 'gud', 'nyt']\n",
      "After stop words removal: ['ever', 'green', 'quote', 'ever', 'told', 'jerry', 'cartoon', 'person', 'irritates', 'u', 'always', 'one', 'loves', 'u', 'vry', 'much', 'fails', 'express', 'gud', 'nyt']\n",
      "After stemming with porters algorithm: ['ever', 'green', 'quot', 'ever', 'told', 'jerri', 'cartoon', 'person', 'irrit', 'alwai', 'on', 'love', 'vry', 'much', 'fail', 'express', 'gud', 'nyt']\n",
      "Tokenized sentence: ['in', 'xam', 'hall', 'boy', 'asked', 'girl', 'tell', 'me', 'the', 'starting', 'term', 'for', 'dis', 'answer', 'i', 'can', 'den', 'manage', 'on', 'my', 'own', 'after', 'lot', 'of', 'hesitation', 'n', 'lookin', 'around', 'silently', 'she', 'said', 'the', 'intha', 'ponnungale', 'ipaditan']\n",
      "After stop words removal: ['xam', 'hall', 'boy', 'asked', 'girl', 'tell', 'starting', 'term', 'dis', 'answer', 'den', 'manage', 'lot', 'hesitation', 'n', 'lookin', 'around', 'silently', 'said', 'intha', 'ponnungale', 'ipaditan']\n",
      "start\n",
      "After stemming with porters algorithm: ['xam', 'hall', 'boi', 'as', 'girl', 'tell', 'star', 'term', 'di', 'answer', 'den', 'manag', 'lot', 'hesit', 'lookin', 'around', 'silent', 'said', 'intha', 'ponnungal', 'ipaditan']\n",
      "Tokenized sentence: ['kate', 'jackson', 'rec', 'center', 'before', 'ish', 'right']\n",
      "After stop words removal: ['kate', 'jackson', 'rec', 'center', 'ish', 'right']\n",
      "After stemming with porters algorithm: ['kate', 'jackson', 'rec', 'center', 'ish', 'right']\n",
      "Tokenized sentence: ['ugh', 'hopefully', 'the', 'asus', 'ppl', 'dont', 'randomly', 'do', 'a', 'reformat']\n",
      "After stop words removal: ['ugh', 'hopefully', 'asus', 'ppl', 'dont', 'randomly', 'reformat']\n",
      "After stemming with porters algorithm: ['ugh', 'hopefulli', 'asu', 'ppl', 'dont', 'randomli', 'reformat']\n",
      "Tokenized sentence: ['no', 'wonder', 'cos', 'i', 'dun', 'rem', 'seeing', 'a', 'silver', 'car', 'but', 'i', 'thk', 'i', 'saw', 'a', 'black', 'one']\n",
      "After stop words removal: ['wonder', 'cos', 'dun', 'rem', 'seeing', 'silver', 'car', 'thk', 'saw', 'black', 'one']\n",
      "see\n",
      "After stemming with porters algorithm: ['wonder', 'co', 'dun', 'rem', 'see', 'silver', 'car', 'thk', 'saw', 'black', 'on']\n",
      "Tokenized sentence: ['hi', 'what', 'you', 'think', 'about', 'match']\n",
      "After stop words removal: ['hi', 'think', 'match']\n",
      "After stemming with porters algorithm: ['think', 'match']\n",
      "Tokenized sentence: ['dear', 'voucher', 'holder', 'to', 'claim', 'this', 'weeks', 'offer', 'at', 'your', 'pc', 'please', 'go', 'to', 'http', 'www', 'wtlp', 'co', 'uk', 'text', 'ts', 'cs', 'apply']\n",
      "After stop words removal: ['dear', 'voucher', 'holder', 'claim', 'weeks', 'offer', 'pc', 'please', 'go', 'http', 'www', 'wtlp', 'co', 'uk', 'text', 'ts', 'cs', 'apply']\n",
      "After stemming with porters algorithm: ['dear', 'voucher', 'holder', 'claim', 'week', 'offer', 'pleas', 'http', 'www', 'wtlp', 'text', 'appli']\n",
      "Tokenized sentence: ['hard', 'live', 'chat', 'just', 'p', 'min', 'choose', 'your', 'girl', 'and', 'connect', 'live', 'call', 'now', 'cheap', 'chat', 'uk', 's', 'biggest', 'live', 'service', 'vu', 'bcm', 'wc', 'n', 'xx']\n",
      "After stop words removal: ['hard', 'live', 'chat', 'p', 'min', 'choose', 'girl', 'connect', 'live', 'call', 'cheap', 'chat', 'uk', 'biggest', 'live', 'service', 'vu', 'bcm', 'wc', 'n', 'xx']\n",
      "After stemming with porters algorithm: ['hard', 'live', 'chat', 'min', 'choos', 'girl', 'connect', 'live', 'call', 'cheap', 'chat', 'biggest', 'live', 'servic', 'bcm']\n",
      "Tokenized sentence: ['shit', 'babe', 'thasa', 'bit', 'messed', 'up', 'yeh']\n",
      "After stop words removal: ['shit', 'babe', 'thasa', 'bit', 'messed', 'yeh']\n",
      "After stemming with porters algorithm: ['shit', 'babe', 'thasa', 'bit', 'mess', 'yeh']\n",
      "Tokenized sentence: ['sure', 'if', 'i', 'get', 'an', 'acknowledgement', 'from', 'you', 'that', 'it', 's', 'astoundingly', 'tactless', 'and', 'generally', 'faggy', 'to', 'demand', 'a', 'blood', 'oath', 'fo']\n",
      "After stop words removal: ['sure', 'get', 'acknowledgement', 'astoundingly', 'tactless', 'generally', 'faggy', 'demand', 'blood', 'oath', 'fo']\n",
      "After stemming with porters algorithm: ['sure', 'get', 'acknowledg', 'astoundingli', 'tactless', 'gener', 'faggi', 'demand', 'blood', 'oath']\n",
      "Tokenized sentence: ['ay', 'wana', 'meet', 'on', 'sat', 'wkg', 'on', 'sat']\n",
      "After stop words removal: ['ay', 'wana', 'meet', 'sat', 'wkg', 'sat']\n",
      "After stemming with porters algorithm: ['wana', 'meet', 'sat', 'wkg', 'sat']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'you', 'last', 'weekends', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'claim', 'code', 's', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'last', 'weekends', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'claim', 'code', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'last', 'weekend', 'draw', 'show', 'priz', 'guaranteed', 'call', 'claim', 'code', 'valid', 'hr']\n",
      "Tokenized sentence: ['i', 'dont', 'know', 'ask', 'to', 'my', 'brother', 'nothing', 'problem', 'some', 'thing', 'that', 'just', 'i', 'told']\n",
      "After stop words removal: ['dont', 'know', 'ask', 'brother', 'nothing', 'problem', 'thing', 'told']\n",
      "noth\n",
      "After stemming with porters algorithm: ['dont', 'know', 'ask', 'brother', 'not', 'problem', 'thing', 'told']\n",
      "Tokenized sentence: ['wiskey', 'brandy', 'rum', 'gin', 'beer', 'vodka', 'scotch', 'shampain', 'wine', 'kudi', 'yarasu', 'dhina', 'vaazhthukkal']\n",
      "After stop words removal: ['wiskey', 'brandy', 'rum', 'gin', 'beer', 'vodka', 'scotch', 'shampain', 'wine', 'kudi', 'yarasu', 'dhina', 'vaazhthukkal']\n",
      "After stemming with porters algorithm: ['wiskei', 'brandi', 'rum', 'gin', 'beer', 'vodka', 'scotch', 'shampain', 'wine', 'kudi', 'yarasu', 'dhina', 'vaazhthukk']\n",
      "Tokenized sentence: ['well', 'keep', 'in', 'mind', 'i', 've', 'only', 'got', 'enough', 'gas', 'for', 'one', 'more', 'round', 'trip', 'barring', 'a', 'sudden', 'influx', 'of', 'cash']\n",
      "After stop words removal: ['well', 'keep', 'mind', 'got', 'enough', 'gas', 'one', 'round', 'trip', 'barring', 'sudden', 'influx', 'cash']\n",
      "barr\n",
      "After stemming with porters algorithm: ['well', 'keep', 'mind', 'got', 'enough', 'ga', 'on', 'round', 'trip', 'bar', 'sudden', 'influx', 'cash']\n",
      "Tokenized sentence: ['for', 'sale', 'arsenal', 'dartboard', 'good', 'condition', 'but', 'no', 'doubles', 'or', 'trebles']\n",
      "After stop words removal: ['sale', 'arsenal', 'dartboard', 'good', 'condition', 'doubles', 'trebles']\n",
      "After stemming with porters algorithm: ['sale', 'arsen', 'dartboard', 'good', 'condit', 'doubl', 'trebl']\n",
      "Tokenized sentence: ['lol', 'your', 'right', 'what', 'diet', 'everyday', 'i', 'cheat', 'anyway', 'i', 'm', 'meant', 'to', 'be', 'a', 'fatty']\n",
      "After stop words removal: ['lol', 'right', 'diet', 'everyday', 'cheat', 'anyway', 'meant', 'fatty']\n",
      "After stemming with porters algorithm: ['lol', 'right', 'diet', 'everydai', 'cheat', 'anywai', 'meant', 'fatti']\n",
      "Tokenized sentence: ['the', 'search', 'happiness', 'is', 'of', 'd', 'main', 'sources', 'of', 'unhappiness', 'accept', 'life', 'the', 'way', 'it', 'comes', 'u', 'will', 'find', 'happiness', 'in', 'every', 'moment', 'u', 'live']\n",
      "After stop words removal: ['search', 'happiness', 'main', 'sources', 'unhappiness', 'accept', 'life', 'way', 'comes', 'u', 'find', 'happiness', 'every', 'moment', 'u', 'live']\n",
      "After stemming with porters algorithm: ['search', 'happi', 'main', 'sourc', 'unhappi', 'accept', 'life', 'wai', 'come', 'find', 'happi', 'everi', 'moment', 'live']\n",
      "Tokenized sentence: ['as', 'i', 'entered', 'my', 'cabin', 'my', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'i', 'felt', 'special', 'she', 'askd', 'me', 'lunch', 'after', 'lunch', 'she', 'invited', 'me', 'to', 'her', 'apartment', 'we', 'went', 'there']\n",
      "After stop words removal: ['entered', 'cabin', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invited', 'apartment', 'went']\n",
      "After stemming with porters algorithm: ['enter', 'cabin', 'said', 'happi', 'dai', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invit', 'apart', 'went']\n",
      "Tokenized sentence: ['if', 'you', 'have', 'belive', 'me', 'come', 'to', 'my', 'home']\n",
      "After stop words removal: ['belive', 'come', 'home']\n",
      "After stemming with porters algorithm: ['beliv', 'come', 'home']\n",
      "Tokenized sentence: ['u', 'in', 'town', 'alone']\n",
      "After stop words removal: ['u', 'town', 'alone']\n",
      "After stemming with porters algorithm: ['town', 'alon']\n",
      "Tokenized sentence: ['can', 'dunno', 'wat', 'to', 'get', 'her']\n",
      "After stop words removal: ['dunno', 'wat', 'get']\n",
      "After stemming with porters algorithm: ['dunno', 'wat', 'get']\n",
      "Tokenized sentence: ['yeah', 'sure', 'i', 'll', 'leave', 'in', 'a', 'min']\n",
      "After stop words removal: ['yeah', 'sure', 'leave', 'min']\n",
      "After stemming with porters algorithm: ['yeah', 'sure', 'leav', 'min']\n",
      "Tokenized sentence: ['r', 'going', 'today', 's', 'meeting']\n",
      "After stop words removal: ['r', 'going', 'today', 'meeting']\n",
      "go\n",
      "meet\n",
      "After stemming with porters algorithm: ['go', 'todai', 'meet']\n",
      "Tokenized sentence: ['lt', 'gt', 'mins', 'but', 'i', 'had', 'to', 'stop', 'somewhere', 'first']\n",
      "After stop words removal: ['lt', 'gt', 'mins', 'stop', 'somewhere', 'first']\n",
      "After stemming with porters algorithm: ['min', 'stop', 'somewher', 'first']\n",
      "Tokenized sentence: ['so', 'gd', 'got', 'free', 'ice', 'cream', 'i', 'oso', 'wan']\n",
      "After stop words removal: ['gd', 'got', 'free', 'ice', 'cream', 'oso', 'wan']\n",
      "After stemming with porters algorithm: ['got', 'free', 'ic', 'cream', 'oso', 'wan']\n",
      "Tokenized sentence: ['i', 'm', 'a', 'guy', 'browsin', 'is', 'compulsory']\n",
      "After stop words removal: ['guy', 'browsin', 'compulsory']\n",
      "After stemming with porters algorithm: ['gui', 'browsin', 'compulsori']\n",
      "Tokenized sentence: ['i', 'know', 'a', 'few', 'people', 'i', 'can', 'hit', 'up', 'and', 'fuck', 'to', 'the', 'yes']\n",
      "After stop words removal: ['know', 'people', 'hit', 'fuck', 'yes']\n",
      "After stemming with porters algorithm: ['know', 'peopl', 'hit', 'fuck', 'ye']\n",
      "Tokenized sentence: ['we', 'currently', 'have', 'a', 'message', 'awaiting', 'your', 'collection', 'to', 'collect', 'your', 'message', 'just', 'call']\n",
      "After stop words removal: ['currently', 'message', 'awaiting', 'collection', 'collect', 'message', 'call']\n",
      "await\n",
      "After stemming with porters algorithm: ['current', 'messag', 'await', 'collect', 'collect', 'messag', 'call']\n",
      "Tokenized sentence: ['yes', 'he', 'have', 'good', 'crickiting', 'mind']\n",
      "After stop words removal: ['yes', 'good', 'crickiting', 'mind']\n",
      "crickit\n",
      "After stemming with porters algorithm: ['ye', 'good', 'crickit', 'mind']\n",
      "Tokenized sentence: ['try', 'to', 'do', 'something', 'dear', 'you', 'read', 'something', 'for', 'exams']\n",
      "After stop words removal: ['try', 'something', 'dear', 'read', 'something', 'exams']\n",
      "someth\n",
      "someth\n",
      "After stemming with porters algorithm: ['try', 'somet', 'dear', 'read', 'somet', 'exam']\n",
      "Tokenized sentence: ['marvel', 'mobile', 'play', 'the', 'official', 'ultimate', 'spider', 'man', 'game', 'on', 'ur', 'mobile', 'right', 'now', 'text', 'spider', 'to', 'for', 'the', 'game', 'we', 'll', 'send', 'u', 'a', 'free', 'ball', 'wallpaper']\n",
      "After stop words removal: ['marvel', 'mobile', 'play', 'official', 'ultimate', 'spider', 'man', 'game', 'ur', 'mobile', 'right', 'text', 'spider', 'game', 'send', 'u', 'free', 'ball', 'wallpaper']\n",
      "After stemming with porters algorithm: ['marvel', 'mobil', 'plai', 'offici', 'ultim', 'spider', 'man', 'game', 'mobil', 'right', 'text', 'spider', 'game', 'send', 'free', 'ball', 'wallpap']\n",
      "Tokenized sentence: ['well', 'she', 's', 'in', 'for', 'a', 'big', 'surprise']\n",
      "After stop words removal: ['well', 'big', 'surprise']\n",
      "After stemming with porters algorithm: ['well', 'big', 'surpris']\n",
      "Tokenized sentence: ['just', 'come', 'home', 'i', 'don', 't', 'want', 'u', 'to', 'be', 'miserable']\n",
      "After stop words removal: ['come', 'home', 'want', 'u', 'miserable']\n",
      "After stemming with porters algorithm: ['come', 'home', 'want', 'miser']\n",
      "Tokenized sentence: ['oh', 'yeah', 'i', 'forgot', 'u', 'can', 'only', 'take', 'out', 'shopping', 'at', 'once']\n",
      "After stop words removal: ['oh', 'yeah', 'forgot', 'u', 'take', 'shopping']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['yeah', 'forgot', 'take', 'shop']\n",
      "Tokenized sentence: ['huh', 'so', 'fast', 'dat', 'means', 'u', 'havent', 'finished', 'painting']\n",
      "After stop words removal: ['huh', 'fast', 'dat', 'means', 'u', 'havent', 'finished', 'painting']\n",
      "paint\n",
      "After stemming with porters algorithm: ['huh', 'fast', 'dat', 'mean', 'havent', 'finis', 'pain']\n",
      "Tokenized sentence: ['nothin', 'comes', 'to', 'my', 'mind', 'help', 'me', 'buy', 'hanger', 'lor', 'ur', 'laptop', 'not', 'heavy']\n",
      "After stop words removal: ['nothin', 'comes', 'mind', 'help', 'buy', 'hanger', 'lor', 'ur', 'laptop', 'heavy']\n",
      "After stemming with porters algorithm: ['nothin', 'come', 'mind', 'help', 'bui', 'hanger', 'lor', 'laptop', 'heavi']\n",
      "Tokenized sentence: ['yes', 'i', 'm', 'in', 'office', 'da']\n",
      "After stop words removal: ['yes', 'office', 'da']\n",
      "After stemming with porters algorithm: ['ye', 'offic']\n",
      "Tokenized sentence: ['sorry', 'brah', 'just', 'finished', 'the', 'last', 'of', 'my', 'exams', 'what', 'up']\n",
      "After stop words removal: ['sorry', 'brah', 'finished', 'last', 'exams']\n",
      "After stemming with porters algorithm: ['sorri', 'brah', 'finis', 'last', 'exam']\n",
      "Tokenized sentence: ['from', 'someone', 'not', 'to', 'smoke', 'when', 'every', 'time', 'i', 've', 'smoked', 'in', 'the', 'last', 'two', 'weeks', 'is', 'because', 'of', 'you', 'calling', 'or', 'texting', 'me', 'that', 'you', 'wanted', 'to', 'smoke']\n",
      "After stop words removal: ['someone', 'smoke', 'every', 'time', 'smoked', 'last', 'two', 'weeks', 'calling', 'texting', 'wanted', 'smoke']\n",
      "call\n",
      "text\n",
      "After stemming with porters algorithm: ['someon', 'smoke', 'everi', 'time', 'smoke', 'last', 'two', 'week', 'call', 'tex', 'wan', 'smoke']\n",
      "Tokenized sentence: ['then', 'we', 'wait', 'u', 'lor', 'no', 'need', 'feel', 'bad', 'lar']\n",
      "After stop words removal: ['wait', 'u', 'lor', 'need', 'feel', 'bad', 'lar']\n",
      "After stemming with porters algorithm: ['wait', 'lor', 'need', 'feel', 'bad', 'lar']\n",
      "Tokenized sentence: ['oh', 'yah', 'we', 'never', 'cancel', 'leh', 'haha']\n",
      "After stop words removal: ['oh', 'yah', 'never', 'cancel', 'leh', 'haha']\n",
      "After stemming with porters algorithm: ['yah', 'never', 'cancel', 'leh', 'haha']\n",
      "Tokenized sentence: ['so', 'when', 'do', 'you', 'wanna', 'gym']\n",
      "After stop words removal: ['wanna', 'gym']\n",
      "After stemming with porters algorithm: ['wanna', 'gym']\n",
      "Tokenized sentence: ['aight', 'sounds', 'good', 'when', 'do', 'you', 'want', 'me', 'to', 'come', 'down']\n",
      "After stop words removal: ['aight', 'sounds', 'good', 'want', 'come']\n",
      "After stemming with porters algorithm: ['aight', 'sound', 'good', 'want', 'come']\n",
      "Tokenized sentence: ['package', 'all', 'your', 'programs', 'well']\n",
      "After stop words removal: ['package', 'programs', 'well']\n",
      "After stemming with porters algorithm: ['packag', 'program', 'well']\n",
      "Tokenized sentence: ['honeybee', 'said', 'i', 'm', 'd', 'sweetest', 'in', 'd', 'world', 'god', 'laughed', 'amp', 'said', 'wait', 'u', 'havnt', 'met', 'd', 'person', 'reading', 'this', 'msg', 'moral', 'even', 'god', 'can', 'crack', 'jokes', 'gm', 'gn', 'ge', 'gn']\n",
      "After stop words removal: ['honeybee', 'said', 'sweetest', 'world', 'god', 'laughed', 'amp', 'said', 'wait', 'u', 'havnt', 'met', 'person', 'reading', 'msg', 'moral', 'even', 'god', 'crack', 'jokes', 'gm', 'gn', 'ge', 'gn']\n",
      "read\n",
      "After stemming with porters algorithm: ['honeybe', 'said', 'sweetest', 'world', 'god', 'laug', 'amp', 'said', 'wait', 'havnt', 'met', 'person', 'read', 'msg', 'moral', 'even', 'god', 'crack', 'joke']\n",
      "Tokenized sentence: ['twenty', 'past', 'five', 'he', 'said', 'will', 'this', 'train', 'have', 'been', 'to', 'durham', 'already', 'or', 'not', 'coz', 'i', 'am', 'in', 'a', 'reserved', 'seat']\n",
      "After stop words removal: ['twenty', 'past', 'five', 'said', 'train', 'durham', 'already', 'coz', 'reserved', 'seat']\n",
      "After stemming with porters algorithm: ['twenti', 'past', 'five', 'said', 'train', 'durham', 'alreadi', 'coz', 'reser', 'seat']\n",
      "Tokenized sentence: ['no', 'need', 'lar', 'i', 'go', 'engin', 'cos', 'my', 'sis', 'at', 'arts', 'today']\n",
      "After stop words removal: ['need', 'lar', 'go', 'engin', 'cos', 'sis', 'arts', 'today']\n",
      "After stemming with porters algorithm: ['need', 'lar', 'engin', 'co', 'si', 'art', 'todai']\n",
      "Tokenized sentence: ['xmas', 'prize', 'draws', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['xmas', 'prize', 'draws', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['xma', 'priz', 'draw', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'valid', 'hr']\n",
      "Tokenized sentence: ['hey', 'morning', 'what', 'you', 'come', 'to', 'ask', 'pa']\n",
      "After stop words removal: ['hey', 'morning', 'come', 'ask', 'pa']\n",
      "morn\n",
      "After stemming with porters algorithm: ['hei', 'mor', 'come', 'ask']\n",
      "Tokenized sentence: ['great', 'never', 'been', 'better', 'each', 'day', 'gives', 'even', 'more', 'reasons', 'to', 'thank', 'god']\n",
      "After stop words removal: ['great', 'never', 'better', 'day', 'gives', 'even', 'reasons', 'thank', 'god']\n",
      "After stemming with porters algorithm: ['great', 'never', 'better', 'dai', 'give', 'even', 'reason', 'thank', 'god']\n",
      "Tokenized sentence: ['am', 'i', 'that', 'much', 'dirty', 'fellow']\n",
      "After stop words removal: ['much', 'dirty', 'fellow']\n",
      "After stemming with porters algorithm: ['much', 'dirti', 'fellow']\n",
      "Tokenized sentence: ['wow', 'the', 'boys', 'r', 'back', 'take', 'that', 'uk', 'tour', 'win', 'vip', 'tickets', 'pre', 'book', 'with', 'vip', 'club', 'txt', 'club', 'to', 'trackmarque', 'ltd', 'info', 'vipclub', 'u']\n",
      "After stop words removal: ['wow', 'boys', 'r', 'back', 'take', 'uk', 'tour', 'win', 'vip', 'tickets', 'pre', 'book', 'vip', 'club', 'txt', 'club', 'trackmarque', 'ltd', 'info', 'vipclub', 'u']\n",
      "After stemming with porters algorithm: ['wow', 'boi', 'back', 'take', 'tour', 'win', 'vip', 'ticket', 'pre', 'book', 'vip', 'club', 'txt', 'club', 'trackmarqu', 'ltd', 'info', 'vipclub']\n",
      "Tokenized sentence: ['gud', 'gud', 'k', 'chikku', 'tke', 'care', 'sleep', 'well', 'gud', 'nyt']\n",
      "After stop words removal: ['gud', 'gud', 'k', 'chikku', 'tke', 'care', 'sleep', 'well', 'gud', 'nyt']\n",
      "After stemming with porters algorithm: ['gud', 'gud', 'chikku', 'tke', 'care', 'sleep', 'well', 'gud', 'nyt']\n",
      "Tokenized sentence: ['i', 'hope', 'you', 'know', 'i', 'm', 'still', 'mad', 'at', 'you']\n",
      "After stop words removal: ['hope', 'know', 'still', 'mad']\n",
      "After stemming with porters algorithm: ['hope', 'know', 'still', 'mad']\n",
      "Tokenized sentence: ['i', 'call', 'you', 'later', 'don', 't', 'have', 'network', 'if', 'urgnt', 'sms', 'me']\n",
      "After stop words removal: ['call', 'later', 'network', 'urgnt', 'sms']\n",
      "After stemming with porters algorithm: ['call', 'later', 'network', 'urgnt', 'sm']\n",
      "Tokenized sentence: ['hey', 'anyway', 'i', 'have', 'to']\n",
      "After stop words removal: ['hey', 'anyway']\n",
      "After stemming with porters algorithm: ['hei', 'anywai']\n",
      "Tokenized sentence: ['i', 'sent', 'you', 'the', 'prices', 'and', 'do', 'you', 'mean', 'the', 'lt', 'gt', 'g']\n",
      "After stop words removal: ['sent', 'prices', 'mean', 'lt', 'gt', 'g']\n",
      "After stemming with porters algorithm: ['sent', 'price', 'mean']\n",
      "Tokenized sentence: ['sorry', 'i', 've', 'not', 'gone', 'to', 'that', 'place', 'i', 'll', 'do', 'so', 'tomorrow', 'really', 'sorry']\n",
      "After stop words removal: ['sorry', 'gone', 'place', 'tomorrow', 'really', 'sorry']\n",
      "After stemming with porters algorithm: ['sorri', 'gone', 'place', 'tomorrow', 'realli', 'sorri']\n",
      "Tokenized sentence: ['at', 'we', 'will', 'go', 'ok', 'na']\n",
      "After stop words removal: ['go', 'ok', 'na']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['free', 'day', 'sexy', 'st', 'george', 's', 'day', 'pic', 'of', 'jordan', 'txt', 'pic', 'to', 'dont', 'miss', 'out', 'then', 'every', 'wk', 'a', 'saucy', 'celeb', 'more', 'pics', 'c', 'pocketbabe', 'co', 'uk', 'wk']\n",
      "After stop words removal: ['free', 'day', 'sexy', 'st', 'george', 'day', 'pic', 'jordan', 'txt', 'pic', 'dont', 'miss', 'every', 'wk', 'saucy', 'celeb', 'pics', 'c', 'pocketbabe', 'co', 'uk', 'wk']\n",
      "After stemming with porters algorithm: ['free', 'dai', 'sexi', 'georg', 'dai', 'pic', 'jordan', 'txt', 'pic', 'dont', 'miss', 'everi', 'sauci', 'celeb', 'pic', 'pocketbab']\n",
      "Tokenized sentence: ['hey', 'sexy', 'buns', 'have', 'i', 'told', 'you', 'i', 'adore', 'you', 'loverboy', 'i', 'hope', 'you', 'remember', 'to', 'thank', 'your', 'sister', 'in', 'law', 'for', 'those', 'meatballs', 'grins', 'i', 'love', 'you', 'babe']\n",
      "After stop words removal: ['hey', 'sexy', 'buns', 'told', 'adore', 'loverboy', 'hope', 'remember', 'thank', 'sister', 'law', 'meatballs', 'grins', 'love', 'babe']\n",
      "After stemming with porters algorithm: ['hei', 'sexi', 'bun', 'told', 'ador', 'loverboi', 'hope', 'rememb', 'thank', 'sister', 'law', 'meatbal', 'grin', 'love', 'babe']\n",
      "Tokenized sentence: ['hello', 'darlin', 'ive', 'finished', 'college', 'now', 'so', 'txt', 'me', 'when', 'u', 'finish', 'if', 'u', 'can', 'love', 'kate', 'xxx']\n",
      "After stop words removal: ['hello', 'darlin', 'ive', 'finished', 'college', 'txt', 'u', 'finish', 'u', 'love', 'kate', 'xxx']\n",
      "After stemming with porters algorithm: ['hello', 'darlin', 'iv', 'finis', 'colleg', 'txt', 'finish', 'love', 'kate', 'xxx']\n",
      "Tokenized sentence: ['i', 'know', 'she', 'called', 'me']\n",
      "After stop words removal: ['know', 'called']\n",
      "After stemming with porters algorithm: ['know', 'call']\n",
      "Tokenized sentence: ['urgent', 'this', 'is', 'our', 'nd', 'attempt', 'to', 'contact', 'u', 'your', 'prize', 'from', 'yesterday', 'is', 'still', 'awaiting', 'collection', 'to', 'claim', 'call', 'now', 'acl', 'pm']\n",
      "After stop words removal: ['urgent', 'nd', 'attempt', 'contact', 'u', 'prize', 'yesterday', 'still', 'awaiting', 'collection', 'claim', 'call', 'acl', 'pm']\n",
      "await\n",
      "After stemming with porters algorithm: ['urgent', 'attempt', 'contact', 'priz', 'yesterdai', 'still', 'await', 'collect', 'claim', 'call', 'acl']\n",
      "Tokenized sentence: ['no', 'polyphonic', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'pt', 'to', 'st', 'tone', 'free', 'so', 'get', 'txtin', 'now', 'and', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stop words removal: ['polyphonic', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'pt', 'st', 'tone', 'free', 'get', 'txtin', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stemming with porters algorithm: ['polyphon', 'tone', 'mob', 'everi', 'week', 'txt', 'tone', 'free', 'get', 'txtin', 'tell', 'friend', 'tone', 'repli', 'info']\n",
      "Tokenized sentence: ['hi', 'customer', 'loyalty', 'offer', 'the', 'new', 'nokia', 'mobile', 'from', 'only', 'at', 'txtauction', 'txt', 'word', 'start', 'to', 'no', 'get', 'yours', 'now', 't', 'ctxt', 'tc', 'p', 'mtmsg']\n",
      "After stop words removal: ['hi', 'customer', 'loyalty', 'offer', 'new', 'nokia', 'mobile', 'txtauction', 'txt', 'word', 'start', 'get', 'ctxt', 'tc', 'p', 'mtmsg']\n",
      "After stemming with porters algorithm: ['custom', 'loyalti', 'offer', 'new', 'nokia', 'mobil', 'txtauct', 'txt', 'word', 'start', 'get', 'ctxt', 'mtmsg']\n",
      "Tokenized sentence: ['am', 'in', 'gobi', 'arts', 'college']\n",
      "After stop words removal: ['gobi', 'arts', 'college']\n",
      "After stemming with porters algorithm: ['gobi', 'art', 'colleg']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['haha', 'yup', 'hopefully', 'we', 'will', 'lose', 'a', 'few', 'kg', 'by', 'mon', 'after', 'hip', 'hop', 'can', 'go', 'orchard', 'and', 'weigh', 'again']\n",
      "After stop words removal: ['haha', 'yup', 'hopefully', 'lose', 'kg', 'mon', 'hip', 'hop', 'go', 'orchard', 'weigh']\n",
      "After stemming with porters algorithm: ['haha', 'yup', 'hopefulli', 'lose', 'mon', 'hip', 'hop', 'orchard', 'weigh']\n",
      "Tokenized sentence: ['carlos', 'is', 'down', 'but', 'i', 'have', 'to', 'pick', 'it', 'up', 'from', 'him', 'so', 'i', 'll', 'swing', 'by', 'usf', 'in', 'a', 'little', 'bit']\n",
      "After stop words removal: ['carlos', 'pick', 'swing', 'usf', 'little', 'bit']\n",
      "After stemming with porters algorithm: ['carlo', 'pick', 'swing', 'usf', 'littl', 'bit']\n",
      "Tokenized sentence: ['hey', 'doc', 'pls', 'i', 'want', 'to', 'get', 'nice', 't', 'shirt', 'for', 'my', 'hubby', 'nice', 'fiting', 'ones', 'my', 'budget', 'is', 'lt', 'gt', 'k', 'help', 'pls', 'i', 'will', 'load', 'd', 'card', 'abi', 'hw', 'keep', 'me', 'posted', 'luv', 'mj']\n",
      "After stop words removal: ['hey', 'doc', 'pls', 'want', 'get', 'nice', 'shirt', 'hubby', 'nice', 'fiting', 'ones', 'budget', 'lt', 'gt', 'k', 'help', 'pls', 'load', 'card', 'abi', 'hw', 'keep', 'posted', 'luv', 'mj']\n",
      "fit\n",
      "After stemming with porters algorithm: ['hei', 'doc', 'pl', 'want', 'get', 'nice', 'shirt', 'hubbi', 'nice', 'fite', 'on', 'budget', 'help', 'pl', 'load', 'card', 'abi', 'keep', 'pos', 'luv']\n",
      "Tokenized sentence: ['whens', 'your', 'radio', 'show']\n",
      "After stop words removal: ['whens', 'radio', 'show']\n",
      "After stemming with porters algorithm: ['when', 'radio', 'show']\n",
      "Tokenized sentence: ['babe', 'how', 'goes', 'that', 'day', 'what', 'are', 'you', 'doing', 'where', 'are', 'you', 'i', 'sip', 'my', 'cappuccino', 'and', 'think', 'of', 'you', 'my', 'love', 'i', 'send', 'a', 'kiss', 'to', 'you', 'from', 'across', 'the', 'sea']\n",
      "After stop words removal: ['babe', 'goes', 'day', 'sip', 'cappuccino', 'think', 'love', 'send', 'kiss', 'across', 'sea']\n",
      "After stemming with porters algorithm: ['babe', 'goe', 'dai', 'sip', 'cappuccino', 'think', 'love', 'send', 'kiss', 'across', 'sea']\n",
      "Tokenized sentence: ['jolly', 'good', 'by', 'the', 'way', 'will', 'give', 'u', 'tickets', 'for', 'sat', 'eve', 'speak', 'before', 'then', 'x']\n",
      "After stop words removal: ['jolly', 'good', 'way', 'give', 'u', 'tickets', 'sat', 'eve', 'speak', 'x']\n",
      "After stemming with porters algorithm: ['jolli', 'good', 'wai', 'give', 'ticket', 'sat', 'ev', 'speak']\n",
      "Tokenized sentence: ['thats', 'a', 'bit', 'weird', 'even', 'where', 'is', 'the', 'do', 'supposed', 'to', 'be', 'happening', 'but', 'good', 'idea', 'sure', 'they', 'will', 'be', 'in', 'pub']\n",
      "After stop words removal: ['thats', 'bit', 'weird', 'even', 'supposed', 'happening', 'good', 'idea', 'sure', 'pub']\n",
      "happen\n",
      "After stemming with porters algorithm: ['that', 'bit', 'weird', 'even', 'suppos', 'happen', 'good', 'idea', 'sure', 'pub']\n",
      "Tokenized sentence: ['sorry', 'i', 'can', 't', 'text', 'amp', 'drive', 'coherently', 'see', 'you', 'in', 'twenty']\n",
      "After stop words removal: ['sorry', 'text', 'amp', 'drive', 'coherently', 'see', 'twenty']\n",
      "After stemming with porters algorithm: ['sorri', 'text', 'amp', 'drive', 'coher', 'see', 'twenti']\n",
      "Tokenized sentence: ['boooo', 'you', 'always', 'work', 'just', 'quit']\n",
      "After stop words removal: ['boooo', 'always', 'work', 'quit']\n",
      "After stemming with porters algorithm: ['boooo', 'alwai', 'work', 'quit']\n",
      "Tokenized sentence: ['ya', 'even', 'those', 'cookies', 'have', 'jelly', 'on', 'them']\n",
      "After stop words removal: ['ya', 'even', 'cookies', 'jelly']\n",
      "After stemming with porters algorithm: ['even', 'cooki', 'jelli']\n",
      "Tokenized sentence: ['k', 'go', 'and', 'sleep', 'well', 'take', 'rest']\n",
      "After stop words removal: ['k', 'go', 'sleep', 'well', 'take', 'rest']\n",
      "After stemming with porters algorithm: ['sleep', 'well', 'take', 'rest']\n",
      "Tokenized sentence: ['you', 'have', 'new', 'message', 'please', 'call']\n",
      "After stop words removal: ['new', 'message', 'please', 'call']\n",
      "After stemming with porters algorithm: ['new', 'messag', 'pleas', 'call']\n",
      "Tokenized sentence: ['just', 'wait', 'till', 'end', 'of', 'march', 'when', 'el', 'nino', 'gets', 'himself', 'oh']\n",
      "After stop words removal: ['wait', 'till', 'end', 'march', 'el', 'nino', 'gets', 'oh']\n",
      "After stemming with porters algorithm: ['wait', 'till', 'end', 'march', 'nino', 'get']\n",
      "Tokenized sentence: ['call', 'listen', 'to', 'extreme', 'dirty', 'live', 'chat', 'going', 'on', 'in', 'the', 'office', 'right', 'now', 'total', 'privacy', 'no', 'one', 'knows', 'your', 'sic', 'listening', 'p', 'min', 'mp']\n",
      "After stop words removal: ['call', 'listen', 'extreme', 'dirty', 'live', 'chat', 'going', 'office', 'right', 'total', 'privacy', 'one', 'knows', 'sic', 'listening', 'p', 'min', 'mp']\n",
      "go\n",
      "listen\n",
      "After stemming with porters algorithm: ['call', 'listen', 'extrem', 'dirti', 'live', 'chat', 'go', 'offic', 'right', 'total', 'privaci', 'on', 'know', 'sic', 'listen', 'min']\n",
      "Tokenized sentence: ['hmm', 'bad', 'news', 'hype', 'park', 'plaza', 'studio', 'taken', 'only', 'left', 'bedrm']\n",
      "After stop words removal: ['hmm', 'bad', 'news', 'hype', 'park', 'plaza', 'studio', 'taken', 'left', 'bedrm']\n",
      "After stemming with porters algorithm: ['hmm', 'bad', 'new', 'hype', 'park', 'plaza', 'studio', 'taken', 'left', 'bedrm']\n",
      "Tokenized sentence: ['i', 'send', 'the', 'print', 'outs', 'da']\n",
      "After stop words removal: ['send', 'print', 'outs', 'da']\n",
      "After stemming with porters algorithm: ['send', 'print', 'out']\n",
      "Tokenized sentence: ['gent', 'we', 'are', 'trying', 'to', 'contact', 'you', 'last', 'weekends', 'draw', 'shows', 'that', 'you', 'won', 'a', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'only', 'ppm']\n",
      "After stop words removal: ['gent', 'trying', 'contact', 'last', 'weekends', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'ppm']\n",
      "After stemming with porters algorithm: ['gent', 'trying', 'contact', 'last', 'weekend', 'draw', 'show', 'priz', 'guaranteed', 'call', 'claim', 'code', 'valid', 'hr', 'ppm']\n",
      "Tokenized sentence: ['lol', 'oh', 'you', 'got', 'a', 'friend', 'for', 'the', 'dog']\n",
      "After stop words removal: ['lol', 'oh', 'got', 'friend', 'dog']\n",
      "After stemming with porters algorithm: ['lol', 'got', 'friend', 'dog']\n",
      "Tokenized sentence: ['if', 'you', 're', 'still', 'up', 'maybe', 'leave', 'the', 'credit', 'card', 'so', 'i', 'can', 'get', 'gas', 'when', 'i', 'get', 'back', 'like', 'he', 'told', 'me', 'to']\n",
      "After stop words removal: ['still', 'maybe', 'leave', 'credit', 'card', 'get', 'gas', 'get', 'back', 'like', 'told']\n",
      "After stemming with porters algorithm: ['still', 'mayb', 'leav', 'credit', 'card', 'get', 'ga', 'get', 'back', 'like', 'told']\n",
      "Tokenized sentence: ['i', 'dont', 'know', 'what', 'to', 'do', 'to', 'come', 'out', 'of', 'this', 'so', 'only', 'am', 'ask', 'questions', 'like', 'this', 'dont', 'mistake', 'me']\n",
      "After stop words removal: ['dont', 'know', 'come', 'ask', 'questions', 'like', 'dont', 'mistake']\n",
      "After stemming with porters algorithm: ['dont', 'know', 'come', 'ask', 'quest', 'like', 'dont', 'mistak']\n",
      "Tokenized sentence: ['i', 'dled', 'd', 'its', 'very', 'imp']\n",
      "After stop words removal: ['dled', 'imp']\n",
      "After stemming with porters algorithm: ['dled', 'imp']\n",
      "Tokenized sentence: ['hello', 'damn', 'this', 'christmas', 'thing', 'i', 'think', 'i', 'have', 'decided', 'to', 'keep', 'this', 'mp', 'that', 'doesnt', 'work']\n",
      "After stop words removal: ['hello', 'damn', 'christmas', 'thing', 'think', 'decided', 'keep', 'mp', 'doesnt', 'work']\n",
      "After stemming with porters algorithm: ['hello', 'damn', 'christma', 'thing', 'think', 'decid', 'keep', 'doesnt', 'work']\n",
      "Tokenized sentence: ['i', 'm', 'going', 'for', 'bath', 'will', 'msg', 'you', 'next', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['going', 'bath', 'msg', 'next', 'lt', 'gt', 'min']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'bath', 'msg', 'next', 'min']\n",
      "Tokenized sentence: ['nope', 'meanwhile', 'she', 'talk', 'say', 'make', 'i', 'greet', 'you']\n",
      "After stop words removal: ['nope', 'meanwhile', 'talk', 'say', 'make', 'greet']\n",
      "After stemming with porters algorithm: ['nope', 'meanwhil', 'talk', 'sai', 'make', 'greet']\n",
      "Tokenized sentence: ['dare', 'i', 'ask', 'any', 'luck', 'with', 'sorting', 'out', 'the', 'car']\n",
      "After stop words removal: ['dare', 'ask', 'luck', 'sorting', 'car']\n",
      "sort\n",
      "After stemming with porters algorithm: ['dare', 'ask', 'luck', 'sor', 'car']\n",
      "Tokenized sentence: ['will', 'be', 'september', 'by', 'then']\n",
      "After stop words removal: ['september']\n",
      "After stemming with porters algorithm: ['septemb']\n",
      "Tokenized sentence: ['i', 'd', 'like', 'to', 'tell', 'you', 'my', 'deepest', 'darkest', 'fantasies', 'call', 'me', 'just', 'p', 'min', 'to', 'stop', 'texts', 'call', 'nat', 'rate']\n",
      "After stop words removal: ['like', 'tell', 'deepest', 'darkest', 'fantasies', 'call', 'p', 'min', 'stop', 'texts', 'call', 'nat', 'rate']\n",
      "After stemming with porters algorithm: ['like', 'tell', 'deepest', 'darkest', 'fantasi', 'call', 'min', 'stop', 'text', 'call', 'nat', 'rate']\n",
      "Tokenized sentence: ['she', 's', 'good', 'she', 'was', 'wondering', 'if', 'you', 'wont', 'say', 'hi', 'but', 'she', 's', 'smiling', 'now', 'so', 'how', 'are', 'you', 'coping', 'with', 'the', 'long', 'distance']\n",
      "After stop words removal: ['good', 'wondering', 'wont', 'say', 'hi', 'smiling', 'coping', 'long', 'distance']\n",
      "wonder\n",
      "smil\n",
      "cop\n",
      "After stemming with porters algorithm: ['good', 'wonder', 'wont', 'sai', 'smile', 'cope', 'long', 'distanc']\n",
      "Tokenized sentence: ['sir', 'i', 'need', 'velusamy', 'sir', 's', 'date', 'of', 'birth', 'and', 'company', 'bank', 'facilities', 'details']\n",
      "After stop words removal: ['sir', 'need', 'velusamy', 'sir', 'date', 'birth', 'company', 'bank', 'facilities', 'details']\n",
      "After stemming with porters algorithm: ['sir', 'need', 'velusami', 'sir', 'date', 'birth', 'compani', 'bank', 'facil', 'detail']\n",
      "Tokenized sentence: ['get', 'the', 'official', 'england', 'poly', 'ringtone', 'or', 'colour', 'flag', 'on', 'yer', 'mobile', 'for', 'tonights', 'game', 'text', 'tone', 'or', 'flag', 'to', 'optout', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stop words removal: ['get', 'official', 'england', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'tonights', 'game', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stemming with porters algorithm: ['get', 'offici', 'england', 'poli', 'rington', 'colour', 'flag', 'yer', 'mobil', 'tonight', 'game', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box']\n",
      "Tokenized sentence: ['oops', 'sorry', 'just', 'to', 'check', 'that', 'you', 'don', 't', 'mind', 'picking', 'me', 'up', 'tomo', 'at', 'half', 'eight', 'from', 'station', 'would', 'that', 'be', 'ok']\n",
      "After stop words removal: ['oops', 'sorry', 'check', 'mind', 'picking', 'tomo', 'half', 'eight', 'station', 'would', 'ok']\n",
      "pick\n",
      "After stemming with porters algorithm: ['oop', 'sorri', 'check', 'mind', 'pic', 'tomo', 'half', 'eight', 'stat', 'would']\n",
      "Tokenized sentence: ['you', 'should', 'change', 'your', 'fb', 'to', 'jaykwon', 'thuglyfe', 'falconerf']\n",
      "After stop words removal: ['change', 'fb', 'jaykwon', 'thuglyfe', 'falconerf']\n",
      "After stemming with porters algorithm: ['chang', 'jaykwon', 'thuglyf', 'falconerf']\n",
      "Tokenized sentence: ['are', 'you', 'still', 'playing', 'with', 'gautham']\n",
      "After stop words removal: ['still', 'playing', 'gautham']\n",
      "play\n",
      "After stemming with porters algorithm: ['still', 'plai', 'gautham']\n",
      "Tokenized sentence: ['for', 'ur', 'chance', 'to', 'win', 'cash', 'every', 'wk', 'txt', 'play', 'to', 't', 's', 'c', 's', 'www', 'music', 'trivia', 'net', 'custcare', 'x', 'p', 'wk']\n",
      "After stop words removal: ['ur', 'chance', 'win', 'cash', 'every', 'wk', 'txt', 'play', 'c', 'www', 'music', 'trivia', 'net', 'custcare', 'x', 'p', 'wk']\n",
      "After stemming with porters algorithm: ['chanc', 'win', 'cash', 'everi', 'txt', 'plai', 'www', 'music', 'trivia', 'net', 'custcar']\n",
      "Tokenized sentence: ['kaiez', 'enjoy', 'ur', 'tuition', 'gee', 'thk', 'e', 'second', 'option', 'sounds', 'beta', 'i', 'll', 'go', 'yan', 'jiu', 'den', 'msg', 'u']\n",
      "After stop words removal: ['kaiez', 'enjoy', 'ur', 'tuition', 'gee', 'thk', 'e', 'second', 'option', 'sounds', 'beta', 'go', 'yan', 'jiu', 'den', 'msg', 'u']\n",
      "After stemming with porters algorithm: ['kaiez', 'enjoi', 'tuit', 'gee', 'thk', 'second', 'opt', 'sound', 'beta', 'yan', 'jiu', 'den', 'msg']\n",
      "Tokenized sentence: ['fair', 'enough', 'anything', 'going', 'on']\n",
      "After stop words removal: ['fair', 'enough', 'anything', 'going']\n",
      "anyth\n",
      "go\n",
      "After stemming with porters algorithm: ['fair', 'enough', 'anyt', 'go']\n",
      "Tokenized sentence: ['oh', 'my', 'love', 'it', 's', 'soooo', 'good', 'to', 'hear', 'from', 'you', 'omg', 'i', 'missed', 'you', 'so', 'much', 'today', 'i', 'm', 'sorry', 'your', 'having', 'problems', 'with', 'the', 'provider', 'but', 'thank', 'you', 'for', 'tming', 'me']\n",
      "After stop words removal: ['oh', 'love', 'soooo', 'good', 'hear', 'omg', 'missed', 'much', 'today', 'sorry', 'problems', 'provider', 'thank', 'tming']\n",
      "After stemming with porters algorithm: ['love', 'soooo', 'good', 'hear', 'omg', 'miss', 'much', 'todai', 'sorri', 'problem', 'provid', 'thank', 'tming']\n",
      "Tokenized sentence: ['oh', 'really', 'perform', 'write', 'a', 'paper', 'go', 'to', 'a', 'movie', 'and', 'be', 'home', 'by', 'midnight', 'huh']\n",
      "After stop words removal: ['oh', 'really', 'perform', 'write', 'paper', 'go', 'movie', 'home', 'midnight', 'huh']\n",
      "After stemming with porters algorithm: ['realli', 'perform', 'write', 'paper', 'movi', 'home', 'midnight', 'huh']\n",
      "Tokenized sentence: ['you', 'have', 'an', 'important', 'customer', 'service', 'announcement', 'call', 'freephone', 'now']\n",
      "After stop words removal: ['important', 'customer', 'service', 'announcement', 'call', 'freephone']\n",
      "After stemming with porters algorithm: ['import', 'custom', 'servic', 'announc', 'call', 'freephon']\n",
      "Tokenized sentence: ['today', 'is', 'song', 'dedicated', 'day', 'which', 'song', 'will', 'u', 'dedicate', 'for', 'me', 'send', 'this', 'to', 'all', 'ur', 'valuable', 'frnds', 'but', 'first', 'rply', 'me']\n",
      "After stop words removal: ['today', 'song', 'dedicated', 'day', 'song', 'u', 'dedicate', 'send', 'ur', 'valuable', 'frnds', 'first', 'rply']\n",
      "dedicate\n",
      "After stemming with porters algorithm: ['todai', 'song', 'dedic', 'dai', 'song', 'dedic', 'send', 'valuab', 'frnd', 'first', 'rply']\n",
      "Tokenized sentence: ['dunno', 'lei', 'i', 'might', 'b', 'eatin', 'wif', 'my', 'frens', 'if', 'wan', 'to', 'eat', 'then', 'i', 'wait', 'lar']\n",
      "After stop words removal: ['dunno', 'lei', 'might', 'b', 'eatin', 'wif', 'frens', 'wan', 'eat', 'wait', 'lar']\n",
      "After stemming with porters algorithm: ['dunno', 'lei', 'might', 'eatin', 'wif', 'fren', 'wan', 'eat', 'wait', 'lar']\n",
      "Tokenized sentence: ['hi', 'msg', 'me', 'i', 'm', 'in', 'office']\n",
      "After stop words removal: ['hi', 'msg', 'office']\n",
      "After stemming with porters algorithm: ['msg', 'offic']\n",
      "Tokenized sentence: ['great', 'new', 'offer', 'double', 'mins', 'double', 'txt', 'on', 'best', 'orange', 'tariffs', 'and', 'get', 'latest', 'camera', 'phones', 'free', 'call', 'mobileupd', 'free', 'on', 'now', 'or', 'stoptxt', 't', 'cs']\n",
      "After stop words removal: ['great', 'new', 'offer', 'double', 'mins', 'double', 'txt', 'best', 'orange', 'tariffs', 'get', 'latest', 'camera', 'phones', 'free', 'call', 'mobileupd', 'free', 'stoptxt', 'cs']\n",
      "After stemming with porters algorithm: ['great', 'new', 'offer', 'doubl', 'min', 'doubl', 'txt', 'best', 'orang', 'tariff', 'get', 'latest', 'camera', 'phone', 'free', 'call', 'mobileupd', 'free', 'stoptxt']\n",
      "Tokenized sentence: ['ffffffffff', 'alright', 'no', 'way', 'i', 'can', 'meet', 'up', 'with', 'you', 'sooner']\n",
      "After stop words removal: ['ffffffffff', 'alright', 'way', 'meet', 'sooner']\n",
      "After stemming with porters algorithm: ['ffffffffff', 'alright', 'wai', 'meet', 'sooner']\n",
      "Tokenized sentence: ['why', 'didn', 't', 'u', 'call', 'on', 'your', 'lunch']\n",
      "After stop words removal: ['u', 'call', 'lunch']\n",
      "After stemming with porters algorithm: ['call', 'lunch']\n",
      "Tokenized sentence: ['ok', 'lor', 'but', 'not', 'too', 'early', 'me', 'still', 'having', 'project', 'meeting', 'now']\n",
      "After stop words removal: ['ok', 'lor', 'early', 'still', 'project', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['lor', 'earli', 'still', 'project', 'meet']\n",
      "Tokenized sentence: ['pls', 'give', 'her', 'the', 'food', 'preferably', 'pap', 'very', 'slowly', 'with', 'loads', 'of', 'sugar', 'you', 'can', 'take', 'up', 'to', 'an', 'hour', 'to', 'give', 'it', 'and', 'then', 'some', 'water', 'very', 'very', 'slowly']\n",
      "After stop words removal: ['pls', 'give', 'food', 'preferably', 'pap', 'slowly', 'loads', 'sugar', 'take', 'hour', 'give', 'water', 'slowly']\n",
      "After stemming with porters algorithm: ['pl', 'give', 'food', 'prefer', 'pap', 'slowli', 'load', 'sugar', 'take', 'hour', 'give', 'water', 'slowli']\n",
      "Tokenized sentence: ['er', 'yeah', 'i', 'will', 'b', 'there', 'at', 'sorry', 'just', 'tell', 'me', 'which', 'pub', 'cafe', 'to', 'sit', 'in', 'and', 'come', 'wen', 'u', 'can']\n",
      "After stop words removal: ['er', 'yeah', 'b', 'sorry', 'tell', 'pub', 'cafe', 'sit', 'come', 'wen', 'u']\n",
      "After stemming with porters algorithm: ['yeah', 'sorri', 'tell', 'pub', 'cafe', 'sit', 'come', 'wen']\n",
      "Tokenized sentence: ['it', 'vl', 'bcum', 'more', 'difficult']\n",
      "After stop words removal: ['vl', 'bcum', 'difficult']\n",
      "After stemming with porters algorithm: ['bcum', 'difficult']\n",
      "Tokenized sentence: ['height', 'of', 'oh', 'shit', 'situation', 'a', 'guy', 'throws', 'a', 'luv', 'letter', 'on', 'a', 'gal', 'but', 'falls', 'on', 'her', 'brothers', 'head', 'whos', 'a', 'gay']\n",
      "After stop words removal: ['height', 'oh', 'shit', 'situation', 'guy', 'throws', 'luv', 'letter', 'gal', 'falls', 'brothers', 'head', 'whos', 'gay']\n",
      "After stemming with porters algorithm: ['height', 'shit', 'situat', 'gui', 'throw', 'luv', 'letter', 'gal', 'fall', 'brother', 'head', 'who', 'gai']\n",
      "Tokenized sentence: ['i', 'surely', 'dont', 'forgot', 'to', 'come', 'i', 'will', 'always', 'be', 'in', 'touch', 'in', 'with', 'you']\n",
      "After stop words removal: ['surely', 'dont', 'forgot', 'come', 'always', 'touch']\n",
      "After stemming with porters algorithm: ['sure', 'dont', 'forgot', 'come', 'alwai', 'touch']\n",
      "Tokenized sentence: ['oh', 'baby', 'of', 'the', 'house', 'how', 'come', 'you', 'dont', 'have', 'any', 'new', 'pictures', 'on', 'facebook']\n",
      "After stop words removal: ['oh', 'baby', 'house', 'come', 'dont', 'new', 'pictures', 'facebook']\n",
      "After stemming with porters algorithm: ['babi', 'hous', 'come', 'dont', 'new', 'pictur', 'facebook']\n",
      "Tokenized sentence: ['yes', 'it', 'completely', 'in', 'out', 'of', 'form', 'clark', 'also', 'utter', 'waste']\n",
      "After stop words removal: ['yes', 'completely', 'form', 'clark', 'also', 'utter', 'waste']\n",
      "After stemming with porters algorithm: ['ye', 'complet', 'form', 'clark', 'also', 'utter', 'wast']\n",
      "Tokenized sentence: ['nope', 'but', 'i', 'm', 'going', 'home', 'now', 'then', 'go', 'pump', 'petrol', 'lor', 'like', 'going', 'rain', 'soon']\n",
      "After stop words removal: ['nope', 'going', 'home', 'go', 'pump', 'petrol', 'lor', 'like', 'going', 'rain', 'soon']\n",
      "go\n",
      "go\n",
      "After stemming with porters algorithm: ['nope', 'go', 'home', 'pump', 'petrol', 'lor', 'like', 'go', 'rain', 'soon']\n",
      "Tokenized sentence: ['is', 'she', 'replying', 'has', 'boye', 'changed', 'his', 'phone', 'number']\n",
      "After stop words removal: ['replying', 'boye', 'changed', 'phone', 'number']\n",
      "reply\n",
      "After stemming with porters algorithm: ['repl', 'boy', 'chan', 'phone', 'number']\n",
      "Tokenized sentence: ['new', 'theory', 'argument', 'wins', 'd', 'situation', 'but', 'loses', 'the', 'person', 'so', 'dont', 'argue', 'with', 'ur', 'friends', 'just', 'kick', 'them', 'amp', 'say', 'i', 'm', 'always', 'correct']\n",
      "After stop words removal: ['new', 'theory', 'argument', 'wins', 'situation', 'loses', 'person', 'dont', 'argue', 'ur', 'friends', 'kick', 'amp', 'say', 'always', 'correct']\n",
      "After stemming with porters algorithm: ['new', 'theori', 'argum', 'win', 'situat', 'lose', 'person', 'dont', 'argu', 'friend', 'kick', 'amp', 'sai', 'alwai', 'correct']\n",
      "Tokenized sentence: ['no', 'worries', 'hope', 'photo', 'shoot', 'went', 'well', 'have', 'a', 'spiffing', 'fun', 'at', 'workage']\n",
      "After stop words removal: ['worries', 'hope', 'photo', 'shoot', 'went', 'well', 'spiffing', 'fun', 'workage']\n",
      "spiff\n",
      "After stemming with porters algorithm: ['worri', 'hope', 'photo', 'shoot', 'went', 'well', 'spif', 'fun', 'workag']\n",
      "Tokenized sentence: ['you', 'gonna', 'ring', 'this', 'weekend', 'or', 'wot']\n",
      "After stop words removal: ['gonna', 'ring', 'weekend', 'wot']\n",
      "After stemming with porters algorithm: ['gonna', 'ring', 'weekend', 'wot']\n",
      "Tokenized sentence: ['it', 's', 'ok', 'i', 'wun', 'b', 'angry', 'msg', 'u', 'aft', 'i', 'come', 'home', 'tonight']\n",
      "After stop words removal: ['ok', 'wun', 'b', 'angry', 'msg', 'u', 'aft', 'come', 'home', 'tonight']\n",
      "After stemming with porters algorithm: ['wun', 'angri', 'msg', 'aft', 'come', 'home', 'tonight']\n",
      "Tokenized sentence: ['nothing', 'will', 'ever', 'be', 'easy', 'but', 'don', 't', 'be', 'looking', 'for', 'a', 'reason', 'not', 'to', 'take', 'a', 'risk', 'on', 'life', 'and', 'love']\n",
      "After stop words removal: ['nothing', 'ever', 'easy', 'looking', 'reason', 'take', 'risk', 'life', 'love']\n",
      "noth\n",
      "look\n",
      "After stemming with porters algorithm: ['not', 'ever', 'easi', 'look', 'reason', 'take', 'risk', 'life', 'love']\n",
      "Tokenized sentence: ['yo', 'come', 'over', 'carlos', 'will', 'be', 'here', 'soon']\n",
      "After stop words removal: ['yo', 'come', 'carlos', 'soon']\n",
      "After stemming with porters algorithm: ['come', 'carlo', 'soon']\n",
      "Tokenized sentence: ['lol', 'you', 'forgot', 'it', 'eh', 'yes', 'i', 'll', 'bring', 'it', 'in', 'babe']\n",
      "After stop words removal: ['lol', 'forgot', 'eh', 'yes', 'bring', 'babe']\n",
      "After stemming with porters algorithm: ['lol', 'forgot', 'ye', 'bring', 'babe']\n",
      "Tokenized sentence: ['hey', 'company', 'elama', 'po', 'mudyadhu']\n",
      "After stop words removal: ['hey', 'company', 'elama', 'po', 'mudyadhu']\n",
      "After stemming with porters algorithm: ['hei', 'compani', 'elama', 'mudyadhu']\n",
      "Tokenized sentence: ['no', 'i', 'm', 'not', 'gonna', 'be', 'able', 'to', 'too', 'late', 'notice', 'i', 'll', 'be', 'home', 'in', 'a', 'few', 'weeks', 'anyway', 'what', 'are', 'the', 'plans']\n",
      "After stop words removal: ['gonna', 'able', 'late', 'notice', 'home', 'weeks', 'anyway', 'plans']\n",
      "After stemming with porters algorithm: ['gonna', 'abl', 'late', 'notic', 'home', 'week', 'anywai', 'plan']\n",
      "Tokenized sentence: ['watching', 'cartoon', 'listening', 'music', 'amp', 'at', 'eve', 'had', 'to', 'go', 'temple', 'amp', 'church', 'what', 'about', 'u']\n",
      "After stop words removal: ['watching', 'cartoon', 'listening', 'music', 'amp', 'eve', 'go', 'temple', 'amp', 'church', 'u']\n",
      "watch\n",
      "listen\n",
      "After stemming with porters algorithm: ['watc', 'cartoon', 'listen', 'music', 'amp', 'ev', 'templ', 'amp', 'church']\n",
      "Tokenized sentence: ['good', 'afternoon', 'my', 'love', 'it', 'was', 'good', 'to', 'see', 'your', 'words', 'on', 'ym', 'and', 'get', 'your', 'tm', 'very', 'smart', 'move', 'my', 'slave', 'smiles', 'i', 'drink', 'my', 'coffee', 'and', 'await', 'you']\n",
      "After stop words removal: ['good', 'afternoon', 'love', 'good', 'see', 'words', 'ym', 'get', 'tm', 'smart', 'move', 'slave', 'smiles', 'drink', 'coffee', 'await']\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'love', 'good', 'see', 'word', 'get', 'smart', 'move', 'slave', 'smile', 'drink', 'coffe', 'await']\n",
      "Tokenized sentence: ['hey', 'very', 'inconvenient', 'for', 'your', 'sis', 'a', 'not', 'huh']\n",
      "After stop words removal: ['hey', 'inconvenient', 'sis', 'huh']\n",
      "After stemming with porters algorithm: ['hei', 'inconveni', 'si', 'huh']\n",
      "Tokenized sentence: ['cool', 'breeze', 'bright', 'sun', 'fresh', 'flower', 'twittering', 'birds', 'all', 'these', 'waiting', 'to', 'wish', 'u', 'goodmorning', 'amp', 'have', 'a', 'nice', 'day']\n",
      "After stop words removal: ['cool', 'breeze', 'bright', 'sun', 'fresh', 'flower', 'twittering', 'birds', 'waiting', 'wish', 'u', 'goodmorning', 'amp', 'nice', 'day']\n",
      "twitter\n",
      "wait\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['cool', 'breez', 'bright', 'sun', 'fresh', 'flower', 'twitter', 'bird', 'wait', 'wish', 'goodmor', 'amp', 'nice', 'dai']\n",
      "Tokenized sentence: ['dear', 'got', 'train', 'and', 'seat', 'mine', 'lower', 'seat']\n",
      "After stop words removal: ['dear', 'got', 'train', 'seat', 'mine', 'lower', 'seat']\n",
      "After stemming with porters algorithm: ['dear', 'got', 'train', 'seat', 'mine', 'lower', 'seat']\n",
      "Tokenized sentence: ['when', 'did', 'i', 'use', 'soc', 'i', 'use', 'it', 'only', 'at', 'home', 'dunno', 'how', 'type', 'it', 'in', 'word', 'ar']\n",
      "After stop words removal: ['use', 'soc', 'use', 'home', 'dunno', 'type', 'word', 'ar']\n",
      "After stemming with porters algorithm: ['us', 'soc', 'us', 'home', 'dunno', 'type', 'word']\n",
      "Tokenized sentence: ['just', 'checking', 'in', 'on', 'you', 'really', 'do', 'miss', 'seeing', 'jeremiah', 'do', 'have', 'a', 'great', 'month']\n",
      "After stop words removal: ['checking', 'really', 'miss', 'seeing', 'jeremiah', 'great', 'month']\n",
      "check\n",
      "see\n",
      "After stemming with porters algorithm: ['chec', 'realli', 'miss', 'see', 'jeremiah', 'great', 'month']\n",
      "Tokenized sentence: ['yo', 'i', 'm', 'at', 'my', 'parents', 'gettin', 'cash', 'good', 'news', 'we', 'picked', 'up', 'a', 'downstem']\n",
      "After stop words removal: ['yo', 'parents', 'gettin', 'cash', 'good', 'news', 'picked', 'downstem']\n",
      "After stemming with porters algorithm: ['parent', 'gettin', 'cash', 'good', 'new', 'pic', 'downstem']\n",
      "Tokenized sentence: ['may', 'i', 'call', 'you', 'later', 'pls']\n",
      "After stop words removal: ['may', 'call', 'later', 'pls']\n",
      "After stemming with porters algorithm: ['mai', 'call', 'later', 'pl']\n",
      "Tokenized sentence: ['i', 'keep', 'ten', 'rs', 'in', 'my', 'shelf', 'buy', 'two', 'egg']\n",
      "After stop words removal: ['keep', 'ten', 'rs', 'shelf', 'buy', 'two', 'egg']\n",
      "After stemming with porters algorithm: ['keep', 'ten', 'shelf', 'bui', 'two', 'egg']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'bonus', 'caller', 'prize', 'call', 'from', 'land', 'line', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'mobile', 'number', 'awarded', 'bonus', 'caller', 'prize', 'call', 'land', 'line', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'number', 'awar', 'bonu', 'caller', 'priz', 'call', 'land', 'line', 'valid', 'hr']\n",
      "Tokenized sentence: ['i', 'love', 'you', 'you', 'set', 'my', 'soul', 'on', 'fire', 'it', 'is', 'not', 'just', 'a', 'spark', 'but', 'it', 'is', 'a', 'flame', 'a', 'big', 'rawring', 'flame', 'xoxo']\n",
      "After stop words removal: ['love', 'set', 'soul', 'fire', 'spark', 'flame', 'big', 'rawring', 'flame', 'xoxo']\n",
      "rawr\n",
      "After stemming with porters algorithm: ['love', 'set', 'soul', 'fire', 'spark', 'flame', 'big', 'raw', 'flame', 'xoxo']\n",
      "Tokenized sentence: ['i', 'should', 'add', 'that', 'i', 'don', 't', 'really', 'care', 'and', 'if', 'you', 'can', 't', 'i', 'can', 'at', 'least', 'get', 'this', 'dude', 'to', 'fuck', 'off', 'but', 'hey', 'your', 'money', 'if', 'you', 'want', 'it']\n",
      "After stop words removal: ['add', 'really', 'care', 'least', 'get', 'dude', 'fuck', 'hey', 'money', 'want']\n",
      "After stemming with porters algorithm: ['add', 'realli', 'care', 'least', 'get', 'dude', 'fuck', 'hei', 'monei', 'want']\n",
      "Tokenized sentence: ['u', 'still', 'havent', 'got', 'urself', 'a', 'jacket', 'ah']\n",
      "After stop words removal: ['u', 'still', 'havent', 'got', 'urself', 'jacket', 'ah']\n",
      "After stemming with porters algorithm: ['still', 'havent', 'got', 'urself', 'jacket']\n",
      "Tokenized sentence: ['ok', 'but', 'knackered', 'just', 'came', 'home', 'and', 'went', 'to', 'sleep', 'not', 'good', 'at', 'this', 'full', 'time', 'work', 'lark']\n",
      "After stop words removal: ['ok', 'knackered', 'came', 'home', 'went', 'sleep', 'good', 'full', 'time', 'work', 'lark']\n",
      "After stemming with porters algorithm: ['knacker', 'came', 'home', 'went', 'sleep', 'good', 'full', 'time', 'work', 'lark']\n",
      "Tokenized sentence: ['there', 'are', 'many', 'company', 'tell', 'me', 'the', 'language']\n",
      "After stop words removal: ['many', 'company', 'tell', 'language']\n",
      "After stemming with porters algorithm: ['mani', 'compani', 'tell', 'languag']\n",
      "Tokenized sentence: ['sorry', 'completely', 'forgot', 'will', 'pop', 'em', 'round', 'this', 'week', 'if', 'your', 'still', 'here']\n",
      "After stop words removal: ['sorry', 'completely', 'forgot', 'pop', 'em', 'round', 'week', 'still']\n",
      "After stemming with porters algorithm: ['sorri', 'complet', 'forgot', 'pop', 'round', 'week', 'still']\n",
      "Tokenized sentence: ['mmmmmmm', 'snuggles', 'into', 'you', 'deep', 'contented', 'sigh', 'whispers', 'i', 'fucking', 'love', 'you', 'so', 'much', 'i', 'can', 'barely', 'stand', 'it']\n",
      "After stop words removal: ['mmmmmmm', 'snuggles', 'deep', 'contented', 'sigh', 'whispers', 'fucking', 'love', 'much', 'barely', 'stand']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['mmmmmmm', 'snuggl', 'deep', 'conten', 'sigh', 'whisper', 'fuc', 'love', 'much', 'bare', 'stand']\n",
      "Tokenized sentence: ['u', 'call', 'me', 'alter', 'at', 'ok']\n",
      "After stop words removal: ['u', 'call', 'alter', 'ok']\n",
      "After stemming with porters algorithm: ['call', 'alter']\n",
      "Tokenized sentence: ['i', 'm', 'leaving', 'my', 'house', 'now']\n",
      "After stop words removal: ['leaving', 'house']\n",
      "leav\n",
      "After stemming with porters algorithm: ['leav', 'hous']\n",
      "Tokenized sentence: ['k', 'k', 'why', 'cant', 'you', 'come', 'here', 'and', 'search', 'job']\n",
      "After stop words removal: ['k', 'k', 'cant', 'come', 'search', 'job']\n",
      "After stemming with porters algorithm: ['cant', 'come', 'search', 'job']\n",
      "Tokenized sentence: ['yeah', 'lol', 'luckily', 'i', 'didn', 't', 'have', 'a', 'starring', 'role', 'like', 'you']\n",
      "After stop words removal: ['yeah', 'lol', 'luckily', 'starring', 'role', 'like']\n",
      "starr\n",
      "After stemming with porters algorithm: ['yeah', 'lol', 'luckili', 'star', 'role', 'like']\n",
      "Tokenized sentence: ['thanks', 'i', 'll', 'keep', 'that', 'in', 'mind']\n",
      "After stop words removal: ['thanks', 'keep', 'mind']\n",
      "After stemming with porters algorithm: ['thank', 'keep', 'mind']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['ok', 'da', 'i', 'already', 'planned', 'i', 'wil', 'pick', 'you']\n",
      "After stop words removal: ['ok', 'da', 'already', 'planned', 'wil', 'pick']\n",
      "After stemming with porters algorithm: ['alreadi', 'plan', 'wil', 'pick']\n",
      "Tokenized sentence: ['i', 'm', 'really', 'not', 'up', 'to', 'it', 'still', 'tonight', 'babe']\n",
      "After stop words removal: ['really', 'still', 'tonight', 'babe']\n",
      "After stemming with porters algorithm: ['realli', 'still', 'tonight', 'babe']\n",
      "Tokenized sentence: ['where', 'to', 'get', 'those']\n",
      "After stop words removal: ['get']\n",
      "After stemming with porters algorithm: ['get']\n",
      "Tokenized sentence: ['also', 'andros', 'ice', 'etc', 'etc']\n",
      "After stop words removal: ['also', 'andros', 'ice', 'etc', 'etc']\n",
      "After stemming with porters algorithm: ['also', 'andro', 'ic', 'etc', 'etc']\n",
      "Tokenized sentence: ['well', 'there', 's', 'not', 'a', 'lot', 'of', 'things', 'happening', 'in', 'lindsay', 'on', 'new', 'years', 'sighs', 'some', 'bars', 'in', 'ptbo', 'and', 'the', 'blue', 'heron', 'has', 'something', 'going']\n",
      "After stop words removal: ['well', 'lot', 'things', 'happening', 'lindsay', 'new', 'years', 'sighs', 'bars', 'ptbo', 'blue', 'heron', 'something', 'going']\n",
      "happen\n",
      "someth\n",
      "go\n",
      "After stemming with porters algorithm: ['well', 'lot', 'thing', 'happen', 'lindsai', 'new', 'year', 'sigh', 'bar', 'ptbo', 'blue', 'heron', 'somet', 'go']\n",
      "Tokenized sentence: ['gr', 'poly', 'tones', 'all', 'mobs', 'direct', 'u', 'rply', 'with', 'poly', 'title', 'to', 'eg', 'poly', 'breathe', 'titles', 'crazyin', 'sleepingwith', 'finest', 'ymca', 'getzed', 'co', 'uk', 'pobox', 'o', 'w', 'wq', 'p']\n",
      "After stop words removal: ['gr', 'poly', 'tones', 'mobs', 'direct', 'u', 'rply', 'poly', 'title', 'eg', 'poly', 'breathe', 'titles', 'crazyin', 'sleepingwith', 'finest', 'ymca', 'getzed', 'co', 'uk', 'pobox', 'w', 'wq', 'p']\n",
      "After stemming with porters algorithm: ['poli', 'tone', 'mob', 'direct', 'rply', 'poli', 'titl', 'poli', 'breath', 'titl', 'crazyin', 'sleepingwith', 'finest', 'ymca', 'getz', 'pobox']\n",
      "Tokenized sentence: ['in', 'case', 'you', 'wake', 'up', 'wondering', 'where', 'i', 'am', 'i', 'forgot', 'i', 'have', 'to', 'take', 'care', 'of', 'something', 'for', 'grandma', 'today', 'should', 'be', 'done', 'before', 'the', 'parade']\n",
      "After stop words removal: ['case', 'wake', 'wondering', 'forgot', 'take', 'care', 'something', 'grandma', 'today', 'done', 'parade']\n",
      "wonder\n",
      "someth\n",
      "After stemming with porters algorithm: ['case', 'wake', 'wonder', 'forgot', 'take', 'care', 'somet', 'grandma', 'todai', 'done', 'parad']\n",
      "Tokenized sentence: ['freemsg', 'you', 'have', 'been', 'awarded', 'a', 'free', 'mini', 'digital', 'camera', 'just', 'reply', 'snap', 'to', 'collect', 'your', 'prize', 'quizclub', 'opt', 'out', 'stop', 'p', 'wk', 'sp', 'rwm', 'ph']\n",
      "After stop words removal: ['freemsg', 'awarded', 'free', 'mini', 'digital', 'camera', 'reply', 'snap', 'collect', 'prize', 'quizclub', 'opt', 'stop', 'p', 'wk', 'sp', 'rwm', 'ph']\n",
      "After stemming with porters algorithm: ['freemsg', 'awar', 'free', 'mini', 'digit', 'camera', 'repli', 'snap', 'collect', 'priz', 'quizclub', 'opt', 'stop', 'rwm']\n",
      "Tokenized sentence: ['it', 's', 'cool', 'we', 'can', 'last', 'a', 'little', 'while', 'getting', 'more', 'any', 'time', 'soon']\n",
      "After stop words removal: ['cool', 'last', 'little', 'getting', 'time', 'soon']\n",
      "gett\n",
      "After stemming with porters algorithm: ['cool', 'last', 'littl', 'get', 'time', 'soon']\n",
      "Tokenized sentence: ['by', 'monday', 'next', 'week', 'give', 'me', 'the', 'full', 'gist']\n",
      "After stop words removal: ['monday', 'next', 'week', 'give', 'full', 'gist']\n",
      "After stemming with porters algorithm: ['mondai', 'next', 'week', 'give', 'full', 'gist']\n",
      "Tokenized sentence: ['how', 'r', 'going', 'to', 'send', 'it', 'to', 'me']\n",
      "After stop words removal: ['r', 'going', 'send']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'send']\n",
      "Tokenized sentence: ['you', 'will', 'be', 'receiving', 'this', 'week', 's', 'triple', 'echo', 'ringtone', 'shortly', 'enjoy', 'it']\n",
      "After stop words removal: ['receiving', 'week', 'triple', 'echo', 'ringtone', 'shortly', 'enjoy']\n",
      "receiv\n",
      "After stemming with porters algorithm: ['receiv', 'week', 'tripl', 'echo', 'rington', 'shortli', 'enjoi']\n",
      "Tokenized sentence: ['great', 'news', 'call', 'freefone', 'to', 'claim', 'your', 'guaranteed', 'cash', 'or', 'gift', 'speak', 'to', 'a', 'live', 'operator', 'now']\n",
      "After stop words removal: ['great', 'news', 'call', 'freefone', 'claim', 'guaranteed', 'cash', 'gift', 'speak', 'live', 'operator']\n",
      "After stemming with porters algorithm: ['great', 'new', 'call', 'freefon', 'claim', 'guaranteed', 'cash', 'gift', 'speak', 'live', 'oper']\n",
      "Tokenized sentence: ['but', 'i', 'm', 'on', 'a', 'diet', 'and', 'i', 'ate', 'too', 'many', 'slices', 'of', 'pizza', 'yesterday', 'ugh', 'i', 'm', 'always', 'on', 'a', 'diet']\n",
      "After stop words removal: ['diet', 'ate', 'many', 'slices', 'pizza', 'yesterday', 'ugh', 'always', 'diet']\n",
      "After stemming with porters algorithm: ['diet', 'at', 'mani', 'slice', 'pizza', 'yesterdai', 'ugh', 'alwai', 'diet']\n",
      "Tokenized sentence: ['nope', 'watching', 'tv', 'at', 'home', 'not', 'going', 'out', 'v', 'bored']\n",
      "After stop words removal: ['nope', 'watching', 'tv', 'home', 'going', 'v', 'bored']\n",
      "watch\n",
      "go\n",
      "After stemming with porters algorithm: ['nope', 'watc', 'home', 'go', 'bore']\n",
      "Tokenized sentence: ['well', 'am', 'officially', 'in', 'a', 'philosophical', 'hole', 'so', 'if', 'u', 'wanna', 'call', 'am', 'at', 'home', 'ready', 'to', 'be', 'saved']\n",
      "After stop words removal: ['well', 'officially', 'philosophical', 'hole', 'u', 'wanna', 'call', 'home', 'ready', 'saved']\n",
      "After stemming with porters algorithm: ['well', 'offici', 'philosoph', 'hole', 'wanna', 'call', 'home', 'readi', 'save']\n",
      "Tokenized sentence: ['pls', 'dont', 'restrict', 'her', 'from', 'eating', 'anythin', 'she', 'likes', 'for', 'the', 'next', 'two', 'days']\n",
      "After stop words removal: ['pls', 'dont', 'restrict', 'eating', 'anythin', 'likes', 'next', 'two', 'days']\n",
      "eat\n",
      "eate\n",
      "After stemming with porters algorithm: ['pl', 'dont', 'restrict', 'eat', 'anythin', 'like', 'next', 'two', 'dai']\n",
      "Tokenized sentence: ['in', 'e', 'msg', 'jus', 'now', 'u', 'said', 'thanks', 'for', 'gift']\n",
      "After stop words removal: ['e', 'msg', 'jus', 'u', 'said', 'thanks', 'gift']\n",
      "After stemming with porters algorithm: ['msg', 'ju', 'said', 'thank', 'gift']\n",
      "Tokenized sentence: ['i', 'm', 'eatin', 'now', 'lor', 'but', 'goin', 'back', 'to', 'work', 'soon', 'e', 'mountain', 'deer', 'show', 'huh', 'i', 'watch', 'b', 'liao', 'very', 'nice']\n",
      "After stop words removal: ['eatin', 'lor', 'goin', 'back', 'work', 'soon', 'e', 'mountain', 'deer', 'show', 'huh', 'watch', 'b', 'liao', 'nice']\n",
      "After stemming with porters algorithm: ['eatin', 'lor', 'goin', 'back', 'work', 'soon', 'mountain', 'deer', 'show', 'huh', 'watch', 'liao', 'nice']\n",
      "Tokenized sentence: ['i', 'have', 'a', 'sore', 'throat', 'it', 's', 'scratches', 'when', 'i', 'talk']\n",
      "After stop words removal: ['sore', 'throat', 'scratches', 'talk']\n",
      "After stemming with porters algorithm: ['sore', 'throat', 'scratch', 'talk']\n",
      "Tokenized sentence: ['change', 'again', 'it', 's', 'e', 'one', 'next', 'to', 'escalator']\n",
      "After stop words removal: ['change', 'e', 'one', 'next', 'escalator']\n",
      "After stemming with porters algorithm: ['chang', 'on', 'next', 'escal']\n",
      "Tokenized sentence: ['sounds', 'great', 'im', 'going', 'to', 'sleep', 'now', 'have', 'a', 'good', 'night']\n",
      "After stop words removal: ['sounds', 'great', 'im', 'going', 'sleep', 'good', 'night']\n",
      "go\n",
      "After stemming with porters algorithm: ['sound', 'great', 'go', 'sleep', 'good', 'night']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'was', 'awarded', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'our', 'nd', 'attempt', 'to', 'contact', 'you', 'call', 'box', 'qu']\n",
      "After stop words removal: ['urgent', 'mobile', 'awarded', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'contact', 'call', 'box', 'qu']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'awar', 'bonu', 'caller', 'priz', 'attempt', 'contact', 'call', 'box']\n",
      "Tokenized sentence: ['ok', 'set', 'let', 'u', 'noe', 'e', 'details', 'later']\n",
      "After stop words removal: ['ok', 'set', 'let', 'u', 'noe', 'e', 'details', 'later']\n",
      "After stemming with porters algorithm: ['set', 'let', 'noe', 'detail', 'later']\n",
      "Tokenized sentence: ['today', 'is', 'accept', 'day', 'u', 'accept', 'me', 'as', 'brother', 'sister', 'lover', 'dear', 'best', 'clos', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'no', 'rply', 'means', 'enemy']\n",
      "After stop words removal: ['today', 'accept', 'day', 'u', 'accept', 'brother', 'sister', 'lover', 'dear', 'best', 'clos', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'rply', 'means', 'enemy']\n",
      "After stemming with porters algorithm: ['todai', 'accept', 'dai', 'accept', 'brother', 'sister', 'lover', 'dear', 'best', 'clo', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'rply', 'mean', 'enemi']\n",
      "Tokenized sentence: ['was', 'the', 'farm', 'open']\n",
      "After stop words removal: ['farm', 'open']\n",
      "After stemming with porters algorithm: ['farm', 'open']\n",
      "Tokenized sentence: ['yup', 'but', 'it', 's', 'not', 'giving', 'me', 'problems', 'now', 'so', 'mayb', 'i', 'll', 'jus', 'leave', 'it']\n",
      "After stop words removal: ['yup', 'giving', 'problems', 'mayb', 'jus', 'leave']\n",
      "giv\n",
      "After stemming with porters algorithm: ['yup', 'give', 'problem', 'mayb', 'ju', 'leav']\n",
      "Tokenized sentence: ['each', 'moment', 'in', 'a', 'day', 'has', 'its', 'own', 'value', 'morning', 'brings', 'hope', 'afternoon', 'brings', 'faith', 'evening', 'brings', 'luv', 'night', 'brings', 'rest', 'wish', 'u', 'find', 'them', 'all', 'today', 'good', 'morning']\n",
      "After stop words removal: ['moment', 'day', 'value', 'morning', 'brings', 'hope', 'afternoon', 'brings', 'faith', 'evening', 'brings', 'luv', 'night', 'brings', 'rest', 'wish', 'u', 'find', 'today', 'good', 'morning']\n",
      "morn\n",
      "even\n",
      "morn\n",
      "After stemming with porters algorithm: ['moment', 'dai', 'valu', 'mor', 'bring', 'hope', 'afternoon', 'bring', 'faith', 'even', 'bring', 'luv', 'night', 'bring', 'rest', 'wish', 'find', 'todai', 'good', 'mor']\n",
      "Tokenized sentence: ['just', 'sent', 'you', 'an', 'email', 'to', 'an', 'address', 'with', 'incomm', 'in', 'it', 'is', 'that', 'right']\n",
      "After stop words removal: ['sent', 'email', 'address', 'incomm', 'right']\n",
      "After stemming with porters algorithm: ['sent', 'email', 'address', 'incomm', 'right']\n",
      "Tokenized sentence: ['how', 'have', 'your', 'little', 'darlings', 'been', 'so', 'far', 'this', 'week', 'need', 'a', 'coffee', 'run', 'tomo', 'can', 't', 'believe', 'it', 's', 'that', 'time', 'of', 'week', 'already']\n",
      "After stop words removal: ['little', 'darlings', 'far', 'week', 'need', 'coffee', 'run', 'tomo', 'believe', 'time', 'week', 'already']\n",
      "darl\n",
      "After stemming with porters algorithm: ['littl', 'darl', 'far', 'week', 'need', 'coffe', 'run', 'tomo', 'believ', 'time', 'week', 'alreadi']\n",
      "Tokenized sentence: ['just', 'wanted', 'to', 'say', 'holy', 'shit', 'you', 'guys', 'weren', 't', 'kidding', 'about', 'this', 'bud']\n",
      "After stop words removal: ['wanted', 'say', 'holy', 'shit', 'guys', 'kidding', 'bud']\n",
      "kidd\n",
      "After stemming with porters algorithm: ['wan', 'sai', 'holi', 'shit', 'gui', 'kid', 'bud']\n",
      "Tokenized sentence: ['sorry', 'that', 'took', 'so', 'long', 'omw', 'now']\n",
      "After stop words removal: ['sorry', 'took', 'long', 'omw']\n",
      "After stemming with porters algorithm: ['sorri', 'took', 'long', 'omw']\n",
      "Tokenized sentence: ['the', 'house', 'is', 'on', 'the', 'water', 'with', 'a', 'dock', 'a', 'boat', 'rolled', 'up', 'with', 'a', 'newscaster', 'who', 'dabbles', 'in', 'jazz', 'flute', 'behind', 'the', 'wheel']\n",
      "After stop words removal: ['house', 'water', 'dock', 'boat', 'rolled', 'newscaster', 'dabbles', 'jazz', 'flute', 'behind', 'wheel']\n",
      "After stemming with porters algorithm: ['hous', 'water', 'dock', 'boat', 'roll', 'newscast', 'dabbl', 'jazz', 'flute', 'behind', 'wheel']\n",
      "Tokenized sentence: ['missed', 'call', 'alert', 'these', 'numbers', 'called', 'but', 'left', 'no', 'message']\n",
      "After stop words removal: ['missed', 'call', 'alert', 'numbers', 'called', 'left', 'message']\n",
      "After stemming with porters algorithm: ['miss', 'call', 'alert', 'number', 'call', 'left', 'messag']\n",
      "Tokenized sentence: ['yup', 'i', 'havent', 'been', 'there', 'before', 'you', 'want', 'to', 'go', 'for', 'the', 'yoga', 'i', 'can', 'call', 'up', 'to', 'book']\n",
      "After stop words removal: ['yup', 'havent', 'want', 'go', 'yoga', 'call', 'book']\n",
      "After stemming with porters algorithm: ['yup', 'havent', 'want', 'yoga', 'call', 'book']\n",
      "Tokenized sentence: ['that', 'seems', 'unnecessarily', 'hostile']\n",
      "After stop words removal: ['seems', 'unnecessarily', 'hostile']\n",
      "After stemming with porters algorithm: ['seem', 'unnecessarili', 'hostil']\n",
      "Tokenized sentence: ['dating', 'i', 'have', 'had', 'two', 'of', 'these', 'only', 'started', 'after', 'i', 'sent', 'a', 'text', 'to', 'talk', 'sport', 'radio', 'last', 'week', 'any', 'connection', 'do', 'you', 'think', 'or', 'coincidence']\n",
      "After stop words removal: ['dating', 'two', 'started', 'sent', 'text', 'talk', 'sport', 'radio', 'last', 'week', 'connection', 'think', 'coincidence']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['date', 'two', 'star', 'sent', 'text', 'talk', 'sport', 'radio', 'last', 'week', 'connect', 'think', 'coincid']\n",
      "Tokenized sentence: ['somebody', 'set', 'up', 'a', 'website', 'where', 'you', 'can', 'play', 'hold', 'em', 'using', 'eve', 'online', 'spacebucks']\n",
      "After stop words removal: ['somebody', 'set', 'website', 'play', 'hold', 'em', 'using', 'eve', 'online', 'spacebucks']\n",
      "us\n",
      "After stemming with porters algorithm: ['somebodi', 'set', 'websit', 'plai', 'hold', 'us', 'ev', 'onlin', 'spacebuck']\n",
      "Tokenized sentence: ['ok', 'no', 'prob']\n",
      "After stop words removal: ['ok', 'prob']\n",
      "After stemming with porters algorithm: ['prob']\n",
      "Tokenized sentence: ['does', 'daddy', 'have', 'a', 'bb', 'now']\n",
      "After stop words removal: ['daddy', 'bb']\n",
      "After stemming with porters algorithm: ['daddi']\n",
      "Tokenized sentence: ['for', 'many', 'things', 'its', 'an', 'antibiotic', 'and', 'it', 'can', 'be', 'used', 'for', 'chest', 'abdomen', 'and', 'gynae', 'infections', 'even', 'bone', 'infections']\n",
      "After stop words removal: ['many', 'things', 'antibiotic', 'used', 'chest', 'abdomen', 'gynae', 'infections', 'even', 'bone', 'infections']\n",
      "After stemming with porters algorithm: ['mani', 'thing', 'antibiot', 'us', 'chest', 'abdomen', 'gynae', 'infect', 'even', 'bone', 'infect']\n",
      "Tokenized sentence: ['don', 'no', 'da', 'whats', 'you', 'plan']\n",
      "After stop words removal: ['da', 'whats', 'plan']\n",
      "After stemming with porters algorithm: ['what', 'plan']\n",
      "Tokenized sentence: ['my', 'friend', 'she', 's', 'studying', 'at', 'warwick', 'we', 've', 'planned', 'to', 'go', 'shopping', 'and', 'to', 'concert', 'tmw', 'but', 'it', 'may', 'be', 'canceled', 'havn', 't', 'seen', 'for', 'ages', 'yeah', 'we', 'should', 'get', 'together', 'sometime']\n",
      "After stop words removal: ['friend', 'studying', 'warwick', 'planned', 'go', 'shopping', 'concert', 'tmw', 'may', 'canceled', 'havn', 'seen', 'ages', 'yeah', 'get', 'together', 'sometime']\n",
      "study\n",
      "shopp\n",
      "After stemming with porters algorithm: ['friend', 'stud', 'warwick', 'plan', 'shop', 'concert', 'tmw', 'mai', 'cancel', 'havn', 'seen', 'ag', 'yeah', 'get', 'togeth', 'sometim']\n",
      "Tokenized sentence: ['send', 'me', 'yetty', 's', 'number', 'pls']\n",
      "After stop words removal: ['send', 'yetty', 'number', 'pls']\n",
      "After stemming with porters algorithm: ['send', 'yetti', 'number', 'pl']\n",
      "Tokenized sentence: ['hello', 'my', 'boytoy', 'i', 'made', 'it', 'home', 'and', 'my', 'constant', 'thought', 'is', 'of', 'you', 'my', 'love', 'i', 'hope', 'your', 'having', 'a', 'nice', 'visit', 'but', 'i', 'can', 't', 'wait', 'till', 'you', 'come', 'home', 'to', 'me', 'kiss']\n",
      "After stop words removal: ['hello', 'boytoy', 'made', 'home', 'constant', 'thought', 'love', 'hope', 'nice', 'visit', 'wait', 'till', 'come', 'home', 'kiss']\n",
      "After stemming with porters algorithm: ['hello', 'boytoi', 'made', 'home', 'constant', 'thought', 'love', 'hope', 'nice', 'visit', 'wait', 'till', 'come', 'home', 'kiss']\n",
      "Tokenized sentence: ['joy', 's', 'father', 'is', 'john', 'then', 'john', 'is', 'the', 'of', 'joy', 's', 'father', 'if', 'u', 'ans', 'ths', 'you', 'hav', 'lt', 'gt', 'iq', 'tis', 's', 'ias', 'question', 'try', 'to', 'answer']\n",
      "After stop words removal: ['joy', 'father', 'john', 'john', 'joy', 'father', 'u', 'ans', 'ths', 'hav', 'lt', 'gt', 'iq', 'tis', 'ias', 'question', 'try', 'answer']\n",
      "After stemming with porters algorithm: ['joi', 'father', 'john', 'john', 'joi', 'father', 'an', 'th', 'hav', 'ti', 'ia', 'quest', 'try', 'answer']\n",
      "Tokenized sentence: ['i', 'wait', 'inside', 'da', 'car', 'park']\n",
      "After stop words removal: ['wait', 'inside', 'da', 'car', 'park']\n",
      "After stemming with porters algorithm: ['wait', 'insid', 'car', 'park']\n",
      "Tokenized sentence: ['take', 'something', 'for', 'pain', 'if', 'it', 'moves', 'however', 'to', 'any', 'side', 'in', 'the', 'next', 'hrs', 'see', 'a', 'doctor']\n",
      "After stop words removal: ['take', 'something', 'pain', 'moves', 'however', 'side', 'next', 'hrs', 'see', 'doctor']\n",
      "someth\n",
      "After stemming with porters algorithm: ['take', 'somet', 'pain', 'move', 'howev', 'side', 'next', 'hr', 'see', 'doctor']\n",
      "Tokenized sentence: ['mmmm', 'fuck', 'not', 'fair', 'you', 'know', 'my', 'weaknesses', 'grins', 'pushes', 'you', 'to', 'your', 'knee', 's', 'exposes', 'my', 'belly', 'and', 'pulls', 'your', 'head', 'to', 'it', 'don', 't', 'forget', 'i', 'know', 'yours', 'too', 'wicked', 'smile']\n",
      "After stop words removal: ['mmmm', 'fuck', 'fair', 'know', 'weaknesses', 'grins', 'pushes', 'knee', 'exposes', 'belly', 'pulls', 'head', 'forget', 'know', 'wicked', 'smile']\n",
      "After stemming with porters algorithm: ['mmmm', 'fuck', 'fair', 'know', 'weak', 'grin', 'push', 'knee', 'expos', 'belli', 'pull', 'head', 'forget', 'know', 'wic', 'smile']\n",
      "Tokenized sentence: ['your', 'pussy', 'is', 'perfect']\n",
      "After stop words removal: ['pussy', 'perfect']\n",
      "After stemming with porters algorithm: ['pussi', 'perfect']\n",
      "Tokenized sentence: ['i', 'was', 'gonna', 'ask', 'you', 'lol', 'but', 'i', 'think', 'its', 'at']\n",
      "After stop words removal: ['gonna', 'ask', 'lol', 'think']\n",
      "After stemming with porters algorithm: ['gonna', 'ask', 'lol', 'think']\n",
      "Tokenized sentence: ['almost', 'there', 'see', 'u', 'in', 'a', 'sec']\n",
      "After stop words removal: ['almost', 'see', 'u', 'sec']\n",
      "After stemming with porters algorithm: ['almost', 'see', 'sec']\n",
      "Tokenized sentence: ['and', 'i', 'don', 't', 'plan', 'on', 'staying', 'the', 'night', 'but', 'i', 'prolly', 'won', 't', 'be', 'back', 'til', 'late']\n",
      "After stop words removal: ['plan', 'staying', 'night', 'prolly', 'back', 'til', 'late']\n",
      "stay\n",
      "After stemming with porters algorithm: ['plan', 'stai', 'night', 'prolli', 'back', 'til', 'late']\n",
      "Tokenized sentence: ['no', 'need', 'to', 'buy', 'lunch', 'for', 'me', 'i', 'eat', 'maggi', 'mee']\n",
      "After stop words removal: ['need', 'buy', 'lunch', 'eat', 'maggi', 'mee']\n",
      "After stemming with porters algorithm: ['need', 'bui', 'lunch', 'eat', 'maggi', 'mee']\n",
      "Tokenized sentence: ['oh', 'ho', 'is', 'this', 'the', 'first', 'time', 'u', 'use', 'these', 'type', 'of', 'words']\n",
      "After stop words removal: ['oh', 'ho', 'first', 'time', 'u', 'use', 'type', 'words']\n",
      "After stemming with porters algorithm: ['first', 'time', 'us', 'type', 'word']\n",
      "Tokenized sentence: ['where', 'u', 'been', 'hiding', 'stranger']\n",
      "After stop words removal: ['u', 'hiding', 'stranger']\n",
      "hid\n",
      "After stemming with porters algorithm: ['hide', 'stranger']\n",
      "Tokenized sentence: ['filthy', 'stories', 'and', 'girls', 'waiting', 'for', 'your']\n",
      "After stop words removal: ['filthy', 'stories', 'girls', 'waiting']\n",
      "wait\n",
      "After stemming with porters algorithm: ['filthi', 'stori', 'girl', 'wait']\n",
      "Tokenized sentence: ['hi', 'hope', 'ur', 'day', 'good', 'back', 'from', 'walk', 'table', 'booked', 'for', 'half', 'eight', 'let', 'me', 'know', 'when', 'ur', 'coming', 'over']\n",
      "After stop words removal: ['hi', 'hope', 'ur', 'day', 'good', 'back', 'walk', 'table', 'booked', 'half', 'eight', 'let', 'know', 'ur', 'coming']\n",
      "com\n",
      "After stemming with porters algorithm: ['hope', 'dai', 'good', 'back', 'walk', 'tabl', 'book', 'half', 'eight', 'let', 'know', 'come']\n",
      "Tokenized sentence: ['the', 'guy', 'kadeem', 'hasn', 't', 'been', 'selling', 'since', 'the', 'break', 'i', 'know', 'one', 'other', 'guy', 'but', 'he', 's', 'paranoid', 'as', 'fuck', 'and', 'doesn', 't', 'like', 'selling', 'without', 'me', 'there', 'and', 'i', 'can', 't', 'be', 'up', 'there', 'til', 'late', 'tonight']\n",
      "After stop words removal: ['guy', 'kadeem', 'selling', 'since', 'break', 'know', 'one', 'guy', 'paranoid', 'fuck', 'like', 'selling', 'without', 'til', 'late', 'tonight']\n",
      "sell\n",
      "sell\n",
      "After stemming with porters algorithm: ['gui', 'kadeem', 'sell', 'sinc', 'break', 'know', 'on', 'gui', 'paranoid', 'fuck', 'like', 'sell', 'without', 'til', 'late', 'tonight']\n",
      "Tokenized sentence: ['guess', 'which', 'pub', 'im', 'in', 'im', 'as', 'happy', 'as', 'a', 'pig', 'in', 'clover', 'or', 'whatever', 'the', 'saying', 'is']\n",
      "After stop words removal: ['guess', 'pub', 'im', 'im', 'happy', 'pig', 'clover', 'whatever', 'saying']\n",
      "say\n",
      "After stemming with porters algorithm: ['guess', 'pub', 'happi', 'pig', 'clover', 'whatev', 'sai']\n",
      "Tokenized sentence: ['what', 'do', 'you', 'do', 'my', 'dog', 'must', 'i', 'always', 'wait', 'till', 'the', 'end', 'of', 'your', 'day', 'to', 'have', 'word', 'from', 'you', 'did', 'you', 'run', 'out', 'of', 'time', 'on', 'your', 'cell', 'already']\n",
      "After stop words removal: ['dog', 'must', 'always', 'wait', 'till', 'end', 'day', 'word', 'run', 'time', 'cell', 'already']\n",
      "After stemming with porters algorithm: ['dog', 'must', 'alwai', 'wait', 'till', 'end', 'dai', 'word', 'run', 'time', 'cell', 'alreadi']\n",
      "Tokenized sentence: ['no', 'current', 'and', 'food', 'here', 'i', 'am', 'alone', 'also']\n",
      "After stop words removal: ['current', 'food', 'alone', 'also']\n",
      "After stemming with porters algorithm: ['current', 'food', 'alon', 'also']\n",
      "Tokenized sentence: ['happy', 'birthday', 'to', 'you', 'dear', 'with', 'lots', 'of', 'love', 'rakhesh', 'nri']\n",
      "After stop words removal: ['happy', 'birthday', 'dear', 'lots', 'love', 'rakhesh', 'nri']\n",
      "After stemming with porters algorithm: ['happi', 'birthdai', 'dear', 'lot', 'love', 'rakhesh', 'nri']\n",
      "Tokenized sentence: ['hey', 'can', 'you', 'tell', 'me', 'blake', 's', 'address', 'carlos', 'wanted', 'me', 'to', 'meet', 'him', 'there', 'but', 'i', 'got', 'lost', 'and', 'he', 's', 'not', 'answering', 'his', 'phone']\n",
      "After stop words removal: ['hey', 'tell', 'blake', 'address', 'carlos', 'wanted', 'meet', 'got', 'lost', 'answering', 'phone']\n",
      "answer\n",
      "After stemming with porters algorithm: ['hei', 'tell', 'blake', 'address', 'carlo', 'wan', 'meet', 'got', 'lost', 'answer', 'phone']\n",
      "Tokenized sentence: ['i', 'think', 'its', 'far', 'more', 'than', 'that', 'but', 'find', 'out', 'check', 'google', 'maps', 'for', 'a', 'place', 'from', 'your', 'dorm']\n",
      "After stop words removal: ['think', 'far', 'find', 'check', 'google', 'maps', 'place', 'dorm']\n",
      "After stemming with porters algorithm: ['think', 'far', 'find', 'check', 'googl', 'map', 'place', 'dorm']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'c', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'c', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['i', 'liked', 'your', 'new', 'house']\n",
      "After stop words removal: ['liked', 'new', 'house']\n",
      "After stemming with porters algorithm: ['like', 'new', 'hous']\n",
      "Tokenized sentence: ['wat', 'doing', 'now']\n",
      "After stop words removal: ['wat']\n",
      "After stemming with porters algorithm: ['wat']\n",
      "Tokenized sentence: ['meeting', 'u', 'is', 'my', 'work', 'tel', 'me', 'when', 'shall', 'i', 'do', 'my', 'work', 'tomorrow']\n",
      "After stop words removal: ['meeting', 'u', 'work', 'tel', 'shall', 'work', 'tomorrow']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'work', 'tel', 'shall', 'work', 'tomorrow']\n",
      "Tokenized sentence: ['free', 'ringtone', 'text', 'first', 'to', 'for', 'a', 'poly', 'or', 'text', 'get', 'to', 'for', 'a', 'true', 'tone', 'help', 'after', 'st', 'free', 'tones', 'are', 'x', 'pw', 'to', 'e', 'nd', 'txt', 'stop']\n",
      "After stop words removal: ['free', 'ringtone', 'text', 'first', 'poly', 'text', 'get', 'true', 'tone', 'help', 'st', 'free', 'tones', 'x', 'pw', 'e', 'nd', 'txt', 'stop']\n",
      "After stemming with porters algorithm: ['free', 'rington', 'text', 'first', 'poli', 'text', 'get', 'true', 'tone', 'help', 'free', 'tone', 'txt', 'stop']\n",
      "Tokenized sentence: ['okmail', 'dear', 'dave', 'this', 'is', 'your', 'final', 'notice', 'to', 'collect', 'your', 'tenerife', 'holiday', 'or', 'cash', 'award', 'call', 'from', 'landline', 'tcs', 'sae', 'box', 'cw', 'wx', 'ppm']\n",
      "After stop words removal: ['okmail', 'dear', 'dave', 'final', 'notice', 'collect', 'tenerife', 'holiday', 'cash', 'award', 'call', 'landline', 'tcs', 'sae', 'box', 'cw', 'wx', 'ppm']\n",
      "After stemming with porters algorithm: ['okmail', 'dear', 'dave', 'final', 'notic', 'collect', 'tenerif', 'holidai', 'cash', 'award', 'call', 'landlin', 'tc', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['not', 'heard', 'from', 'u', 'a', 'while', 'call', 'me', 'now', 'am', 'here', 'all', 'night', 'with', 'just', 'my', 'knickers', 'on', 'make', 'me', 'beg', 'for', 'it', 'like', 'u', 'did', 'last', 'time', 'xx', 'luv', 'nikiyu', 'net']\n",
      "After stop words removal: ['heard', 'u', 'call', 'night', 'knickers', 'make', 'beg', 'like', 'u', 'last', 'time', 'xx', 'luv', 'nikiyu', 'net']\n",
      "After stemming with porters algorithm: ['heard', 'call', 'night', 'knicker', 'make', 'beg', 'like', 'last', 'time', 'luv', 'nikiyu', 'net']\n",
      "Tokenized sentence: ['we', 'confirm', 'eating', 'at', 'esplanade']\n",
      "After stop words removal: ['confirm', 'eating', 'esplanade']\n",
      "eat\n",
      "eate\n",
      "After stemming with porters algorithm: ['confirm', 'eat', 'esplanad']\n",
      "Tokenized sentence: ['january', 'male', 'sale', 'hot', 'gay', 'chat', 'now', 'cheaper', 'call', 'national', 'rate', 'from', 'p', 'min', 'cheap', 'to', 'p', 'min', 'peak', 'to', 'stop', 'texts', 'call', 'p', 'min']\n",
      "After stop words removal: ['january', 'male', 'sale', 'hot', 'gay', 'chat', 'cheaper', 'call', 'national', 'rate', 'p', 'min', 'cheap', 'p', 'min', 'peak', 'stop', 'texts', 'call', 'p', 'min']\n",
      "After stemming with porters algorithm: ['januari', 'male', 'sale', 'hot', 'gai', 'chat', 'cheaper', 'call', 'nat', 'rate', 'min', 'cheap', 'min', 'peak', 'stop', 'text', 'call', 'min']\n",
      "Tokenized sentence: ['oic', 'i', 'saw', 'him', 'too', 'but', 'i', 'tot', 'he', 'din', 'c', 'me', 'i', 'found', 'a', 'group', 'liao']\n",
      "After stop words removal: ['oic', 'saw', 'tot', 'din', 'c', 'found', 'group', 'liao']\n",
      "After stemming with porters algorithm: ['oic', 'saw', 'tot', 'din', 'found', 'group', 'liao']\n",
      "Tokenized sentence: ['see', 'i', 'knew', 'giving', 'you', 'a', 'break', 'a', 'few', 'times', 'woul', 'lead', 'to', 'you', 'always', 'wanting', 'to', 'miss', 'curfew', 'i', 'was', 'gonna', 'gibe', 'you', 'til', 'one', 'but', 'a', 'midnight', 'movie', 'is', 'not', 'gonna', 'get', 'out', 'til', 'after', 'you', 'need', 'to', 'come', 'home', 'you', 'need', 'to', 'getsleep', 'and', 'if', 'anything', 'you', 'need', 'to', 'b', 'studdying', 'ear', 'training']\n",
      "After stop words removal: ['see', 'knew', 'giving', 'break', 'times', 'woul', 'lead', 'always', 'wanting', 'miss', 'curfew', 'gonna', 'gibe', 'til', 'one', 'midnight', 'movie', 'gonna', 'get', 'til', 'need', 'come', 'home', 'need', 'getsleep', 'anything', 'need', 'b', 'studdying', 'ear', 'training']\n",
      "giv\n",
      "want\n",
      "anyth\n",
      "studdy\n",
      "train\n",
      "After stemming with porters algorithm: ['see', 'knew', 'give', 'break', 'time', 'woul', 'lead', 'alwai', 'wan', 'miss', 'curfew', 'gonna', 'gibe', 'til', 'on', 'midnight', 'movi', 'gonna', 'get', 'til', 'need', 'come', 'home', 'need', 'getsleep', 'anyt', 'need', 'studd', 'ear', 'train']\n",
      "Tokenized sentence: ['short', 'but', 'cute', 'be', 'a', 'good', 'person']\n",
      "After stop words removal: ['short', 'cute', 'good', 'person']\n",
      "After stemming with porters algorithm: ['short', 'cute', 'good', 'person']\n",
      "Tokenized sentence: ['thesmszone', 'com', 'lets', 'you', 'send', 'free', 'anonymous', 'and', 'masked', 'messages', 'im', 'sending', 'this', 'message', 'from', 'there', 'do', 'you', 'see', 'the', 'potential', 'for', 'abuse']\n",
      "After stop words removal: ['thesmszone', 'com', 'lets', 'send', 'free', 'anonymous', 'masked', 'messages', 'im', 'sending', 'message', 'see', 'potential', 'abuse']\n",
      "send\n",
      "After stemming with porters algorithm: ['thesmszon', 'com', 'let', 'send', 'free', 'anonym', 'mas', 'messag', 'sen', 'messag', 'see', 'potenti', 'abus']\n",
      "Tokenized sentence: ['ok', 'then', 'no', 'need', 'to', 'tell', 'me', 'anything', 'i', 'am', 'going', 'to', 'sleep', 'good', 'night']\n",
      "After stop words removal: ['ok', 'need', 'tell', 'anything', 'going', 'sleep', 'good', 'night']\n",
      "anyth\n",
      "go\n",
      "After stemming with porters algorithm: ['need', 'tell', 'anyt', 'go', 'sleep', 'good', 'night']\n",
      "Tokenized sentence: ['can', 't', 'i', 'feel', 'nauseous', 'i', 'm', 'so', 'pissed', 'i', 'didn', 't', 'eat', 'any', 'sweets', 'all', 'week', 'cause', 'today', 'i', 'was', 'planning', 'to', 'pig', 'out', 'i', 'was', 'dieting', 'all', 'week', 'and', 'now', 'i', 'm', 'not', 'hungry']\n",
      "After stop words removal: ['feel', 'nauseous', 'pissed', 'eat', 'sweets', 'week', 'cause', 'today', 'planning', 'pig', 'dieting', 'week', 'hungry']\n",
      "plann\n",
      "diet\n",
      "After stemming with porters algorithm: ['feel', 'nauseou', 'piss', 'eat', 'sweet', 'week', 'caus', 'todai', 'plan', 'pig', 'diet', 'week', 'hungri']\n",
      "Tokenized sentence: ['oh', 'god', 'taken', 'the', 'teeth', 'is', 'it', 'paining']\n",
      "After stop words removal: ['oh', 'god', 'taken', 'teeth', 'paining']\n",
      "pain\n",
      "After stemming with porters algorithm: ['god', 'taken', 'teeth', 'pain']\n",
      "Tokenized sentence: ['lol', 'wtf', 'random', 'btw', 'is', 'that', 'your', 'lunch', 'break']\n",
      "After stop words removal: ['lol', 'wtf', 'random', 'btw', 'lunch', 'break']\n",
      "After stemming with porters algorithm: ['lol', 'wtf', 'random', 'btw', 'lunch', 'break']\n",
      "Tokenized sentence: ['good', 'evening', 'sir', 'hope', 'you', 'are', 'having', 'a', 'nice', 'day', 'i', 'wanted', 'to', 'bring', 'it', 'to', 'your', 'notice', 'that', 'i', 'have', 'been', 'late', 'in', 'paying', 'rent', 'for', 'the', 'past', 'few', 'months', 'and', 'have', 'had', 'to', 'pay', 'a', 'lt', 'gt', 'charge', 'i', 'felt', 'it', 'would', 'be', 'inconsiderate', 'of', 'me', 'to', 'nag', 'about', 'something', 'you', 'give', 'at', 'great', 'cost', 'to', 'yourself', 'and', 'that', 's', 'why', 'i', 'didnt', 'speak', 'up', 'i', 'however', 'am', 'in', 'a', 'recession', 'and', 'wont', 'be', 'able', 'to', 'pay', 'the', 'charge', 'this', 'month', 'hence', 'my', 'askin', 'well', 'ahead', 'of', 'month', 's', 'end', 'can', 'you', 'please', 'help', 'thank', 'you', 'for', 'everything']\n",
      "After stop words removal: ['good', 'evening', 'sir', 'hope', 'nice', 'day', 'wanted', 'bring', 'notice', 'late', 'paying', 'rent', 'past', 'months', 'pay', 'lt', 'gt', 'charge', 'felt', 'would', 'inconsiderate', 'nag', 'something', 'give', 'great', 'cost', 'didnt', 'speak', 'however', 'recession', 'wont', 'able', 'pay', 'charge', 'month', 'hence', 'askin', 'well', 'ahead', 'month', 'end', 'please', 'help', 'thank', 'everything']\n",
      "even\n",
      "pay\n",
      "someth\n",
      "everyth\n",
      "After stemming with porters algorithm: ['good', 'even', 'sir', 'hope', 'nice', 'dai', 'wan', 'bring', 'notic', 'late', 'pai', 'rent', 'past', 'month', 'pai', 'charg', 'felt', 'would', 'inconsider', 'nag', 'somet', 'give', 'great', 'cost', 'didnt', 'speak', 'howev', 'recess', 'wont', 'abl', 'pai', 'charg', 'month', 'henc', 'askin', 'well', 'ahead', 'month', 'end', 'pleas', 'help', 'thank', 'everyt']\n",
      "Tokenized sentence: ['velly', 'good', 'yes', 'please']\n",
      "After stop words removal: ['velly', 'good', 'yes', 'please']\n",
      "After stemming with porters algorithm: ['velli', 'good', 'ye', 'pleas']\n",
      "Tokenized sentence: ['this', 'message', 'is', 'from', 'a', 'great', 'doctor', 'in', 'india', 'do', 'not', 'drink', 'appy', 'fizz', 'it', 'contains', 'cancer', 'causing', 'age']\n",
      "After stop words removal: ['message', 'great', 'doctor', 'india', 'drink', 'appy', 'fizz', 'contains', 'cancer', 'causing', 'age']\n",
      "caus\n",
      "After stemming with porters algorithm: ['messag', 'great', 'doctor', 'india', 'drink', 'appi', 'fizz', 'contain', 'cancer', 'caus', 'ag']\n",
      "Tokenized sentence: ['beautiful', 'tomorrow', 'never', 'comes', 'when', 'it', 'comes', 'it', 's', 'already', 'today', 'in', 'the', 'hunt', 'of', 'beautiful', 'tomorrow', 'don', 't', 'waste', 'your', 'wonderful', 'today', 'goodmorning']\n",
      "After stop words removal: ['beautiful', 'tomorrow', 'never', 'comes', 'comes', 'already', 'today', 'hunt', 'beautiful', 'tomorrow', 'waste', 'wonderful', 'today', 'goodmorning']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['beauti', 'tomorrow', 'never', 'come', 'come', 'alreadi', 'todai', 'hunt', 'beauti', 'tomorrow', 'wast', 'wonder', 'todai', 'goodmor']\n",
      "Tokenized sentence: ['i', 'm', 'taking', 'derek', 'amp', 'taylor', 'to', 'walmart', 'if', 'i', 'm', 'not', 'back', 'by', 'the', 'time', 'you', 're', 'done', 'just', 'leave', 'the', 'mouse', 'on', 'my', 'desk', 'and', 'i', 'll', 'text', 'you', 'when', 'priscilla', 's', 'ready']\n",
      "After stop words removal: ['taking', 'derek', 'amp', 'taylor', 'walmart', 'back', 'time', 'done', 'leave', 'mouse', 'desk', 'text', 'priscilla', 'ready']\n",
      "tak\n",
      "After stemming with porters algorithm: ['take', 'derek', 'amp', 'taylor', 'walmart', 'back', 'time', 'done', 'leav', 'mous', 'desk', 'text', 'priscilla', 'readi']\n",
      "Tokenized sentence: ['how', 'about', 'getting', 'in', 'touch', 'with', 'folks', 'waiting', 'for', 'company', 'just', 'txt', 'back', 'your', 'name', 'and', 'age', 'to', 'opt', 'in', 'enjoy', 'the', 'community', 'p', 'sms']\n",
      "After stop words removal: ['getting', 'touch', 'folks', 'waiting', 'company', 'txt', 'back', 'name', 'age', 'opt', 'enjoy', 'community', 'p', 'sms']\n",
      "gett\n",
      "wait\n",
      "After stemming with porters algorithm: ['get', 'touch', 'folk', 'wait', 'compani', 'txt', 'back', 'name', 'ag', 'opt', 'enjoi', 'commun', 'sm']\n",
      "Tokenized sentence: ['sunshine', 'hols', 'to', 'claim', 'ur', 'med', 'holiday', 'send', 'a', 'stamped', 'self', 'address', 'envelope', 'to', 'drinks', 'on', 'us', 'uk', 'po', 'box', 'bray', 'wicklow', 'eire', 'quiz', 'starts', 'saturday', 'unsub', 'stop']\n",
      "After stop words removal: ['sunshine', 'hols', 'claim', 'ur', 'med', 'holiday', 'send', 'stamped', 'self', 'address', 'envelope', 'drinks', 'us', 'uk', 'po', 'box', 'bray', 'wicklow', 'eire', 'quiz', 'starts', 'saturday', 'unsub', 'stop']\n",
      "After stemming with porters algorithm: ['sunshin', 'hol', 'claim', 'med', 'holidai', 'send', 'stam', 'self', 'address', 'envelop', 'drink', 'box', 'brai', 'wicklow', 'eir', 'quiz', 'start', 'saturdai', 'unsub', 'stop']\n",
      "Tokenized sentence: ['nothing', 'much', 'chillin', 'at', 'home', 'any', 'super', 'bowl', 'plan']\n",
      "After stop words removal: ['nothing', 'much', 'chillin', 'home', 'super', 'bowl', 'plan']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not', 'much', 'chillin', 'home', 'super', 'bowl', 'plan']\n",
      "Tokenized sentence: ['arms', 'fine', 'how', 's', 'cardiff', 'and', 'uni']\n",
      "After stop words removal: ['arms', 'fine', 'cardiff', 'uni']\n",
      "After stemming with porters algorithm: ['arm', 'fine', 'cardiff', 'uni']\n",
      "Tokenized sentence: ['i', 're', 'met', 'alex', 'nichols', 'from', 'middle', 'school', 'and', 'it', 'turns', 'out', 'he', 's', 'dealing']\n",
      "After stop words removal: ['met', 'alex', 'nichols', 'middle', 'school', 'turns', 'dealing']\n",
      "deal\n",
      "After stemming with porters algorithm: ['met', 'alex', 'nichol', 'middl', 'school', 'turn', 'deal']\n",
      "Tokenized sentence: ['yep', 'then', 'is', 'fine', 'or', 'for', 'ice', 'age']\n",
      "After stop words removal: ['yep', 'fine', 'ice', 'age']\n",
      "After stemming with porters algorithm: ['yep', 'fine', 'ic', 'ag']\n",
      "Tokenized sentence: ['dunno', 'lei', 'shd', 'b', 'driving', 'lor', 'cos', 'i', 'go', 'sch', 'hr', 'oni']\n",
      "After stop words removal: ['dunno', 'lei', 'shd', 'b', 'driving', 'lor', 'cos', 'go', 'sch', 'hr', 'oni']\n",
      "driv\n",
      "After stemming with porters algorithm: ['dunno', 'lei', 'shd', 'drive', 'lor', 'co', 'sch', 'oni']\n",
      "Tokenized sentence: ['p', 'alfie', 'moon', 's', 'children', 'in', 'need', 'song', 'on', 'ur', 'mob', 'tell', 'ur', 'm', 's', 'txt', 'tone', 'charity', 'to', 'for', 'nokias', 'or', 'poly', 'charity', 'for', 'polys', 'zed', 'profit', 'charity']\n",
      "After stop words removal: ['p', 'alfie', 'moon', 'children', 'need', 'song', 'ur', 'mob', 'tell', 'ur', 'txt', 'tone', 'charity', 'nokias', 'poly', 'charity', 'polys', 'zed', 'profit', 'charity']\n",
      "After stemming with porters algorithm: ['alfi', 'moon', 'children', 'need', 'song', 'mob', 'tell', 'txt', 'tone', 'chariti', 'nokia', 'poli', 'chariti', 'poli', 'zed', 'profit', 'chariti']\n",
      "Tokenized sentence: ['wonders', 'in', 'my', 'world', 'th', 'you', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'and', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "After stop words removal: ['wonders', 'world', 'th', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "morn\n",
      "After stemming with porters algorithm: ['wonder', 'world', 'style', 'smile', 'person', 'natur', 'sm', 'love', 'friendship', 'good', 'mor', 'dear']\n",
      "Tokenized sentence: ['yes', 'i', 'posted', 'a', 'couple', 'of', 'pics', 'on', 'fb', 'there', 's', 'still', 'snow', 'outside', 'too', 'i', 'm', 'just', 'waking', 'up']\n",
      "After stop words removal: ['yes', 'posted', 'couple', 'pics', 'fb', 'still', 'snow', 'outside', 'waking']\n",
      "wak\n",
      "After stemming with porters algorithm: ['ye', 'pos', 'coupl', 'pic', 'still', 'snow', 'outsid', 'wake']\n",
      "Tokenized sentence: ['u', 've', 'bin', 'awarded', 'to', 'play', 'instant', 'cash', 'call', 'to', 'claim', 'every', 'th', 'player', 'wins', 'min', 'optout']\n",
      "After stop words removal: ['u', 'bin', 'awarded', 'play', 'instant', 'cash', 'call', 'claim', 'every', 'th', 'player', 'wins', 'min', 'optout']\n",
      "After stemming with porters algorithm: ['bin', 'awar', 'plai', 'instant', 'cash', 'call', 'claim', 'everi', 'player', 'win', 'min', 'optout']\n",
      "Tokenized sentence: ['hiya', 'comin', 'bristol', 'st', 'week', 'in', 'april', 'les', 'got', 'off', 'rudi', 'on', 'new', 'yrs', 'eve', 'but', 'i', 'was', 'snoring', 'they', 'were', 'drunk', 'u', 'bak', 'at', 'college', 'yet', 'my', 'work', 'sends', 'ink', 'bath']\n",
      "After stop words removal: ['hiya', 'comin', 'bristol', 'st', 'week', 'april', 'les', 'got', 'rudi', 'new', 'yrs', 'eve', 'snoring', 'drunk', 'u', 'bak', 'college', 'yet', 'work', 'sends', 'ink', 'bath']\n",
      "snor\n",
      "After stemming with porters algorithm: ['hiya', 'comin', 'bristol', 'week', 'april', 'le', 'got', 'rudi', 'new', 'yr', 'ev', 'snore', 'drunk', 'bak', 'colleg', 'yet', 'work', 'send', 'ink', 'bath']\n",
      "Tokenized sentence: ['theoretically', 'yeah', 'he', 'could', 'be', 'able', 'to', 'come']\n",
      "After stop words removal: ['theoretically', 'yeah', 'could', 'able', 'come']\n",
      "After stemming with porters algorithm: ['theoret', 'yeah', 'could', 'abl', 'come']\n",
      "Tokenized sentence: ['dont', 'search', 'love', 'let', 'love', 'find', 'u', 'thats', 'why', 'its', 'called', 'falling', 'in', 'love', 'bcoz', 'u', 'dont', 'force', 'yourself', 'u', 'just', 'fall', 'and', 'u', 'know', 'there', 'is', 'smeone', 'to', 'hold', 'u', 'bslvyl']\n",
      "After stop words removal: ['dont', 'search', 'love', 'let', 'love', 'find', 'u', 'thats', 'called', 'falling', 'love', 'bcoz', 'u', 'dont', 'force', 'u', 'fall', 'u', 'know', 'smeone', 'hold', 'u', 'bslvyl']\n",
      "fall\n",
      "After stemming with porters algorithm: ['dont', 'search', 'love', 'let', 'love', 'find', 'that', 'call', 'fall', 'love', 'bcoz', 'dont', 'forc', 'fall', 'know', 'smeon', 'hold', 'bslvyl']\n",
      "Tokenized sentence: ['ur', 'balance', 'is', 'now', 'ur', 'next', 'question', 'is', 'who', 'sang', 'uptown', 'girl', 'in', 'the', 's', 'answer', 'txt', 'ur', 'answer', 'to', 'good', 'luck']\n",
      "After stop words removal: ['ur', 'balance', 'ur', 'next', 'question', 'sang', 'uptown', 'girl', 'answer', 'txt', 'ur', 'answer', 'good', 'luck']\n",
      "After stemming with porters algorithm: ['balanc', 'next', 'quest', 'sang', 'uptown', 'girl', 'answer', 'txt', 'answer', 'good', 'luck']\n",
      "Tokenized sentence: ['already', 'am', 'squatting', 'is', 'the', 'new', 'way', 'of', 'walking']\n",
      "After stop words removal: ['already', 'squatting', 'new', 'way', 'walking']\n",
      "squatt\n",
      "walk\n",
      "After stemming with porters algorithm: ['alreadi', 'squat', 'new', 'wai', 'wal']\n",
      "Tokenized sentence: ['die', 'i', 'accidentally', 'deleted', 'e', 'msg', 'i', 'suppose', 'put', 'in', 'e', 'sim', 'archive', 'haiz', 'i', 'so', 'sad']\n",
      "After stop words removal: ['die', 'accidentally', 'deleted', 'e', 'msg', 'suppose', 'put', 'e', 'sim', 'archive', 'haiz', 'sad']\n",
      "After stemming with porters algorithm: ['die', 'accid', 'delet', 'msg', 'suppos', 'put', 'sim', 'archiv', 'haiz', 'sad']\n",
      "Tokenized sentence: ['forwarded', 'from', 'free', 'entry', 'into', 'our', 'weekly', 'comp', 'just', 'send', 'the', 'word', 'enter', 'to', 'now', 't', 'c', 'www', 'textcomp', 'com']\n",
      "After stop words removal: ['forwarded', 'free', 'entry', 'weekly', 'comp', 'send', 'word', 'enter', 'c', 'www', 'textcomp', 'com']\n",
      "After stemming with porters algorithm: ['forwar', 'free', 'entri', 'weekli', 'comp', 'send', 'word', 'enter', 'www', 'textcomp', 'com']\n",
      "Tokenized sentence: ['someone', 'u', 'know', 'has', 'asked', 'our', 'dating', 'service', 'contact', 'you', 'cant', 'guess', 'who', 'call', 'now', 'all', 'will', 'be', 'revealed', 'pobox', 'ls', 'hb', 'p']\n",
      "After stop words removal: ['someone', 'u', 'know', 'asked', 'dating', 'service', 'contact', 'cant', 'guess', 'call', 'revealed', 'pobox', 'ls', 'hb', 'p']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someon', 'know', 'as', 'date', 'servic', 'contact', 'cant', 'guess', 'call', 'reveal', 'pobox']\n",
      "Tokenized sentence: ['hiya', 'have', 'u', 'been', 'paying', 'money', 'into', 'my', 'account', 'if', 'so', 'thanks', 'got', 'a', 'pleasant', 'surprise', 'when', 'i', 'checked', 'my', 'balance', 'u', 'c', 'i', 'don', 't', 'get', 'statements', 'that', 'acc']\n",
      "After stop words removal: ['hiya', 'u', 'paying', 'money', 'account', 'thanks', 'got', 'pleasant', 'surprise', 'checked', 'balance', 'u', 'c', 'get', 'statements', 'acc']\n",
      "pay\n",
      "After stemming with porters algorithm: ['hiya', 'pai', 'monei', 'account', 'thank', 'got', 'pleasant', 'surpris', 'chec', 'balanc', 'get', 'statem', 'acc']\n",
      "Tokenized sentence: ['hi', 'this', 'is', 'amy', 'we', 'will', 'be', 'sending', 'you', 'a', 'free', 'phone', 'number', 'in', 'a', 'couple', 'of', 'days', 'which', 'will', 'give', 'you', 'an', 'access', 'to', 'all', 'the', 'adult', 'parties']\n",
      "After stop words removal: ['hi', 'amy', 'sending', 'free', 'phone', 'number', 'couple', 'days', 'give', 'access', 'adult', 'parties']\n",
      "send\n",
      "After stemming with porters algorithm: ['ami', 'sen', 'free', 'phone', 'number', 'coupl', 'dai', 'give', 'access', 'adult', 'parti']\n",
      "Tokenized sentence: ['height', 'of', 'confidence', 'all', 'the', 'aeronautics', 'professors', 'wer', 'calld', 'amp', 'they', 'wer', 'askd', 'sit', 'in', 'an', 'aeroplane', 'aftr', 'they', 'sat', 'they', 'wer', 'told', 'dat', 'the', 'plane', 'ws', 'made', 'by', 'their', 'students', 'dey', 'all', 'hurried', 'out', 'of', 'd', 'plane', 'bt', 'only', 'didnt', 'move', 'he', 'said', 'if', 'it', 'is', 'made', 'by', 'my', 'students']\n",
      "After stop words removal: ['height', 'confidence', 'aeronautics', 'professors', 'wer', 'calld', 'amp', 'wer', 'askd', 'sit', 'aeroplane', 'aftr', 'sat', 'wer', 'told', 'dat', 'plane', 'ws', 'made', 'students', 'dey', 'hurried', 'plane', 'bt', 'didnt', 'move', 'said', 'made', 'students']\n",
      "After stemming with porters algorithm: ['height', 'confid', 'aeronaut', 'professor', 'wer', 'calld', 'amp', 'wer', 'askd', 'sit', 'aeroplan', 'aftr', 'sat', 'wer', 'told', 'dat', 'plane', 'made', 'student', 'dei', 'hurri', 'plane', 'didnt', 'move', 'said', 'made', 'student']\n",
      "Tokenized sentence: ['the', 'bus', 'leaves', 'at', 'lt', 'gt']\n",
      "After stop words removal: ['bus', 'leaves', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['bu', 'leav']\n",
      "Tokenized sentence: ['s', 'if', 'we', 'have', 'one', 'good', 'partnership', 'going', 'we', 'will', 'take', 'lead']\n",
      "After stop words removal: ['one', 'good', 'partnership', 'going', 'take', 'lead']\n",
      "go\n",
      "After stemming with porters algorithm: ['on', 'good', 'partnership', 'go', 'take', 'lead']\n",
      "Tokenized sentence: ['sms', 'auction', 'a', 'brand', 'new', 'nokia', 'is', 'up', 'auction', 'today', 'auction', 'is', 'free', 'join', 'take', 'part', 'txt', 'nokia', 'to', 'now', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stop words removal: ['sms', 'auction', 'brand', 'new', 'nokia', 'auction', 'today', 'auction', 'free', 'join', 'take', 'part', 'txt', 'nokia', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stemming with porters algorithm: ['sm', 'auct', 'brand', 'new', 'nokia', 'auct', 'todai', 'auct', 'free', 'join', 'take', 'part', 'txt', 'nokia', 'suit', 'land', 'row']\n",
      "Tokenized sentence: ['tell', 'them', 'no', 'need', 'to', 'investigate', 'about', 'me', 'anywhere']\n",
      "After stop words removal: ['tell', 'need', 'investigate', 'anywhere']\n",
      "After stemming with porters algorithm: ['tell', 'need', 'investig', 'anywher']\n",
      "Tokenized sentence: ['greetings', 'me', 'consider', 'yourself', 'excused']\n",
      "After stop words removal: ['greetings', 'consider', 'excused']\n",
      "greet\n",
      "After stemming with porters algorithm: ['greet', 'consid', 'excus']\n",
      "Tokenized sentence: ['ps', 'u', 'no', 'ur', 'a', 'grown', 'up', 'now', 'right']\n",
      "After stop words removal: ['ps', 'u', 'ur', 'grown', 'right']\n",
      "After stemming with porters algorithm: ['grown', 'right']\n",
      "Tokenized sentence: ['thought', 'i', 'didn', 't', 'see', 'you']\n",
      "After stop words removal: ['thought', 'see']\n",
      "After stemming with porters algorithm: ['thought', 'see']\n",
      "Tokenized sentence: ['i', 'wnt', 'to', 'buy', 'a', 'bmw', 'car', 'urgently', 'its', 'vry', 'urgent', 'but', 'hv', 'a', 'shortage', 'of', 'lt', 'gt', 'lacs', 'there', 'is', 'no', 'source', 'to', 'arng', 'dis', 'amt', 'lt', 'gt', 'lacs', 'thats', 'my', 'prob']\n",
      "After stop words removal: ['wnt', 'buy', 'bmw', 'car', 'urgently', 'vry', 'urgent', 'hv', 'shortage', 'lt', 'gt', 'lacs', 'source', 'arng', 'dis', 'amt', 'lt', 'gt', 'lacs', 'thats', 'prob']\n",
      "After stemming with porters algorithm: ['wnt', 'bui', 'bmw', 'car', 'urgent', 'vry', 'urgent', 'shortag', 'lac', 'sourc', 'arng', 'di', 'amt', 'lac', 'that', 'prob']\n",
      "Tokenized sentence: ['dont', 'pack', 'what', 'you', 'can', 'buy', 'at', 'any', 'store', 'like', 'cereals', 'if', 'you', 'must', 'pack', 'food', 'pack', 'gari', 'or', 'something', 'ja', 'that', 'you', 'will', 'miss']\n",
      "After stop words removal: ['dont', 'pack', 'buy', 'store', 'like', 'cereals', 'must', 'pack', 'food', 'pack', 'gari', 'something', 'ja', 'miss']\n",
      "someth\n",
      "After stemming with porters algorithm: ['dont', 'pack', 'bui', 'store', 'like', 'cereal', 'must', 'pack', 'food', 'pack', 'gari', 'somet', 'miss']\n",
      "Tokenized sentence: ['collect', 'your', 'valentine', 's', 'weekend', 'to', 'paris', 'inc', 'flight', 'hotel', 'prize', 'guaranteed', 'text', 'paris', 'to', 'no', 'www', 'rtf', 'sphosting', 'com']\n",
      "After stop words removal: ['collect', 'valentine', 'weekend', 'paris', 'inc', 'flight', 'hotel', 'prize', 'guaranteed', 'text', 'paris', 'www', 'rtf', 'sphosting', 'com']\n",
      "sphost\n",
      "After stemming with porters algorithm: ['collect', 'valentin', 'weekend', 'pari', 'inc', 'flight', 'hotel', 'priz', 'guaranteed', 'text', 'pari', 'www', 'rtf', 'sphos', 'com']\n",
      "Tokenized sentence: ['you', 'please', 'give', 'us', 'connection', 'today', 'itself', 'before', 'lt', 'decimal', 'gt', 'or', 'refund', 'the', 'bill']\n",
      "After stop words removal: ['please', 'give', 'us', 'connection', 'today', 'lt', 'decimal', 'gt', 'refund', 'bill']\n",
      "After stemming with porters algorithm: ['pleas', 'give', 'connect', 'todai', 'decim', 'refund', 'bill']\n",
      "Tokenized sentence: ['sounds', 'like', 'there', 'could', 'be', 'a', 'lot', 'of', 'time', 'spent', 'in', 'that', 'chastity', 'device', 'boy', 'grins', 'or', 'take', 'your', 'beatings', 'like', 'a', 'good', 'dog', 'going', 'to', 'lounge', 'in', 'a', 'nice', 'long', 'bath', 'now']\n",
      "After stop words removal: ['sounds', 'like', 'could', 'lot', 'time', 'spent', 'chastity', 'device', 'boy', 'grins', 'take', 'beatings', 'like', 'good', 'dog', 'going', 'lounge', 'nice', 'long', 'bath']\n",
      "beat\n",
      "beate\n",
      "go\n",
      "After stemming with porters algorithm: ['sound', 'like', 'could', 'lot', 'time', 'spent', 'chastiti', 'devic', 'boi', 'grin', 'take', 'beat', 'like', 'good', 'dog', 'go', 'loung', 'nice', 'long', 'bath']\n",
      "Tokenized sentence: ['did', 'you', 'hear', 'about', 'the', 'new', 'divorce', 'barbie', 'it', 'comes', 'with', 'all', 'of', 'ken', 's', 'stuff']\n",
      "After stop words removal: ['hear', 'new', 'divorce', 'barbie', 'comes', 'ken', 'stuff']\n",
      "After stemming with porters algorithm: ['hear', 'new', 'divorc', 'barbi', 'come', 'ken', 'stuff']\n",
      "Tokenized sentence: ['min', 'later', 'k']\n",
      "After stop words removal: ['min', 'later', 'k']\n",
      "After stemming with porters algorithm: ['min', 'later']\n",
      "Tokenized sentence: ['mths', 'half', 'price', 'orange', 'line', 'rental', 'latest', 'camera', 'phones', 'free', 'had', 'your', 'phone', 'mths', 'call', 'mobilesdirect', 'free', 'on', 'to', 'update', 'now', 'or', 'stoptxt', 't', 'cs']\n",
      "After stop words removal: ['mths', 'half', 'price', 'orange', 'line', 'rental', 'latest', 'camera', 'phones', 'free', 'phone', 'mths', 'call', 'mobilesdirect', 'free', 'update', 'stoptxt', 'cs']\n",
      "After stemming with porters algorithm: ['mth', 'half', 'price', 'orang', 'line', 'rental', 'latest', 'camera', 'phone', 'free', 'phone', 'mth', 'call', 'mobilesdirect', 'free', 'updat', 'stoptxt']\n",
      "Tokenized sentence: ['hi', 'neva', 'worry', 'bout', 'da', 'truth', 'coz', 'the', 'truth', 'will', 'lead', 'me', 'ur', 'heart', 'it', 's', 'the', 'least', 'a', 'unique', 'person', 'like', 'u', 'deserve', 'sleep', 'tight', 'or', 'morning']\n",
      "After stop words removal: ['hi', 'neva', 'worry', 'bout', 'da', 'truth', 'coz', 'truth', 'lead', 'ur', 'heart', 'least', 'unique', 'person', 'like', 'u', 'deserve', 'sleep', 'tight', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['neva', 'worri', 'bout', 'truth', 'coz', 'truth', 'lead', 'heart', 'least', 'uniqu', 'person', 'like', 'deserv', 'sleep', 'tight', 'mor']\n",
      "Tokenized sentence: ['yeah', 'if', 'we', 'do', 'have', 'to', 'get', 'a', 'random', 'dude', 'we', 'need', 'to', 'change', 'our', 'info', 'sheets', 'to', 'party', 'lt', 'gt', 'never', 'study', 'just', 'to', 'be', 'safe']\n",
      "After stop words removal: ['yeah', 'get', 'random', 'dude', 'need', 'change', 'info', 'sheets', 'party', 'lt', 'gt', 'never', 'study', 'safe']\n",
      "After stemming with porters algorithm: ['yeah', 'get', 'random', 'dude', 'need', 'chang', 'info', 'sheet', 'parti', 'never', 'studi', 'safe']\n",
      "Tokenized sentence: ['hope', 'things', 'went', 'well', 'at', 'doctors', 'reminds', 'me', 'i', 'still', 'need', 'go', 'did', 'u', 'c', 'd', 'little', 'thing', 'i', 'left', 'in', 'the', 'lounge']\n",
      "After stop words removal: ['hope', 'things', 'went', 'well', 'doctors', 'reminds', 'still', 'need', 'go', 'u', 'c', 'little', 'thing', 'left', 'lounge']\n",
      "After stemming with porters algorithm: ['hope', 'thing', 'went', 'well', 'doctor', 'remind', 'still', 'need', 'littl', 'thing', 'left', 'loung']\n",
      "Tokenized sentence: ['ok', 'i', 'am', 'on', 'the', 'way', 'to', 'home', 'hi', 'hi']\n",
      "After stop words removal: ['ok', 'way', 'home', 'hi', 'hi']\n",
      "After stemming with porters algorithm: ['wai', 'home']\n",
      "Tokenized sentence: ['i', 'have', 'no', 'idea', 'where', 'you', 'are']\n",
      "After stop words removal: ['idea']\n",
      "After stemming with porters algorithm: ['idea']\n",
      "Tokenized sentence: ['congratulations', 'ur', 'awarded', 'of', 'cd', 'vouchers', 'or', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music', 'to', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stop words removal: ['congratulations', 'ur', 'awarded', 'cd', 'vouchers', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stemming with porters algorithm: ['congratul', 'awar', 'voucher', 'gift', 'guaranteed', 'free', 'entri', 'wkly', 'draw', 'txt', 'music', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'ag']\n",
      "Tokenized sentence: ['by', 'the', 'way', 'make', 'sure', 'u', 'get', 'train', 'to', 'worc', 'foregate', 'street', 'not', 'shrub', 'hill', 'have', 'fun', 'night', 'x']\n",
      "After stop words removal: ['way', 'make', 'sure', 'u', 'get', 'train', 'worc', 'foregate', 'street', 'shrub', 'hill', 'fun', 'night', 'x']\n",
      "After stemming with porters algorithm: ['wai', 'make', 'sure', 'get', 'train', 'worc', 'foreg', 'street', 'shrub', 'hill', 'fun', 'night']\n",
      "Tokenized sentence: ['so', 'no', 'messages', 'had', 'food']\n",
      "After stop words removal: ['messages', 'food']\n",
      "After stemming with porters algorithm: ['messag', 'food']\n",
      "Tokenized sentence: ['k', 'k', 'i', 'm', 'also', 'fine', 'when', 'will', 'you', 'complete', 'the', 'course']\n",
      "After stop words removal: ['k', 'k', 'also', 'fine', 'complete', 'course']\n",
      "After stemming with porters algorithm: ['also', 'fine', 'complet', 'cours']\n",
      "Tokenized sentence: ['i', 'love', 'to', 'cuddle', 'i', 'want', 'to', 'hold', 'you', 'in', 'my', 'strong', 'arms', 'right', 'now']\n",
      "After stop words removal: ['love', 'cuddle', 'want', 'hold', 'strong', 'arms', 'right']\n",
      "After stemming with porters algorithm: ['love', 'cuddl', 'want', 'hold', 'strong', 'arm', 'right']\n",
      "Tokenized sentence: ['ok', 'now', 'i', 'am', 'in', 'bus', 'if', 'i', 'come', 'soon', 'i', 'will', 'come', 'otherwise', 'tomorrow']\n",
      "After stop words removal: ['ok', 'bus', 'come', 'soon', 'come', 'otherwise', 'tomorrow']\n",
      "After stemming with porters algorithm: ['bu', 'come', 'soon', 'come', 'otherwis', 'tomorrow']\n",
      "Tokenized sentence: ['hello', 'my', 'little', 'party', 'animal', 'i', 'just', 'thought', 'i', 'd', 'buzz', 'you', 'as', 'you', 'were', 'with', 'your', 'friends', 'grins', 'reminding', 'you', 'were', 'loved', 'and', 'send', 'a', 'naughty', 'adoring', 'kiss']\n",
      "After stop words removal: ['hello', 'little', 'party', 'animal', 'thought', 'buzz', 'friends', 'grins', 'reminding', 'loved', 'send', 'naughty', 'adoring', 'kiss']\n",
      "remind\n",
      "ador\n",
      "After stemming with porters algorithm: ['hello', 'littl', 'parti', 'anim', 'thought', 'buzz', 'friend', 'grin', 'remin', 'love', 'send', 'naughti', 'ador', 'kiss']\n",
      "Tokenized sentence: ['i', 'll', 'get', 'there', 'tomorrow', 'and', 'send', 'it', 'to', 'you']\n",
      "After stop words removal: ['get', 'tomorrow', 'send']\n",
      "After stemming with porters algorithm: ['get', 'tomorrow', 'send']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['yes', 'but', 'they', 'said', 'its', 'it']\n",
      "After stop words removal: ['yes', 'said']\n",
      "After stemming with porters algorithm: ['ye', 'said']\n",
      "Tokenized sentence: ['i', 'll', 'have', 'a', 'look', 'at', 'the', 'frying', 'pan', 'in', 'case', 'it', 's', 'cheap', 'or', 'a', 'book', 'perhaps', 'no', 'that', 's', 'silly', 'a', 'frying', 'pan', 'isn', 't', 'likely', 'to', 'be', 'a', 'book']\n",
      "After stop words removal: ['look', 'frying', 'pan', 'case', 'cheap', 'book', 'perhaps', 'silly', 'frying', 'pan', 'likely', 'book']\n",
      "After stemming with porters algorithm: ['look', 'frying', 'pan', 'case', 'cheap', 'book', 'perhap', 'silli', 'frying', 'pan', 'like', 'book']\n",
      "Tokenized sentence: ['hey', 'why', 'dont', 'we', 'just', 'go', 'watch', 'x', 'men', 'and', 'have', 'lunch', 'haha']\n",
      "After stop words removal: ['hey', 'dont', 'go', 'watch', 'x', 'men', 'lunch', 'haha']\n",
      "After stemming with porters algorithm: ['hei', 'dont', 'watch', 'men', 'lunch', 'haha']\n",
      "Tokenized sentence: ['yeah', 'i', 'think', 'my', 'usual', 'guy', 's', 'still', 'passed', 'out', 'from', 'last', 'night', 'if', 'you', 'get', 'ahold', 'of', 'anybody', 'let', 'me', 'know', 'and', 'i', 'll', 'throw', 'down']\n",
      "After stop words removal: ['yeah', 'think', 'usual', 'guy', 'still', 'passed', 'last', 'night', 'get', 'ahold', 'anybody', 'let', 'know', 'throw']\n",
      "After stemming with porters algorithm: ['yeah', 'think', 'usual', 'gui', 'still', 'pass', 'last', 'night', 'get', 'ahold', 'anybodi', 'let', 'know', 'throw']\n",
      "Tokenized sentence: ['reply', 'to', 'win', 'weekly', 'what', 'professional', 'sport', 'does', 'tiger', 'woods', 'play', 'send', 'stop', 'to', 'to', 'end', 'service']\n",
      "After stop words removal: ['reply', 'win', 'weekly', 'professional', 'sport', 'tiger', 'woods', 'play', 'send', 'stop', 'end', 'service']\n",
      "After stemming with porters algorithm: ['repli', 'win', 'weekli', 'profess', 'sport', 'tiger', 'wood', 'plai', 'send', 'stop', 'end', 'servic']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['hey', 'there', 'babe', 'how', 'u', 'doin', 'wot', 'u', 'up', 'nite', 'love', 'annie', 'x']\n",
      "After stop words removal: ['hey', 'babe', 'u', 'doin', 'wot', 'u', 'nite', 'love', 'annie', 'x']\n",
      "After stemming with porters algorithm: ['hei', 'babe', 'doin', 'wot', 'nite', 'love', 'anni']\n",
      "Tokenized sentence: ['i', 'might', 'come', 'to', 'kerala', 'for', 'days', 'so', 'you', 'can', 'be', 'prepared', 'to', 'take', 'a', 'leave', 'once', 'i', 'finalise', 'dont', 'plan', 'any', 'travel', 'during', 'my', 'visit', 'need', 'to', 'finish', 'urgent', 'works']\n",
      "After stop words removal: ['might', 'come', 'kerala', 'days', 'prepared', 'take', 'leave', 'finalise', 'dont', 'plan', 'travel', 'visit', 'need', 'finish', 'urgent', 'works']\n",
      "After stemming with porters algorithm: ['might', 'come', 'kerala', 'dai', 'prepar', 'take', 'leav', 'finalis', 'dont', 'plan', 'travel', 'visit', 'need', 'finish', 'urgent', 'work']\n",
      "Tokenized sentence: ['hey', 'babe', 'sorry', 'i', 'didn', 't', 'get', 'sooner', 'gary', 'can', 'come', 'and', 'fix', 'it', 'cause', 'he', 'thinks', 'he', 'knows', 'what', 'it', 'is', 'but', 'he', 'doesn', 't', 'go', 'as', 'far', 'a', 'ptbo', 'and', 'he', 'says', 'it', 'will', 'cost', 'lt', 'gt', 'bucks', 'i', 'don', 't', 'know', 'if', 'it', 'might', 'be', 'cheaper', 'to', 'find', 'someone', 'there', 'we', 'don', 't', 'have', 'any', 'second', 'hand', 'machines', 'at', 'all', 'right', 'now', 'let', 'me', 'know', 'what', 'you', 'want', 'to', 'do', 'babe']\n",
      "After stop words removal: ['hey', 'babe', 'sorry', 'get', 'sooner', 'gary', 'come', 'fix', 'cause', 'thinks', 'knows', 'go', 'far', 'ptbo', 'says', 'cost', 'lt', 'gt', 'bucks', 'know', 'might', 'cheaper', 'find', 'someone', 'second', 'hand', 'machines', 'right', 'let', 'know', 'want', 'babe']\n",
      "After stemming with porters algorithm: ['hei', 'babe', 'sorri', 'get', 'sooner', 'gari', 'come', 'fix', 'caus', 'think', 'know', 'far', 'ptbo', 'sai', 'cost', 'buck', 'know', 'might', 'cheaper', 'find', 'someon', 'second', 'hand', 'machin', 'right', 'let', 'know', 'want', 'babe']\n",
      "Tokenized sentence: ['theyre', 'doing', 'it', 'to', 'lots', 'of', 'places', 'only', 'hospitals', 'and', 'medical', 'places', 'are', 'safe']\n",
      "After stop words removal: ['theyre', 'lots', 'places', 'hospitals', 'medical', 'places', 'safe']\n",
      "After stemming with porters algorithm: ['theyr', 'lot', 'place', 'hospit', 'medic', 'place', 'safe']\n",
      "Tokenized sentence: ['one', 'of', 'the', 'joys', 'in', 'lifeis', 'waking', 'up', 'each', 'daywith', 'thoughts', 'that', 'somewheresomeone', 'cares', 'enough', 'tosend', 'a', 'warm', 'morning', 'greeting']\n",
      "After stop words removal: ['one', 'joys', 'lifeis', 'waking', 'daywith', 'thoughts', 'somewheresomeone', 'cares', 'enough', 'tosend', 'warm', 'morning', 'greeting']\n",
      "wak\n",
      "morn\n",
      "greet\n",
      "After stemming with porters algorithm: ['on', 'joi', 'lifei', 'wake', 'daywith', 'thought', 'somewheresomeon', 'care', 'enough', 'tosend', 'warm', 'mor', 'greet']\n",
      "Tokenized sentence: ['k', 'k', 'good', 'study', 'well']\n",
      "After stop words removal: ['k', 'k', 'good', 'study', 'well']\n",
      "After stemming with porters algorithm: ['good', 'studi', 'well']\n",
      "Tokenized sentence: ['life', 'is', 'nothing', 'wen', 'v', 'get', 'everything', 'but', 'life', 'is', 'everything', 'wen', 'v', 'miss', 'something', 'real', 'value', 'of', 'people', 'wil', 'be', 'realized', 'only', 'in', 'their', 'absence', 'gud', 'mrng']\n",
      "After stop words removal: ['life', 'nothing', 'wen', 'v', 'get', 'everything', 'life', 'everything', 'wen', 'v', 'miss', 'something', 'real', 'value', 'people', 'wil', 'realized', 'absence', 'gud', 'mrng']\n",
      "noth\n",
      "everyth\n",
      "everyth\n",
      "someth\n",
      "realize\n",
      "After stemming with porters algorithm: ['life', 'not', 'wen', 'get', 'everyt', 'life', 'everyt', 'wen', 'miss', 'somet', 'real', 'valu', 'peopl', 'wil', 'realiz', 'absenc', 'gud', 'mrng']\n",
      "Tokenized sentence: ['sorry', 'im', 'stil', 'fucked', 'after', 'last', 'nite', 'went', 'tobed', 'at', 'got', 'up', 'work', 'at']\n",
      "After stop words removal: ['sorry', 'im', 'stil', 'fucked', 'last', 'nite', 'went', 'tobed', 'got', 'work']\n",
      "After stemming with porters algorithm: ['sorri', 'stil', 'fuc', 'last', 'nite', 'went', 'tobe', 'got', 'work']\n",
      "Tokenized sentence: ['i', 'm', 'so', 'in', 'love', 'with', 'you', 'i', 'm', 'excited', 'each', 'day', 'i', 'spend', 'with', 'you', 'you', 'make', 'me', 'so', 'happy']\n",
      "After stop words removal: ['love', 'excited', 'day', 'spend', 'make', 'happy']\n",
      "After stemming with porters algorithm: ['love', 'excit', 'dai', 'spend', 'make', 'happi']\n",
      "Tokenized sentence: ['okie']\n",
      "After stop words removal: ['okie']\n",
      "After stemming with porters algorithm: ['oki']\n",
      "Tokenized sentence: ['men', 'always', 'needs', 'a', 'beautiful', 'intelligent', 'caring', 'loving', 'adjustable', 'cooperative', 'wife', 'but', 'the', 'law', 'allows', 'only', 'one', 'wife']\n",
      "After stop words removal: ['men', 'always', 'needs', 'beautiful', 'intelligent', 'caring', 'loving', 'adjustable', 'cooperative', 'wife', 'law', 'allows', 'one', 'wife']\n",
      "car\n",
      "lov\n",
      "After stemming with porters algorithm: ['men', 'alwai', 'need', 'beauti', 'intellig', 'care', 'love', 'adjust', 'cooper', 'wife', 'law', 'allow', 'on', 'wife']\n",
      "Tokenized sentence: ['moon', 'has', 'come', 'to', 'color', 'your', 'dreams', 'stars', 'to', 'make', 'them', 'musical', 'and', 'my', 'sms', 'to', 'give', 'you', 'warm', 'and', 'peaceful', 'sleep', 'good', 'night']\n",
      "After stop words removal: ['moon', 'come', 'color', 'dreams', 'stars', 'make', 'musical', 'sms', 'give', 'warm', 'peaceful', 'sleep', 'good', 'night']\n",
      "After stemming with porters algorithm: ['moon', 'come', 'color', 'dream', 'star', 'make', 'music', 'sm', 'give', 'warm', 'peac', 'sleep', 'good', 'night']\n",
      "Tokenized sentence: ['that', 's', 'necessarily', 'respectful']\n",
      "After stop words removal: ['necessarily', 'respectful']\n",
      "After stemming with porters algorithm: ['necessarili', 'respect']\n",
      "Tokenized sentence: ['first', 'has', 'she', 'gained', 'more', 'than', 'lt', 'gt', 'kg', 'since', 'she', 'took', 'in', 'second', 'has', 'she', 'done', 'the', 'blood', 'sugar', 'tests', 'if', 'she', 'has', 'and', 'its', 'ok', 'and', 'her', 'blood', 'pressure', 'is', 'within', 'normal', 'limits', 'then', 'no', 'worries']\n",
      "After stop words removal: ['first', 'gained', 'lt', 'gt', 'kg', 'since', 'took', 'second', 'done', 'blood', 'sugar', 'tests', 'ok', 'blood', 'pressure', 'within', 'normal', 'limits', 'worries']\n",
      "After stemming with porters algorithm: ['first', 'gain', 'sinc', 'took', 'second', 'done', 'blood', 'sugar', 'test', 'blood', 'pressur', 'within', 'normal', 'limit', 'worri']\n",
      "Tokenized sentence: ['k', 'then', 'come', 'wenever', 'u', 'lik', 'to', 'come', 'and', 'also', 'tel', 'vikky', 'to', 'come', 'by', 'getting', 'free', 'time']\n",
      "After stop words removal: ['k', 'come', 'wenever', 'u', 'lik', 'come', 'also', 'tel', 'vikky', 'come', 'getting', 'free', 'time']\n",
      "gett\n",
      "After stemming with porters algorithm: ['come', 'wenev', 'lik', 'come', 'also', 'tel', 'vikki', 'come', 'get', 'free', 'time']\n",
      "Tokenized sentence: ['my', 'drive', 'can', 'only', 'be', 'read', 'i', 'need', 'to', 'write']\n",
      "After stop words removal: ['drive', 'read', 'need', 'write']\n",
      "After stemming with porters algorithm: ['drive', 'read', 'need', 'write']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'ringtone', 'order', 'reference', 't', 'you', 'will', 'be', 'charged', 'gbp', 'per', 'week', 'you', 'can', 'unsubscribe', 'at', 'anytime', 'by', 'calling', 'customer', 'services', 'on']\n",
      "After stop words removal: ['thanks', 'ringtone', 'order', 'reference', 'charged', 'gbp', 'per', 'week', 'unsubscribe', 'anytime', 'calling', 'customer', 'services']\n",
      "call\n",
      "After stemming with porters algorithm: ['thank', 'rington', 'order', 'refer', 'char', 'gbp', 'per', 'week', 'unsubscrib', 'anytim', 'call', 'custom', 'servic']\n",
      "Tokenized sentence: ['and', 'is', 'there', 'a', 'way', 'you', 'can', 'send', 'shade', 's', 'stuff', 'to', 'her', 'and', 'she', 'has', 'been', 'wonderful', 'too']\n",
      "After stop words removal: ['way', 'send', 'shade', 'stuff', 'wonderful']\n",
      "After stemming with porters algorithm: ['wai', 'send', 'shade', 'stuff', 'wonder']\n",
      "Tokenized sentence: ['some', 'friends', 'want', 'me', 'to', 'drive', 'em', 'someplace', 'probably', 'take', 'a', 'while']\n",
      "After stop words removal: ['friends', 'want', 'drive', 'em', 'someplace', 'probably', 'take']\n",
      "After stemming with porters algorithm: ['friend', 'want', 'drive', 'someplac', 'probab', 'take']\n",
      "Tokenized sentence: ['also', 'sir', 'i', 'sent', 'you', 'an', 'email', 'about', 'how', 'to', 'log', 'into', 'the', 'usc', 'payment', 'portal', 'i', 'll', 'send', 'you', 'another', 'message', 'that', 'should', 'explain', 'how', 'things', 'are', 'back', 'home', 'have', 'a', 'great', 'weekend']\n",
      "After stop words removal: ['also', 'sir', 'sent', 'email', 'log', 'usc', 'payment', 'portal', 'send', 'another', 'message', 'explain', 'things', 'back', 'home', 'great', 'weekend']\n",
      "After stemming with porters algorithm: ['also', 'sir', 'sent', 'email', 'log', 'usc', 'payment', 'portal', 'send', 'anoth', 'messag', 'explain', 'thing', 'back', 'home', 'great', 'weekend']\n",
      "Tokenized sentence: ['i', 'dunno', 'they', 'close', 'oredi', 'not', 'v', 'ma', 'fan']\n",
      "After stop words removal: ['dunno', 'close', 'oredi', 'v', 'fan']\n",
      "After stemming with porters algorithm: ['dunno', 'close', 'oredi', 'fan']\n",
      "Tokenized sentence: ['what', 'year', 'and', 'how', 'many', 'miles']\n",
      "After stop words removal: ['year', 'many', 'miles']\n",
      "After stemming with porters algorithm: ['year', 'mani', 'mile']\n",
      "Tokenized sentence: ['hmmm', 'mayb', 'can', 'try', 'e', 'shoppin', 'area', 'one', 'but', 'forgot', 'e', 'name', 'of', 'hotel']\n",
      "After stop words removal: ['hmmm', 'mayb', 'try', 'e', 'shoppin', 'area', 'one', 'forgot', 'e', 'name', 'hotel']\n",
      "After stemming with porters algorithm: ['hmmm', 'mayb', 'try', 'shoppin', 'area', 'on', 'forgot', 'name', 'hotel']\n",
      "Tokenized sentence: ['c', 'movie', 'is', 'juz', 'last', 'minute', 'decision', 'mah', 'juz', 'watch', 'lar', 'but', 'i', 'tot', 'not', 'interested']\n",
      "After stop words removal: ['c', 'movie', 'juz', 'last', 'minute', 'decision', 'mah', 'juz', 'watch', 'lar', 'tot', 'interested']\n",
      "After stemming with porters algorithm: ['movi', 'juz', 'last', 'minut', 'decis', 'mah', 'juz', 'watch', 'lar', 'tot', 'interes']\n",
      "Tokenized sentence: ['since', 'when', 'which', 'side', 'any', 'fever', 'any', 'vomitin']\n",
      "After stop words removal: ['since', 'side', 'fever', 'vomitin']\n",
      "After stemming with porters algorithm: ['sinc', 'side', 'fever', 'vomitin']\n",
      "Tokenized sentence: ['ah', 'poor', 'baby', 'hope', 'urfeeling', 'bettersn', 'luv', 'probthat', 'overdose', 'of', 'work', 'hey', 'go', 'careful', 'spk', 'u', 'sn', 'lots', 'of', 'lovejen', 'xxx']\n",
      "After stop words removal: ['ah', 'poor', 'baby', 'hope', 'urfeeling', 'bettersn', 'luv', 'probthat', 'overdose', 'work', 'hey', 'go', 'careful', 'spk', 'u', 'sn', 'lots', 'lovejen', 'xxx']\n",
      "urfeel\n",
      "After stemming with porters algorithm: ['poor', 'babi', 'hope', 'urfeel', 'bettersn', 'luv', 'probthat', 'overdos', 'work', 'hei', 'care', 'spk', 'lot', 'lovejen', 'xxx']\n",
      "Tokenized sentence: ['raji', 'pls', 'do', 'me', 'a', 'favour', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimya', 'pls', 'today', 'is', 'her', 'birthday']\n",
      "After stop words removal: ['raji', 'pls', 'favour', 'pls', 'convey', 'birthday', 'wishes', 'nimya', 'pls', 'today', 'birthday']\n",
      "After stemming with porters algorithm: ['raji', 'pl', 'favour', 'pl', 'convei', 'birthdai', 'wish', 'nimya', 'pl', 'todai', 'birthdai']\n",
      "Tokenized sentence: ['hiya', 'do', 'u', 'like', 'the', 'hlday', 'pics', 'looked', 'horrible', 'in', 'them', 'so', 'took', 'mo', 'out', 'hows', 'the', 'camp', 'amrca', 'thing', 'speak', 'soon', 'serena']\n",
      "After stop words removal: ['hiya', 'u', 'like', 'hlday', 'pics', 'looked', 'horrible', 'took', 'mo', 'hows', 'camp', 'amrca', 'thing', 'speak', 'soon', 'serena']\n",
      "After stemming with porters algorithm: ['hiya', 'like', 'hldai', 'pic', 'look', 'horrib', 'took', 'how', 'camp', 'amrca', 'thing', 'speak', 'soon', 'serena']\n",
      "Tokenized sentence: ['christmas', 'is', 'an', 'occasion', 'that', 'is', 'celebrated', 'as', 'a', 'reflection', 'of', 'ur', 'values', 'desires', 'affections', 'amp', 'traditions', 'have', 'an', 'ideal', 'christmas']\n",
      "After stop words removal: ['christmas', 'occasion', 'celebrated', 'reflection', 'ur', 'values', 'desires', 'affections', 'amp', 'traditions', 'ideal', 'christmas']\n",
      "celebrate\n",
      "After stemming with porters algorithm: ['christma', 'occas', 'celebr', 'reflect', 'valu', 'desir', 'affect', 'amp', 'tradit', 'ideal', 'christma']\n",
      "Tokenized sentence: ['k', 'i', 'will', 'send', 'in', 'lt', 'gt', 'min']\n",
      "After stop words removal: ['k', 'send', 'lt', 'gt', 'min']\n",
      "After stemming with porters algorithm: ['send', 'min']\n",
      "Tokenized sentence: ['i', 'call', 'you', 'later', 'don', 't', 'have', 'network', 'if', 'urgnt', 'sms', 'me']\n",
      "After stop words removal: ['call', 'later', 'network', 'urgnt', 'sms']\n",
      "After stemming with porters algorithm: ['call', 'later', 'network', 'urgnt', 'sm']\n",
      "Tokenized sentence: ['dai', 'lt', 'gt', 'naal', 'eruku']\n",
      "After stop words removal: ['dai', 'lt', 'gt', 'naal', 'eruku']\n",
      "After stemming with porters algorithm: ['dai', 'naal', 'eruku']\n",
      "Tokenized sentence: ['that', 's', 'what', 'i', 'love', 'to', 'hear', 'v', 'see', 'you', 'sundayish', 'then']\n",
      "After stop words removal: ['love', 'hear', 'v', 'see', 'sundayish']\n",
      "After stemming with porters algorithm: ['love', 'hear', 'see', 'sundayish']\n",
      "Tokenized sentence: ['s', 'i', 'm', 'watching', 'it', 'in', 'live']\n",
      "After stop words removal: ['watching', 'live']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'live']\n",
      "Tokenized sentence: ['marrow', 'only', 'wed', 'at', 'lt', 'gt', 'to', 'aha']\n",
      "After stop words removal: ['marrow', 'wed', 'lt', 'gt', 'aha']\n",
      "After stemming with porters algorithm: ['marrow', 'wed', 'aha']\n",
      "Tokenized sentence: ['like', 'lt', 'gt', 'same', 'question']\n",
      "After stop words removal: ['like', 'lt', 'gt', 'question']\n",
      "After stemming with porters algorithm: ['like', 'quest']\n",
      "Tokenized sentence: ['jus', 'finished', 'avatar', 'nigro']\n",
      "After stop words removal: ['jus', 'finished', 'avatar', 'nigro']\n",
      "After stemming with porters algorithm: ['ju', 'finis', 'avatar', 'nigro']\n",
      "Tokenized sentence: ['y', 'so', 'late', 'but', 'i', 'need', 'to', 'go', 'n', 'get', 'da', 'laptop']\n",
      "After stop words removal: ['late', 'need', 'go', 'n', 'get', 'da', 'laptop']\n",
      "After stemming with porters algorithm: ['late', 'need', 'get', 'laptop']\n",
      "Tokenized sentence: ['hurt', 'me', 'tease', 'me', 'make', 'me', 'cry', 'but', 'in', 'the', 'end', 'of', 'my', 'life', 'when', 'i', 'die', 'plz', 'keep', 'one', 'rose', 'on', 'my', 'grave', 'and', 'say', 'stupid', 'i', 'miss', 'u', 'have', 'a', 'nice', 'day', 'bslvyl']\n",
      "After stop words removal: ['hurt', 'tease', 'make', 'cry', 'end', 'life', 'die', 'plz', 'keep', 'one', 'rose', 'grave', 'say', 'stupid', 'miss', 'u', 'nice', 'day', 'bslvyl']\n",
      "After stemming with porters algorithm: ['hurt', 'teas', 'make', 'cry', 'end', 'life', 'die', 'plz', 'keep', 'on', 'rose', 'grave', 'sai', 'stupid', 'miss', 'nice', 'dai', 'bslvyl']\n",
      "Tokenized sentence: ['dear', 'good', 'morning', 'how', 'you', 'feeling', 'dear']\n",
      "After stop words removal: ['dear', 'good', 'morning', 'feeling', 'dear']\n",
      "morn\n",
      "feel\n",
      "After stemming with porters algorithm: ['dear', 'good', 'mor', 'feel', 'dear']\n",
      "Tokenized sentence: ['i', 'know', 'you', 'are', 'serving', 'i', 'mean', 'what', 'are', 'you', 'doing', 'now']\n",
      "After stop words removal: ['know', 'serving', 'mean']\n",
      "serv\n",
      "After stemming with porters algorithm: ['know', 'ser', 'mean']\n",
      "Tokenized sentence: ['yep', 'by', 'the', 'pretty', 'sculpture']\n",
      "After stop words removal: ['yep', 'pretty', 'sculpture']\n",
      "After stemming with porters algorithm: ['yep', 'pretti', 'sculptur']\n",
      "Tokenized sentence: ['wat', 'uniform', 'in', 'where', 'get']\n",
      "After stop words removal: ['wat', 'uniform', 'get']\n",
      "After stemming with porters algorithm: ['wat', 'uniform', 'get']\n",
      "Tokenized sentence: ['oh', 'oh', 'wasted', 'den', 'muz', 'chiong', 'on', 'sat', 'n', 'sun', 'liao']\n",
      "After stop words removal: ['oh', 'oh', 'wasted', 'den', 'muz', 'chiong', 'sat', 'n', 'sun', 'liao']\n",
      "After stemming with porters algorithm: ['was', 'den', 'muz', 'chiong', 'sat', 'sun', 'liao']\n",
      "Tokenized sentence: ['g', 'w', 'r']\n",
      "After stop words removal: ['g', 'w', 'r']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['no', 'prob', 'i', 'will', 'send', 'to', 'your', 'email']\n",
      "After stop words removal: ['prob', 'send', 'email']\n",
      "After stemming with porters algorithm: ['prob', 'send', 'email']\n",
      "Tokenized sentence: ['eh', 'u', 'remember', 'how', 'spell', 'his', 'name', 'yes', 'i', 'did', 'he', 'v', 'naughty', 'make', 'until', 'i', 'v', 'wet']\n",
      "After stop words removal: ['eh', 'u', 'remember', 'spell', 'name', 'yes', 'v', 'naughty', 'make', 'v', 'wet']\n",
      "After stemming with porters algorithm: ['rememb', 'spell', 'name', 'ye', 'naughti', 'make', 'wet']\n",
      "Tokenized sentence: ['the', 'greatest', 'test', 'of', 'courage', 'on', 'earth', 'is', 'to', 'bear', 'defeat', 'without', 'losing', 'heart', 'gn', 'tc']\n",
      "After stop words removal: ['greatest', 'test', 'courage', 'earth', 'bear', 'defeat', 'without', 'losing', 'heart', 'gn', 'tc']\n",
      "los\n",
      "After stemming with porters algorithm: ['greatest', 'test', 'courag', 'earth', 'bear', 'defeat', 'without', 'lose', 'heart']\n",
      "Tokenized sentence: ['dear', 'voucher', 'holder', 'claim', 'your', 'st', 'class', 'airport', 'lounge', 'passes', 'when', 'using', 'your', 'holiday', 'voucher', 'call', 'when', 'booking', 'quote', 'st', 'class', 'x']\n",
      "After stop words removal: ['dear', 'voucher', 'holder', 'claim', 'st', 'class', 'airport', 'lounge', 'passes', 'using', 'holiday', 'voucher', 'call', 'booking', 'quote', 'st', 'class', 'x']\n",
      "us\n",
      "book\n",
      "After stemming with porters algorithm: ['dear', 'voucher', 'holder', 'claim', 'class', 'airport', 'loung', 'pass', 'us', 'holidai', 'voucher', 'call', 'book', 'quot', 'class']\n",
      "Tokenized sentence: ['the', 'guy', 'at', 'the', 'car', 'shop', 'who', 'was', 'flirting', 'with', 'me', 'got', 'my', 'phone', 'number', 'from', 'the', 'paperwork', 'and', 'called', 'and', 'texted', 'me', 'i', 'm', 'nervous', 'because', 'of', 'course', 'now', 'he', 'may', 'have', 'my', 'address', 'should', 'i', 'call', 'his', 'boss', 'and', 'tell', 'him', 'knowing', 'this', 'may', 'get', 'him', 'fired']\n",
      "After stop words removal: ['guy', 'car', 'shop', 'flirting', 'got', 'phone', 'number', 'paperwork', 'called', 'texted', 'nervous', 'course', 'may', 'address', 'call', 'boss', 'tell', 'knowing', 'may', 'get', 'fired']\n",
      "flirt\n",
      "know\n",
      "After stemming with porters algorithm: ['gui', 'car', 'shop', 'flir', 'got', 'phone', 'number', 'paperwork', 'call', 'tex', 'nervou', 'cours', 'mai', 'address', 'call', 'boss', 'tell', 'knowe', 'mai', 'get', 'fire']\n",
      "Tokenized sentence: ['this', 'pen', 'thing', 'is', 'beyond', 'a', 'joke', 'wont', 'a', 'biro', 'do', 'don', 't', 'do', 'a', 'masters', 'as', 'can', 't', 'do', 'this', 'ever', 'again']\n",
      "After stop words removal: ['pen', 'thing', 'beyond', 'joke', 'wont', 'biro', 'masters', 'ever']\n",
      "After stemming with porters algorithm: ['pen', 'thing', 'beyond', 'joke', 'wont', 'biro', 'master', 'ever']\n",
      "Tokenized sentence: ['playin', 'space', 'poker', 'u']\n",
      "After stop words removal: ['playin', 'space', 'poker', 'u']\n",
      "After stemming with porters algorithm: ['playin', 'space', 'poker']\n",
      "Tokenized sentence: ['the', 'monthly', 'amount', 'is', 'not', 'that', 'terrible', 'and', 'you', 'will', 'not', 'pay', 'anything', 'till', 'months', 'after', 'finishing', 'school']\n",
      "After stop words removal: ['monthly', 'amount', 'terrible', 'pay', 'anything', 'till', 'months', 'finishing', 'school']\n",
      "anyth\n",
      "finish\n",
      "After stemming with porters algorithm: ['monthli', 'amount', 'terrib', 'pai', 'anyt', 'till', 'month', 'finis', 'school']\n",
      "Tokenized sentence: ['noice', 'text', 'me', 'when', 'you', 're', 'here']\n",
      "After stop words removal: ['noice', 'text']\n",
      "After stemming with porters algorithm: ['noic', 'text']\n",
      "Tokenized sentence: ['oh', 'my', 'god', 'i', 'm', 'almost', 'home']\n",
      "After stop words removal: ['oh', 'god', 'almost', 'home']\n",
      "After stemming with porters algorithm: ['god', 'almost', 'home']\n",
      "Tokenized sentence: ['he', 'is', 'a', 'womdarfull', 'actor']\n",
      "After stop words removal: ['womdarfull', 'actor']\n",
      "After stemming with porters algorithm: ['womdarful', 'actor']\n",
      "Tokenized sentence: ['and', 'several', 'to', 'you', 'sir']\n",
      "After stop words removal: ['several', 'sir']\n",
      "After stemming with porters algorithm: ['sever', 'sir']\n",
      "Tokenized sentence: ['ok', 'lor']\n",
      "After stop words removal: ['ok', 'lor']\n",
      "After stemming with porters algorithm: ['lor']\n",
      "Tokenized sentence: ['take', 'some', 'small', 'dose', 'tablet', 'for', 'fever']\n",
      "After stop words removal: ['take', 'small', 'dose', 'tablet', 'fever']\n",
      "After stemming with porters algorithm: ['take', 'small', 'dose', 'tablet', 'fever']\n",
      "Tokenized sentence: ['u', 're', 'welcome', 'caught', 'u', 'using', 'broken', 'english', 'again']\n",
      "After stop words removal: ['u', 'welcome', 'caught', 'u', 'using', 'broken', 'english']\n",
      "us\n",
      "After stemming with porters algorithm: ['welcom', 'caught', 'us', 'broken', 'english']\n",
      "Tokenized sentence: ['ya', 'very', 'nice', 'be', 'ready', 'on', 'thursday']\n",
      "After stop words removal: ['ya', 'nice', 'ready', 'thursday']\n",
      "After stemming with porters algorithm: ['nice', 'readi', 'thursdai']\n",
      "Tokenized sentence: ['k', 'so', 'am', 'i', 'how', 'much', 'for', 'an', 'th', 'fifty']\n",
      "After stop words removal: ['k', 'much', 'th', 'fifty']\n",
      "After stemming with porters algorithm: ['much', 'fifti']\n",
      "Tokenized sentence: ['no', 'message', 'no', 'responce', 'what', 'happend']\n",
      "After stop words removal: ['message', 'responce', 'happend']\n",
      "After stemming with porters algorithm: ['messag', 'responc', 'happend']\n",
      "Tokenized sentence: ['hurry', 'home', 'soup', 'is', 'done']\n",
      "After stop words removal: ['hurry', 'home', 'soup', 'done']\n",
      "After stemming with porters algorithm: ['hurri', 'home', 'soup', 'done']\n",
      "Tokenized sentence: ['dip', 's', 'cell', 'dead', 'so', 'i', 'm', 'coming', 'with', 'him', 'u', 'better', 'respond', 'else', 'we', 'shall', 'come', 'back']\n",
      "After stop words removal: ['dip', 'cell', 'dead', 'coming', 'u', 'better', 'respond', 'else', 'shall', 'come', 'back']\n",
      "com\n",
      "After stemming with porters algorithm: ['dip', 'cell', 'dead', 'come', 'better', 'respond', 'els', 'shall', 'come', 'back']\n",
      "Tokenized sentence: ['lor', 'change', 'suntec', 'wat', 'time', 'u', 'coming']\n",
      "After stop words removal: ['lor', 'change', 'suntec', 'wat', 'time', 'u', 'coming']\n",
      "com\n",
      "After stemming with porters algorithm: ['lor', 'chang', 'suntec', 'wat', 'time', 'come']\n",
      "Tokenized sentence: ['cos', 'i', 'was', 'out', 'shopping', 'wif', 'darren', 'jus', 'now', 'n', 'i', 'called', 'him', 'ask', 'wat', 'present', 'he', 'wan', 'lor', 'then', 'he', 'started', 'guessing', 'who', 'i', 'was', 'wif', 'n', 'he', 'finally', 'guessed', 'darren', 'lor']\n",
      "After stop words removal: ['cos', 'shopping', 'wif', 'darren', 'jus', 'n', 'called', 'ask', 'wat', 'present', 'wan', 'lor', 'started', 'guessing', 'wif', 'n', 'finally', 'guessed', 'darren', 'lor']\n",
      "shopp\n",
      "guess\n",
      "After stemming with porters algorithm: ['co', 'shop', 'wif', 'darren', 'ju', 'call', 'ask', 'wat', 'present', 'wan', 'lor', 'star', 'guess', 'wif', 'final', 'guess', 'darren', 'lor']\n",
      "Tokenized sentence: ['says', 'the', 'lt', 'gt', 'year', 'old', 'with', 'a', 'man', 'and', 'money', 'i', 'm', 'down', 'to', 'my', 'last', 'lt', 'gt', 'still', 'waiting', 'for', 'that', 'check']\n",
      "After stop words removal: ['says', 'lt', 'gt', 'year', 'old', 'man', 'money', 'last', 'lt', 'gt', 'still', 'waiting', 'check']\n",
      "wait\n",
      "After stemming with porters algorithm: ['sai', 'year', 'old', 'man', 'monei', 'last', 'still', 'wait', 'check']\n",
      "Tokenized sentence: ['superb', 'thought', 'be', 'grateful', 'that', 'u', 'dont', 'have', 'everything', 'u', 'want', 'that', 'means', 'u', 'still', 'have', 'an', 'opportunity', 'to', 'be', 'happier', 'tomorrow', 'than', 'u', 'are', 'today']\n",
      "After stop words removal: ['superb', 'thought', 'grateful', 'u', 'dont', 'everything', 'u', 'want', 'means', 'u', 'still', 'opportunity', 'happier', 'tomorrow', 'u', 'today']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['superb', 'thought', 'grate', 'dont', 'everyt', 'want', 'mean', 'still', 'opportun', 'happier', 'tomorrow', 'todai']\n",
      "Tokenized sentence: ['hai', 'priya', 'are', 'you', 'right', 'what', 'doctor', 'said', 'pa', 'where', 'are', 'you']\n",
      "After stop words removal: ['hai', 'priya', 'right', 'doctor', 'said', 'pa']\n",
      "After stemming with porters algorithm: ['hai', 'priya', 'right', 'doctor', 'said']\n",
      "Tokenized sentence: ['yeah', 'i', 'll', 'leave', 'in', 'a', 'couple', 'minutes', 'amp', 'let', 'you', 'know', 'when', 'i', 'get', 'to', 'mu']\n",
      "After stop words removal: ['yeah', 'leave', 'couple', 'minutes', 'amp', 'let', 'know', 'get', 'mu']\n",
      "After stemming with porters algorithm: ['yeah', 'leav', 'coupl', 'minut', 'amp', 'let', 'know', 'get']\n",
      "Tokenized sentence: ['jokin', 'only', 'lar', 'depends', 'on', 'which', 'phone', 'my', 'father', 'can', 'get', 'lor']\n",
      "After stop words removal: ['jokin', 'lar', 'depends', 'phone', 'father', 'get', 'lor']\n",
      "After stemming with porters algorithm: ['jokin', 'lar', 'depend', 'phone', 'father', 'get', 'lor']\n",
      "Tokenized sentence: ['sorry', 'i', 'din', 'lock', 'my', 'keypad']\n",
      "After stop words removal: ['sorry', 'din', 'lock', 'keypad']\n",
      "After stemming with porters algorithm: ['sorri', 'din', 'lock', 'keypad']\n",
      "Tokenized sentence: ['pls', 'ask', 'macho', 'how', 'much', 'is', 'budget', 'for', 'bb', 'bold', 'is', 'cos', 'i', 'saw', 'a', 'new', 'one', 'for', 'lt', 'gt', 'dollars']\n",
      "After stop words removal: ['pls', 'ask', 'macho', 'much', 'budget', 'bb', 'bold', 'cos', 'saw', 'new', 'one', 'lt', 'gt', 'dollars']\n",
      "After stemming with porters algorithm: ['pl', 'ask', 'macho', 'much', 'budget', 'bold', 'co', 'saw', 'new', 'on', 'dollar']\n",
      "Tokenized sentence: ['the', 'evo', 'i', 'just', 'had', 'to', 'download', 'flash', 'jealous']\n",
      "After stop words removal: ['evo', 'download', 'flash', 'jealous']\n",
      "After stemming with porters algorithm: ['evo', 'download', 'flash', 'jealou']\n",
      "Tokenized sentence: ['ask', 'g', 'or', 'iouri', 'i', 've', 'told', 'the', 'story', 'like', 'ten', 'times', 'already']\n",
      "After stop words removal: ['ask', 'g', 'iouri', 'told', 'story', 'like', 'ten', 'times', 'already']\n",
      "After stemming with porters algorithm: ['ask', 'iouri', 'told', 'stori', 'like', 'ten', 'time', 'alreadi']\n",
      "Tokenized sentence: ['easy', 'ah', 'sen', 'got', 'selected', 'means', 'its', 'good']\n",
      "After stop words removal: ['easy', 'ah', 'sen', 'got', 'selected', 'means', 'good']\n",
      "After stemming with porters algorithm: ['easi', 'sen', 'got', 'selec', 'mean', 'good']\n",
      "Tokenized sentence: ['you', 'unbelievable', 'faglord']\n",
      "After stop words removal: ['unbelievable', 'faglord']\n",
      "After stemming with porters algorithm: ['unbeliev', 'faglord']\n",
      "Tokenized sentence: ['okie']\n",
      "After stop words removal: ['okie']\n",
      "After stemming with porters algorithm: ['oki']\n",
      "Tokenized sentence: ['play', 'w', 'computer', 'aiyah', 'i', 'tok', 'u', 'lor']\n",
      "After stop words removal: ['play', 'w', 'computer', 'aiyah', 'tok', 'u', 'lor']\n",
      "After stemming with porters algorithm: ['plai', 'comput', 'aiyah', 'tok', 'lor']\n",
      "Tokenized sentence: ['if', 'i', 'die', 'i', 'want', 'u', 'to', 'have', 'all', 'my', 'stuffs']\n",
      "After stop words removal: ['die', 'want', 'u', 'stuffs']\n",
      "After stemming with porters algorithm: ['die', 'want', 'stuff']\n",
      "Tokenized sentence: ['thanks', 'honey', 'but', 'still', 'haven', 't', 'heard', 'anything', 'i', 'will', 'leave', 'it', 'a', 'bit', 'longer', 'so', 'not', 'crowd', 'him', 'and', 'will', 'try', 'later', 'great', 'advice', 'thanks', 'hope', 'cardiff', 'is', 'still', 'there']\n",
      "After stop words removal: ['thanks', 'honey', 'still', 'heard', 'anything', 'leave', 'bit', 'longer', 'crowd', 'try', 'later', 'great', 'advice', 'thanks', 'hope', 'cardiff', 'still']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['thank', 'honei', 'still', 'heard', 'anyt', 'leav', 'bit', 'longer', 'crowd', 'try', 'later', 'great', 'advic', 'thank', 'hope', 'cardiff', 'still']\n",
      "Tokenized sentence: ['no', 'de', 'but', 'call', 'me', 'after', 'some', 'time', 'ill', 'tell', 'you', 'k']\n",
      "After stop words removal: ['de', 'call', 'time', 'ill', 'tell', 'k']\n",
      "After stemming with porters algorithm: ['call', 'time', 'ill', 'tell']\n",
      "Tokenized sentence: ['he', 'needs', 'to', 'stop', 'going', 'to', 'bed', 'and', 'make', 'with', 'the', 'fucking', 'dealing']\n",
      "After stop words removal: ['needs', 'stop', 'going', 'bed', 'make', 'fucking', 'dealing']\n",
      "go\n",
      "fuck\n",
      "deal\n",
      "After stemming with porters algorithm: ['need', 'stop', 'go', 'bed', 'make', 'fuc', 'deal']\n",
      "Tokenized sentence: ['the', 'xmas', 'story', 'is', 'peace', 'the', 'xmas', 'msg', 'is', 'love', 'the', 'xmas', 'miracle', 'is', 'jesus', 'hav', 'a', 'blessed', 'month', 'ahead', 'amp', 'wish', 'u', 'merry', 'xmas']\n",
      "After stop words removal: ['xmas', 'story', 'peace', 'xmas', 'msg', 'love', 'xmas', 'miracle', 'jesus', 'hav', 'blessed', 'month', 'ahead', 'amp', 'wish', 'u', 'merry', 'xmas']\n",
      "After stemming with porters algorithm: ['xma', 'stori', 'peac', 'xma', 'msg', 'love', 'xma', 'mirac', 'jesu', 'hav', 'bless', 'month', 'ahead', 'amp', 'wish', 'merri', 'xma']\n",
      "Tokenized sentence: ['free', 'game', 'get', 'rayman', 'golf', 'free', 'from', 'the', 'o', 'games', 'arcade', 'st', 'get', 'ur', 'games', 'settings', 'reply', 'post', 'then', 'save', 'activ', 'press', 'key', 'for', 'arcade', 'termsapply']\n",
      "After stop words removal: ['free', 'game', 'get', 'rayman', 'golf', 'free', 'games', 'arcade', 'st', 'get', 'ur', 'games', 'settings', 'reply', 'post', 'save', 'activ', 'press', 'key', 'arcade', 'termsapply']\n",
      "sett\n",
      "After stemming with porters algorithm: ['free', 'game', 'get', 'rayman', 'golf', 'free', 'game', 'arcad', 'get', 'game', 'set', 'repli', 'post', 'save', 'activ', 'press', 'kei', 'arcad', 'termsappli']\n",
      "Tokenized sentence: ['aathi', 'where', 'are', 'you', 'dear']\n",
      "After stop words removal: ['aathi', 'dear']\n",
      "After stemming with porters algorithm: ['aathi', 'dear']\n",
      "Tokenized sentence: ['eh', 'sorry', 'leh', 'i', 'din', 'c', 'ur', 'msg', 'not', 'sad', 'already', 'lar', 'me', 'watching', 'tv', 'now', 'u', 'still', 'in', 'office']\n",
      "After stop words removal: ['eh', 'sorry', 'leh', 'din', 'c', 'ur', 'msg', 'sad', 'already', 'lar', 'watching', 'tv', 'u', 'still', 'office']\n",
      "watch\n",
      "After stemming with porters algorithm: ['sorri', 'leh', 'din', 'msg', 'sad', 'alreadi', 'lar', 'watc', 'still', 'offic']\n",
      "Tokenized sentence: ['wait', 'do', 'you', 'know', 'if', 'wesleys', 'in', 'town', 'i', 'bet', 'she', 'does', 'hella', 'drugs']\n",
      "After stop words removal: ['wait', 'know', 'wesleys', 'town', 'bet', 'hella', 'drugs']\n",
      "After stemming with porters algorithm: ['wait', 'know', 'weslei', 'town', 'bet', 'hella', 'drug']\n",
      "Tokenized sentence: ['gent', 'we', 'are', 'trying', 'to', 'contact', 'you', 'last', 'weekends', 'draw', 'shows', 'that', 'you', 'won', 'a', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'only', 'ppm']\n",
      "After stop words removal: ['gent', 'trying', 'contact', 'last', 'weekends', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'ppm']\n",
      "After stemming with porters algorithm: ['gent', 'trying', 'contact', 'last', 'weekend', 'draw', 'show', 'priz', 'guaranteed', 'call', 'claim', 'code', 'valid', 'hr', 'ppm']\n",
      "Tokenized sentence: ['yupz', 'i', 've', 'oredi', 'booked', 'slots', 'my', 'weekends', 'liao']\n",
      "After stop words removal: ['yupz', 'oredi', 'booked', 'slots', 'weekends', 'liao']\n",
      "After stemming with porters algorithm: ['yupz', 'oredi', 'book', 'slot', 'weekend', 'liao']\n",
      "Tokenized sentence: ['fuck', 'babe', 'i', 'miss', 'you', 'already', 'you', 'know', 'can', 't', 'you', 'let', 'me', 'send', 'you', 'some', 'money', 'towards', 'your', 'net', 'i', 'need', 'you', 'i', 'want', 'you', 'i', 'crave', 'you']\n",
      "After stop words removal: ['fuck', 'babe', 'miss', 'already', 'know', 'let', 'send', 'money', 'towards', 'net', 'need', 'want', 'crave']\n",
      "After stemming with porters algorithm: ['fuck', 'babe', 'miss', 'alreadi', 'know', 'let', 'send', 'monei', 'toward', 'net', 'need', 'want', 'crave']\n",
      "Tokenized sentence: ['i', 'will', 'take', 'care', 'of', 'financial', 'problem', 'i', 'will', 'help']\n",
      "After stop words removal: ['take', 'care', 'financial', 'problem', 'help']\n",
      "After stemming with porters algorithm: ['take', 'care', 'financi', 'problem', 'help']\n",
      "Tokenized sentence: ['i', 'finished', 'my', 'lunch', 'already', 'u', 'wake', 'up', 'already']\n",
      "After stop words removal: ['finished', 'lunch', 'already', 'u', 'wake', 'already']\n",
      "After stemming with porters algorithm: ['finis', 'lunch', 'alreadi', 'wake', 'alreadi']\n",
      "Tokenized sentence: ['k', 'jason', 'says', 'he', 's', 'gonna', 'be', 'around', 'so', 'i', 'll', 'be', 'up', 'there', 'around', 'lt', 'gt']\n",
      "After stop words removal: ['k', 'jason', 'says', 'gonna', 'around', 'around', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['jason', 'sai', 'gonna', 'around', 'around']\n",
      "Tokenized sentence: ['ok', 'thanx']\n",
      "After stop words removal: ['ok', 'thanx']\n",
      "After stemming with porters algorithm: ['thanx']\n",
      "Tokenized sentence: ['thanks', 'love', 'but', 'am', 'i', 'doing', 'torch', 'or', 'bold']\n",
      "After stop words removal: ['thanks', 'love', 'torch', 'bold']\n",
      "After stemming with porters algorithm: ['thank', 'love', 'torch', 'bold']\n",
      "Tokenized sentence: ['yo', 'you', 'gonna', 'still', 'be', 'in', 'stock', 'tomorrow', 'today', 'i', 'm', 'trying', 'to', 'get', 'a', 'dubsack']\n",
      "After stop words removal: ['yo', 'gonna', 'still', 'stock', 'tomorrow', 'today', 'trying', 'get', 'dubsack']\n",
      "After stemming with porters algorithm: ['gonna', 'still', 'stock', 'tomorrow', 'todai', 'trying', 'get', 'dubsack']\n",
      "Tokenized sentence: ['hi', 'hope', 'u', 'get', 'this', 'txt', 'journey', 'hasnt', 'been', 'gd', 'now', 'about', 'mins', 'late', 'i', 'think']\n",
      "After stop words removal: ['hi', 'hope', 'u', 'get', 'txt', 'journey', 'hasnt', 'gd', 'mins', 'late', 'think']\n",
      "After stemming with porters algorithm: ['hope', 'get', 'txt', 'journei', 'hasnt', 'min', 'late', 'think']\n",
      "Tokenized sentence: ['jus', 'finish', 'watching', 'tv', 'u']\n",
      "After stop words removal: ['jus', 'finish', 'watching', 'tv', 'u']\n",
      "watch\n",
      "After stemming with porters algorithm: ['ju', 'finish', 'watc']\n",
      "Tokenized sentence: ['yup', 'i', 'thk', 'cine', 'is', 'better', 'cos', 'no', 'need', 'go', 'down', 'plaza', 'mah']\n",
      "After stop words removal: ['yup', 'thk', 'cine', 'better', 'cos', 'need', 'go', 'plaza', 'mah']\n",
      "After stemming with porters algorithm: ['yup', 'thk', 'cine', 'better', 'co', 'need', 'plaza', 'mah']\n",
      "Tokenized sentence: ['k', 'k', 'apo', 'k', 'good', 'movie']\n",
      "After stop words removal: ['k', 'k', 'apo', 'k', 'good', 'movie']\n",
      "After stemming with porters algorithm: ['apo', 'good', 'movi']\n",
      "Tokenized sentence: ['finally', 'the', 'match', 'heading', 'towards', 'draw', 'as', 'your', 'prediction']\n",
      "After stop words removal: ['finally', 'match', 'heading', 'towards', 'draw', 'prediction']\n",
      "head\n",
      "After stemming with porters algorithm: ['final', 'match', 'head', 'toward', 'draw', 'predict']\n",
      "Tokenized sentence: ['ya', 'ok', 'then', 'had', 'dinner']\n",
      "After stop words removal: ['ya', 'ok', 'dinner']\n",
      "After stemming with porters algorithm: ['dinner']\n",
      "Tokenized sentence: ['lmao', 'but', 'its', 'so', 'fun']\n",
      "After stop words removal: ['lmao', 'fun']\n",
      "After stemming with porters algorithm: ['lmao', 'fun']\n",
      "Tokenized sentence: ['den', 'only', 'weekdays', 'got', 'special', 'price', 'haiz', 'cant', 'eat', 'liao', 'cut', 'nails', 'oso', 'muz', 'wait', 'until', 'i', 'finish', 'drivin', 'wat', 'lunch', 'still', 'muz', 'eat', 'wat']\n",
      "After stop words removal: ['den', 'weekdays', 'got', 'special', 'price', 'haiz', 'cant', 'eat', 'liao', 'cut', 'nails', 'oso', 'muz', 'wait', 'finish', 'drivin', 'wat', 'lunch', 'still', 'muz', 'eat', 'wat']\n",
      "After stemming with porters algorithm: ['den', 'weekdai', 'got', 'special', 'price', 'haiz', 'cant', 'eat', 'liao', 'cut', 'nail', 'oso', 'muz', 'wait', 'finish', 'drivin', 'wat', 'lunch', 'still', 'muz', 'eat', 'wat']\n",
      "Tokenized sentence: ['going', 'to', 'join', 'tomorrow']\n",
      "After stop words removal: ['going', 'join', 'tomorrow']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'join', 'tomorrow']\n",
      "Tokenized sentence: ['sounds', 'like', 'something', 'that', 'someone', 'testing', 'me', 'would', 'sayy']\n",
      "After stop words removal: ['sounds', 'like', 'something', 'someone', 'testing', 'would', 'sayy']\n",
      "someth\n",
      "test\n",
      "After stemming with porters algorithm: ['sound', 'like', 'somet', 'someon', 'tes', 'would', 'sayi']\n",
      "Tokenized sentence: ['santa', 'calling', 'would', 'your', 'little', 'ones', 'like', 'a', 'call', 'from', 'santa', 'xmas', 'eve', 'call', 'to', 'book', 'your', 'time']\n",
      "After stop words removal: ['santa', 'calling', 'would', 'little', 'ones', 'like', 'call', 'santa', 'xmas', 'eve', 'call', 'book', 'time']\n",
      "call\n",
      "After stemming with porters algorithm: ['santa', 'call', 'would', 'littl', 'on', 'like', 'call', 'santa', 'xma', 'ev', 'call', 'book', 'time']\n",
      "Tokenized sentence: ['good', 'morning', 'my', 'love', 'i', 'go', 'to', 'sleep', 'now', 'and', 'wish', 'you', 'a', 'great', 'day', 'full', 'of', 'feeling', 'better', 'and', 'opportunity', 'you', 'are', 'my', 'last', 'thought', 'babe', 'i', 'love', 'you', 'kiss']\n",
      "After stop words removal: ['good', 'morning', 'love', 'go', 'sleep', 'wish', 'great', 'day', 'full', 'feeling', 'better', 'opportunity', 'last', 'thought', 'babe', 'love', 'kiss']\n",
      "morn\n",
      "feel\n",
      "After stemming with porters algorithm: ['good', 'mor', 'love', 'sleep', 'wish', 'great', 'dai', 'full', 'feel', 'better', 'opportun', 'last', 'thought', 'babe', 'love', 'kiss']\n",
      "Tokenized sentence: ['yup', 'i', 'thk', 'so', 'until', 'e', 'shop', 'closes', 'lor']\n",
      "After stop words removal: ['yup', 'thk', 'e', 'shop', 'closes', 'lor']\n",
      "After stemming with porters algorithm: ['yup', 'thk', 'shop', 'close', 'lor']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'enjoying', 'this', 'semester', 'take', 'care', 'brother']\n",
      "After stop words removal: ['enjoying', 'semester', 'take', 'care', 'brother']\n",
      "enjoy\n",
      "After stemming with porters algorithm: ['enjoi', 'semest', 'take', 'care', 'brother']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'c', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'c', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['anything', 'lor', 'but', 'toa', 'payoh', 'got', 'place', 'walk', 'meh']\n",
      "After stop words removal: ['anything', 'lor', 'toa', 'payoh', 'got', 'place', 'walk', 'meh']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'lor', 'toa', 'payoh', 'got', 'place', 'walk', 'meh']\n",
      "Tokenized sentence: ['how', 'come', 'she', 'can', 'get', 'it', 'should', 'b', 'quite', 'diff', 'to', 'guess', 'rite']\n",
      "After stop words removal: ['come', 'get', 'b', 'quite', 'diff', 'guess', 'rite']\n",
      "After stemming with porters algorithm: ['come', 'get', 'quit', 'diff', 'guess', 'rite']\n",
      "Tokenized sentence: ['my', 'uncles', 'in', 'atlanta', 'wish', 'you', 'guys', 'a', 'great', 'semester']\n",
      "After stop words removal: ['uncles', 'atlanta', 'wish', 'guys', 'great', 'semester']\n",
      "After stemming with porters algorithm: ['uncl', 'atlanta', 'wish', 'gui', 'great', 'semest']\n",
      "Tokenized sentence: ['today', 'is', 'sorry', 'day', 'if', 'ever', 'i', 'was', 'angry', 'with', 'you', 'if', 'ever', 'i', 'misbehaved', 'or', 'hurt', 'you', 'plz', 'plz', 'just', 'slap', 'urself', 'bcoz', 'its', 'ur', 'fault', 'i', 'm', 'basically', 'good']\n",
      "After stop words removal: ['today', 'sorry', 'day', 'ever', 'angry', 'ever', 'misbehaved', 'hurt', 'plz', 'plz', 'slap', 'urself', 'bcoz', 'ur', 'fault', 'basically', 'good']\n",
      "After stemming with porters algorithm: ['todai', 'sorri', 'dai', 'ever', 'angri', 'ever', 'misbehav', 'hurt', 'plz', 'plz', 'slap', 'urself', 'bcoz', 'fault', 'basic', 'good']\n",
      "Tokenized sentence: ['hey', 'mate', 'hows', 'u', 'honey', 'did', 'u', 'ave', 'good', 'holiday', 'gimmi', 'de', 'goss', 'x']\n",
      "After stop words removal: ['hey', 'mate', 'hows', 'u', 'honey', 'u', 'ave', 'good', 'holiday', 'gimmi', 'de', 'goss', 'x']\n",
      "After stemming with porters algorithm: ['hei', 'mate', 'how', 'honei', 'av', 'good', 'holidai', 'gimmi', 'goss']\n",
      "Tokenized sentence: ['the', 'world', 'is', 'running', 'and', 'i', 'am', 'still', 'maybe', 'all', 'are', 'feeling', 'the', 'same', 'so', 'be', 'it', 'or', 'i', 'have', 'to', 'admit', 'i', 'am', 'mad', 'then', 'where', 'is', 'the', 'correction', 'or', 'let', 'me', 'call', 'this', 'is', 'life', 'and', 'keep', 'running', 'with', 'the', 'world', 'may', 'be', 'u', 'r', 'also', 'running', 'lets', 'run']\n",
      "After stop words removal: ['world', 'running', 'still', 'maybe', 'feeling', 'admit', 'mad', 'correction', 'let', 'call', 'life', 'keep', 'running', 'world', 'may', 'u', 'r', 'also', 'running', 'lets', 'run']\n",
      "runn\n",
      "feel\n",
      "runn\n",
      "runn\n",
      "After stemming with porters algorithm: ['world', 'run', 'still', 'mayb', 'feel', 'admit', 'mad', 'correct', 'let', 'call', 'life', 'keep', 'run', 'world', 'mai', 'also', 'run', 'let', 'run']\n",
      "Tokenized sentence: ['lol', 'i', 'really', 'need', 'to', 'remember', 'to', 'eat', 'when', 'i', 'm', 'drinking', 'but', 'i', 'do', 'appreciate', 'you', 'keeping', 'me', 'company', 'that', 'night', 'babe', 'smiles']\n",
      "After stop words removal: ['lol', 'really', 'need', 'remember', 'eat', 'drinking', 'appreciate', 'keeping', 'company', 'night', 'babe', 'smiles']\n",
      "drink\n",
      "keep\n",
      "After stemming with porters algorithm: ['lol', 'realli', 'need', 'rememb', 'eat', 'drin', 'appreci', 'keep', 'compani', 'night', 'babe', 'smile']\n",
      "Tokenized sentence: ['as', 'a', 'valued', 'customer', 'i', 'am', 'pleased', 'to', 'advise', 'you', 'that', 'following', 'recent', 'review', 'of', 'your', 'mob', 'no', 'you', 'are', 'awarded', 'with', 'a', 'bonus', 'prize', 'call']\n",
      "After stop words removal: ['valued', 'customer', 'pleased', 'advise', 'following', 'recent', 'review', 'mob', 'awarded', 'bonus', 'prize', 'call']\n",
      "follow\n",
      "After stemming with porters algorithm: ['valu', 'custom', 'pleas', 'advis', 'follow', 'recent', 'review', 'mob', 'awar', 'bonu', 'priz', 'call']\n",
      "Tokenized sentence: ['oops', 'am', 'at', 'my', 'mum', 's', 'in', 'somerset', 'bit', 'far', 'back', 'tomo', 'see', 'you', 'soon', 'x']\n",
      "After stop words removal: ['oops', 'mum', 'somerset', 'bit', 'far', 'back', 'tomo', 'see', 'soon', 'x']\n",
      "After stemming with porters algorithm: ['oop', 'mum', 'somerset', 'bit', 'far', 'back', 'tomo', 'see', 'soon']\n",
      "Tokenized sentence: ['me', 'too', 'baby', 'i', 'promise', 'to', 'treat', 'you', 'well', 'i', 'bet', 'you', 'will', 'take', 'good', 'care', 'of', 'me']\n",
      "After stop words removal: ['baby', 'promise', 'treat', 'well', 'bet', 'take', 'good', 'care']\n",
      "After stemming with porters algorithm: ['babi', 'promis', 'treat', 'well', 'bet', 'take', 'good', 'care']\n",
      "Tokenized sentence: ['they', 'are', 'just', 'making', 'it', 'easy', 'to', 'pay', 'back', 'i', 'have', 'lt', 'gt', 'yrs', 'to', 'say', 'but', 'i', 'can', 'pay', 'back', 'earlier', 'you', 'get']\n",
      "After stop words removal: ['making', 'easy', 'pay', 'back', 'lt', 'gt', 'yrs', 'say', 'pay', 'back', 'earlier', 'get']\n",
      "mak\n",
      "After stemming with porters algorithm: ['make', 'easi', 'pai', 'back', 'yr', 'sai', 'pai', 'back', 'earlier', 'get']\n",
      "Tokenized sentence: ['ok', 'i', 'm', 'gonna', 'head', 'up', 'to', 'usf', 'in', 'like', 'fifteen', 'minutes']\n",
      "After stop words removal: ['ok', 'gonna', 'head', 'usf', 'like', 'fifteen', 'minutes']\n",
      "After stemming with porters algorithm: ['gonna', 'head', 'usf', 'like', 'fifteen', 'minut']\n",
      "Tokenized sentence: ['mom', 'wants', 'to', 'know', 'where', 'you', 'at']\n",
      "After stop words removal: ['mom', 'wants', 'know']\n",
      "After stemming with porters algorithm: ['mom', 'want', 'know']\n",
      "Tokenized sentence: ['what', 'you', 'did', 'in', 'leave']\n",
      "After stop words removal: ['leave']\n",
      "After stemming with porters algorithm: ['leav']\n",
      "Tokenized sentence: ['i', 'jus', 'reached', 'home', 'i', 'go', 'bathe', 'first', 'but', 'my', 'sis', 'using', 'net', 'tell', 'u', 'when', 'she', 'finishes', 'k']\n",
      "After stop words removal: ['jus', 'reached', 'home', 'go', 'bathe', 'first', 'sis', 'using', 'net', 'tell', 'u', 'finishes', 'k']\n",
      "us\n",
      "After stemming with porters algorithm: ['ju', 'reac', 'home', 'bath', 'first', 'si', 'us', 'net', 'tell', 'finish']\n",
      "Tokenized sentence: ['uncle', 'abbey', 'happy', 'new', 'year', 'abiola']\n",
      "After stop words removal: ['uncle', 'abbey', 'happy', 'new', 'year', 'abiola']\n",
      "After stemming with porters algorithm: ['uncl', 'abbei', 'happi', 'new', 'year', 'abiola']\n",
      "Tokenized sentence: ['alright', 'i', 'have', 'a', 'new', 'goal', 'now']\n",
      "After stop words removal: ['alright', 'new', 'goal']\n",
      "After stemming with porters algorithm: ['alright', 'new', 'goal']\n",
      "Tokenized sentence: ['and', 'stop', 'being', 'an', 'old', 'man', 'you', 'get', 'to', 'build', 'snowman', 'snow', 'angels', 'and', 'snowball', 'fights']\n",
      "After stop words removal: ['stop', 'old', 'man', 'get', 'build', 'snowman', 'snow', 'angels', 'snowball', 'fights']\n",
      "After stemming with porters algorithm: ['stop', 'old', 'man', 'get', 'build', 'snowman', 'snow', 'angel', 'snowbal', 'fight']\n",
      "Tokenized sentence: ['it', 's', 'that', 'time', 'of', 'the', 'week', 'again', 'ryan']\n",
      "After stop words removal: ['time', 'week', 'ryan']\n",
      "After stemming with porters algorithm: ['time', 'week', 'ryan']\n",
      "Tokenized sentence: ['just', 'got', 'some', 'gas', 'money', 'any', 'chance', 'you', 'and', 'the', 'gang', 'want', 'to', 'go', 'on', 'a', 'grand', 'nature', 'adventure']\n",
      "After stop words removal: ['got', 'gas', 'money', 'chance', 'gang', 'want', 'go', 'grand', 'nature', 'adventure']\n",
      "After stemming with porters algorithm: ['got', 'ga', 'monei', 'chanc', 'gang', 'want', 'grand', 'natur', 'adventur']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'town', 'now', 'so', 'i', 'll', 'jus', 'take', 'mrt', 'down', 'later']\n",
      "After stop words removal: ['town', 'jus', 'take', 'mrt', 'later']\n",
      "After stemming with porters algorithm: ['town', 'ju', 'take', 'mrt', 'later']\n",
      "Tokenized sentence: ['i', 'dont', 'can', 'you', 'send', 'it', 'to', 'me', 'plus', 'how', 's', 'mode']\n",
      "After stop words removal: ['dont', 'send', 'plus', 'mode']\n",
      "After stemming with porters algorithm: ['dont', 'send', 'plu', 'mode']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['is', 'it', 'ok', 'if', 'i', 'stay', 'the', 'night', 'here', 'xavier', 'has', 'a', 'sleeping', 'bag', 'and', 'i', 'm', 'getting', 'tired']\n",
      "After stop words removal: ['ok', 'stay', 'night', 'xavier', 'sleeping', 'bag', 'getting', 'tired']\n",
      "sleep\n",
      "gett\n",
      "After stemming with porters algorithm: ['stai', 'night', 'xavier', 'sleep', 'bag', 'get', 'tire']\n",
      "Tokenized sentence: ['do', 'not', 'b', 'late', 'love', 'mum']\n",
      "After stop words removal: ['b', 'late', 'love', 'mum']\n",
      "After stemming with porters algorithm: ['late', 'love', 'mum']\n",
      "Tokenized sentence: ['nice', 'nice', 'how', 'is', 'it', 'working']\n",
      "After stop words removal: ['nice', 'nice', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['nice', 'nice', 'wor']\n",
      "Tokenized sentence: ['even', 'i', 'cant', 'close', 'my', 'eyes', 'you', 'are', 'in', 'me', 'our', 'vava', 'playing', 'umma', 'd']\n",
      "After stop words removal: ['even', 'cant', 'close', 'eyes', 'vava', 'playing', 'umma']\n",
      "play\n",
      "After stemming with porters algorithm: ['even', 'cant', 'close', 'ey', 'vava', 'plai', 'umma']\n",
      "Tokenized sentence: ['hack', 'chat', 'get', 'backdoor', 'entry', 'into', 'chat', 'rooms', 'at', 'a', 'fraction', 'of', 'the', 'cost', 'reply', 'neo', 'or', 'call', 'to', 'subscribe', 'p', 'pm', 'dps', 'bcm', 'box', 'ldn', 'wc', 'n', 'xx']\n",
      "After stop words removal: ['hack', 'chat', 'get', 'backdoor', 'entry', 'chat', 'rooms', 'fraction', 'cost', 'reply', 'neo', 'call', 'subscribe', 'p', 'pm', 'dps', 'bcm', 'box', 'ldn', 'wc', 'n', 'xx']\n",
      "After stemming with porters algorithm: ['hack', 'chat', 'get', 'backdoor', 'entri', 'chat', 'room', 'fract', 'cost', 'repli', 'neo', 'call', 'subscrib', 'dp', 'bcm', 'box', 'ldn']\n",
      "Tokenized sentence: ['lt', 'gt', 'is', 'fast', 'approaching', 'so', 'wish', 'u', 'a', 'very', 'happy', 'new', 'year', 'happy', 'sankranti', 'happy', 'republic', 'day', 'happy', 'valentines', 'day', 'happy', 'shivratri', 'happy', 'ugadi', 'happy', 'fools', 'day', 'happy', 'may', 'day', 'happy', 'independence', 'day', 'happy', 'friendship', 'mother', 'father', 'teachers', 'childrens', 'day', 'amp', 'happy', 'birthday', 'u', 'happy', 'ganesh', 'festival', 'happy', 'dasara', 'happy', 'diwali', 'happy', 'christmas', 'lt', 'gt', 'good', 'mornings', 'afternoons', 'evenings', 'nights', 'rememberi', 'am', 'the', 'first', 'to', 'wishing', 'u', 'all', 'these', 'your', 's', 'raj']\n",
      "After stop words removal: ['lt', 'gt', 'fast', 'approaching', 'wish', 'u', 'happy', 'new', 'year', 'happy', 'sankranti', 'happy', 'republic', 'day', 'happy', 'valentines', 'day', 'happy', 'shivratri', 'happy', 'ugadi', 'happy', 'fools', 'day', 'happy', 'may', 'day', 'happy', 'independence', 'day', 'happy', 'friendship', 'mother', 'father', 'teachers', 'childrens', 'day', 'amp', 'happy', 'birthday', 'u', 'happy', 'ganesh', 'festival', 'happy', 'dasara', 'happy', 'diwali', 'happy', 'christmas', 'lt', 'gt', 'good', 'mornings', 'afternoons', 'evenings', 'nights', 'rememberi', 'first', 'wishing', 'u', 'raj']\n",
      "approach\n",
      "morn\n",
      "even\n",
      "wish\n",
      "After stemming with porters algorithm: ['fast', 'approac', 'wish', 'happi', 'new', 'year', 'happi', 'sankranti', 'happi', 'repub', 'dai', 'happi', 'valentin', 'dai', 'happi', 'shivratri', 'happi', 'ugadi', 'happi', 'fool', 'dai', 'happi', 'mai', 'dai', 'happi', 'independ', 'dai', 'happi', 'friendship', 'mother', 'father', 'teacher', 'children', 'dai', 'amp', 'happi', 'birthdai', 'happi', 'ganesh', 'festiv', 'happi', 'dasara', 'happi', 'diwali', 'happi', 'christma', 'good', 'mor', 'afternoon', 'even', 'night', 'rememberi', 'first', 'wis', 'raj']\n",
      "Tokenized sentence: ['your', 'credits', 'have', 'been', 'topped', 'up', 'for', 'http', 'www', 'bubbletext', 'com', 'your', 'renewal', 'pin', 'is', 'tgxxrz']\n",
      "After stop words removal: ['credits', 'topped', 'http', 'www', 'bubbletext', 'com', 'renewal', 'pin', 'tgxxrz']\n",
      "After stemming with porters algorithm: ['credit', 'top', 'http', 'www', 'bubbletext', 'com', 'renew', 'pin', 'tgxxrz']\n",
      "Tokenized sentence: ['yo', 'you', 'guys', 'ever', 'figure', 'out', 'how', 'much', 'we', 'need', 'for', 'alcohol', 'jay', 'and', 'i', 'are', 'trying', 'to', 'figure', 'out', 'how', 'much', 'we', 'can', 'safely', 'spend', 'on', 'weed']\n",
      "After stop words removal: ['yo', 'guys', 'ever', 'figure', 'much', 'need', 'alcohol', 'jay', 'trying', 'figure', 'much', 'safely', 'spend', 'weed']\n",
      "After stemming with porters algorithm: ['gui', 'ever', 'figur', 'much', 'need', 'alcohol', 'jai', 'trying', 'figur', 'much', 'safe', 'spend', 'weed']\n",
      "Tokenized sentence: ['yeah', 'we', 'got', 'one', 'lined', 'up', 'for', 'us']\n",
      "After stop words removal: ['yeah', 'got', 'one', 'lined', 'us']\n",
      "After stemming with porters algorithm: ['yeah', 'got', 'on', 'line']\n",
      "Tokenized sentence: ['quite', 'lor', 'but', 'dun', 'tell', 'him', 'wait', 'he', 'get', 'complacent']\n",
      "After stop words removal: ['quite', 'lor', 'dun', 'tell', 'wait', 'get', 'complacent']\n",
      "After stemming with porters algorithm: ['quit', 'lor', 'dun', 'tell', 'wait', 'get', 'complac']\n",
      "Tokenized sentence: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'm', 'valid', 'hrs', 'only']\n",
      "After stop words removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
      "After stemming with porters algorithm: ['urgent', 'trying', 'contact', 'todai', 'draw', 'show', 'priz', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
      "Tokenized sentence: ['can', 'help', 'u', 'swoop', 'by', 'picking', 'u', 'up', 'from', 'wherever', 'ur', 'other', 'birds', 'r', 'meeting', 'if', 'u', 'want']\n",
      "After stop words removal: ['help', 'u', 'swoop', 'picking', 'u', 'wherever', 'ur', 'birds', 'r', 'meeting', 'u', 'want']\n",
      "pick\n",
      "meet\n",
      "After stemming with porters algorithm: ['help', 'swoop', 'pic', 'wherev', 'bird', 'meet', 'want']\n",
      "Tokenized sentence: ['naughty', 'little', 'thought', 'its', 'better', 'to', 'flirt', 'flirt', 'n', 'flirt', 'rather', 'than', 'loving', 'someone', 'n', 'gettin', 'hurt', 'hurt', 'n', 'hurt', 'gud', 'nyt']\n",
      "After stop words removal: ['naughty', 'little', 'thought', 'better', 'flirt', 'flirt', 'n', 'flirt', 'rather', 'loving', 'someone', 'n', 'gettin', 'hurt', 'hurt', 'n', 'hurt', 'gud', 'nyt']\n",
      "lov\n",
      "After stemming with porters algorithm: ['naughti', 'littl', 'thought', 'better', 'flirt', 'flirt', 'flirt', 'rather', 'love', 'someon', 'gettin', 'hurt', 'hurt', 'hurt', 'gud', 'nyt']\n",
      "Tokenized sentence: ['lol', 'u', 'still', 'feeling', 'sick']\n",
      "After stop words removal: ['lol', 'u', 'still', 'feeling', 'sick']\n",
      "feel\n",
      "After stemming with porters algorithm: ['lol', 'still', 'feel', 'sick']\n",
      "Tokenized sentence: ['love', 'you', 'aathi', 'love', 'u', 'lot']\n",
      "After stop words removal: ['love', 'aathi', 'love', 'u', 'lot']\n",
      "After stemming with porters algorithm: ['love', 'aathi', 'love', 'lot']\n",
      "Tokenized sentence: ['she', 's', 'fine', 'good', 'to', 'hear', 'from', 'you', 'how', 'are', 'you', 'my', 'dear', 'happy', 'new', 'year', 'oh']\n",
      "After stop words removal: ['fine', 'good', 'hear', 'dear', 'happy', 'new', 'year', 'oh']\n",
      "After stemming with porters algorithm: ['fine', 'good', 'hear', 'dear', 'happi', 'new', 'year']\n",
      "Tokenized sentence: ['wait', 'that', 's', 'still', 'not', 'all', 'that', 'clear', 'were', 'you', 'not', 'sure', 'about', 'me', 'being', 'sarcastic', 'or', 'that', 'that', 's', 'why', 'x', 'doesn', 't', 'want', 'to', 'live', 'with', 'us']\n",
      "After stop words removal: ['wait', 'still', 'clear', 'sure', 'sarcastic', 'x', 'want', 'live', 'us']\n",
      "After stemming with porters algorithm: ['wait', 'still', 'clear', 'sure', 'sarcast', 'want', 'live']\n",
      "Tokenized sentence: ['an', 'amazing', 'quote', 'sometimes', 'in', 'life', 'its', 'difficult', 'to', 'decide', 'whats', 'wrong', 'a', 'lie', 'that', 'brings', 'a', 'smile', 'or', 'the', 'truth', 'that', 'brings', 'a', 'tear']\n",
      "After stop words removal: ['amazing', 'quote', 'sometimes', 'life', 'difficult', 'decide', 'whats', 'wrong', 'lie', 'brings', 'smile', 'truth', 'brings', 'tear']\n",
      "amaz\n",
      "After stemming with porters algorithm: ['amaz', 'quot', 'sometim', 'life', 'difficult', 'decid', 'what', 'wrong', 'lie', 'bring', 'smile', 'truth', 'bring', 'tear']\n",
      "Tokenized sentence: ['all', 'write', 'or', 'wat']\n",
      "After stop words removal: ['write', 'wat']\n",
      "After stemming with porters algorithm: ['write', 'wat']\n",
      "Tokenized sentence: ['how', 'much', 'is', 'torch', 'in', 'ja']\n",
      "After stop words removal: ['much', 'torch', 'ja']\n",
      "After stemming with porters algorithm: ['much', 'torch']\n",
      "Tokenized sentence: ['dear', 'we', 'are', 'going', 'to', 'our', 'rubber', 'place']\n",
      "After stop words removal: ['dear', 'going', 'rubber', 'place']\n",
      "go\n",
      "After stemming with porters algorithm: ['dear', 'go', 'rubber', 'place']\n",
      "Tokenized sentence: ['i', 'm', 'on', 'da', 'bus', 'going', 'home']\n",
      "After stop words removal: ['da', 'bus', 'going', 'home']\n",
      "go\n",
      "After stemming with porters algorithm: ['bu', 'go', 'home']\n",
      "Tokenized sentence: ['aight', 'you', 'close', 'by', 'or', 'still', 'down', 'around', 'alex', 's', 'place']\n",
      "After stop words removal: ['aight', 'close', 'still', 'around', 'alex', 'place']\n",
      "After stemming with porters algorithm: ['aight', 'close', 'still', 'around', 'alex', 'place']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['height', 'of', 'recycling', 'read', 'twice', 'people', 'spend', 'time', 'for', 'earning', 'money', 'and', 'the', 'same', 'money', 'is', 'spent', 'for', 'spending', 'time', 'good', 'morning', 'keep', 'smiling']\n",
      "After stop words removal: ['height', 'recycling', 'read', 'twice', 'people', 'spend', 'time', 'earning', 'money', 'money', 'spent', 'spending', 'time', 'good', 'morning', 'keep', 'smiling']\n",
      "recycl\n",
      "earn\n",
      "spend\n",
      "morn\n",
      "smil\n",
      "After stemming with porters algorithm: ['height', 'recycl', 'read', 'twice', 'peopl', 'spend', 'time', 'ear', 'monei', 'monei', 'spent', 'spen', 'time', 'good', 'mor', 'keep', 'smile']\n",
      "Tokenized sentence: ['want', 'get', 'laid', 'tonight', 'want', 'real', 'dogging', 'locations', 'sent', 'direct', 'ur', 'mob', 'join', 'the', 'uk', 's', 'largest', 'dogging', 'network', 'bt', 'txting', 'gravel', 'to', 'nt', 'ec', 'a', 'p', 'msg', 'p']\n",
      "After stop words removal: ['want', 'get', 'laid', 'tonight', 'want', 'real', 'dogging', 'locations', 'sent', 'direct', 'ur', 'mob', 'join', 'uk', 'largest', 'dogging', 'network', 'bt', 'txting', 'gravel', 'nt', 'ec', 'p', 'msg', 'p']\n",
      "dogg\n",
      "dogg\n",
      "After stemming with porters algorithm: ['want', 'get', 'laid', 'tonight', 'want', 'real', 'dog', 'locat', 'sent', 'direct', 'mob', 'join', 'largest', 'dog', 'network', 'txting', 'gravel', 'msg']\n",
      "Tokenized sentence: ['not', 'yet', 'chikku', 'going', 'to', 'room', 'nw', 'i', 'm', 'in', 'bus']\n",
      "After stop words removal: ['yet', 'chikku', 'going', 'room', 'nw', 'bus']\n",
      "go\n",
      "After stemming with porters algorithm: ['yet', 'chikku', 'go', 'room', 'bu']\n",
      "Tokenized sentence: ['if', 'you', 'still', 'havent', 'collected', 'the', 'dough', 'pls', 'let', 'me', 'know', 'so', 'i', 'can', 'go', 'to', 'the', 'place', 'i', 'sent', 'it', 'to', 'get', 'the', 'control', 'number']\n",
      "After stop words removal: ['still', 'havent', 'collected', 'dough', 'pls', 'let', 'know', 'go', 'place', 'sent', 'get', 'control', 'number']\n",
      "After stemming with porters algorithm: ['still', 'havent', 'collec', 'dough', 'pl', 'let', 'know', 'place', 'sent', 'get', 'control', 'number']\n",
      "Tokenized sentence: ['hi', 'babe', 'its', 'jordan', 'how', 'r', 'u', 'im', 'home', 'from', 'abroad', 'and', 'lonely', 'text', 'me', 'back', 'if', 'u', 'wanna', 'chat', 'xxsp', 'visionsms', 'com', 'text', 'stop', 'to', 'stopcost', 'p']\n",
      "After stop words removal: ['hi', 'babe', 'jordan', 'r', 'u', 'im', 'home', 'abroad', 'lonely', 'text', 'back', 'u', 'wanna', 'chat', 'xxsp', 'visionsms', 'com', 'text', 'stop', 'stopcost', 'p']\n",
      "After stemming with porters algorithm: ['babe', 'jordan', 'home', 'abroad', 'lone', 'text', 'back', 'wanna', 'chat', 'xxsp', 'visionsm', 'com', 'text', 'stop', 'stopcost']\n",
      "Tokenized sentence: ['honey', 'can', 'you', 'pls', 'find', 'out', 'how', 'much', 'they', 'sell', 'predicte', 'in', 'nigeria', 'and', 'how', 'many', 'times', 'can', 'it', 'be', 'used', 'its', 'very', 'important', 'to', 'have', 'a', 'reply', 'before', 'monday']\n",
      "After stop words removal: ['honey', 'pls', 'find', 'much', 'sell', 'predicte', 'nigeria', 'many', 'times', 'used', 'important', 'reply', 'monday']\n",
      "After stemming with porters algorithm: ['honei', 'pl', 'find', 'much', 'sell', 'predict', 'nigeria', 'mani', 'time', 'us', 'import', 'repli', 'mondai']\n",
      "Tokenized sentence: ['i', 'm', 'on', 'my', 'way', 'home', 'went', 'to', 'change', 'batt', 'my', 'watch', 'then', 'go', 'shop', 'a', 'bit', 'lor']\n",
      "After stop words removal: ['way', 'home', 'went', 'change', 'batt', 'watch', 'go', 'shop', 'bit', 'lor']\n",
      "After stemming with porters algorithm: ['wai', 'home', 'went', 'chang', 'batt', 'watch', 'shop', 'bit', 'lor']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'school', 'now', 'n', 'i', 'll', 'be', 'in', 'da', 'lab', 'doing', 'some', 'stuff', 'give', 'me', 'a', 'call', 'when', 'r', 'done']\n",
      "After stop words removal: ['school', 'n', 'da', 'lab', 'stuff', 'give', 'call', 'r', 'done']\n",
      "After stemming with porters algorithm: ['school', 'lab', 'stuff', 'give', 'call', 'done']\n",
      "Tokenized sentence: ['surely', 'result', 'will', 'offer']\n",
      "After stop words removal: ['surely', 'result', 'offer']\n",
      "After stemming with porters algorithm: ['sure', 'result', 'offer']\n",
      "Tokenized sentence: ['somewhr', 'someone', 'is', 'surely', 'made', 'u', 'and', 'god', 'has', 'decided', 'a', 'perfect', 'time', 'to', 'make', 'u', 'meet', 'dat', 'person', 'till', 'den', 'enjoy', 'ur', 'crushes']\n",
      "After stop words removal: ['somewhr', 'someone', 'surely', 'made', 'u', 'god', 'decided', 'perfect', 'time', 'make', 'u', 'meet', 'dat', 'person', 'till', 'den', 'enjoy', 'ur', 'crushes']\n",
      "After stemming with porters algorithm: ['somewhr', 'someon', 'sure', 'made', 'god', 'decid', 'perfect', 'time', 'make', 'meet', 'dat', 'person', 'till', 'den', 'enjoi', 'crush']\n",
      "Tokenized sentence: ['you', 'have', 'been', 'specially', 'selected', 'to', 'receive', 'a', 'pound', 'award', 'call', 'before', 'the', 'lines', 'close', 'cost', 'ppm', 't', 'cs', 'apply', 'ag', 'promo']\n",
      "After stop words removal: ['specially', 'selected', 'receive', 'pound', 'award', 'call', 'lines', 'close', 'cost', 'ppm', 'cs', 'apply', 'ag', 'promo']\n",
      "After stemming with porters algorithm: ['special', 'selec', 'receiv', 'pound', 'award', 'call', 'line', 'close', 'cost', 'ppm', 'appli', 'promo']\n",
      "Tokenized sentence: ['u', 'have', 'won', 'a', 'nokia', 'plus', 'a', 'free', 'digital', 'camera', 'this', 'is', 'what', 'u', 'get', 'when', 'u', 'win', 'our', 'free', 'auction', 'to', 'take', 'part', 'send', 'nokia', 'to', 'now', 'pobox', 'tcr', 'w']\n",
      "After stop words removal: ['u', 'nokia', 'plus', 'free', 'digital', 'camera', 'u', 'get', 'u', 'win', 'free', 'auction', 'take', 'part', 'send', 'nokia', 'pobox', 'tcr', 'w']\n",
      "After stemming with porters algorithm: ['nokia', 'plu', 'free', 'digit', 'camera', 'get', 'win', 'free', 'auct', 'take', 'part', 'send', 'nokia', 'pobox', 'tcr']\n",
      "Tokenized sentence: ['i', 'would', 'but', 'i', 'm', 'still', 'cozy', 'and', 'exhausted', 'from', 'last', 'night', 'nobody', 'went', 'to', 'school', 'or', 'work', 'everything', 'is', 'closed']\n",
      "After stop words removal: ['would', 'still', 'cozy', 'exhausted', 'last', 'night', 'nobody', 'went', 'school', 'work', 'everything', 'closed']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['would', 'still', 'cozi', 'exhaus', 'last', 'night', 'nobodi', 'went', 'school', 'work', 'everyt', 'close']\n",
      "Tokenized sentence: ['just', 'do', 'what', 'ever', 'is', 'easier', 'for', 'you']\n",
      "After stop words removal: ['ever', 'easier']\n",
      "After stemming with porters algorithm: ['ever', 'easier']\n",
      "Tokenized sentence: ['good', 'evening', 'how', 'are', 'you']\n",
      "After stop words removal: ['good', 'evening']\n",
      "even\n",
      "After stemming with porters algorithm: ['good', 'even']\n",
      "Tokenized sentence: ['yar', 'lor', 'he', 'wan', 'go', 'c', 'horse', 'racing', 'today', 'mah', 'so', 'eat', 'earlier', 'lor', 'i', 'ate', 'chicken', 'rice', 'u']\n",
      "After stop words removal: ['yar', 'lor', 'wan', 'go', 'c', 'horse', 'racing', 'today', 'mah', 'eat', 'earlier', 'lor', 'ate', 'chicken', 'rice', 'u']\n",
      "rac\n",
      "After stemming with porters algorithm: ['yar', 'lor', 'wan', 'hors', 'race', 'todai', 'mah', 'eat', 'earlier', 'lor', 'at', 'chicken', 'rice']\n",
      "Tokenized sentence: ['sorry', 'man', 'accidentally', 'left', 'my', 'phone', 'on', 'silent', 'last', 'night', 'and', 'didn', 't', 'check', 'it', 'til', 'i', 'got', 'up']\n",
      "After stop words removal: ['sorry', 'man', 'accidentally', 'left', 'phone', 'silent', 'last', 'night', 'check', 'til', 'got']\n",
      "After stemming with porters algorithm: ['sorri', 'man', 'accid', 'left', 'phone', 'silent', 'last', 'night', 'check', 'til', 'got']\n",
      "Tokenized sentence: ['congrats', 'year', 'special', 'cinema', 'pass', 'for', 'is', 'yours', 'call', 'now', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'all', 'free', 'bx', 'ip', 'we', 'pm', 'dont', 'miss', 'out']\n",
      "After stop words removal: ['congrats', 'year', 'special', 'cinema', 'pass', 'call', 'c', 'suprman', 'v', 'matrix', 'starwars', 'etc', 'free', 'bx', 'ip', 'pm', 'dont', 'miss']\n",
      "After stemming with porters algorithm: ['congrat', 'year', 'special', 'cinema', 'pass', 'call', 'suprman', 'matrix', 'starwar', 'etc', 'free', 'dont', 'miss']\n",
      "Tokenized sentence: ['awesome', 'question', 'with', 'a', 'cute', 'answer', 'someone', 'asked', 'a', 'boy', 'how', 'is', 'ur', 'life', 'he', 'smiled', 'amp', 'answered', 'she', 'is', 'fine', 'gudnite']\n",
      "After stop words removal: ['awesome', 'question', 'cute', 'answer', 'someone', 'asked', 'boy', 'ur', 'life', 'smiled', 'amp', 'answered', 'fine', 'gudnite']\n",
      "After stemming with porters algorithm: ['awesom', 'quest', 'cute', 'answer', 'someon', 'as', 'boi', 'life', 'smile', 'amp', 'answer', 'fine', 'gudnit']\n",
      "Tokenized sentence: ['anyway', 'many', 'good', 'evenings', 'to', 'u', 's']\n",
      "After stop words removal: ['anyway', 'many', 'good', 'evenings', 'u']\n",
      "even\n",
      "After stemming with porters algorithm: ['anywai', 'mani', 'good', 'even']\n",
      "Tokenized sentence: ['my', 'sister', 'cleared', 'two', 'round', 'in', 'birla', 'soft', 'yesterday']\n",
      "After stop words removal: ['sister', 'cleared', 'two', 'round', 'birla', 'soft', 'yesterday']\n",
      "After stemming with porters algorithm: ['sister', 'clear', 'two', 'round', 'birla', 'soft', 'yesterdai']\n",
      "Tokenized sentence: ['not', 'yet', 'chikku', 'wat', 'abt', 'u']\n",
      "After stop words removal: ['yet', 'chikku', 'wat', 'abt', 'u']\n",
      "After stemming with porters algorithm: ['yet', 'chikku', 'wat', 'abt']\n",
      "Tokenized sentence: ['fuck', 'babe', 'i', 'miss', 'you', 'sooooo', 'much', 'i', 'wish', 'you', 'were', 'here', 'to', 'sleep', 'with', 'me', 'my', 'bed', 'is', 'so', 'lonely', 'i', 'go', 'now', 'to', 'sleep', 'to', 'dream', 'of', 'you', 'my', 'love']\n",
      "After stop words removal: ['fuck', 'babe', 'miss', 'sooooo', 'much', 'wish', 'sleep', 'bed', 'lonely', 'go', 'sleep', 'dream', 'love']\n",
      "After stemming with porters algorithm: ['fuck', 'babe', 'miss', 'sooooo', 'much', 'wish', 'sleep', 'bed', 'lone', 'sleep', 'dream', 'love']\n",
      "Tokenized sentence: ['no', 'jst', 'change', 'tat', 'only']\n",
      "After stop words removal: ['jst', 'change', 'tat']\n",
      "After stemming with porters algorithm: ['jst', 'chang', 'tat']\n",
      "Tokenized sentence: ['dear', 'subscriber', 'ur', 'draw', 'gift', 'voucher', 'will', 'b', 'entered', 'on', 'receipt', 'of', 'a', 'correct', 'ans', 'when', 'was', 'elvis', 'presleys', 'birthday', 'txt', 'answer', 'to']\n",
      "After stop words removal: ['dear', 'subscriber', 'ur', 'draw', 'gift', 'voucher', 'b', 'entered', 'receipt', 'correct', 'ans', 'elvis', 'presleys', 'birthday', 'txt', 'answer']\n",
      "After stemming with porters algorithm: ['dear', 'subscrib', 'draw', 'gift', 'voucher', 'enter', 'receipt', 'correct', 'an', 'elvi', 'preslei', 'birthdai', 'txt', 'answer']\n",
      "Tokenized sentence: ['wake', 'me', 'up', 'at', 'lt', 'gt', 'am', 'morning']\n",
      "After stop words removal: ['wake', 'lt', 'gt', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['wake', 'mor']\n",
      "Tokenized sentence: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'a', 'top', 'sony', 'dvd', 'player', 'if', 'u', 'know', 'which', 'country', 'liverpool', 'played', 'in', 'mid', 'week', 'txt', 'ansr', 'to', 'sp', 'tyrone']\n",
      "After stop words removal: ['sunshine', 'quiz', 'wkly', 'q', 'win', 'top', 'sony', 'dvd', 'player', 'u', 'know', 'country', 'liverpool', 'played', 'mid', 'week', 'txt', 'ansr', 'sp', 'tyrone']\n",
      "After stemming with porters algorithm: ['sunshin', 'quiz', 'wkly', 'win', 'top', 'soni', 'dvd', 'player', 'know', 'countri', 'liverpool', 'plai', 'mid', 'week', 'txt', 'ansr', 'tyrone']\n",
      "Tokenized sentence: ['turns', 'out', 'my', 'friends', 'are', 'staying', 'for', 'the', 'whole', 'show', 'and', 'won', 't', 'be', 'back', 'til', 'lt', 'gt', 'so', 'feel', 'free', 'to', 'go', 'ahead', 'and', 'smoke', 'that', 'lt', 'gt', 'worth']\n",
      "After stop words removal: ['turns', 'friends', 'staying', 'whole', 'show', 'back', 'til', 'lt', 'gt', 'feel', 'free', 'go', 'ahead', 'smoke', 'lt', 'gt', 'worth']\n",
      "stay\n",
      "After stemming with porters algorithm: ['turn', 'friend', 'stai', 'whole', 'show', 'back', 'til', 'feel', 'free', 'ahead', 'smoke', 'worth']\n",
      "Tokenized sentence: ['and', 'very', 'importantly', 'all', 'we', 'discuss', 'is', 'between', 'u', 'and', 'i', 'only']\n",
      "After stop words removal: ['importantly', 'discuss', 'u']\n",
      "After stemming with porters algorithm: ['importantli', 'discuss']\n",
      "Tokenized sentence: ['we', 'have', 'pizza', 'if', 'u', 'want']\n",
      "After stop words removal: ['pizza', 'u', 'want']\n",
      "After stemming with porters algorithm: ['pizza', 'want']\n",
      "Tokenized sentence: ['send', 'ur', 'birthdate', 'with', 'month', 'and', 'year', 'i', 'will', 'tel', 'u', 'ur', 'life', 'partner', 's', 'name', 'and', 'the', 'method', 'of', 'calculation', 'reply', 'must']\n",
      "After stop words removal: ['send', 'ur', 'birthdate', 'month', 'year', 'tel', 'u', 'ur', 'life', 'partner', 'name', 'method', 'calculation', 'reply', 'must']\n",
      "After stemming with porters algorithm: ['send', 'birthdat', 'month', 'year', 'tel', 'life', 'partner', 'name', 'method', 'calcul', 'repli', 'must']\n",
      "Tokenized sentence: ['is', 'there', 'coming', 'friday', 'is', 'leave', 'for', 'pongal', 'do', 'you', 'get', 'any', 'news', 'from', 'your', 'work', 'place']\n",
      "After stop words removal: ['coming', 'friday', 'leave', 'pongal', 'get', 'news', 'work', 'place']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'fridai', 'leav', 'pongal', 'get', 'new', 'work', 'place']\n",
      "Tokenized sentence: ['shall', 'i', 'bring', 'us', 'a', 'bottle', 'of', 'wine', 'to', 'keep', 'us', 'amused', 'only', 'joking', 'i', 'll', 'bring', 'one', 'anyway']\n",
      "After stop words removal: ['shall', 'bring', 'us', 'bottle', 'wine', 'keep', 'us', 'amused', 'joking', 'bring', 'one', 'anyway']\n",
      "jok\n",
      "After stemming with porters algorithm: ['shall', 'bring', 'bottl', 'wine', 'keep', 'amus', 'joke', 'bring', 'on', 'anywai']\n",
      "Tokenized sentence: ['wow', 'so', 'healthy', 'old', 'airport', 'rd', 'lor', 'cant', 'thk', 'of', 'anything', 'else', 'but', 'i', 'll', 'b', 'bathing', 'my', 'dog', 'later']\n",
      "After stop words removal: ['wow', 'healthy', 'old', 'airport', 'rd', 'lor', 'cant', 'thk', 'anything', 'else', 'b', 'bathing', 'dog', 'later']\n",
      "anyth\n",
      "bath\n",
      "After stemming with porters algorithm: ['wow', 'healthi', 'old', 'airport', 'lor', 'cant', 'thk', 'anyt', 'els', 'bat', 'dog', 'later']\n",
      "Tokenized sentence: ['ok', 'but', 'bag', 'again']\n",
      "After stop words removal: ['ok', 'bag']\n",
      "After stemming with porters algorithm: ['bag']\n",
      "Tokenized sentence: ['nope', 'juz', 'off', 'from', 'work']\n",
      "After stop words removal: ['nope', 'juz', 'work']\n",
      "After stemming with porters algorithm: ['nope', 'juz', 'work']\n",
      "Tokenized sentence: ['freemsg', 'claim', 'ur', 'sms', 'messages', 'text', 'ok', 'to', 'now', 'use', 'web', 'mobile', 'ur', 'mates', 'etc', 'join', 'txt', 'com', 'for', 'p', 'wk', 't', 'c', 'box', 'la', 'wu', 'remove', 'txtx', 'or', 'stop']\n",
      "After stop words removal: ['freemsg', 'claim', 'ur', 'sms', 'messages', 'text', 'ok', 'use', 'web', 'mobile', 'ur', 'mates', 'etc', 'join', 'txt', 'com', 'p', 'wk', 'c', 'box', 'la', 'wu', 'remove', 'txtx', 'stop']\n",
      "After stemming with porters algorithm: ['freemsg', 'claim', 'sm', 'messag', 'text', 'us', 'web', 'mobil', 'mate', 'etc', 'join', 'txt', 'com', 'box', 'remov', 'txtx', 'stop']\n",
      "Tokenized sentence: ['will', 'you', 'be', 'here', 'for', 'food']\n",
      "After stop words removal: ['food']\n",
      "After stemming with porters algorithm: ['food']\n",
      "Tokenized sentence: ['wonders', 'in', 'my', 'world', 'th', 'you', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'and', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "After stop words removal: ['wonders', 'world', 'th', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "morn\n",
      "After stemming with porters algorithm: ['wonder', 'world', 'style', 'smile', 'person', 'natur', 'sm', 'love', 'friendship', 'good', 'mor', 'dear']\n",
      "Tokenized sentence: ['good', 'morning', 'pookie', 'pie', 'lol', 'hope', 'i', 'didn', 't', 'wake', 'u', 'up']\n",
      "After stop words removal: ['good', 'morning', 'pookie', 'pie', 'lol', 'hope', 'wake', 'u']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'pooki', 'pie', 'lol', 'hope', 'wake']\n",
      "Tokenized sentence: ['my', 'battery', 'is', 'low', 'babe']\n",
      "After stop words removal: ['battery', 'low', 'babe']\n",
      "After stemming with porters algorithm: ['batteri', 'low', 'babe']\n",
      "Tokenized sentence: ['dear', 'we', 'got', 'lt', 'gt', 'dollars', 'hi', 'hi']\n",
      "After stop words removal: ['dear', 'got', 'lt', 'gt', 'dollars', 'hi', 'hi']\n",
      "After stemming with porters algorithm: ['dear', 'got', 'dollar']\n",
      "Tokenized sentence: ['i', 'might', 'go', 'sch', 'yar', 'at', 'e', 'salon', 'now', 'v', 'boring']\n",
      "After stop words removal: ['might', 'go', 'sch', 'yar', 'e', 'salon', 'v', 'boring']\n",
      "bor\n",
      "After stemming with porters algorithm: ['might', 'sch', 'yar', 'salon', 'bore']\n",
      "Tokenized sentence: ['hey', 'check', 'it', 'da', 'i', 'have', 'listed', 'da']\n",
      "After stop words removal: ['hey', 'check', 'da', 'listed', 'da']\n",
      "After stemming with porters algorithm: ['hei', 'check', 'lis']\n",
      "Tokenized sentence: ['ok', 'cool', 'see', 'ya', 'then']\n",
      "After stop words removal: ['ok', 'cool', 'see', 'ya']\n",
      "After stemming with porters algorithm: ['cool', 'see']\n",
      "Tokenized sentence: ['hey', 'i', 'almost', 'forgot', 'happy', 'b', 'day', 'babe', 'i', 'love', 'ya']\n",
      "After stop words removal: ['hey', 'almost', 'forgot', 'happy', 'b', 'day', 'babe', 'love', 'ya']\n",
      "After stemming with porters algorithm: ['hei', 'almost', 'forgot', 'happi', 'dai', 'babe', 'love']\n",
      "Tokenized sentence: ['i', 'want', 'to', 'be', 'inside', 'you', 'every', 'night']\n",
      "After stop words removal: ['want', 'inside', 'every', 'night']\n",
      "After stemming with porters algorithm: ['want', 'insid', 'everi', 'night']\n",
      "Tokenized sentence: ['good', 'morning', 'at', 'the', 'repair', 'shop', 'the', 'only', 'reason', 'i', 'm', 'up', 'at', 'this', 'hour']\n",
      "After stop words removal: ['good', 'morning', 'repair', 'shop', 'reason', 'hour']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'repair', 'shop', 'reason', 'hour']\n",
      "Tokenized sentence: ['mum', 'ask', 'to', 'buy', 'food', 'home']\n",
      "After stop words removal: ['mum', 'ask', 'buy', 'food', 'home']\n",
      "After stemming with porters algorithm: ['mum', 'ask', 'bui', 'food', 'home']\n",
      "Tokenized sentence: ['you', 've', 'won', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tc', 's', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stop words removal: ['costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'sk', 'xh', 'cost', 'pm', 'max', 'mins']\n",
      "After stemming with porters algorithm: ['costa', 'del', 'sol', 'holidai', 'await', 'collect', 'call', 'toclaim', 'sae', 'pobox', 'stockport', 'cost', 'max', 'min']\n",
      "Tokenized sentence: ['pls', 'send', 'me', 'a', 'comprehensive', 'mail', 'about', 'who', 'i', 'm', 'paying', 'when', 'and', 'how', 'much']\n",
      "After stop words removal: ['pls', 'send', 'comprehensive', 'mail', 'paying', 'much']\n",
      "pay\n",
      "After stemming with porters algorithm: ['pl', 'send', 'comprehens', 'mail', 'pai', 'much']\n",
      "Tokenized sentence: ['i', 'm', 'sick', 'i', 'm', 'needy', 'i', 'want', 'you', 'pouts', 'stomps', 'feet', 'where', 'are', 'you', 'pouts', 'stomps', 'feet', 'i', 'want', 'my', 'slave', 'i', 'want', 'him', 'now']\n",
      "After stop words removal: ['sick', 'needy', 'want', 'pouts', 'stomps', 'feet', 'pouts', 'stomps', 'feet', 'want', 'slave', 'want']\n",
      "After stemming with porters algorithm: ['sick', 'needi', 'want', 'pout', 'stomp', 'feet', 'pout', 'stomp', 'feet', 'want', 'slave', 'want']\n",
      "Tokenized sentence: ['s', 'but', 'he', 'had', 'some', 'luck', 'catches', 'put', 'down']\n",
      "After stop words removal: ['luck', 'catches', 'put']\n",
      "After stemming with porters algorithm: ['luck', 'catch', 'put']\n",
      "Tokenized sentence: ['lil', 'fever', 'now', 'fine']\n",
      "After stop words removal: ['lil', 'fever', 'fine']\n",
      "After stemming with porters algorithm: ['lil', 'fever', 'fine']\n",
      "Tokenized sentence: ['thats', 'cool', 'sometimes', 'slow', 'and', 'gentle', 'sonetimes', 'rough', 'and', 'hard']\n",
      "After stop words removal: ['thats', 'cool', 'sometimes', 'slow', 'gentle', 'sonetimes', 'rough', 'hard']\n",
      "After stemming with porters algorithm: ['that', 'cool', 'sometim', 'slow', 'gentl', 'sonetim', 'rough', 'hard']\n",
      "Tokenized sentence: ['two', 'fundamentals', 'of', 'cool', 'life', 'walk']\n",
      "After stop words removal: ['two', 'fundamentals', 'cool', 'life', 'walk']\n",
      "After stemming with porters algorithm: ['two', 'fundam', 'cool', 'life', 'walk']\n",
      "Tokenized sentence: ['t', 'mobile', 'customer', 'you', 'may', 'now', 'claim', 'your', 'free', 'camera', 'phone', 'upgrade', 'a', 'pay', 'go', 'sim', 'card', 'for', 'your', 'loyalty', 'call', 'on', 'offer', 'ends', 'thfeb', 't', 'c', 's', 'apply']\n",
      "After stop words removal: ['mobile', 'customer', 'may', 'claim', 'free', 'camera', 'phone', 'upgrade', 'pay', 'go', 'sim', 'card', 'loyalty', 'call', 'offer', 'ends', 'thfeb', 'c', 'apply']\n",
      "After stemming with porters algorithm: ['mobil', 'custom', 'mai', 'claim', 'free', 'camera', 'phone', 'upgrad', 'pai', 'sim', 'card', 'loyalti', 'call', 'offer', 'end', 'thfeb', 'appli']\n",
      "Tokenized sentence: ['yup', 'it', 's', 'at', 'paragon', 'i', 'havent', 'decided', 'whether', 'cut', 'yet', 'hee']\n",
      "After stop words removal: ['yup', 'paragon', 'havent', 'decided', 'whether', 'cut', 'yet', 'hee']\n",
      "After stemming with porters algorithm: ['yup', 'paragon', 'havent', 'decid', 'whether', 'cut', 'yet', 'hee']\n",
      "Tokenized sentence: ['that', 'said', 'can', 'you', 'text', 'him', 'one', 'more', 'time']\n",
      "After stop words removal: ['said', 'text', 'one', 'time']\n",
      "After stemming with porters algorithm: ['said', 'text', 'on', 'time']\n",
      "Tokenized sentence: ['win', 'a', 'cash', 'prize', 'or', 'a', 'prize', 'worth']\n",
      "After stop words removal: ['win', 'cash', 'prize', 'prize', 'worth']\n",
      "After stemming with porters algorithm: ['win', 'cash', 'priz', 'priz', 'worth']\n",
      "Tokenized sentence: ['todays', 'voda', 'numbers', 'ending', 'are', 'selected', 'to', 'receive', 'a', 'award', 'if', 'you', 'have', 'a', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "After stop words removal: ['todays', 'voda', 'numbers', 'ending', 'selected', 'receive', 'award', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
      "end\n",
      "quot\n",
      "After stemming with porters algorithm: ['todai', 'voda', 'number', 'en', 'selec', 'receiv', 'award', 'match', 'pleas', 'call', 'quot', 'claim', 'code', 'standard', 'rate', 'app']\n",
      "Tokenized sentence: ['think', 'da', 'you', 'wil', 'do']\n",
      "After stop words removal: ['think', 'da', 'wil']\n",
      "After stemming with porters algorithm: ['think', 'wil']\n",
      "Tokenized sentence: ['do', 'you', 'like', 'shaking', 'your', 'booty', 'on', 'the', 'dance', 'floor']\n",
      "After stop words removal: ['like', 'shaking', 'booty', 'dance', 'floor']\n",
      "shak\n",
      "After stemming with porters algorithm: ['like', 'shake', 'booti', 'danc', 'floor']\n",
      "Tokenized sentence: ['goodnight', 'da', 'thangam', 'i', 'really', 'miss', 'u', 'dear']\n",
      "After stop words removal: ['goodnight', 'da', 'thangam', 'really', 'miss', 'u', 'dear']\n",
      "After stemming with porters algorithm: ['goodnight', 'thangam', 'realli', 'miss', 'dear']\n",
      "Tokenized sentence: ['me', 'not', 'waking', 'up', 'until', 'in', 'the', 'afternoon', 'sup']\n",
      "After stop words removal: ['waking', 'afternoon', 'sup']\n",
      "wak\n",
      "After stemming with porters algorithm: ['wake', 'afternoon', 'sup']\n",
      "Tokenized sentence: ['loan', 'for', 'any', 'purpose', 'homeowners', 'tenants', 'welcome', 'have', 'you', 'been', 'previously', 'refused', 'we', 'can', 'still', 'help', 'call', 'free', 'or', 'text', 'back', 'help']\n",
      "After stop words removal: ['loan', 'purpose', 'homeowners', 'tenants', 'welcome', 'previously', 'refused', 'still', 'help', 'call', 'free', 'text', 'back', 'help']\n",
      "After stemming with porters algorithm: ['loan', 'purpos', 'homeown', 'tenant', 'welcom', 'previous', 'refus', 'still', 'help', 'call', 'free', 'text', 'back', 'help']\n",
      "Tokenized sentence: ['dear', 'sir', 'salam', 'alaikkum', 'pride', 'and', 'pleasure', 'meeting', 'you', 'today', 'at', 'the', 'tea', 'shop', 'we', 'are', 'pleased', 'to', 'send', 'you', 'our', 'contact', 'number', 'at', 'qatar', 'rakhesh', 'an', 'indian', 'pls', 'save', 'our', 'number', 'respectful', 'regards']\n",
      "After stop words removal: ['dear', 'sir', 'salam', 'alaikkum', 'pride', 'pleasure', 'meeting', 'today', 'tea', 'shop', 'pleased', 'send', 'contact', 'number', 'qatar', 'rakhesh', 'indian', 'pls', 'save', 'number', 'respectful', 'regards']\n",
      "meet\n",
      "After stemming with porters algorithm: ['dear', 'sir', 'salam', 'alaikkum', 'pride', 'pleasur', 'meet', 'todai', 'tea', 'shop', 'pleas', 'send', 'contact', 'number', 'qatar', 'rakhesh', 'indian', 'pl', 'save', 'number', 'respect', 'regard']\n",
      "Tokenized sentence: ['only', 'saturday', 'and', 'sunday', 'holiday', 'so', 'its', 'very', 'difficult']\n",
      "After stop words removal: ['saturday', 'sunday', 'holiday', 'difficult']\n",
      "After stemming with porters algorithm: ['saturdai', 'sundai', 'holidai', 'difficult']\n",
      "Tokenized sentence: ['imagine', 'you', 'finally', 'get', 'to', 'sink', 'into', 'that', 'bath', 'after', 'i', 'have', 'put', 'you', 'through', 'your', 'paces', 'maybe', 'even', 'having', 'you', 'eat', 'me', 'for', 'a', 'while', 'before', 'i', 'left', 'but', 'also', 'imagine', 'the', 'feel', 'of', 'that', 'cage', 'on', 'your', 'cock', 'surrounded', 'by', 'the', 'bath', 'water', 'reminding', 'you', 'always', 'who', 'owns', 'you', 'enjoy', 'my', 'cuck']\n",
      "After stop words removal: ['imagine', 'finally', 'get', 'sink', 'bath', 'put', 'paces', 'maybe', 'even', 'eat', 'left', 'also', 'imagine', 'feel', 'cage', 'cock', 'surrounded', 'bath', 'water', 'reminding', 'always', 'owns', 'enjoy', 'cuck']\n",
      "remind\n",
      "After stemming with porters algorithm: ['imagin', 'final', 'get', 'sink', 'bath', 'put', 'pace', 'mayb', 'even', 'eat', 'left', 'also', 'imagin', 'feel', 'cage', 'cock', 'surroun', 'bath', 'water', 'remin', 'alwai', 'own', 'enjoi', 'cuck']\n",
      "Tokenized sentence: ['i', 'think', 'your', 'mentor', 'is', 'but', 'not', 'percent', 'sure']\n",
      "After stop words removal: ['think', 'mentor', 'percent', 'sure']\n",
      "After stemming with porters algorithm: ['think', 'mentor', 'percent', 'sure']\n",
      "Tokenized sentence: ['sexy', 'sexy', 'cum', 'and', 'text', 'me', 'im', 'wet', 'and', 'warm', 'and', 'ready', 'for', 'some', 'porn', 'u', 'up', 'for', 'some', 'fun', 'this', 'msg', 'is', 'free', 'recd', 'msgs', 'p', 'inc', 'vat', 'cancel', 'text', 'stop']\n",
      "After stop words removal: ['sexy', 'sexy', 'cum', 'text', 'im', 'wet', 'warm', 'ready', 'porn', 'u', 'fun', 'msg', 'free', 'recd', 'msgs', 'p', 'inc', 'vat', 'cancel', 'text', 'stop']\n",
      "After stemming with porters algorithm: ['sexi', 'sexi', 'cum', 'text', 'wet', 'warm', 'readi', 'porn', 'fun', 'msg', 'free', 'recd', 'msg', 'inc', 'vat', 'cancel', 'text', 'stop']\n",
      "Tokenized sentence: ['oh', 'kay', 'on', 'sat', 'right']\n",
      "After stop words removal: ['oh', 'kay', 'sat', 'right']\n",
      "After stemming with porters algorithm: ['kai', 'sat', 'right']\n",
      "Tokenized sentence: ['not', 'yet', 'chikku', 'k', 'then', 'wat', 'abt', 'tht', 'guy', 'did', 'he', 'stopped', 'irritating', 'or', 'msging', 'to', 'u']\n",
      "After stop words removal: ['yet', 'chikku', 'k', 'wat', 'abt', 'tht', 'guy', 'stopped', 'irritating', 'msging', 'u']\n",
      "irritat\n",
      "irritate\n",
      "After stemming with porters algorithm: ['yet', 'chikku', 'wat', 'abt', 'tht', 'gui', 'stop', 'irrit', 'msging']\n",
      "Tokenized sentence: ['well', 'welp', 'is', 'sort', 'of', 'a', 'semiobscure', 'internet', 'thing']\n",
      "After stop words removal: ['well', 'welp', 'sort', 'semiobscure', 'internet', 'thing']\n",
      "After stemming with porters algorithm: ['well', 'welp', 'sort', 'semiobscur', 'internet', 'thing']\n",
      "Tokenized sentence: ['lol', 'well', 'don', 't', 'do', 'it', 'without', 'me', 'we', 'could', 'have', 'a', 'big', 'sale', 'together']\n",
      "After stop words removal: ['lol', 'well', 'without', 'could', 'big', 'sale', 'together']\n",
      "After stemming with porters algorithm: ['lol', 'well', 'without', 'could', 'big', 'sale', 'togeth']\n",
      "Tokenized sentence: ['customer', 'service', 'annoncement', 'you', 'have', 'a', 'new', 'years', 'delivery', 'waiting', 'for', 'you', 'please', 'call', 'now', 'to', 'arrange', 'delivery']\n",
      "After stop words removal: ['customer', 'service', 'annoncement', 'new', 'years', 'delivery', 'waiting', 'please', 'call', 'arrange', 'delivery']\n",
      "wait\n",
      "After stemming with porters algorithm: ['custom', 'servic', 'annonc', 'new', 'year', 'deliveri', 'wait', 'pleas', 'call', 'arrang', 'deliveri']\n",
      "Tokenized sentence: ['then', 'she', 'buying', 'today', 'no', 'need', 'to', 'c', 'meh']\n",
      "After stop words removal: ['buying', 'today', 'need', 'c', 'meh']\n",
      "buy\n",
      "After stemming with porters algorithm: ['bui', 'todai', 'need', 'meh']\n",
      "Tokenized sentence: ['how', 'i', 'noe', 'did', 'specify', 'da', 'domain', 'as', 'nusstu', 'still', 'in', 'sch']\n",
      "After stop words removal: ['noe', 'specify', 'da', 'domain', 'nusstu', 'still', 'sch']\n",
      "After stemming with porters algorithm: ['noe', 'specifi', 'domain', 'nusstu', 'still', 'sch']\n",
      "Tokenized sentence: ['you', 'also', 'didnt', 'get', 'na', 'hi', 'hi', 'hi', 'hi', 'hi']\n",
      "After stop words removal: ['also', 'didnt', 'get', 'na', 'hi', 'hi', 'hi', 'hi', 'hi']\n",
      "After stemming with porters algorithm: ['also', 'didnt', 'get']\n",
      "Tokenized sentence: ['i', 'tot', 'u', 'outside', 'cos', 'darren', 'say', 'u', 'come', 'shopping', 'of', 'course', 'we', 'nice', 'wat', 'we', 'jus', 'went', 'sim', 'lim', 'look', 'at', 'mp', 'player']\n",
      "After stop words removal: ['tot', 'u', 'outside', 'cos', 'darren', 'say', 'u', 'come', 'shopping', 'course', 'nice', 'wat', 'jus', 'went', 'sim', 'lim', 'look', 'mp', 'player']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['tot', 'outsid', 'co', 'darren', 'sai', 'come', 'shop', 'cours', 'nice', 'wat', 'ju', 'went', 'sim', 'lim', 'look', 'player']\n",
      "Tokenized sentence: ['god', 'picked', 'up', 'a', 'flower', 'and', 'dippeditinadew', 'lovingly', 'touched', 'itwhichturnedinto', 'u', 'and', 'the', 'he', 'gifted', 'tomeandsaid', 'this', 'friend', 'is', 'u']\n",
      "After stop words removal: ['god', 'picked', 'flower', 'dippeditinadew', 'lovingly', 'touched', 'itwhichturnedinto', 'u', 'gifted', 'tomeandsaid', 'friend', 'u']\n",
      "After stemming with porters algorithm: ['god', 'pic', 'flower', 'dippeditinadew', 'lovingli', 'touc', 'itwhichturnedinto', 'gif', 'tomeandsaid', 'friend']\n",
      "Tokenized sentence: ['dude', 'just', 'saw', 'a', 'parked', 'car', 'with', 'its', 'sunroof', 'popped', 'up', 'sux']\n",
      "After stop words removal: ['dude', 'saw', 'parked', 'car', 'sunroof', 'popped', 'sux']\n",
      "After stemming with porters algorithm: ['dude', 'saw', 'par', 'car', 'sunroof', 'pop', 'sux']\n",
      "Tokenized sentence: ['k', 'still', 'are', 'you', 'loving', 'me']\n",
      "After stop words removal: ['k', 'still', 'loving']\n",
      "lov\n",
      "After stemming with porters algorithm: ['still', 'love']\n",
      "Tokenized sentence: ['yeah', 'probably', 'earlier', 'than', 'that']\n",
      "After stop words removal: ['yeah', 'probably', 'earlier']\n",
      "After stemming with porters algorithm: ['yeah', 'probab', 'earlier']\n",
      "Tokenized sentence: ['you', 've', 'always', 'been', 'the', 'brainy', 'one']\n",
      "After stop words removal: ['always', 'brainy', 'one']\n",
      "After stemming with porters algorithm: ['alwai', 'braini', 'on']\n",
      "Tokenized sentence: ['free', 'entry', 'into', 'our', 'weekly', 'comp', 'just', 'send', 'the', 'word', 'enter', 'to', 'now', 't', 'c', 'www', 'textcomp', 'com', 'cust', 'care']\n",
      "After stop words removal: ['free', 'entry', 'weekly', 'comp', 'send', 'word', 'enter', 'c', 'www', 'textcomp', 'com', 'cust', 'care']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'weekli', 'comp', 'send', 'word', 'enter', 'www', 'textcomp', 'com', 'cust', 'care']\n",
      "Tokenized sentence: ['aiyar', 'dun', 'disturb', 'u', 'liao', 'thk', 'u', 'have', 'lots', 'do', 'aft', 'ur', 'cupboard', 'come']\n",
      "After stop words removal: ['aiyar', 'dun', 'disturb', 'u', 'liao', 'thk', 'u', 'lots', 'aft', 'ur', 'cupboard', 'come']\n",
      "After stemming with porters algorithm: ['aiyar', 'dun', 'disturb', 'liao', 'thk', 'lot', 'aft', 'cupboard', 'come']\n",
      "Tokenized sentence: ['at', 'home', 'by', 'the', 'way']\n",
      "After stop words removal: ['home', 'way']\n",
      "After stemming with porters algorithm: ['home', 'wai']\n",
      "Tokenized sentence: ['i', 'dont', 'thnk', 'its', 'a', 'wrong', 'calling', 'between', 'us']\n",
      "After stop words removal: ['dont', 'thnk', 'wrong', 'calling', 'us']\n",
      "call\n",
      "After stemming with porters algorithm: ['dont', 'thnk', 'wrong', 'call']\n",
      "Tokenized sentence: ['well', 'at', 'this', 'right', 'i', 'm', 'gonna', 'have', 'to', 'get', 'up', 'and', 'check', 'today', 's', 'steam', 'sales', 'pee', 'so', 'text', 'me', 'when', 'you', 'want', 'me', 'to', 'come', 'get', 'you']\n",
      "After stop words removal: ['well', 'right', 'gonna', 'get', 'check', 'today', 'steam', 'sales', 'pee', 'text', 'want', 'come', 'get']\n",
      "After stemming with porters algorithm: ['well', 'right', 'gonna', 'get', 'check', 'todai', 'steam', 'sale', 'pee', 'text', 'want', 'come', 'get']\n",
      "Tokenized sentence: ['thanx', 'day', 'u', 'r', 'a', 'goodmate', 'i', 'think', 'ur', 'rite', 'sary', 'asusual', 'u', 'cheered', 'me', 'up', 'love', 'u', 'franyxxxxx']\n",
      "After stop words removal: ['thanx', 'day', 'u', 'r', 'goodmate', 'think', 'ur', 'rite', 'sary', 'asusual', 'u', 'cheered', 'love', 'u', 'franyxxxxx']\n",
      "After stemming with porters algorithm: ['thanx', 'dai', 'goodmat', 'think', 'rite', 'sari', 'asusu', 'cheer', 'love', 'franyxxxxx']\n",
      "Tokenized sentence: ['eek', 'that', 's', 'a', 'lot', 'of', 'time', 'especially', 'since', 'american', 'pie', 'is', 'like', 'minutes', 'long', 'i', 'can', 't', 'stop', 'singing', 'it']\n",
      "After stop words removal: ['eek', 'lot', 'time', 'especially', 'since', 'american', 'pie', 'like', 'minutes', 'long', 'stop', 'singing']\n",
      "sing\n",
      "After stemming with porters algorithm: ['eek', 'lot', 'time', 'especi', 'sinc', 'american', 'pie', 'like', 'minut', 'long', 'stop', 'sin']\n",
      "Tokenized sentence: ['i', 'm', 'really', 'sorry', 'i', 'lit', 'your', 'hair', 'on', 'fire']\n",
      "After stop words removal: ['really', 'sorry', 'lit', 'hair', 'fire']\n",
      "After stemming with porters algorithm: ['realli', 'sorri', 'lit', 'hair', 'fire']\n",
      "Tokenized sentence: ['get', 'the', 'official', 'england', 'poly', 'ringtone', 'or', 'colour', 'flag', 'on', 'yer', 'mobile', 'for', 'tonights', 'game', 'text', 'tone', 'or', 'flag', 'to', 'optout', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stop words removal: ['get', 'official', 'england', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'tonights', 'game', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stemming with porters algorithm: ['get', 'offici', 'england', 'poli', 'rington', 'colour', 'flag', 'yer', 'mobil', 'tonight', 'game', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box']\n",
      "Tokenized sentence: ['pls', 'come', 'quick', 'cant', 'bare', 'this']\n",
      "After stop words removal: ['pls', 'come', 'quick', 'cant', 'bare']\n",
      "After stemming with porters algorithm: ['pl', 'come', 'quick', 'cant', 'bare']\n",
      "Tokenized sentence: ['your', 'right', 'i', 'll', 'make', 'the', 'appointment', 'right', 'now']\n",
      "After stop words removal: ['right', 'make', 'appointment', 'right']\n",
      "After stemming with porters algorithm: ['right', 'make', 'appoint', 'right']\n",
      "Tokenized sentence: ['sms', 'ac', 'sptv', 'the', 'new', 'jersey', 'devils', 'and', 'the', 'detroit', 'red', 'wings', 'play', 'ice', 'hockey', 'correct', 'or', 'incorrect', 'end', 'reply', 'end', 'sptv']\n",
      "After stop words removal: ['sms', 'ac', 'sptv', 'new', 'jersey', 'devils', 'detroit', 'red', 'wings', 'play', 'ice', 'hockey', 'correct', 'incorrect', 'end', 'reply', 'end', 'sptv']\n",
      "After stemming with porters algorithm: ['sm', 'sptv', 'new', 'jersei', 'devil', 'detroit', 'red', 'wing', 'plai', 'ic', 'hockei', 'correct', 'incorrect', 'end', 'repli', 'end', 'sptv']\n",
      "Tokenized sentence: ['this', 'weeks', 'savamob', 'member', 'offers', 'are', 'now', 'accessible', 'just', 'call', 'for', 'details', 'savamob', 'pobox', 'la', 'wu', 'only', 'week', 'savamob', 'offers', 'mobile']\n",
      "After stop words removal: ['weeks', 'savamob', 'member', 'offers', 'accessible', 'call', 'details', 'savamob', 'pobox', 'la', 'wu', 'week', 'savamob', 'offers', 'mobile']\n",
      "After stemming with porters algorithm: ['week', 'savamob', 'member', 'offer', 'access', 'call', 'detail', 'savamob', 'pobox', 'week', 'savamob', 'offer', 'mobil']\n",
      "Tokenized sentence: ['ur', 'cash', 'balance', 'is', 'currently', 'pounds', 'to', 'maximize', 'ur', 'cash', 'in', 'now', 'send', 'cash', 'to', 'only', 'p', 'msg', 'cc', 'po', 'box', 'tcr', 'w']\n",
      "After stop words removal: ['ur', 'cash', 'balance', 'currently', 'pounds', 'maximize', 'ur', 'cash', 'send', 'cash', 'p', 'msg', 'cc', 'po', 'box', 'tcr', 'w']\n",
      "After stemming with porters algorithm: ['cash', 'balanc', 'current', 'pound', 'maxim', 'cash', 'send', 'cash', 'msg', 'box', 'tcr']\n",
      "Tokenized sentence: ['maybe', 'say', 'hi', 'to', 'and', 'find', 'out', 'if', 'got', 'his', 'card', 'great', 'escape', 'or', 'wetherspoons']\n",
      "After stop words removal: ['maybe', 'say', 'hi', 'find', 'got', 'card', 'great', 'escape', 'wetherspoons']\n",
      "After stemming with porters algorithm: ['mayb', 'sai', 'find', 'got', 'card', 'great', 'escap', 'wetherspoon']\n",
      "Tokenized sentence: ['sms', 'auction', 'you', 'have', 'won', 'a', 'nokia', 'i', 'this', 'is', 'what', 'you', 'get', 'when', 'you', 'win', 'our', 'free', 'auction', 'to', 'take', 'part', 'send', 'nokia', 'to', 'now', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stop words removal: ['sms', 'auction', 'nokia', 'get', 'win', 'free', 'auction', 'take', 'part', 'send', 'nokia', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stemming with porters algorithm: ['sm', 'auct', 'nokia', 'get', 'win', 'free', 'auct', 'take', 'part', 'send', 'nokia', 'suit', 'land', 'row', 'jhl']\n",
      "Tokenized sentence: ['hmm', 'my', 'uncle', 'just', 'informed', 'me', 'that', 'he', 's', 'paying', 'the', 'school', 'directly', 'so', 'pls', 'buy', 'food']\n",
      "After stop words removal: ['hmm', 'uncle', 'informed', 'paying', 'school', 'directly', 'pls', 'buy', 'food']\n",
      "pay\n",
      "After stemming with porters algorithm: ['hmm', 'uncl', 'infor', 'pai', 'school', 'directli', 'pl', 'bui', 'food']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'contact', 'you', 're', 'your', 'reply', 'to', 'our', 'offer', 'of', 'a', 'video', 'handset', 'anytime', 'networks', 'mins', 'unlimited', 'text', 'camcorder', 'reply', 'or', 'call', 'now']\n",
      "After stop words removal: ['tried', 'contact', 'reply', 'offer', 'video', 'handset', 'anytime', 'networks', 'mins', 'unlimited', 'text', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['tri', 'contact', 'repli', 'offer', 'video', 'handset', 'anytim', 'network', 'min', 'unlimit', 'text', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['if', 'you', 'ask', 'her', 'or', 'she', 'say', 'any', 'please', 'message']\n",
      "After stop words removal: ['ask', 'say', 'please', 'message']\n",
      "After stemming with porters algorithm: ['ask', 'sai', 'pleas', 'messag']\n",
      "Tokenized sentence: ['not', 'heard', 'from', 'u', 'a', 'while', 'call', 'rude', 'chat', 'private', 'line', 'to', 'cum', 'wan', 'c', 'pics', 'of', 'me', 'gettin', 'shagged', 'then', 'text', 'pix', 'to', 'end', 'send', 'stop', 'sam', 'xxx']\n",
      "After stop words removal: ['heard', 'u', 'call', 'rude', 'chat', 'private', 'line', 'cum', 'wan', 'c', 'pics', 'gettin', 'shagged', 'text', 'pix', 'end', 'send', 'stop', 'sam', 'xxx']\n",
      "After stemming with porters algorithm: ['heard', 'call', 'rude', 'chat', 'privat', 'line', 'cum', 'wan', 'pic', 'gettin', 'shag', 'text', 'pix', 'end', 'send', 'stop', 'sam', 'xxx']\n",
      "Tokenized sentence: ['er', 'hello', 'things', 'didn', 't', 'quite', 'go', 'to', 'plan', 'is', 'limping', 'slowly', 'home', 'followed', 'by', 'aa', 'and', 'with', 'exhaust', 'hanging', 'off']\n",
      "After stop words removal: ['er', 'hello', 'things', 'quite', 'go', 'plan', 'limping', 'slowly', 'home', 'followed', 'aa', 'exhaust', 'hanging']\n",
      "limp\n",
      "hang\n",
      "After stemming with porters algorithm: ['hello', 'thing', 'quit', 'plan', 'lim', 'slowli', 'home', 'follow', 'exhaust', 'han']\n",
      "Tokenized sentence: ['but', 'you', 'dint', 'in', 'touch', 'with', 'me']\n",
      "After stop words removal: ['dint', 'touch']\n",
      "After stemming with porters algorithm: ['dint', 'touch']\n",
      "Tokenized sentence: ['it', 's', 'ok', 'lar', 'u', 'sleep', 'early', 'too', 'nite']\n",
      "After stop words removal: ['ok', 'lar', 'u', 'sleep', 'early', 'nite']\n",
      "After stemming with porters algorithm: ['lar', 'sleep', 'earli', 'nite']\n",
      "Tokenized sentence: ['damn', 'poor', 'zac', 'doesn', 't', 'stand', 'a', 'chance']\n",
      "After stop words removal: ['damn', 'poor', 'zac', 'stand', 'chance']\n",
      "After stemming with porters algorithm: ['damn', 'poor', 'zac', 'stand', 'chanc']\n",
      "Tokenized sentence: ['yup', 'having', 'my', 'lunch', 'buffet', 'now', 'u', 'eat', 'already']\n",
      "After stop words removal: ['yup', 'lunch', 'buffet', 'u', 'eat', 'already']\n",
      "After stemming with porters algorithm: ['yup', 'lunch', 'buffet', 'eat', 'alreadi']\n",
      "Tokenized sentence: ['shit', 'that', 'is', 'really', 'shocking', 'and', 'scary', 'cant', 'imagine', 'for', 'a', 'second', 'def', 'up', 'for', 'night', 'out', 'do', 'u', 'think', 'there', 'is', 'somewhere', 'i', 'could', 'crash', 'for', 'night', 'save', 'on', 'taxi']\n",
      "After stop words removal: ['shit', 'really', 'shocking', 'scary', 'cant', 'imagine', 'second', 'def', 'night', 'u', 'think', 'somewhere', 'could', 'crash', 'night', 'save', 'taxi']\n",
      "shock\n",
      "After stemming with porters algorithm: ['shit', 'realli', 'shoc', 'scari', 'cant', 'imagin', 'second', 'def', 'night', 'think', 'somewher', 'could', 'crash', 'night', 'save', 'taxi']\n",
      "Tokenized sentence: ['our', 'records', 'indicate', 'u', 'maybe', 'entitled', 'to', 'pounds', 'in', 'compensation', 'for', 'the', 'accident', 'you', 'had', 'to', 'claim', 'free', 'reply', 'with', 'claim', 'to', 'this', 'msg', 'stop', 'txt', 'stop']\n",
      "After stop words removal: ['records', 'indicate', 'u', 'maybe', 'entitled', 'pounds', 'compensation', 'accident', 'claim', 'free', 'reply', 'claim', 'msg', 'stop', 'txt', 'stop']\n",
      "After stemming with porters algorithm: ['record', 'indic', 'mayb', 'entit', 'pound', 'compens', 'accid', 'claim', 'free', 'repli', 'claim', 'msg', 'stop', 'txt', 'stop']\n",
      "Tokenized sentence: ['evry', 'emotion', 'dsn', 't', 'hav', 'words', 'evry', 'wish', 'dsn', 't', 'hav', 'prayrs', 'if', 'u', 'smile', 'd', 'world', 'is', 'wit', 'u', 'othrwise', 'even', 'd', 'drop', 'of', 'tear', 'dsn', 't', 'lik', 'stay', 'wit', 'u', 'so', 'b', 'happy', 'good', 'morning', 'keep', 'smiling']\n",
      "After stop words removal: ['evry', 'emotion', 'dsn', 'hav', 'words', 'evry', 'wish', 'dsn', 'hav', 'prayrs', 'u', 'smile', 'world', 'wit', 'u', 'othrwise', 'even', 'drop', 'tear', 'dsn', 'lik', 'stay', 'wit', 'u', 'b', 'happy', 'good', 'morning', 'keep', 'smiling']\n",
      "morn\n",
      "smil\n",
      "After stemming with porters algorithm: ['evri', 'emot', 'dsn', 'hav', 'word', 'evri', 'wish', 'dsn', 'hav', 'prayr', 'smile', 'world', 'wit', 'othrwis', 'even', 'drop', 'tear', 'dsn', 'lik', 'stai', 'wit', 'happi', 'good', 'mor', 'keep', 'smile']\n",
      "Tokenized sentence: ['v', 'nice', 'off', 'sheffield', 'tom', 'air', 'my', 'opinions', 'on', 'categories', 'b', 'used', 'measure', 'ethnicity', 'in', 'next', 'census', 'busy', 'transcribing']\n",
      "After stop words removal: ['v', 'nice', 'sheffield', 'tom', 'air', 'opinions', 'categories', 'b', 'used', 'measure', 'ethnicity', 'next', 'census', 'busy', 'transcribing']\n",
      "transcrib\n",
      "After stemming with porters algorithm: ['nice', 'sheffield', 'tom', 'air', 'opinion', 'categori', 'us', 'measur', 'ethnic', 'next', 'censu', 'busi', 'transcrib']\n",
      "Tokenized sentence: ['but', 'if', 'she', 's', 'drinkin', 'i', 'm', 'ok']\n",
      "After stop words removal: ['drinkin', 'ok']\n",
      "After stemming with porters algorithm: ['drinkin']\n",
      "Tokenized sentence: ['ok', 'enjoy', 'r', 'u', 'there', 'in', 'home']\n",
      "After stop words removal: ['ok', 'enjoy', 'r', 'u', 'home']\n",
      "After stemming with porters algorithm: ['enjoi', 'home']\n",
      "Tokenized sentence: ['ever', 'green', 'quote', 'ever', 'told', 'by', 'jerry', 'in', 'cartoon', 'a', 'person', 'who', 'irritates', 'u', 'always', 'is', 'the', 'one', 'who', 'loves', 'u', 'vry', 'much', 'but', 'fails', 'to', 'express', 'it', 'gud', 'nyt']\n",
      "After stop words removal: ['ever', 'green', 'quote', 'ever', 'told', 'jerry', 'cartoon', 'person', 'irritates', 'u', 'always', 'one', 'loves', 'u', 'vry', 'much', 'fails', 'express', 'gud', 'nyt']\n",
      "After stemming with porters algorithm: ['ever', 'green', 'quot', 'ever', 'told', 'jerri', 'cartoon', 'person', 'irrit', 'alwai', 'on', 'love', 'vry', 'much', 'fail', 'express', 'gud', 'nyt']\n",
      "Tokenized sentence: ['i', 'dont', 'understand', 'your', 'message']\n",
      "After stop words removal: ['dont', 'understand', 'message']\n",
      "After stemming with porters algorithm: ['dont', 'understand', 'messag']\n",
      "Tokenized sentence: ['so', 'dont', 'use', 'hook', 'up', 'any', 'how']\n",
      "After stop words removal: ['dont', 'use', 'hook']\n",
      "After stemming with porters algorithm: ['dont', 'us', 'hook']\n",
      "Tokenized sentence: ['hey', 'babe', 'far', 'spun', 'out', 'spk', 'at', 'da', 'mo', 'dead', 'da', 'wrld', 'been', 'sleeping', 'on', 'da', 'sofa', 'all', 'day']\n",
      "After stop words removal: ['hey', 'babe', 'far', 'spun', 'spk', 'da', 'mo', 'dead', 'da', 'wrld', 'sleeping', 'da', 'sofa', 'day']\n",
      "sleep\n",
      "After stemming with porters algorithm: ['hei', 'babe', 'far', 'spun', 'spk', 'dead', 'wrld', 'sleep', 'sofa', 'dai']\n",
      "Tokenized sentence: ['carlos', 'took', 'a', 'while', 'again', 'we', 'leave', 'in', 'a', 'minute']\n",
      "After stop words removal: ['carlos', 'took', 'leave', 'minute']\n",
      "After stemming with porters algorithm: ['carlo', 'took', 'leav', 'minut']\n",
      "Tokenized sentence: ['fighting', 'with', 'the', 'world', 'is', 'easy', 'u', 'either', 'win', 'or', 'lose', 'bt', 'fightng', 'with', 'some', 'who', 'is', 'close', 'to', 'u', 'is', 'dificult', 'if', 'u', 'lose', 'u', 'lose', 'if', 'u', 'win', 'u', 'still', 'lose']\n",
      "After stop words removal: ['fighting', 'world', 'easy', 'u', 'either', 'win', 'lose', 'bt', 'fightng', 'close', 'u', 'dificult', 'u', 'lose', 'u', 'lose', 'u', 'win', 'u', 'still', 'lose']\n",
      "fight\n",
      "After stemming with porters algorithm: ['figh', 'world', 'easi', 'either', 'win', 'lose', 'fightng', 'close', 'dificult', 'lose', 'lose', 'win', 'still', 'lose']\n",
      "Tokenized sentence: ['you', 'are', 'everywhere', 'dirt', 'on', 'the', 'floor', 'the', 'windows', 'even', 'on', 'my', 'shirt', 'and', 'sometimes', 'when', 'i', 'open', 'my', 'mouth', 'you', 'are', 'all', 'that', 'comes', 'flowing', 'out', 'i', 'dream', 'of', 'my', 'world', 'without', 'you', 'then', 'half', 'my', 'chores', 'are', 'out', 'too', 'a', 'time', 'of', 'joy', 'for', 'me', 'lots', 'of', 'tv', 'shows', 'i', 'll', 'see', 'but', 'i', 'guess', 'like', 'all', 'things', 'you', 'just', 'must', 'exist', 'like', 'rain', 'hail', 'and', 'mist', 'and', 'when', 'my', 'time', 'here', 'is', 'done', 'you', 'and', 'i', 'become', 'one']\n",
      "After stop words removal: ['everywhere', 'dirt', 'floor', 'windows', 'even', 'shirt', 'sometimes', 'open', 'mouth', 'comes', 'flowing', 'dream', 'world', 'without', 'half', 'chores', 'time', 'joy', 'lots', 'tv', 'shows', 'see', 'guess', 'like', 'things', 'must', 'exist', 'like', 'rain', 'hail', 'mist', 'time', 'done', 'become', 'one']\n",
      "flow\n",
      "After stemming with porters algorithm: ['everywher', 'dirt', 'floor', 'window', 'even', 'shirt', 'sometim', 'open', 'mouth', 'come', 'flowe', 'dream', 'world', 'without', 'half', 'chore', 'time', 'joi', 'lot', 'show', 'see', 'guess', 'like', 'thing', 'must', 'exist', 'like', 'rain', 'hail', 'mist', 'time', 'done', 'becom', 'on']\n",
      "Tokenized sentence: ['for', 'ur', 'chance', 'to', 'win', 'a', 'cash', 'every', 'wk', 'txt', 'action', 'to', 't', 's', 'c', 's', 'www', 'movietrivia', 'tv', 'custcare', 'x', 'p', 'wk']\n",
      "After stop words removal: ['ur', 'chance', 'win', 'cash', 'every', 'wk', 'txt', 'action', 'c', 'www', 'movietrivia', 'tv', 'custcare', 'x', 'p', 'wk']\n",
      "After stemming with porters algorithm: ['chanc', 'win', 'cash', 'everi', 'txt', 'act', 'www', 'movietrivia', 'custcar']\n",
      "Tokenized sentence: ['ok', 'no', 'prob', 'i', 'll', 'come', 'after', 'lunch', 'then']\n",
      "After stop words removal: ['ok', 'prob', 'come', 'lunch']\n",
      "After stemming with porters algorithm: ['prob', 'come', 'lunch']\n",
      "Tokenized sentence: ['text', 'get', 'more', 'ringtones', 'logos', 'and', 'games', 'from', 'www', 'txt', 'com', 'questions', 'info', 'txt', 'co', 'uk']\n",
      "After stop words removal: ['text', 'get', 'ringtones', 'logos', 'games', 'www', 'txt', 'com', 'questions', 'info', 'txt', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['text', 'get', 'rington', 'logo', 'game', 'www', 'txt', 'com', 'quest', 'info', 'txt']\n",
      "Tokenized sentence: ['sian', 'aft', 'meeting', 'supervisor', 'got', 'work', 'do', 'liao', 'u', 'working', 'now']\n",
      "After stop words removal: ['sian', 'aft', 'meeting', 'supervisor', 'got', 'work', 'liao', 'u', 'working']\n",
      "meet\n",
      "work\n",
      "After stemming with porters algorithm: ['sian', 'aft', 'meet', 'supervisor', 'got', 'work', 'liao', 'wor']\n",
      "Tokenized sentence: ['it', 's', 'ok', 'i', 'noe', 'u', 're', 'busy', 'but', 'i', 'm', 'really', 'too', 'bored', 'so', 'i', 'msg', 'u', 'i', 'oso', 'dunno', 'wat', 'colour', 'she', 'choose', 'me', 'one']\n",
      "After stop words removal: ['ok', 'noe', 'u', 'busy', 'really', 'bored', 'msg', 'u', 'oso', 'dunno', 'wat', 'colour', 'choose', 'one']\n",
      "After stemming with porters algorithm: ['noe', 'busi', 'realli', 'bore', 'msg', 'oso', 'dunno', 'wat', 'colour', 'choos', 'on']\n",
      "Tokenized sentence: ['its', 'sarcasm', 'nt', 'scarcasim']\n",
      "After stop words removal: ['sarcasm', 'nt', 'scarcasim']\n",
      "After stemming with porters algorithm: ['sarcasm', 'scarcasim']\n",
      "Tokenized sentence: ['dont', 'think', 'so', 'it', 'turns', 'off', 'like', 'randomlly', 'within', 'min', 'of', 'opening']\n",
      "After stop words removal: ['dont', 'think', 'turns', 'like', 'randomlly', 'within', 'min', 'opening']\n",
      "open\n",
      "After stemming with porters algorithm: ['dont', 'think', 'turn', 'like', 'randomlli', 'within', 'min', 'open']\n",
      "Tokenized sentence: ['what', 'i', 'told', 'before', 'i', 'tell', 'stupid', 'hear', 'after', 'i', 'wont', 'tell', 'anything', 'to', 'you', 'you', 'dad', 'called', 'to', 'my', 'brother', 'and', 'spoken', 'not', 'with', 'me']\n",
      "After stop words removal: ['told', 'tell', 'stupid', 'hear', 'wont', 'tell', 'anything', 'dad', 'called', 'brother', 'spoken']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['told', 'tell', 'stupid', 'hear', 'wont', 'tell', 'anyt', 'dad', 'call', 'brother', 'spoken']\n",
      "Tokenized sentence: ['waiting', 'in', 'e', 'car', 'my', 'mum', 'lor', 'u', 'leh', 'reach', 'home', 'already']\n",
      "After stop words removal: ['waiting', 'e', 'car', 'mum', 'lor', 'u', 'leh', 'reach', 'home', 'already']\n",
      "wait\n",
      "After stemming with porters algorithm: ['wait', 'car', 'mum', 'lor', 'leh', 'reach', 'home', 'alreadi']\n",
      "Tokenized sentence: ['oh', 'k', 'k', 'but', 'he', 'is', 'not', 'a', 'big', 'hitter', 'anyway', 'good']\n",
      "After stop words removal: ['oh', 'k', 'k', 'big', 'hitter', 'anyway', 'good']\n",
      "After stemming with porters algorithm: ['big', 'hitter', 'anywai', 'good']\n",
      "Tokenized sentence: ['but', 'my', 'family', 'not', 'responding', 'for', 'anything', 'now', 'am', 'in', 'room', 'not', 'went', 'to', 'home', 'for', 'diwali', 'but', 'no', 'one', 'called', 'me', 'and', 'why', 'not', 'coming', 'it', 'makes', 'me', 'feel', 'like', 'died']\n",
      "After stop words removal: ['family', 'responding', 'anything', 'room', 'went', 'home', 'diwali', 'one', 'called', 'coming', 'makes', 'feel', 'like', 'died']\n",
      "respond\n",
      "anyth\n",
      "com\n",
      "After stemming with porters algorithm: ['famili', 'respon', 'anyt', 'room', 'went', 'home', 'diwali', 'on', 'call', 'come', 'make', 'feel', 'like', 'di']\n",
      "Tokenized sentence: ['apo', 'all', 'other', 'are', 'mokka', 'players', 'only']\n",
      "After stop words removal: ['apo', 'mokka', 'players']\n",
      "After stemming with porters algorithm: ['apo', 'mokka', 'player']\n",
      "Tokenized sentence: ['is', 'there', 'any', 'training', 'tomorrow']\n",
      "After stop words removal: ['training', 'tomorrow']\n",
      "train\n",
      "After stemming with porters algorithm: ['train', 'tomorrow']\n",
      "Tokenized sentence: ['i', 'enjoy', 'watching', 'and', 'playing', 'football', 'and', 'basketball', 'anything', 'outdoors', 'and', 'you']\n",
      "After stop words removal: ['enjoy', 'watching', 'playing', 'football', 'basketball', 'anything', 'outdoors']\n",
      "watch\n",
      "play\n",
      "anyth\n",
      "After stemming with porters algorithm: ['enjoi', 'watc', 'plai', 'footbal', 'basketbal', 'anyt', 'outdoor']\n",
      "Tokenized sentence: ['its', 'going', 'good', 'no', 'problem', 'but', 'still', 'need', 'little', 'experience', 'to', 'understand', 'american', 'customer', 'voice']\n",
      "After stop words removal: ['going', 'good', 'problem', 'still', 'need', 'little', 'experience', 'understand', 'american', 'customer', 'voice']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'good', 'problem', 'still', 'need', 'littl', 'experi', 'understand', 'american', 'custom', 'voic']\n",
      "Tokenized sentence: ['detroit', 'the', 'home', 'of', 'snow', 'enjoy', 'it']\n",
      "After stop words removal: ['detroit', 'home', 'snow', 'enjoy']\n",
      "After stemming with porters algorithm: ['detroit', 'home', 'snow', 'enjoi']\n",
      "Tokenized sentence: ['he', 'says', 'he', 'll', 'give', 'me', 'a', 'call', 'when', 'his', 'friend', 's', 'got', 'the', 'money', 'but', 'that', 'he', 's', 'definitely', 'buying', 'before', 'the', 'end', 'of', 'the', 'week']\n",
      "After stop words removal: ['says', 'give', 'call', 'friend', 'got', 'money', 'definitely', 'buying', 'end', 'week']\n",
      "buy\n",
      "After stemming with porters algorithm: ['sai', 'give', 'call', 'friend', 'got', 'monei', 'definit', 'bui', 'end', 'week']\n",
      "Tokenized sentence: ['tension', 'ah', 'what', 'machi', 'any', 'problem']\n",
      "After stop words removal: ['tension', 'ah', 'machi', 'problem']\n",
      "After stemming with porters algorithm: ['tension', 'machi', 'problem']\n",
      "Tokenized sentence: ['k', 'i', 'deleted', 'my', 'contact', 'that', 'why']\n",
      "After stop words removal: ['k', 'deleted', 'contact']\n",
      "After stemming with porters algorithm: ['delet', 'contact']\n",
      "Tokenized sentence: ['mm', 'i', 'had', 'my', 'food', 'da', 'from', 'out']\n",
      "After stop words removal: ['mm', 'food', 'da']\n",
      "After stemming with porters algorithm: ['food']\n",
      "Tokenized sentence: ['oh', 'ho', 'is', 'this', 'the', 'first', 'time', 'u', 'use', 'these', 'type', 'of', 'words']\n",
      "After stop words removal: ['oh', 'ho', 'first', 'time', 'u', 'use', 'type', 'words']\n",
      "After stemming with porters algorithm: ['first', 'time', 'us', 'type', 'word']\n",
      "Tokenized sentence: ['i', 'love', 'your', 'ass', 'do', 'you', 'enjoy', 'doggy', 'style']\n",
      "After stop words removal: ['love', 'ass', 'enjoy', 'doggy', 'style']\n",
      "After stemming with porters algorithm: ['love', 'ass', 'enjoi', 'doggi', 'style']\n",
      "Tokenized sentence: ['i', 'will', 'reach', 'ur', 'home', 'in', 'lt', 'gt', 'minutes']\n",
      "After stop words removal: ['reach', 'ur', 'home', 'lt', 'gt', 'minutes']\n",
      "After stemming with porters algorithm: ['reach', 'home', 'minut']\n",
      "Tokenized sentence: ['themob', 'hit', 'the', 'link', 'to', 'get', 'a', 'premium', 'pink', 'panther', 'game', 'the', 'new', 'no', 'from', 'sugababes', 'a', 'crazy', 'zebra', 'animation', 'or', 'a', 'badass', 'hoody', 'wallpaper', 'all', 'free']\n",
      "After stop words removal: ['themob', 'hit', 'link', 'get', 'premium', 'pink', 'panther', 'game', 'new', 'sugababes', 'crazy', 'zebra', 'animation', 'badass', 'hoody', 'wallpaper', 'free']\n",
      "After stemming with porters algorithm: ['themob', 'hit', 'link', 'get', 'premium', 'pink', 'panther', 'game', 'new', 'sugabab', 'crazi', 'zebra', 'anim', 'badass', 'hoodi', 'wallpap', 'free']\n",
      "Tokenized sentence: ['i', 'm', 'working', 'technical', 'support', 'voice', 'process']\n",
      "After stop words removal: ['working', 'technical', 'support', 'voice', 'process']\n",
      "work\n",
      "After stemming with porters algorithm: ['wor', 'technic', 'support', 'voic', 'process']\n",
      "Tokenized sentence: ['shopping', 'eh', 'ger', 'i', 'toking', 'abt', 'syd', 'leh', 'haha']\n",
      "After stop words removal: ['shopping', 'eh', 'ger', 'toking', 'abt', 'syd', 'leh', 'haha']\n",
      "shopp\n",
      "tok\n",
      "After stemming with porters algorithm: ['shop', 'ger', 'toke', 'abt', 'syd', 'leh', 'haha']\n",
      "Tokenized sentence: ['urgent', 'call', 'from', 'your', 'landline', 'your', 'complimentary', 'ibiza', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
      "After stop words removal: ['urgent', 'call', 'landline', 'complimentary', 'ibiza', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'call', 'landlin', 'complimentari', 'ibiza', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['gibbs', 'unsold', 'mike', 'hussey']\n",
      "After stop words removal: ['gibbs', 'unsold', 'mike', 'hussey']\n",
      "After stemming with porters algorithm: ['gibb', 'unsold', 'mike', 'hussei']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call']\n",
      "Tokenized sentence: ['best', 'msg', 'it', 's', 'hard', 'to', 'be', 'with', 'a', 'person', 'when', 'u', 'know', 'that', 'one', 'more', 'step', 'foward', 'will', 'make', 'u', 'fall', 'in', 'love', 'amp', 'one', 'step', 'back', 'can', 'ruin', 'ur', 'friendship', 'good', 'night']\n",
      "After stop words removal: ['best', 'msg', 'hard', 'person', 'u', 'know', 'one', 'step', 'foward', 'make', 'u', 'fall', 'love', 'amp', 'one', 'step', 'back', 'ruin', 'ur', 'friendship', 'good', 'night']\n",
      "After stemming with porters algorithm: ['best', 'msg', 'hard', 'person', 'know', 'on', 'step', 'foward', 'make', 'fall', 'love', 'amp', 'on', 'step', 'back', 'ruin', 'friendship', 'good', 'night']\n",
      "Tokenized sentence: ['good', 'do', 'you', 'think', 'you', 'could', 'send', 'me', 'some', 'pix', 'i', 'would', 'love', 'to', 'see', 'your', 'top', 'and', 'bottom']\n",
      "After stop words removal: ['good', 'think', 'could', 'send', 'pix', 'would', 'love', 'see', 'top', 'bottom']\n",
      "After stemming with porters algorithm: ['good', 'think', 'could', 'send', 'pix', 'would', 'love', 'see', 'top', 'bottom']\n",
      "Tokenized sentence: ['ur', 'cash', 'balance', 'is', 'currently', 'pounds', 'to', 'maximize', 'ur', 'cash', 'in', 'now', 'send', 'go', 'to', 'only', 'p', 'msg', 'cc', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stop words removal: ['ur', 'cash', 'balance', 'currently', 'pounds', 'maximize', 'ur', 'cash', 'send', 'go', 'p', 'msg', 'cc', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stemming with porters algorithm: ['cash', 'balanc', 'current', 'pound', 'maxim', 'cash', 'send', 'msg', 'suit', 'land', 'row']\n",
      "Tokenized sentence: ['nice', 'line', 'said', 'by', 'a', 'broken', 'heart', 'plz', 'don', 't', 'cum', 'more', 'times', 'infront', 'of', 'me', 'other', 'wise', 'once', 'again', 'i', 'll', 'trust', 'u', 'good', 't']\n",
      "After stop words removal: ['nice', 'line', 'said', 'broken', 'heart', 'plz', 'cum', 'times', 'infront', 'wise', 'trust', 'u', 'good']\n",
      "After stemming with porters algorithm: ['nice', 'line', 'said', 'broken', 'heart', 'plz', 'cum', 'time', 'infront', 'wise', 'trust', 'good']\n",
      "Tokenized sentence: ['someone', 'has', 'contacted', 'our', 'dating', 'service', 'and', 'entered', 'your', 'phone', 'because', 'they', 'fancy', 'you', 'to', 'find', 'out', 'who', 'it', 'is', 'call', 'from', 'a', 'landline', 'pobox', 'n', 'tf', 'p']\n",
      "After stop words removal: ['someone', 'contacted', 'dating', 'service', 'entered', 'phone', 'fancy', 'find', 'call', 'landline', 'pobox', 'n', 'tf', 'p']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['someon', 'contac', 'date', 'servic', 'enter', 'phone', 'fanci', 'find', 'call', 'landlin', 'pobox']\n",
      "Tokenized sentence: ['u', 'free', 'on', 'sat', 'rite', 'u', 'wan', 'watch', 'infernal', 'affairs', 'wif', 'me', 'n', 'darren', 'n', 'mayb', 'xy']\n",
      "After stop words removal: ['u', 'free', 'sat', 'rite', 'u', 'wan', 'watch', 'infernal', 'affairs', 'wif', 'n', 'darren', 'n', 'mayb', 'xy']\n",
      "After stemming with porters algorithm: ['free', 'sat', 'rite', 'wan', 'watch', 'infern', 'affair', 'wif', 'darren', 'mayb']\n",
      "Tokenized sentence: ['its', 'a', 'part', 'of', 'checking', 'iq']\n",
      "After stop words removal: ['part', 'checking', 'iq']\n",
      "check\n",
      "After stemming with porters algorithm: ['part', 'chec']\n",
      "Tokenized sentence: ['hi', 'darlin', 'i', 'cantdo', 'anythingtomorrow', 'as', 'myparents', 'aretaking', 'me', 'outfor', 'a', 'meal', 'when', 'are', 'u', 'free', 'katexxx']\n",
      "After stop words removal: ['hi', 'darlin', 'cantdo', 'anythingtomorrow', 'myparents', 'aretaking', 'outfor', 'meal', 'u', 'free', 'katexxx']\n",
      "aretak\n",
      "After stemming with porters algorithm: ['darlin', 'cantdo', 'anythingtomorrow', 'myparent', 'aretak', 'outfor', 'meal', 'free', 'katexxx']\n",
      "Tokenized sentence: ['hi', 'babe', 'u', 'r', 'most', 'likely', 'to', 'be', 'in', 'bed', 'but', 'im', 'so', 'sorry', 'about', 'tonight', 'i', 'really', 'wanna', 'see', 'u', 'tomorrow', 'so', 'call', 'me', 'at', 'love', 'me', 'xxx']\n",
      "After stop words removal: ['hi', 'babe', 'u', 'r', 'likely', 'bed', 'im', 'sorry', 'tonight', 'really', 'wanna', 'see', 'u', 'tomorrow', 'call', 'love', 'xxx']\n",
      "After stemming with porters algorithm: ['babe', 'like', 'bed', 'sorri', 'tonight', 'realli', 'wanna', 'see', 'tomorrow', 'call', 'love', 'xxx']\n",
      "Tokenized sentence: ['i', 'am', 'going', 'to', 'film', 'day', 'da', 'at', 'pm', 'sorry', 'da']\n",
      "After stop words removal: ['going', 'film', 'day', 'da', 'pm', 'sorry', 'da']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'film', 'dai', 'sorri']\n",
      "Tokenized sentence: ['boy', 'you', 'best', 'get', 'yo', 'ass', 'out', 'here', 'quick']\n",
      "After stop words removal: ['boy', 'best', 'get', 'yo', 'ass', 'quick']\n",
      "After stemming with porters algorithm: ['boi', 'best', 'get', 'ass', 'quick']\n",
      "Tokenized sentence: ['what', 'class', 'of', 'lt', 'gt', 'reunion']\n",
      "After stop words removal: ['class', 'lt', 'gt', 'reunion']\n",
      "After stemming with porters algorithm: ['class', 'reunion']\n",
      "Tokenized sentence: ['my', 'parents', 'my', 'kidz', 'my', 'friends', 'n', 'my', 'colleagues', 'all', 'screaming', 'surprise', 'and', 'i', 'was', 'waiting', 'on', 'the', 'sofa', 'naked']\n",
      "After stop words removal: ['parents', 'kidz', 'friends', 'n', 'colleagues', 'screaming', 'surprise', 'waiting', 'sofa', 'naked']\n",
      "scream\n",
      "wait\n",
      "After stemming with porters algorithm: ['parent', 'kidz', 'friend', 'colleagu', 'scream', 'surpris', 'wait', 'sofa', 'nake']\n",
      "Tokenized sentence: ['oh', 'fuck', 'juswoke', 'up', 'in', 'a', 'bed', 'on', 'a', 'boatin', 'the', 'docks', 'slept', 'wid', 'year', 'old', 'spinout', 'giv', 'u', 'da', 'gossip', 'l', 'r', 'xxx']\n",
      "After stop words removal: ['oh', 'fuck', 'juswoke', 'bed', 'boatin', 'docks', 'slept', 'wid', 'year', 'old', 'spinout', 'giv', 'u', 'da', 'gossip', 'l', 'r', 'xxx']\n",
      "After stemming with porters algorithm: ['fuck', 'juswok', 'bed', 'boatin', 'dock', 'slept', 'wid', 'year', 'old', 'spinout', 'giv', 'gossip', 'xxx']\n",
      "Tokenized sentence: ['amazing', 'if', 'you', 'rearrange', 'these', 'letters', 'it', 'gives', 'the', 'same', 'meaning', 'dormitory', 'dirty', 'room', 'astronomer', 'moon', 'starer', 'the', 'eyes', 'they', 'see', 'election', 'results', 'lies', 'lets', 'recount', 'mother', 'in', 'law', 'woman', 'hitler', 'eleven', 'plus', 'two', 'twelve', 'plus', 'one', 'its', 'amazing']\n",
      "After stop words removal: ['amazing', 'rearrange', 'letters', 'gives', 'meaning', 'dormitory', 'dirty', 'room', 'astronomer', 'moon', 'starer', 'eyes', 'see', 'election', 'results', 'lies', 'lets', 'recount', 'mother', 'law', 'woman', 'hitler', 'eleven', 'plus', 'two', 'twelve', 'plus', 'one', 'amazing']\n",
      "amaz\n",
      "mean\n",
      "amaz\n",
      "After stemming with porters algorithm: ['amaz', 'rearrang', 'letter', 'give', 'mean', 'dormitori', 'dirti', 'room', 'astronom', 'moon', 'starer', 'ey', 'see', 'elect', 'result', 'li', 'let', 'recount', 'mother', 'law', 'woman', 'hitler', 'eleven', 'plu', 'two', 'twelv', 'plu', 'on', 'amaz']\n",
      "Tokenized sentence: ['anything', 'lar', 'then', 'not', 'going', 'home', 'dinner']\n",
      "After stop words removal: ['anything', 'lar', 'going', 'home', 'dinner']\n",
      "anyth\n",
      "go\n",
      "After stemming with porters algorithm: ['anyt', 'lar', 'go', 'home', 'dinner']\n",
      "Tokenized sentence: ['pick', 'ur', 'fone', 'up', 'now', 'u', 'dumb']\n",
      "After stop words removal: ['pick', 'ur', 'fone', 'u', 'dumb']\n",
      "After stemming with porters algorithm: ['pick', 'fone', 'dumb']\n",
      "Tokenized sentence: ['bugis', 'oso', 'near', 'wat']\n",
      "After stop words removal: ['bugis', 'oso', 'near', 'wat']\n",
      "After stemming with porters algorithm: ['bugi', 'oso', 'near', 'wat']\n",
      "Tokenized sentence: ['no', 'calls', 'messages', 'missed', 'calls']\n",
      "After stop words removal: ['calls', 'messages', 'missed', 'calls']\n",
      "After stemming with porters algorithm: ['call', 'messag', 'miss', 'call']\n",
      "Tokenized sentence: ['what', 'type', 'of', 'stuff', 'do', 'you', 'sing']\n",
      "After stop words removal: ['type', 'stuff', 'sing']\n",
      "After stemming with porters algorithm: ['type', 'stuff', 'sing']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'a', 'nokia', 'i', 'this', 'is', 'what', 'you', 'get', 'when', 'you', 'win', 'our', 'free', 'auction', 'to', 'take', 'part', 'send', 'nokia', 'to', 'now', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stop words removal: ['nokia', 'get', 'win', 'free', 'auction', 'take', 'part', 'send', 'nokia', 'hg', 'suite', 'lands', 'row', 'w', 'jhl']\n",
      "After stemming with porters algorithm: ['nokia', 'get', 'win', 'free', 'auct', 'take', 'part', 'send', 'nokia', 'suit', 'land', 'row', 'jhl']\n",
      "Tokenized sentence: ['nope', 'i', 'm', 'not', 'drivin', 'i', 'neva', 'develop', 'da', 'photos', 'lei']\n",
      "After stop words removal: ['nope', 'drivin', 'neva', 'develop', 'da', 'photos', 'lei']\n",
      "After stemming with porters algorithm: ['nope', 'drivin', 'neva', 'develop', 'photo', 'lei']\n",
      "Tokenized sentence: ['what', 'you', 'need', 'you', 'have', 'a', 'person', 'to', 'give', 'na']\n",
      "After stop words removal: ['need', 'person', 'give', 'na']\n",
      "After stemming with porters algorithm: ['need', 'person', 'give']\n",
      "Tokenized sentence: ['r', 'u', 'sure', 'they', 'll', 'understand', 'that', 'wine', 'good', 'idea', 'just', 'had', 'a', 'slurp']\n",
      "After stop words removal: ['r', 'u', 'sure', 'understand', 'wine', 'good', 'idea', 'slurp']\n",
      "After stemming with porters algorithm: ['sure', 'understand', 'wine', 'good', 'idea', 'slurp']\n",
      "Tokenized sentence: ['i', 'noe', 'la', 'u', 'wana', 'pei', 'bf', 'oso', 'rite', 'k', 'lor', 'other', 'days', 'den']\n",
      "After stop words removal: ['noe', 'la', 'u', 'wana', 'pei', 'bf', 'oso', 'rite', 'k', 'lor', 'days', 'den']\n",
      "After stemming with porters algorithm: ['noe', 'wana', 'pei', 'oso', 'rite', 'lor', 'dai', 'den']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'winner', 'you', 'have', 'been', 'specially', 'selected', 'to', 'receive', 'cash', 'or', 'a', 'award', 'speak', 'to', 'a', 'live', 'operator', 'to', 'claim', 'call', 'am', 'pm', 'cost', 'p']\n",
      "After stop words removal: ['winner', 'specially', 'selected', 'receive', 'cash', 'award', 'speak', 'live', 'operator', 'claim', 'call', 'pm', 'cost', 'p']\n",
      "After stemming with porters algorithm: ['winner', 'special', 'selec', 'receiv', 'cash', 'award', 'speak', 'live', 'oper', 'claim', 'call', 'cost']\n",
      "Tokenized sentence: ['you', 'didn', 't', 'hear', 'it', 'from', 'me']\n",
      "After stop words removal: ['hear']\n",
      "After stemming with porters algorithm: ['hear']\n",
      "Tokenized sentence: ['hi', 'chikku', 'send', 'some', 'nice', 'msgs']\n",
      "After stop words removal: ['hi', 'chikku', 'send', 'nice', 'msgs']\n",
      "After stemming with porters algorithm: ['chikku', 'send', 'nice', 'msg']\n",
      "Tokenized sentence: ['get', 'the', 'door', 'i', 'm', 'here']\n",
      "After stop words removal: ['get', 'door']\n",
      "After stemming with porters algorithm: ['get', 'door']\n",
      "Tokenized sentence: ['alright', 'if', 'you', 're', 'sure', 'let', 'me', 'know', 'when', 'you', 're', 'leaving']\n",
      "After stop words removal: ['alright', 'sure', 'let', 'know', 'leaving']\n",
      "leav\n",
      "After stemming with porters algorithm: ['alright', 'sure', 'let', 'know', 'leav']\n",
      "Tokenized sentence: ['oh', 'yes', 'i', 'can', 'speak', 'txt', 'u', 'no', 'hmm', 'did', 'u', 'get', 'email']\n",
      "After stop words removal: ['oh', 'yes', 'speak', 'txt', 'u', 'hmm', 'u', 'get', 'email']\n",
      "After stemming with porters algorithm: ['ye', 'speak', 'txt', 'hmm', 'get', 'email']\n",
      "Tokenized sentence: ['okay', 'name', 'ur', 'price', 'as', 'long', 'as', 'its', 'legal', 'wen', 'can', 'i', 'pick', 'them', 'up', 'y', 'u', 'ave', 'x', 'ams', 'xx']\n",
      "After stop words removal: ['okay', 'name', 'ur', 'price', 'long', 'legal', 'wen', 'pick', 'u', 'ave', 'x', 'ams', 'xx']\n",
      "After stemming with porters algorithm: ['okai', 'name', 'price', 'long', 'legal', 'wen', 'pick', 'av', 'am']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['uk', 'break', 'accommodationvouchers', 'terms', 'conditions', 'apply', 'claim', 'you', 'mustprovide', 'your', 'claim', 'number', 'which', 'is']\n",
      "After stop words removal: ['uk', 'break', 'accommodationvouchers', 'terms', 'conditions', 'apply', 'claim', 'mustprovide', 'claim', 'number']\n",
      "After stemming with porters algorithm: ['break', 'accommodationvouch', 'term', 'condit', 'appli', 'claim', 'mustprovid', 'claim', 'number']\n",
      "Tokenized sentence: ['havent', 'planning', 'to', 'buy', 'later', 'i', 'check', 'already', 'lido', 'only', 'got', 'show', 'in', 'e', 'afternoon', 'u', 'finish', 'work', 'already']\n",
      "After stop words removal: ['havent', 'planning', 'buy', 'later', 'check', 'already', 'lido', 'got', 'show', 'e', 'afternoon', 'u', 'finish', 'work', 'already']\n",
      "plann\n",
      "After stemming with porters algorithm: ['havent', 'plan', 'bui', 'later', 'check', 'alreadi', 'lido', 'got', 'show', 'afternoon', 'finish', 'work', 'alreadi']\n",
      "Tokenized sentence: ['haha', 'yeah', 'oz', 'is', 'kind', 'of', 'a', 'shitload']\n",
      "After stop words removal: ['haha', 'yeah', 'oz', 'kind', 'shitload']\n",
      "After stemming with porters algorithm: ['haha', 'yeah', 'kind', 'shitload']\n",
      "Tokenized sentence: ['up', 'to', 'wan', 'come', 'then', 'come', 'lor', 'but', 'i', 'din', 'c', 'any', 'stripes', 'skirt']\n",
      "After stop words removal: ['wan', 'come', 'come', 'lor', 'din', 'c', 'stripes', 'skirt']\n",
      "After stemming with porters algorithm: ['wan', 'come', 'come', 'lor', 'din', 'stripe', 'skirt']\n",
      "Tokenized sentence: ['i', 'll', 'be', 'in', 'sch', 'fr', 'i', 'dun', 'haf', 'da', 'book', 'in', 'sch', 'it', 's', 'at', 'home']\n",
      "After stop words removal: ['sch', 'fr', 'dun', 'haf', 'da', 'book', 'sch', 'home']\n",
      "After stemming with porters algorithm: ['sch', 'dun', 'haf', 'book', 'sch', 'home']\n",
      "Tokenized sentence: ['ok', 'no', 'prob']\n",
      "After stop words removal: ['ok', 'prob']\n",
      "After stemming with porters algorithm: ['prob']\n",
      "Tokenized sentence: ['what', 'you', 'doing', 'how', 'are', 'you']\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['no', 'but', 'we', 'found', 'a', 'diff', 'farm', 'shop', 'to', 'buy', 'some', 'cheese', 'on', 'way', 'back', 'now', 'can', 'i', 'call', 'in']\n",
      "After stop words removal: ['found', 'diff', 'farm', 'shop', 'buy', 'cheese', 'way', 'back', 'call']\n",
      "After stemming with porters algorithm: ['found', 'diff', 'farm', 'shop', 'bui', 'chees', 'wai', 'back', 'call']\n",
      "Tokenized sentence: ['yup', 'no', 'need', 'i', 'll', 'jus', 'wait', 'e', 'rain', 'stop']\n",
      "After stop words removal: ['yup', 'need', 'jus', 'wait', 'e', 'rain', 'stop']\n",
      "After stemming with porters algorithm: ['yup', 'need', 'ju', 'wait', 'rain', 'stop']\n",
      "Tokenized sentence: ['aight', 'lemme', 'know', 'what', 's', 'up']\n",
      "After stop words removal: ['aight', 'lemme', 'know']\n",
      "After stemming with porters algorithm: ['aight', 'lemm', 'know']\n",
      "Tokenized sentence: ['i', 'only', 'work', 'from', 'mon', 'to', 'thurs', 'but', 'sat', 'i', 'cant', 'leh', 'booked', 'liao', 'which', 'other', 'day', 'u', 'free']\n",
      "After stop words removal: ['work', 'mon', 'thurs', 'sat', 'cant', 'leh', 'booked', 'liao', 'day', 'u', 'free']\n",
      "After stemming with porters algorithm: ['work', 'mon', 'thur', 'sat', 'cant', 'leh', 'book', 'liao', 'dai', 'free']\n",
      "Tokenized sentence: ['how', 'long', 'does', 'applebees', 'fucking', 'take']\n",
      "After stop words removal: ['long', 'applebees', 'fucking', 'take']\n",
      "fuck\n",
      "After stemming with porters algorithm: ['long', 'applebe', 'fuc', 'take']\n",
      "Tokenized sentence: ['thanks', 'a', 'lot', 'for', 'your', 'wishes', 'on', 'my', 'birthday', 'thanks', 'you', 'for', 'making', 'my', 'birthday', 'truly', 'memorable']\n",
      "After stop words removal: ['thanks', 'lot', 'wishes', 'birthday', 'thanks', 'making', 'birthday', 'truly', 'memorable']\n",
      "mak\n",
      "After stemming with porters algorithm: ['thank', 'lot', 'wish', 'birthdai', 'thank', 'make', 'birthdai', 'truli', 'memor']\n",
      "Tokenized sentence: ['you', 'can', 'stop', 'further', 'club', 'tones', 'by', 'replying', 'stop', 'mix', 'see', 'my', 'tone', 'com', 'enjoy', 'html', 'for', 'terms', 'club', 'tones', 'cost', 'gbp', 'week', 'mfl']\n",
      "After stop words removal: ['stop', 'club', 'tones', 'replying', 'stop', 'mix', 'see', 'tone', 'com', 'enjoy', 'html', 'terms', 'club', 'tones', 'cost', 'gbp', 'week', 'mfl']\n",
      "reply\n",
      "After stemming with porters algorithm: ['stop', 'club', 'tone', 'repl', 'stop', 'mix', 'see', 'tone', 'com', 'enjoi', 'html', 'term', 'club', 'tone', 'cost', 'gbp', 'week', 'mfl']\n",
      "Tokenized sentence: ['no', 'just', 'send', 'to', 'you', 'bec', 'you', 'in', 'temple', 'na']\n",
      "After stop words removal: ['send', 'bec', 'temple', 'na']\n",
      "After stemming with porters algorithm: ['send', 'bec', 'templ']\n",
      "Tokenized sentence: ['webpage', 's', 'not', 'available']\n",
      "After stop words removal: ['webpage', 'available']\n",
      "After stemming with porters algorithm: ['webpag', 'avail']\n",
      "Tokenized sentence: ['it', 's', 'justbeen', 'overa', 'week', 'since', 'we', 'broke', 'up', 'and', 'already', 'our', 'brains', 'are', 'going', 'to', 'mush']\n",
      "After stop words removal: ['justbeen', 'overa', 'week', 'since', 'broke', 'already', 'brains', 'going', 'mush']\n",
      "go\n",
      "After stemming with porters algorithm: ['justbeen', 'overa', 'week', 'sinc', 'broke', 'alreadi', 'brain', 'go', 'mush']\n",
      "Tokenized sentence: ['how', 'much', 'is', 'blackberry', 'bold', 'in', 'nigeria']\n",
      "After stop words removal: ['much', 'blackberry', 'bold', 'nigeria']\n",
      "After stemming with porters algorithm: ['much', 'blackberri', 'bold', 'nigeria']\n",
      "Tokenized sentence: ['check', 'wid', 'corect', 'speling', 'i', 'e', 'sarcasm']\n",
      "After stop words removal: ['check', 'wid', 'corect', 'speling', 'e', 'sarcasm']\n",
      "spel\n",
      "After stemming with porters algorithm: ['check', 'wid', 'corect', 'spele', 'sarcasm']\n",
      "Tokenized sentence: ['i', 'm', 'always', 'looking', 'for', 'an', 'excuse', 'to', 'be', 'in', 'the', 'city']\n",
      "After stop words removal: ['always', 'looking', 'excuse', 'city']\n",
      "look\n",
      "After stemming with porters algorithm: ['alwai', 'look', 'excus', 'citi']\n",
      "Tokenized sentence: ['thank', 'you', 'so', 'much', 'when', 'we', 'skyped', 'wit', 'kz', 'and', 'sura', 'we', 'didnt', 'get', 'the', 'pleasure', 'of', 'your', 'company', 'hope', 'you', 'are', 'good', 'we', 've', 'given', 'you', 'ultimatum', 'oh', 'we', 'are', 'countin', 'down', 'to', 'aburo', 'enjoy', 'this', 'is', 'the', 'message', 'i', 'sent', 'days', 'ago']\n",
      "After stop words removal: ['thank', 'much', 'skyped', 'wit', 'kz', 'sura', 'didnt', 'get', 'pleasure', 'company', 'hope', 'good', 'given', 'ultimatum', 'oh', 'countin', 'aburo', 'enjoy', 'message', 'sent', 'days', 'ago']\n",
      "After stemming with porters algorithm: ['thank', 'much', 'skyped', 'wit', 'sura', 'didnt', 'get', 'pleasur', 'compani', 'hope', 'good', 'given', 'ultimatum', 'countin', 'aburo', 'enjoi', 'messag', 'sent', 'dai', 'ago']\n",
      "Tokenized sentence: ['ok', 'lor', 'thanx', 'in', 'school']\n",
      "After stop words removal: ['ok', 'lor', 'thanx', 'school']\n",
      "After stemming with porters algorithm: ['lor', 'thanx', 'school']\n",
      "Tokenized sentence: ['thanx', 'the', 'time', 'we', 've', 'spent', 'geva', 'its', 'bin', 'mint', 'ur', 'my', 'baby', 'and', 'all', 'i', 'want', 'is', 'u', 'xxxx']\n",
      "After stop words removal: ['thanx', 'time', 'spent', 'geva', 'bin', 'mint', 'ur', 'baby', 'want', 'u', 'xxxx']\n",
      "After stemming with porters algorithm: ['thanx', 'time', 'spent', 'geva', 'bin', 'mint', 'babi', 'want', 'xxxx']\n",
      "Tokenized sentence: ['good', 'afternoon', 'my', 'boytoy', 'how', 'goes', 'that', 'walking', 'here', 'and', 'there', 'day', 'did', 'you', 'get', 'that', 'police', 'abstract', 'are', 'you', 'still', 'out', 'and', 'about', 'i', 'wake', 'and', 'miss', 'you', 'babe']\n",
      "After stop words removal: ['good', 'afternoon', 'boytoy', 'goes', 'walking', 'day', 'get', 'police', 'abstract', 'still', 'wake', 'miss', 'babe']\n",
      "walk\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'boytoi', 'goe', 'wal', 'dai', 'get', 'polic', 'abstract', 'still', 'wake', 'miss', 'babe']\n",
      "Tokenized sentence: ['ya', 'that', 'one', 'is', 'slow', 'as', 'poo']\n",
      "After stop words removal: ['ya', 'one', 'slow', 'poo']\n",
      "After stemming with porters algorithm: ['on', 'slow', 'poo']\n",
      "Tokenized sentence: ['sorry', 'im', 'getting', 'up', 'now', 'feel', 'really', 'bad', 'totally', 'rejected', 'that', 'kinda', 'me', 'thing']\n",
      "After stop words removal: ['sorry', 'im', 'getting', 'feel', 'really', 'bad', 'totally', 'rejected', 'kinda', 'thing']\n",
      "gett\n",
      "After stemming with porters algorithm: ['sorri', 'get', 'feel', 'realli', 'bad', 'total', 'rejec', 'kinda', 'thing']\n",
      "Tokenized sentence: ['then', 'u', 'ask', 'darren', 'go', 'n', 'pick', 'u', 'lor', 'but', 'i', 'oso', 'sian', 'tmr', 'haf', 'meet', 'lect']\n",
      "After stop words removal: ['u', 'ask', 'darren', 'go', 'n', 'pick', 'u', 'lor', 'oso', 'sian', 'tmr', 'haf', 'meet', 'lect']\n",
      "After stemming with porters algorithm: ['ask', 'darren', 'pick', 'lor', 'oso', 'sian', 'tmr', 'haf', 'meet', 'lect']\n",
      "Tokenized sentence: ['last', 'chance', 'claim', 'ur', 'worth', 'of', 'discount', 'vouchers', 'today', 'text', 'shop', 'to', 'now', 'savamob', 'offers', 'mobile', 't', 'cs', 'savamob', 'pobox', 'm', 'uz', 'sub']\n",
      "After stop words removal: ['last', 'chance', 'claim', 'ur', 'worth', 'discount', 'vouchers', 'today', 'text', 'shop', 'savamob', 'offers', 'mobile', 'cs', 'savamob', 'pobox', 'uz', 'sub']\n",
      "After stemming with porters algorithm: ['last', 'chanc', 'claim', 'worth', 'discount', 'voucher', 'todai', 'text', 'shop', 'savamob', 'offer', 'mobil', 'savamob', 'pobox', 'sub']\n",
      "Tokenized sentence: ['ok', 'there', 'may', 'be', 'a', 'free', 'gym', 'about']\n",
      "After stop words removal: ['ok', 'may', 'free', 'gym']\n",
      "After stemming with porters algorithm: ['mai', 'free', 'gym']\n",
      "Tokenized sentence: ['go', 'fool', 'dont', 'cheat', 'others', 'ok']\n",
      "After stop words removal: ['go', 'fool', 'dont', 'cheat', 'others', 'ok']\n",
      "After stemming with porters algorithm: ['fool', 'dont', 'cheat', 'other']\n",
      "Tokenized sentence: ['mark', 'works', 'tomorrow', 'he', 'gets', 'out', 'at', 'his', 'work', 'is', 'by', 'your', 'house', 'so', 'he', 'can', 'meet', 'u', 'afterwards']\n",
      "After stop words removal: ['mark', 'works', 'tomorrow', 'gets', 'work', 'house', 'meet', 'u', 'afterwards']\n",
      "After stemming with porters algorithm: ['mark', 'work', 'tomorrow', 'get', 'work', 'hous', 'meet', 'afterward']\n",
      "Tokenized sentence: ['from', 'lost', 'pound', 'help']\n",
      "After stop words removal: ['lost', 'pound', 'help']\n",
      "After stemming with porters algorithm: ['lost', 'pound', 'help']\n",
      "Tokenized sentence: ['home', 'so', 'we', 'can', 'always', 'chat']\n",
      "After stop words removal: ['home', 'always', 'chat']\n",
      "After stemming with porters algorithm: ['home', 'alwai', 'chat']\n",
      "Tokenized sentence: ['collecting', 'ur', 'laptop', 'then', 'going', 'to', 'configure', 'da', 'settings', 'izzit']\n",
      "After stop words removal: ['collecting', 'ur', 'laptop', 'going', 'configure', 'da', 'settings', 'izzit']\n",
      "collect\n",
      "go\n",
      "sett\n",
      "After stemming with porters algorithm: ['collec', 'laptop', 'go', 'configur', 'set', 'izzit']\n",
      "Tokenized sentence: ['ya', 'srsly', 'better', 'than', 'yi', 'tho']\n",
      "After stop words removal: ['ya', 'srsly', 'better', 'yi', 'tho']\n",
      "After stemming with porters algorithm: ['srsly', 'better', 'tho']\n",
      "Tokenized sentence: ['forwarded', 'from', 'hi', 'this', 'is', 'your', 'mailbox', 'messaging', 'sms', 'alert', 'you', 'have', 'messages', 'you', 'have', 'matches', 'please', 'call', 'back', 'on', 'to', 'retrieve', 'your', 'messages', 'and', 'matches']\n",
      "After stop words removal: ['forwarded', 'hi', 'mailbox', 'messaging', 'sms', 'alert', 'messages', 'matches', 'please', 'call', 'back', 'retrieve', 'messages', 'matches']\n",
      "messag\n",
      "After stemming with porters algorithm: ['forwar', 'mailbox', 'messag', 'sm', 'alert', 'messag', 'match', 'pleas', 'call', 'back', 'retriev', 'messag', 'match']\n",
      "Tokenized sentence: ['no', 'i', 'am', 'not', 'having', 'not', 'any', 'movies', 'in', 'my', 'laptop']\n",
      "After stop words removal: ['movies', 'laptop']\n",
      "After stemming with porters algorithm: ['movi', 'laptop']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'a', 'meeting', 'call', 'me', 'later', 'at']\n",
      "After stop words removal: ['meeting', 'call', 'later']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'call', 'later']\n",
      "Tokenized sentence: ['very', 'strange', 'and', 'are', 'watching', 'the', 'nd', 'one', 'now', 'but', 'i', 'm', 'in', 'bed', 'sweet', 'dreams', 'miss', 'u']\n",
      "After stop words removal: ['strange', 'watching', 'nd', 'one', 'bed', 'sweet', 'dreams', 'miss', 'u']\n",
      "watch\n",
      "After stemming with porters algorithm: ['strang', 'watc', 'on', 'bed', 'sweet', 'dream', 'miss']\n",
      "Tokenized sentence: ['reminder', 'you', 'have', 'not', 'downloaded', 'the', 'content', 'you', 'have', 'already', 'paid', 'for', 'goto', 'http', 'doit', 'mymoby', 'tv', 'to', 'collect', 'your', 'content']\n",
      "After stop words removal: ['reminder', 'downloaded', 'content', 'already', 'paid', 'goto', 'http', 'doit', 'mymoby', 'tv', 'collect', 'content']\n",
      "After stemming with porters algorithm: ['remind', 'download', 'content', 'alreadi', 'paid', 'goto', 'http', 'doit', 'mymobi', 'collect', 'content']\n",
      "Tokenized sentence: ['miles', 'and', 'smiles', 'r', 'made', 'frm', 'same', 'letters', 'but', 'do', 'u', 'know', 'd', 'difference', 'smile', 'on', 'ur', 'face', 'keeps', 'me', 'happy', 'even', 'though', 'i', 'am', 'miles', 'away', 'from', 'u', 'keep', 'smiling', 'good', 'nyt']\n",
      "After stop words removal: ['miles', 'smiles', 'r', 'made', 'frm', 'letters', 'u', 'know', 'difference', 'smile', 'ur', 'face', 'keeps', 'happy', 'even', 'though', 'miles', 'away', 'u', 'keep', 'smiling', 'good', 'nyt']\n",
      "smil\n",
      "After stemming with porters algorithm: ['mile', 'smile', 'made', 'frm', 'letter', 'know', 'differ', 'smile', 'face', 'keep', 'happi', 'even', 'though', 'mile', 'awai', 'keep', 'smile', 'good', 'nyt']\n",
      "Tokenized sentence: ['aight', 'let', 'me', 'know', 'when', 'you', 're', 'gonna', 'be', 'around', 'usf']\n",
      "After stop words removal: ['aight', 'let', 'know', 'gonna', 'around', 'usf']\n",
      "After stemming with porters algorithm: ['aight', 'let', 'know', 'gonna', 'around', 'usf']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'xxxxxxxxx', 'won', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'reach', 'you', 'call', 'asap']\n",
      "After stop words removal: ['urgent', 'mobile', 'xxxxxxxxx', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'reach', 'call', 'asap']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'xxxxxxxxx', 'bonu', 'caller', 'priz', 'attempt', 'reach', 'call', 'asap']\n",
      "Tokenized sentence: ['what', 'was', 'she', 'looking', 'for']\n",
      "After stop words removal: ['looking']\n",
      "look\n",
      "After stemming with porters algorithm: ['look']\n",
      "Tokenized sentence: ['goodnight', 'sleep', 'well', 'da', 'please', 'take', 'care', 'pa', 'please']\n",
      "After stop words removal: ['goodnight', 'sleep', 'well', 'da', 'please', 'take', 'care', 'pa', 'please']\n",
      "After stemming with porters algorithm: ['goodnight', 'sleep', 'well', 'pleas', 'take', 'care', 'pleas']\n",
      "Tokenized sentence: ['reading', 'gud', 'habit', 'nan', 'bari', 'hudgi', 'yorge', 'pataistha', 'ertini', 'kano']\n",
      "After stop words removal: ['reading', 'gud', 'habit', 'nan', 'bari', 'hudgi', 'yorge', 'pataistha', 'ertini', 'kano']\n",
      "read\n",
      "After stemming with porters algorithm: ['read', 'gud', 'habit', 'nan', 'bari', 'hudgi', 'yorg', 'pataistha', 'ertini', 'kano']\n",
      "Tokenized sentence: ['machan', 'you', 'go', 'to', 'gym', 'tomorrow', 'i', 'wil', 'come', 'late', 'goodnight']\n",
      "After stop words removal: ['machan', 'go', 'gym', 'tomorrow', 'wil', 'come', 'late', 'goodnight']\n",
      "After stemming with porters algorithm: ['machan', 'gym', 'tomorrow', 'wil', 'come', 'late', 'goodnight']\n",
      "Tokenized sentence: ['save', 'yourself', 'the', 'stress', 'if', 'the', 'person', 'has', 'a', 'dorm', 'account', 'just', 'send', 'your', 'account', 'details', 'and', 'the', 'money', 'will', 'be', 'sent', 'to', 'you']\n",
      "After stop words removal: ['save', 'stress', 'person', 'dorm', 'account', 'send', 'account', 'details', 'money', 'sent']\n",
      "After stemming with porters algorithm: ['save', 'stress', 'person', 'dorm', 'account', 'send', 'account', 'detail', 'monei', 'sent']\n",
      "Tokenized sentence: ['not', 'that', 'i', 'know', 'of', 'most', 'people', 'up', 'here', 'are', 'still', 'out', 'of', 'town']\n",
      "After stop words removal: ['know', 'people', 'still', 'town']\n",
      "After stemming with porters algorithm: ['know', 'peopl', 'still', 'town']\n",
      "Tokenized sentence: ['mm', 'i', 'am', 'on', 'the', 'way', 'to', 'railway']\n",
      "After stop words removal: ['mm', 'way', 'railway']\n",
      "After stemming with porters algorithm: ['wai', 'railwai']\n",
      "Tokenized sentence: ['awesome', 'i', 'remember', 'the', 'last', 'time', 'we', 'got', 'somebody', 'high', 'for', 'the', 'first', 'time', 'with', 'diesel', 'v']\n",
      "After stop words removal: ['awesome', 'remember', 'last', 'time', 'got', 'somebody', 'high', 'first', 'time', 'diesel', 'v']\n",
      "After stemming with porters algorithm: ['awesom', 'rememb', 'last', 'time', 'got', 'somebodi', 'high', 'first', 'time', 'diesel']\n",
      "Tokenized sentence: ['i', 'dont', 'have', 'any', 'of', 'your', 'file', 'in', 'my', 'bag', 'i', 'was', 'in', 'work', 'when', 'you', 'called', 'me', 'i', 'll', 'tell', 'you', 'if', 'i', 'find', 'anything', 'in', 'my', 'room']\n",
      "After stop words removal: ['dont', 'file', 'bag', 'work', 'called', 'tell', 'find', 'anything', 'room']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['dont', 'file', 'bag', 'work', 'call', 'tell', 'find', 'anyt', 'room']\n",
      "Tokenized sentence: ['hi', 'babe', 'uawake', 'feellikw', 'shit', 'justfound', 'out', 'via', 'aletter', 'thatmum', 'gotmarried', 'thnov', 'behind', 'ourbacks', 'fuckinnice', 'selfish']\n",
      "After stop words removal: ['hi', 'babe', 'uawake', 'feellikw', 'shit', 'justfound', 'via', 'aletter', 'thatmum', 'gotmarried', 'thnov', 'behind', 'ourbacks', 'fuckinnice', 'selfish']\n",
      "After stemming with porters algorithm: ['babe', 'uawak', 'feellikw', 'shit', 'justfound', 'via', 'alett', 'thatmum', 'gotmarri', 'thnov', 'behind', 'ourback', 'fuckinnic', 'selfish']\n",
      "Tokenized sentence: ['s', 'antha', 'num', 'corrct', 'dane']\n",
      "After stop words removal: ['antha', 'num', 'corrct', 'dane']\n",
      "After stemming with porters algorithm: ['antha', 'num', 'corrct', 'dane']\n",
      "Tokenized sentence: ['this', 'message', 'is', 'brought', 'to', 'you', 'by', 'gmw', 'ltd', 'and', 'is', 'not', 'connected', 'to', 'the']\n",
      "After stop words removal: ['message', 'brought', 'gmw', 'ltd', 'connected']\n",
      "After stemming with porters algorithm: ['messag', 'brought', 'gmw', 'ltd', 'connec']\n",
      "Tokenized sentence: ['don', 't', 'worry', 'is', 'easy', 'once', 'have', 'ingredients']\n",
      "After stop words removal: ['worry', 'easy', 'ingredients']\n",
      "After stemming with porters algorithm: ['worri', 'easi', 'ingredi']\n",
      "Tokenized sentence: ['call', 'and', 'send', 'our', 'girls', 'into', 'erotic', 'ecstacy', 'just', 'p', 'min', 'to', 'stop', 'texts', 'call', 'nat', 'rate']\n",
      "After stop words removal: ['call', 'send', 'girls', 'erotic', 'ecstacy', 'p', 'min', 'stop', 'texts', 'call', 'nat', 'rate']\n",
      "After stemming with porters algorithm: ['call', 'send', 'girl', 'erot', 'ecstaci', 'min', 'stop', 'text', 'call', 'nat', 'rate']\n",
      "Tokenized sentence: ['hey', 'next', 'sun', 'there', 's', 'a', 'basic', 'yoga', 'course', 'at', 'bugis', 'we', 'can', 'go', 'for', 'that', 'pilates', 'intro', 'next', 'sat', 'tell', 'me', 'what', 'time', 'you', 'r', 'free']\n",
      "After stop words removal: ['hey', 'next', 'sun', 'basic', 'yoga', 'course', 'bugis', 'go', 'pilates', 'intro', 'next', 'sat', 'tell', 'time', 'r', 'free']\n",
      "After stemming with porters algorithm: ['hei', 'next', 'sun', 'basic', 'yoga', 'cours', 'bugi', 'pilat', 'intro', 'next', 'sat', 'tell', 'time', 'free']\n",
      "Tokenized sentence: ['i', 'm', 'parked', 'next', 'to', 'a', 'mini', 'when', 'are', 'you', 'coming', 'in', 'today', 'do', 'you', 'think']\n",
      "After stop words removal: ['parked', 'next', 'mini', 'coming', 'today', 'think']\n",
      "com\n",
      "After stemming with porters algorithm: ['par', 'next', 'mini', 'come', 'todai', 'think']\n",
      "Tokenized sentence: ['i', 'tot', 'u', 'reach', 'liao', 'he', 'said', 't', 'shirt']\n",
      "After stop words removal: ['tot', 'u', 'reach', 'liao', 'said', 'shirt']\n",
      "After stemming with porters algorithm: ['tot', 'reach', 'liao', 'said', 'shirt']\n",
      "Tokenized sentence: ['if', 'u', 'laugh', 'really', 'loud', 'if', 'u', 'talk', 'spontaneously', 'if', 'u', 'dont', 'care', 'what', 'others', 'feel', 'u', 'are', 'probably', 'with', 'your', 'dear', 'amp', 'best', 'friends', 'goodevening', 'dear']\n",
      "After stop words removal: ['u', 'laugh', 'really', 'loud', 'u', 'talk', 'spontaneously', 'u', 'dont', 'care', 'others', 'feel', 'u', 'probably', 'dear', 'amp', 'best', 'friends', 'goodevening', 'dear']\n",
      "goodeven\n",
      "After stemming with porters algorithm: ['laugh', 'realli', 'loud', 'talk', 'spontan', 'dont', 'care', 'other', 'feel', 'probab', 'dear', 'amp', 'best', 'friend', 'goodeven', 'dear']\n",
      "Tokenized sentence: ['i', 'emailed', 'yifeng', 'my', 'part', 'oredi', 'can', 'get', 'it', 'fr', 'him']\n",
      "After stop words removal: ['emailed', 'yifeng', 'part', 'oredi', 'get', 'fr']\n",
      "After stemming with porters algorithm: ['email', 'yifeng', 'part', 'oredi', 'get']\n",
      "Tokenized sentence: ['i', 'think', 'asking', 'for', 'a', 'gym', 'is', 'the', 'excuse', 'for', 'lazy', 'people', 'i', 'jog']\n",
      "After stop words removal: ['think', 'asking', 'gym', 'excuse', 'lazy', 'people', 'jog']\n",
      "ask\n",
      "After stemming with porters algorithm: ['think', 'as', 'gym', 'excus', 'lazi', 'peopl', 'jog']\n",
      "Tokenized sentence: ['please', 'call', 'immediately', 'as', 'there', 'is', 'an', 'urgent', 'message', 'waiting', 'for', 'you']\n",
      "After stop words removal: ['please', 'call', 'immediately', 'urgent', 'message', 'waiting']\n",
      "wait\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'immedi', 'urgent', 'messag', 'wait']\n",
      "Tokenized sentence: ['you', 'are', 'guaranteed', 'the', 'latest', 'nokia', 'phone', 'a', 'gb', 'ipod', 'mp', 'player', 'or', 'a', 'prize', 'txt', 'word', 'collect', 'to', 'no', 'ibhltd', 'ldnw', 'h', 'p', 'mtmsgrcvd']\n",
      "After stop words removal: ['guaranteed', 'latest', 'nokia', 'phone', 'gb', 'ipod', 'mp', 'player', 'prize', 'txt', 'word', 'collect', 'ibhltd', 'ldnw', 'h', 'p', 'mtmsgrcvd']\n",
      "After stemming with porters algorithm: ['guaranteed', 'latest', 'nokia', 'phone', 'ipod', 'player', 'priz', 'txt', 'word', 'collect', 'ibhltd', 'ldnw', 'mtmsgrcvd']\n",
      "Tokenized sentence: ['free', 'msg', 'get', 'gnarls', 'barkleys', 'crazy', 'ringtone', 'totally', 'free', 'just', 'reply', 'go', 'to', 'this', 'message', 'right', 'now']\n",
      "After stop words removal: ['free', 'msg', 'get', 'gnarls', 'barkleys', 'crazy', 'ringtone', 'totally', 'free', 'reply', 'go', 'message', 'right']\n",
      "After stemming with porters algorithm: ['free', 'msg', 'get', 'gnarl', 'barklei', 'crazi', 'rington', 'total', 'free', 'repli', 'messag', 'right']\n",
      "Tokenized sentence: ['oh', 'k', 'i', 'will', 'come', 'tomorrow']\n",
      "After stop words removal: ['oh', 'k', 'come', 'tomorrow']\n",
      "After stemming with porters algorithm: ['come', 'tomorrow']\n",
      "Tokenized sentence: ['we', 'left', 'already', 'we', 'at', 'orchard', 'now']\n",
      "After stop words removal: ['left', 'already', 'orchard']\n",
      "After stemming with porters algorithm: ['left', 'alreadi', 'orchard']\n",
      "Tokenized sentence: ['k', 'k', 'when', 'are', 'you', 'going']\n",
      "After stop words removal: ['k', 'k', 'going']\n",
      "go\n",
      "After stemming with porters algorithm: ['go']\n",
      "Tokenized sentence: ['hello', 'just', 'got', 'here', 'st', 'andrews', 'boy', 'its', 'a', 'long', 'way', 'its', 'cold', 'i', 'will', 'keep', 'you', 'posted']\n",
      "After stop words removal: ['hello', 'got', 'st', 'andrews', 'boy', 'long', 'way', 'cold', 'keep', 'posted']\n",
      "After stemming with porters algorithm: ['hello', 'got', 'andrew', 'boi', 'long', 'wai', 'cold', 'keep', 'pos']\n",
      "Tokenized sentence: ['i', 'hope', 'your', 'alright', 'babe', 'i', 'worry', 'that', 'you', 'might', 'have', 'felt', 'a', 'bit', 'desparate', 'when', 'you', 'learned', 'the', 'job', 'was', 'a', 'fake', 'i', 'am', 'here', 'waiting', 'when', 'you', 'come', 'back', 'my', 'love']\n",
      "After stop words removal: ['hope', 'alright', 'babe', 'worry', 'might', 'felt', 'bit', 'desparate', 'learned', 'job', 'fake', 'waiting', 'come', 'back', 'love']\n",
      "wait\n",
      "After stemming with porters algorithm: ['hope', 'alright', 'babe', 'worri', 'might', 'felt', 'bit', 'despar', 'lear', 'job', 'fake', 'wait', 'come', 'back', 'love']\n",
      "Tokenized sentence: ['talk', 'sexy', 'make', 'new', 'friends', 'or', 'fall', 'in', 'love', 'in', 'the', 'worlds', 'most', 'discreet', 'text', 'dating', 'service', 'just', 'text', 'vip', 'to', 'and', 'see', 'who', 'you', 'could', 'meet']\n",
      "After stop words removal: ['talk', 'sexy', 'make', 'new', 'friends', 'fall', 'love', 'worlds', 'discreet', 'text', 'dating', 'service', 'text', 'vip', 'see', 'could', 'meet']\n",
      "dat\n",
      "date\n",
      "After stemming with porters algorithm: ['talk', 'sexi', 'make', 'new', 'friend', 'fall', 'love', 'world', 'discreet', 'text', 'date', 'servic', 'text', 'vip', 'see', 'could', 'meet']\n",
      "Tokenized sentence: ['sorry', 'i', 'missed', 'you', 'babe', 'i', 'was', 'up', 'late', 'and', 'slept', 'in', 'i', 'hope', 'you', 'enjoy', 'your', 'driving', 'lesson', 'boytoy', 'i', 'miss', 'you', 'too', 'teasing', 'kiss']\n",
      "After stop words removal: ['sorry', 'missed', 'babe', 'late', 'slept', 'hope', 'enjoy', 'driving', 'lesson', 'boytoy', 'miss', 'teasing', 'kiss']\n",
      "driv\n",
      "teas\n",
      "After stemming with porters algorithm: ['sorri', 'miss', 'babe', 'late', 'slept', 'hope', 'enjoi', 'drive', 'lesson', 'boytoi', 'miss', 'teas', 'kiss']\n",
      "Tokenized sentence: ['my', 'friends', 'use', 'to', 'call', 'the', 'same']\n",
      "After stop words removal: ['friends', 'use', 'call']\n",
      "After stemming with porters algorithm: ['friend', 'us', 'call']\n",
      "Tokenized sentence: ['aight', 'we', 'can', 'pick', 'some', 'up', 'you', 'open', 'before', 'tonight']\n",
      "After stop words removal: ['aight', 'pick', 'open', 'tonight']\n",
      "After stemming with porters algorithm: ['aight', 'pick', 'open', 'tonight']\n",
      "Tokenized sentence: ['lt', 'decimal', 'gt', 'm', 'but', 'its', 'not', 'a', 'common', 'car', 'here', 'so', 'its', 'better', 'to', 'buy', 'from', 'china', 'or', 'asia', 'or', 'if', 'i', 'find', 'it', 'less', 'expensive', 'i', 'll', 'holla']\n",
      "After stop words removal: ['lt', 'decimal', 'gt', 'common', 'car', 'better', 'buy', 'china', 'asia', 'find', 'less', 'expensive', 'holla']\n",
      "After stemming with porters algorithm: ['decim', 'common', 'car', 'better', 'bui', 'china', 'asia', 'find', 'less', 'expens', 'holla']\n",
      "Tokenized sentence: ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'freephone', 'between', 'am', 'pm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
      "After stop words removal: ['please', 'call', 'customer', 'service', 'representative', 'freephone', 'pm', 'guaranteed', 'cash', 'prize']\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'custom', 'servic', 'repres', 'freephon', 'guaranteed', 'cash', 'priz']\n",
      "Tokenized sentence: ['that', 's', 'the', 'trouble', 'with', 'classes', 'that', 'go', 'well', 'you', 're', 'due', 'a', 'dodgey', 'one', 'expecting', 'mine', 'tomo', 'see', 'you', 'for', 'recovery', 'same', 'time', 'same', 'place']\n",
      "After stop words removal: ['trouble', 'classes', 'go', 'well', 'due', 'dodgey', 'one', 'expecting', 'mine', 'tomo', 'see', 'recovery', 'time', 'place']\n",
      "expect\n",
      "After stemming with porters algorithm: ['troubl', 'class', 'well', 'due', 'dodgei', 'on', 'expec', 'mine', 'tomo', 'see', 'recoveri', 'time', 'place']\n",
      "Tokenized sentence: ['our', 'mobile', 'number', 'has', 'won', 'to', 'claim', 'calls', 'us', 'back', 'or', 'ring', 'the', 'claims', 'hot', 'line', 'on']\n",
      "After stop words removal: ['mobile', 'number', 'claim', 'calls', 'us', 'back', 'ring', 'claims', 'hot', 'line']\n",
      "After stemming with porters algorithm: ['mobil', 'number', 'claim', 'call', 'back', 'ring', 'claim', 'hot', 'line']\n",
      "Tokenized sentence: ['in', 'the', 'simpsons', 'movie', 'released', 'in', 'july', 'name', 'the', 'band', 'that', 'died', 'at', 'the', 'start', 'of', 'the', 'film', 'a', 'green', 'day', 'b', 'blue', 'day', 'c', 'red', 'day', 'send', 'a', 'b', 'or', 'c']\n",
      "After stop words removal: ['simpsons', 'movie', 'released', 'july', 'name', 'band', 'died', 'start', 'film', 'green', 'day', 'b', 'blue', 'day', 'c', 'red', 'day', 'send', 'b', 'c']\n",
      "After stemming with porters algorithm: ['simpson', 'movi', 'releas', 'juli', 'name', 'band', 'di', 'start', 'film', 'green', 'dai', 'blue', 'dai', 'red', 'dai', 'send']\n",
      "Tokenized sentence: ['gud', 'mrng', 'dear', 'have', 'a', 'nice', 'day']\n",
      "After stop words removal: ['gud', 'mrng', 'dear', 'nice', 'day']\n",
      "After stemming with porters algorithm: ['gud', 'mrng', 'dear', 'nice', 'dai']\n",
      "Tokenized sentence: ['storming', 'msg', 'wen', 'u', 'lift', 'd', 'phne', 'u', 'say', 'hello', 'do', 'u', 'knw', 'wt', 'is', 'd', 'real', 'meaning', 'of', 'hello', 'it', 's', 'd', 'name', 'of', 'a', 'girl', 'yes', 'and', 'u', 'knw', 'who', 'is', 'dat', 'girl', 'margaret', 'hello', 'she', 'is', 'd', 'girlfrnd', 'f', 'grahmbell', 'who', 'invnted', 'telphone', 'moral', 'one', 'can', 'get', 'd', 'name', 'of', 'a', 'person']\n",
      "After stop words removal: ['storming', 'msg', 'wen', 'u', 'lift', 'phne', 'u', 'say', 'hello', 'u', 'knw', 'wt', 'real', 'meaning', 'hello', 'name', 'girl', 'yes', 'u', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'f', 'grahmbell', 'invnted', 'telphone', 'moral', 'one', 'get', 'name', 'person']\n",
      "storm\n",
      "mean\n",
      "After stemming with porters algorithm: ['stor', 'msg', 'wen', 'lift', 'phne', 'sai', 'hello', 'knw', 'real', 'mean', 'hello', 'name', 'girl', 'ye', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'grahmbel', 'invn', 'telphon', 'moral', 'on', 'get', 'name', 'person']\n",
      "Tokenized sentence: ['jus', 'finish', 'bathing']\n",
      "After stop words removal: ['jus', 'finish', 'bathing']\n",
      "bath\n",
      "After stemming with porters algorithm: ['ju', 'finish', 'bat']\n",
      "Tokenized sentence: ['leave', 'it', 'u', 'will', 'always', 'be', 'ignorant']\n",
      "After stop words removal: ['leave', 'u', 'always', 'ignorant']\n",
      "After stemming with porters algorithm: ['leav', 'alwai', 'ignor']\n",
      "Tokenized sentence: ['hey', 'sexy', 'buns', 'what', 'of', 'that', 'day', 'no', 'word', 'from', 'you', 'this', 'morning', 'on', 'ym', 'i', 'think', 'of', 'you']\n",
      "After stop words removal: ['hey', 'sexy', 'buns', 'day', 'word', 'morning', 'ym', 'think']\n",
      "morn\n",
      "After stemming with porters algorithm: ['hei', 'sexi', 'bun', 'dai', 'word', 'mor', 'think']\n",
      "Tokenized sentence: ['he', 'remains', 'a', 'bro', 'amongst', 'bros']\n",
      "After stop words removal: ['remains', 'bro', 'amongst', 'bros']\n",
      "After stemming with porters algorithm: ['remain', 'bro', 'amongst', 'bro']\n",
      "Tokenized sentence: ['yes', 'sura', 'in', 'sun', 'tv', 'lol']\n",
      "After stop words removal: ['yes', 'sura', 'sun', 'tv', 'lol']\n",
      "After stemming with porters algorithm: ['ye', 'sura', 'sun', 'lol']\n",
      "Tokenized sentence: ['we', 'know', 'taj', 'mahal', 'as', 'symbol', 'of', 'love', 'but', 'the', 'other', 'lesser', 'known', 'facts', 'mumtaz', 'was', 'shahjahan', 's', 'th', 'wife', 'out', 'of', 'his', 'wifes', 'shahjahan', 'killed', 'mumtaz', 's', 'husband', 'to', 'marry', 'her', 'mumtaz', 'died', 'in', 'her', 'lt', 'gt', 'th', 'delivery', 'he', 'then', 'married', 'mumtaz', 's', 'sister', 'question', 'arises', 'where', 'the', 'hell', 'is', 'the', 'love', 'the', 'great', 'hari']\n",
      "After stop words removal: ['know', 'taj', 'mahal', 'symbol', 'love', 'lesser', 'known', 'facts', 'mumtaz', 'shahjahan', 'th', 'wife', 'wifes', 'shahjahan', 'killed', 'mumtaz', 'husband', 'marry', 'mumtaz', 'died', 'lt', 'gt', 'th', 'delivery', 'married', 'mumtaz', 'sister', 'question', 'arises', 'hell', 'love', 'great', 'hari']\n",
      "After stemming with porters algorithm: ['know', 'taj', 'mahal', 'symbol', 'love', 'lesser', 'known', 'fact', 'mumtaz', 'shahjahan', 'wife', 'wife', 'shahjahan', 'kill', 'mumtaz', 'husband', 'marri', 'mumtaz', 'di', 'deliveri', 'marri', 'mumtaz', 'sister', 'quest', 'aris', 'hell', 'love', 'great', 'hari']\n",
      "Tokenized sentence: ['go', 'chase', 'after', 'her', 'and', 'run', 'her', 'over', 'while', 'she', 's', 'crossing', 'the', 'street']\n",
      "After stop words removal: ['go', 'chase', 'run', 'crossing', 'street']\n",
      "cross\n",
      "After stemming with porters algorithm: ['chase', 'run', 'cross', 'street']\n",
      "Tokenized sentence: ['let', 'me', 'know', 'how', 'to', 'contact', 'you', 'i', 've', 'you', 'settled', 'in', 'a', 'room', 'lets', 'know', 'you', 'are', 'ok']\n",
      "After stop words removal: ['let', 'know', 'contact', 'settled', 'room', 'lets', 'know', 'ok']\n",
      "After stemming with porters algorithm: ['let', 'know', 'contact', 'settl', 'room', 'let', 'know']\n",
      "Tokenized sentence: ['wonders', 'in', 'my', 'world', 'th', 'you', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'and', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "After stop words removal: ['wonders', 'world', 'th', 'th', 'ur', 'style', 'th', 'ur', 'smile', 'th', 'ur', 'personality', 'rd', 'ur', 'nature', 'nd', 'ur', 'sms', 'st', 'ur', 'lovely', 'friendship', 'good', 'morning', 'dear']\n",
      "morn\n",
      "After stemming with porters algorithm: ['wonder', 'world', 'style', 'smile', 'person', 'natur', 'sm', 'love', 'friendship', 'good', 'mor', 'dear']\n",
      "Tokenized sentence: ['hope', 'you', 'are', 'having', 'a', 'good', 'week', 'just', 'checking', 'in']\n",
      "After stop words removal: ['hope', 'good', 'week', 'checking']\n",
      "check\n",
      "After stemming with porters algorithm: ['hope', 'good', 'week', 'chec']\n",
      "Tokenized sentence: ['s', 'fine', 'anytime', 'all', 'the', 'best', 'with', 'it']\n",
      "After stop words removal: ['fine', 'anytime', 'best']\n",
      "After stemming with porters algorithm: ['fine', 'anytim', 'best']\n",
      "Tokenized sentence: ['deep', 'sigh', 'i', 'miss', 'you', 'i', 'am', 'really', 'surprised', 'you', 'haven', 't', 'gone', 'to', 'the', 'net', 'cafe', 'yet', 'to', 'get', 'to', 'me', 'don', 't', 'you', 'miss', 'me']\n",
      "After stop words removal: ['deep', 'sigh', 'miss', 'really', 'surprised', 'gone', 'net', 'cafe', 'yet', 'get', 'miss']\n",
      "After stemming with porters algorithm: ['deep', 'sigh', 'miss', 'realli', 'surpris', 'gone', 'net', 'cafe', 'yet', 'get', 'miss']\n",
      "Tokenized sentence: ['what', 's', 'a', 'feathery', 'bowa', 'is', 'that', 'something', 'guys', 'have', 'that', 'i', 'don', 't', 'know', 'about']\n",
      "After stop words removal: ['feathery', 'bowa', 'something', 'guys', 'know']\n",
      "someth\n",
      "After stemming with porters algorithm: ['featheri', 'bowa', 'somet', 'gui', 'know']\n",
      "Tokenized sentence: ['hi', 'if', 'ur', 'lookin', 'saucy', 'daytime', 'fun', 'wiv', 'busty', 'married', 'woman', 'am', 'free', 'all', 'next', 'week', 'chat', 'now', 'sort', 'time', 'janinexx', 'calls', 'minmobsmorelkpobox', 'hp', 'fl']\n",
      "After stop words removal: ['hi', 'ur', 'lookin', 'saucy', 'daytime', 'fun', 'wiv', 'busty', 'married', 'woman', 'free', 'next', 'week', 'chat', 'sort', 'time', 'janinexx', 'calls', 'minmobsmorelkpobox', 'hp', 'fl']\n",
      "After stemming with porters algorithm: ['lookin', 'sauci', 'daytim', 'fun', 'wiv', 'busti', 'marri', 'woman', 'free', 'next', 'week', 'chat', 'sort', 'time', 'janinexx', 'call', 'minmobsmorelkpobox']\n",
      "Tokenized sentence: ['claim', 'a', 'shopping', 'spree', 'just', 'call', 'now', 'have', 'you', 'won', 'mobstorequiz', 'ppm']\n",
      "After stop words removal: ['claim', 'shopping', 'spree', 'call', 'mobstorequiz', 'ppm']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['claim', 'shop', 'spree', 'call', 'mobstorequiz', 'ppm']\n",
      "Tokenized sentence: ['okie']\n",
      "After stop words removal: ['okie']\n",
      "After stemming with porters algorithm: ['oki']\n",
      "Tokenized sentence: ['god', 'created', 'gap', 'btwn', 'ur', 'fingers', 'so', 'dat', 'sum', 'vry', 'special', 'will', 'fill', 'those', 'gaps', 'by', 'holding', 'ur', 'hands', 'now', 'plz', 'dont', 'ask', 'y', 'he', 'created', 'so', 'much', 'gap', 'between', 'legs']\n",
      "After stop words removal: ['god', 'created', 'gap', 'btwn', 'ur', 'fingers', 'dat', 'sum', 'vry', 'special', 'fill', 'gaps', 'holding', 'ur', 'hands', 'plz', 'dont', 'ask', 'created', 'much', 'gap', 'legs']\n",
      "create\n",
      "hold\n",
      "create\n",
      "After stemming with porters algorithm: ['god', 'creat', 'gap', 'btwn', 'finger', 'dat', 'sum', 'vry', 'special', 'fill', 'gap', 'hol', 'hand', 'plz', 'dont', 'ask', 'creat', 'much', 'gap', 'leg']\n",
      "Tokenized sentence: ['goodmorning', 'today', 'i', 'am', 'late', 'for', 'hr']\n",
      "After stop words removal: ['goodmorning', 'today', 'late', 'hr']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['goodmor', 'todai', 'late']\n",
      "Tokenized sentence: ['got', 'hella', 'gas', 'money', 'want', 'to', 'go', 'on', 'a', 'grand', 'nature', 'adventure', 'with', 'galileo', 'in', 'a', 'little', 'bit']\n",
      "After stop words removal: ['got', 'hella', 'gas', 'money', 'want', 'go', 'grand', 'nature', 'adventure', 'galileo', 'little', 'bit']\n",
      "After stemming with porters algorithm: ['got', 'hella', 'ga', 'monei', 'want', 'grand', 'natur', 'adventur', 'galileo', 'littl', 'bit']\n",
      "Tokenized sentence: ['sex', 'up', 'ur', 'mobile', 'with', 'a', 'free', 'sexy', 'pic', 'of', 'jordan', 'just', 'text', 'babe', 'to', 'then', 'every', 'wk', 'get', 'a', 'sexy', 'celeb', 'pocketbabe', 'co', 'uk', 'more', 'pics', 'wk']\n",
      "After stop words removal: ['sex', 'ur', 'mobile', 'free', 'sexy', 'pic', 'jordan', 'text', 'babe', 'every', 'wk', 'get', 'sexy', 'celeb', 'pocketbabe', 'co', 'uk', 'pics', 'wk']\n",
      "After stemming with porters algorithm: ['sex', 'mobil', 'free', 'sexi', 'pic', 'jordan', 'text', 'babe', 'everi', 'get', 'sexi', 'celeb', 'pocketbab', 'pic']\n",
      "Tokenized sentence: ['ok', 'lor', 'anyway', 'i', 'thk', 'we', 'cant', 'get', 'tickets', 'now', 'cos', 'like', 'quite', 'late', 'already', 'u', 'wan', 'go', 'look', 'ur', 'frens', 'a', 'not', 'darren', 'is', 'wif', 'them', 'now']\n",
      "After stop words removal: ['ok', 'lor', 'anyway', 'thk', 'cant', 'get', 'tickets', 'cos', 'like', 'quite', 'late', 'already', 'u', 'wan', 'go', 'look', 'ur', 'frens', 'darren', 'wif']\n",
      "After stemming with porters algorithm: ['lor', 'anywai', 'thk', 'cant', 'get', 'ticket', 'co', 'like', 'quit', 'late', 'alreadi', 'wan', 'look', 'fren', 'darren', 'wif']\n",
      "Tokenized sentence: ['ok', 'i', 'vl', 'do', 'u', 'know', 'i', 'got', 'adsense', 'approved']\n",
      "After stop words removal: ['ok', 'vl', 'u', 'know', 'got', 'adsense', 'approved']\n",
      "After stemming with porters algorithm: ['know', 'got', 'adsens', 'approv']\n",
      "Tokenized sentence: ['enjoy', 'urself', 'tmr']\n",
      "After stop words removal: ['enjoy', 'urself', 'tmr']\n",
      "After stemming with porters algorithm: ['enjoi', 'urself', 'tmr']\n",
      "Tokenized sentence: ['yup', 'no', 'more', 'already', 'thanx', 'printing', 'n', 'handing', 'it', 'up']\n",
      "After stop words removal: ['yup', 'already', 'thanx', 'printing', 'n', 'handing']\n",
      "print\n",
      "hand\n",
      "After stemming with porters algorithm: ['yup', 'alreadi', 'thanx', 'prin', 'han']\n",
      "Tokenized sentence: ['we', 'not', 'leaving', 'yet', 'ok', 'lor', 'then', 'we', 'go', 'elsewhere', 'n', 'eat', 'u', 'thk']\n",
      "After stop words removal: ['leaving', 'yet', 'ok', 'lor', 'go', 'elsewhere', 'n', 'eat', 'u', 'thk']\n",
      "leav\n",
      "After stemming with porters algorithm: ['leav', 'yet', 'lor', 'elsewher', 'eat', 'thk']\n",
      "Tokenized sentence: ['purity', 'of', 'friendship', 'between', 'two', 'is', 'not', 'about', 'smiling', 'after', 'reading', 'the', 'forwarded', 'message', 'its', 'about', 'smiling', 'just', 'by', 'seeing', 'the', 'name', 'gud', 'evng', 'musthu']\n",
      "After stop words removal: ['purity', 'friendship', 'two', 'smiling', 'reading', 'forwarded', 'message', 'smiling', 'seeing', 'name', 'gud', 'evng', 'musthu']\n",
      "smil\n",
      "read\n",
      "smil\n",
      "see\n",
      "After stemming with porters algorithm: ['puriti', 'friendship', 'two', 'smile', 'read', 'forwar', 'messag', 'smile', 'see', 'name', 'gud', 'evng', 'musthu']\n",
      "Tokenized sentence: ['it', 's', 'still', 'not', 'working', 'and', 'this', 'time', 'i', 'also', 'tried', 'adding', 'zeros', 'that', 'was', 'the', 'savings', 'the', 'checking', 'is', 'lt', 'gt']\n",
      "After stop words removal: ['still', 'working', 'time', 'also', 'tried', 'adding', 'zeros', 'savings', 'checking', 'lt', 'gt']\n",
      "work\n",
      "add\n",
      "sav\n",
      "check\n",
      "After stemming with porters algorithm: ['still', 'wor', 'time', 'also', 'tri', 'ad', 'zero', 'save', 'chec']\n",
      "Tokenized sentence: ['as', 'a', 'valued', 'customer', 'i', 'am', 'pleased', 'to', 'advise', 'you', 'that', 'following', 'recent', 'review', 'of', 'your', 'mob', 'no', 'you', 'are', 'awarded', 'with', 'a', 'bonus', 'prize', 'call']\n",
      "After stop words removal: ['valued', 'customer', 'pleased', 'advise', 'following', 'recent', 'review', 'mob', 'awarded', 'bonus', 'prize', 'call']\n",
      "follow\n",
      "After stemming with porters algorithm: ['valu', 'custom', 'pleas', 'advis', 'follow', 'recent', 'review', 'mob', 'awar', 'bonu', 'priz', 'call']\n",
      "Tokenized sentence: ['they', 'have', 'a', 'thread', 'on', 'the', 'wishlist', 'section', 'of', 'the', 'forums', 'where', 'ppl', 'post', 'nitro', 'requests', 'start', 'from', 'the', 'last', 'page', 'and', 'collect', 'from', 'the', 'bottom', 'up']\n",
      "After stop words removal: ['thread', 'wishlist', 'section', 'forums', 'ppl', 'post', 'nitro', 'requests', 'start', 'last', 'page', 'collect', 'bottom']\n",
      "After stemming with porters algorithm: ['thread', 'wishlist', 'sect', 'forum', 'ppl', 'post', 'nitro', 'request', 'start', 'last', 'page', 'collect', 'bottom']\n",
      "Tokenized sentence: ['sorry', 'to', 'be', 'a', 'pain', 'is', 'it', 'ok', 'if', 'we', 'meet', 'another', 'night', 'i', 'spent', 'late', 'afternoon', 'in', 'casualty', 'and', 'that', 'means', 'i', 'haven', 't', 'done', 'any', 'of', 'y', 'stuff', 'moro', 'and', 'that', 'includes', 'all', 'my', 'time', 'sheets', 'and', 'that', 'sorry']\n",
      "After stop words removal: ['sorry', 'pain', 'ok', 'meet', 'another', 'night', 'spent', 'late', 'afternoon', 'casualty', 'means', 'done', 'stuff', 'moro', 'includes', 'time', 'sheets', 'sorry']\n",
      "After stemming with porters algorithm: ['sorri', 'pain', 'meet', 'anoth', 'night', 'spent', 'late', 'afternoon', 'casualti', 'mean', 'done', 'stuff', 'moro', 'includ', 'time', 'sheet', 'sorri']\n",
      "Tokenized sentence: ['been', 'running', 'but', 'only', 'managed', 'minutes', 'and', 'then', 'needed', 'oxygen', 'might', 'have', 'to', 'resort', 'to', 'the', 'roller', 'option']\n",
      "After stop words removal: ['running', 'managed', 'minutes', 'needed', 'oxygen', 'might', 'resort', 'roller', 'option']\n",
      "runn\n",
      "After stemming with porters algorithm: ['run', 'manag', 'minut', 'need', 'oxygen', 'might', 'resort', 'roller', 'opt']\n",
      "Tokenized sentence: ['don', 't', 'give', 'a', 'flying', 'monkeys', 'wot', 'they', 'think', 'and', 'i', 'certainly', 'don', 't', 'mind', 'any', 'friend', 'of', 'mine', 'and', 'all', 'that']\n",
      "After stop words removal: ['give', 'flying', 'monkeys', 'wot', 'think', 'certainly', 'mind', 'friend', 'mine']\n",
      "After stemming with porters algorithm: ['give', 'flying', 'monkei', 'wot', 'think', 'certainli', 'mind', 'friend', 'mine']\n",
      "Tokenized sentence: ['dont', 'show', 'yourself', 'how', 'far', 'put', 'new', 'pictures', 'up', 'on', 'facebook']\n",
      "After stop words removal: ['dont', 'show', 'far', 'put', 'new', 'pictures', 'facebook']\n",
      "After stemming with porters algorithm: ['dont', 'show', 'far', 'put', 'new', 'pictur', 'facebook']\n",
      "Tokenized sentence: ['bishan', 'lar', 'nearer', 'no', 'need', 'buy', 'so', 'early', 'cos', 'if', 'buy', 'now', 'i', 'gotta', 'park', 'my', 'car']\n",
      "After stop words removal: ['bishan', 'lar', 'nearer', 'need', 'buy', 'early', 'cos', 'buy', 'gotta', 'park', 'car']\n",
      "After stemming with porters algorithm: ['bishan', 'lar', 'nearer', 'need', 'bui', 'earli', 'co', 'bui', 'gotta', 'park', 'car']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'ringtone', 'order', 'ref', 'number', 'k', 'your', 'mobile', 'will', 'be', 'charged', 'should', 'your', 'tone', 'not', 'arrive', 'please', 'call', 'customer', 'services', 'on']\n",
      "After stop words removal: ['thanks', 'ringtone', 'order', 'ref', 'number', 'k', 'mobile', 'charged', 'tone', 'arrive', 'please', 'call', 'customer', 'services']\n",
      "After stemming with porters algorithm: ['thank', 'rington', 'order', 'ref', 'number', 'mobil', 'char', 'tone', 'arriv', 'pleas', 'call', 'custom', 'servic']\n",
      "Tokenized sentence: ['eh', 'ur', 'laptop', 'got', 'no', 'stock', 'lei', 'he', 'say', 'mon', 'muz', 'come', 'again', 'to', 'take', 'a', 'look', 'c', 'got', 'a', 'not']\n",
      "After stop words removal: ['eh', 'ur', 'laptop', 'got', 'stock', 'lei', 'say', 'mon', 'muz', 'come', 'take', 'look', 'c', 'got']\n",
      "After stemming with porters algorithm: ['laptop', 'got', 'stock', 'lei', 'sai', 'mon', 'muz', 'come', 'take', 'look', 'got']\n",
      "Tokenized sentence: ['wat', 'time', 'u', 'finish', 'ur', 'lect', 'today']\n",
      "After stop words removal: ['wat', 'time', 'u', 'finish', 'ur', 'lect', 'today']\n",
      "After stemming with porters algorithm: ['wat', 'time', 'finish', 'lect', 'todai']\n",
      "Tokenized sentence: ['i', 'lost', 'pounds', 'since', 'my', 'doc', 'visit', 'last', 'week', 'woot', 'woot', 'now', 'i', 'm', 'gonna', 'celebrate', 'by', 'stuffing', 'my', 'face']\n",
      "After stop words removal: ['lost', 'pounds', 'since', 'doc', 'visit', 'last', 'week', 'woot', 'woot', 'gonna', 'celebrate', 'stuffing', 'face']\n",
      "stuff\n",
      "After stemming with porters algorithm: ['lost', 'pound', 'sinc', 'doc', 'visit', 'last', 'week', 'woot', 'woot', 'gonna', 'celebr', 'stuf', 'face']\n",
      "Tokenized sentence: ['as', 'a', 'registered', 'optin', 'subscriber', 'ur', 'draw', 'gift', 'voucher', 'will', 'be', 'entered', 'on', 'receipt', 'of', 'a', 'correct', 'ans', 'to', 'whats', 'no', 'in', 'the', 'bbc', 'charts']\n",
      "After stop words removal: ['registered', 'optin', 'subscriber', 'ur', 'draw', 'gift', 'voucher', 'entered', 'receipt', 'correct', 'ans', 'whats', 'bbc', 'charts']\n",
      "After stemming with porters algorithm: ['regist', 'optin', 'subscrib', 'draw', 'gift', 'voucher', 'enter', 'receipt', 'correct', 'an', 'what', 'bbc', 'chart']\n",
      "Tokenized sentence: ['have', 'a', 'nice', 'day', 'my', 'dear']\n",
      "After stop words removal: ['nice', 'day', 'dear']\n",
      "After stemming with porters algorithm: ['nice', 'dai', 'dear']\n",
      "Tokenized sentence: ['call', 'me', 'when', 'u', 'finish', 'then', 'i', 'come', 'n', 'pick', 'u']\n",
      "After stop words removal: ['call', 'u', 'finish', 'come', 'n', 'pick', 'u']\n",
      "After stemming with porters algorithm: ['call', 'finish', 'come', 'pick']\n",
      "Tokenized sentence: ['hi', 'baby', 'im', 'sat', 'on', 'the', 'bloody', 'bus', 'at', 'the', 'mo', 'and', 'i', 'wont', 'be', 'home', 'until', 'about', 'wanna', 'do', 'somethin', 'later', 'call', 'me', 'later', 'ortxt', 'back', 'jess', 'xx']\n",
      "After stop words removal: ['hi', 'baby', 'im', 'sat', 'bloody', 'bus', 'mo', 'wont', 'home', 'wanna', 'somethin', 'later', 'call', 'later', 'ortxt', 'back', 'jess', 'xx']\n",
      "After stemming with porters algorithm: ['babi', 'sat', 'bloodi', 'bu', 'wont', 'home', 'wanna', 'somethin', 'later', 'call', 'later', 'ortxt', 'back', 'jess']\n",
      "Tokenized sentence: ['one', 'small', 'prestige', 'problem', 'now']\n",
      "After stop words removal: ['one', 'small', 'prestige', 'problem']\n",
      "After stemming with porters algorithm: ['on', 'small', 'prestig', 'problem']\n",
      "Tokenized sentence: ['i', 'sent', 'your', 'maga', 'that', 'money', 'yesterday', 'oh']\n",
      "After stop words removal: ['sent', 'maga', 'money', 'yesterday', 'oh']\n",
      "After stemming with porters algorithm: ['sent', 'maga', 'monei', 'yesterdai']\n",
      "Tokenized sentence: ['http', 'tms', 'widelive', 'com', 'index', 'wml', 'id', 'ad', 'a', 'first', 'true', 'c', 'c', 'ringtone']\n",
      "After stop words removal: ['http', 'tms', 'widelive', 'com', 'index', 'wml', 'id', 'ad', 'first', 'true', 'c', 'c', 'ringtone']\n",
      "After stemming with porters algorithm: ['http', 'tm', 'widel', 'com', 'index', 'wml', 'first', 'true', 'rington']\n",
      "Tokenized sentence: ['pls', 'speak', 'with', 'me', 'i', 'wont', 'ask', 'anything', 'other', 'then', 'you', 'friendship']\n",
      "After stop words removal: ['pls', 'speak', 'wont', 'ask', 'anything', 'friendship']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['pl', 'speak', 'wont', 'ask', 'anyt', 'friendship']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['okay', 'lor', 'will', 'they', 'still', 'let', 'us', 'go', 'a', 'not', 'ah', 'coz', 'they', 'will', 'not', 'know', 'until', 'later', 'we', 'drop', 'our', 'cards', 'into', 'the', 'box', 'right']\n",
      "After stop words removal: ['okay', 'lor', 'still', 'let', 'us', 'go', 'ah', 'coz', 'know', 'later', 'drop', 'cards', 'box', 'right']\n",
      "After stemming with porters algorithm: ['okai', 'lor', 'still', 'let', 'coz', 'know', 'later', 'drop', 'card', 'box', 'right']\n",
      "Tokenized sentence: ['ever', 'thought', 'about', 'living', 'a', 'good', 'life', 'with', 'a', 'perfect', 'partner', 'just', 'txt', 'back', 'name', 'and', 'age', 'to', 'join', 'the', 'mobile', 'community', 'p', 'sms']\n",
      "After stop words removal: ['ever', 'thought', 'living', 'good', 'life', 'perfect', 'partner', 'txt', 'back', 'name', 'age', 'join', 'mobile', 'community', 'p', 'sms']\n",
      "liv\n",
      "After stemming with porters algorithm: ['ever', 'thought', 'live', 'good', 'life', 'perfect', 'partner', 'txt', 'back', 'name', 'ag', 'join', 'mobil', 'commun', 'sm']\n",
      "Tokenized sentence: ['wot', 'u', 'up', 'thout', 'u', 'were', 'gonna', 'call', 'me', 'txt', 'bak', 'luv', 'k']\n",
      "After stop words removal: ['wot', 'u', 'thout', 'u', 'gonna', 'call', 'txt', 'bak', 'luv', 'k']\n",
      "After stemming with porters algorithm: ['wot', 'thout', 'gonna', 'call', 'txt', 'bak', 'luv']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'a', 'new', 'video', 'handset', 'anytime', 'any', 'network', 'mins', 'half', 'price', 'line', 'rental', 'camcorder', 'reply', 'or', 'call', 'for', 'delivery', 'tomorrow']\n",
      "After stop words removal: ['want', 'new', 'video', 'handset', 'anytime', 'network', 'mins', 'half', 'price', 'line', 'rental', 'camcorder', 'reply', 'call', 'delivery', 'tomorrow']\n",
      "After stemming with porters algorithm: ['want', 'new', 'video', 'handset', 'anytim', 'network', 'min', 'half', 'price', 'line', 'rental', 'camcord', 'repli', 'call', 'deliveri', 'tomorrow']\n",
      "Tokenized sentence: ['happy', 'new', 'year', 'to', 'u', 'too']\n",
      "After stop words removal: ['happy', 'new', 'year', 'u']\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year']\n",
      "Tokenized sentence: ['xmas', 'new', 'years', 'eve', 'tickets', 'are', 'now', 'on', 'sale', 'from', 'the', 'club', 'during', 'the', 'day', 'from', 'am', 'till', 'pm', 'and', 'on', 'thurs', 'fri', 'sat', 'night', 'this', 'week', 'they', 're', 'selling', 'fast']\n",
      "After stop words removal: ['xmas', 'new', 'years', 'eve', 'tickets', 'sale', 'club', 'day', 'till', 'pm', 'thurs', 'fri', 'sat', 'night', 'week', 'selling', 'fast']\n",
      "sell\n",
      "After stemming with porters algorithm: ['xma', 'new', 'year', 'ev', 'ticket', 'sale', 'club', 'dai', 'till', 'thur', 'fri', 'sat', 'night', 'week', 'sell', 'fast']\n",
      "Tokenized sentence: ['ela', 'kano', 'il', 'download', 'come', 'wen', 'ur', 'free']\n",
      "After stop words removal: ['ela', 'kano', 'il', 'download', 'come', 'wen', 'ur', 'free']\n",
      "After stemming with porters algorithm: ['ela', 'kano', 'download', 'come', 'wen', 'free']\n",
      "Tokenized sentence: ['yetunde', 'i', 'm', 'in', 'class', 'can', 'you', 'not', 'run', 'water', 'on', 'it', 'to', 'make', 'it', 'ok', 'pls', 'now']\n",
      "After stop words removal: ['yetunde', 'class', 'run', 'water', 'make', 'ok', 'pls']\n",
      "After stemming with porters algorithm: ['yetund', 'class', 'run', 'water', 'make', 'pl']\n",
      "Tokenized sentence: ['even', 'my', 'brother', 'is', 'not', 'like', 'to', 'speak', 'with', 'me', 'they', 'treat', 'me', 'like', 'aids', 'patent']\n",
      "After stop words removal: ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent']\n",
      "After stemming with porters algorithm: ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent']\n",
      "Tokenized sentence: ['hiya', 'how', 'was', 'last', 'night', 'i', 've', 'been', 'naughty', 'and', 'bought', 'myself', 'clothes', 'and', 'very', 'little', 'ready', 'for', 'more', 'shopping', 'tho', 'what', 'kind', 'of', 'time', 'do', 'you', 'wanna', 'meet']\n",
      "After stop words removal: ['hiya', 'last', 'night', 'naughty', 'bought', 'clothes', 'little', 'ready', 'shopping', 'tho', 'kind', 'time', 'wanna', 'meet']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['hiya', 'last', 'night', 'naughti', 'bought', 'cloth', 'littl', 'readi', 'shop', 'tho', 'kind', 'time', 'wanna', 'meet']\n",
      "Tokenized sentence: ['i', 'm', 'good', 'have', 'you', 'registered', 'to', 'vote']\n",
      "After stop words removal: ['good', 'registered', 'vote']\n",
      "After stemming with porters algorithm: ['good', 'regist', 'vote']\n",
      "Tokenized sentence: ['no', 'he', 'joined', 'today', 'itself']\n",
      "After stop words removal: ['joined', 'today']\n",
      "After stemming with porters algorithm: ['join', 'todai']\n",
      "Tokenized sentence: ['do', 'you', 'know', 'where', 'my', 'lab', 'goggles', 'went']\n",
      "After stop words removal: ['know', 'lab', 'goggles', 'went']\n",
      "After stemming with porters algorithm: ['know', 'lab', 'goggl', 'went']\n",
      "Tokenized sentence: ['hi', 'i', 'm', 'sue', 'i', 'am', 'years', 'old', 'and', 'work', 'as', 'a', 'lapdancer', 'i', 'love', 'sex', 'text', 'me', 'live', 'i', 'm', 'i', 'my', 'bedroom', 'now', 'text', 'sue', 'to', 'by', 'textoperator', 'g', 'da', 'ppmsg']\n",
      "After stop words removal: ['hi', 'sue', 'years', 'old', 'work', 'lapdancer', 'love', 'sex', 'text', 'live', 'bedroom', 'text', 'sue', 'textoperator', 'g', 'da', 'ppmsg']\n",
      "After stemming with porters algorithm: ['sue', 'year', 'old', 'work', 'lapdanc', 'love', 'sex', 'text', 'live', 'bedroom', 'text', 'sue', 'textoper', 'ppmsg']\n",
      "Tokenized sentence: ['that', 's', 'very', 'rude', 'you', 'on', 'campus']\n",
      "After stop words removal: ['rude', 'campus']\n",
      "After stemming with porters algorithm: ['rude', 'campu']\n",
      "Tokenized sentence: ['ok', 'i', 've', 'sent', 'u', 'da', 'latest', 'version', 'of', 'da', 'project']\n",
      "After stop words removal: ['ok', 'sent', 'u', 'da', 'latest', 'version', 'da', 'project']\n",
      "After stemming with porters algorithm: ['sent', 'latest', 'version', 'project']\n",
      "Tokenized sentence: ['i', 'will', 'treasure', 'every', 'moment', 'we', 'spend', 'together']\n",
      "After stop words removal: ['treasure', 'every', 'moment', 'spend', 'together']\n",
      "After stemming with porters algorithm: ['treasur', 'everi', 'moment', 'spend', 'togeth']\n",
      "Tokenized sentence: ['i', 've', 'got', 'ten', 'bucks', 'jay', 'is', 'being', 'noncomittal']\n",
      "After stop words removal: ['got', 'ten', 'bucks', 'jay', 'noncomittal']\n",
      "After stemming with porters algorithm: ['got', 'ten', 'buck', 'jai', 'noncomitt']\n",
      "Tokenized sentence: ['you', 'have', 'an', 'important', 'customer', 'service', 'announcement', 'from', 'premier', 'call', 'freephone', 'now']\n",
      "After stop words removal: ['important', 'customer', 'service', 'announcement', 'premier', 'call', 'freephone']\n",
      "After stemming with porters algorithm: ['import', 'custom', 'servic', 'announc', 'premier', 'call', 'freephon']\n",
      "Tokenized sentence: ['ah', 'you', 'see', 'you', 'have', 'to', 'be', 'in', 'the', 'lingo', 'i', 'will', 'let', 'you', 'know', 'wot', 'on', 'earth', 'it', 'is', 'when', 'has', 'finished', 'making', 'it']\n",
      "After stop words removal: ['ah', 'see', 'lingo', 'let', 'know', 'wot', 'earth', 'finished', 'making']\n",
      "mak\n",
      "After stemming with porters algorithm: ['see', 'lingo', 'let', 'know', 'wot', 'earth', 'finis', 'make']\n",
      "Tokenized sentence: ['lol', 'yeah', 'at', 'this', 'point', 'i', 'guess', 'not']\n",
      "After stop words removal: ['lol', 'yeah', 'point', 'guess']\n",
      "After stemming with porters algorithm: ['lol', 'yeah', 'point', 'guess']\n",
      "Tokenized sentence: ['message', 'some', 'text', 'missing', 'sender', 'name', 'missing', 'number', 'missing', 'sent', 'date', 'missing', 'missing', 'u', 'a', 'lot', 'thats', 'y', 'everything', 'is', 'missing', 'sent', 'via', 'fullonsms', 'com']\n",
      "After stop words removal: ['message', 'text', 'missing', 'sender', 'name', 'missing', 'number', 'missing', 'sent', 'date', 'missing', 'missing', 'u', 'lot', 'thats', 'everything', 'missing', 'sent', 'via', 'fullonsms', 'com']\n",
      "miss\n",
      "miss\n",
      "miss\n",
      "miss\n",
      "miss\n",
      "everyth\n",
      "miss\n",
      "After stemming with porters algorithm: ['messag', 'text', 'miss', 'sender', 'name', 'miss', 'number', 'miss', 'sent', 'date', 'miss', 'miss', 'lot', 'that', 'everyt', 'miss', 'sent', 'via', 'fullonsm', 'com']\n",
      "Tokenized sentence: ['new', 'theory', 'argument', 'wins', 'd', 'situation', 'but', 'loses', 'the', 'person', 'so', 'dont', 'argue', 'with', 'ur', 'friends', 'just', 'kick', 'them', 'amp', 'say', 'i', 'm', 'always', 'correct']\n",
      "After stop words removal: ['new', 'theory', 'argument', 'wins', 'situation', 'loses', 'person', 'dont', 'argue', 'ur', 'friends', 'kick', 'amp', 'say', 'always', 'correct']\n",
      "After stemming with porters algorithm: ['new', 'theori', 'argum', 'win', 'situat', 'lose', 'person', 'dont', 'argu', 'friend', 'kick', 'amp', 'sai', 'alwai', 'correct']\n",
      "Tokenized sentence: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'it', 'seriously', 'coz', 'being', 'angry', 'is', 'd', 'most', 'childish', 'n', 'true', 'way', 'of', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'have', 'nice', 'day', 'da']\n",
      "After stop words removal: ['wen', 'ur', 'lovable', 'bcums', 'angry', 'wid', 'u', 'dnt', 'take', 'seriously', 'coz', 'angry', 'childish', 'n', 'true', 'way', 'showing', 'deep', 'affection', 'care', 'n', 'luv', 'kettoda', 'manda', 'nice', 'day', 'da']\n",
      "show\n",
      "After stemming with porters algorithm: ['wen', 'lovab', 'bcum', 'angri', 'wid', 'dnt', 'take', 'serious', 'coz', 'angri', 'childish', 'true', 'wai', 'showe', 'deep', 'affect', 'care', 'luv', 'kettoda', 'manda', 'nice', 'dai']\n",
      "Tokenized sentence: ['yep', 'i', 'do', 'like', 'the', 'pink', 'furniture', 'tho']\n",
      "After stop words removal: ['yep', 'like', 'pink', 'furniture', 'tho']\n",
      "After stemming with porters algorithm: ['yep', 'like', 'pink', 'furnitur', 'tho']\n",
      "Tokenized sentence: ['not', 'yet', 'had', 'ya', 'sapna', 'aunty', 'manege', 'y', 'day', 'hogidhe', 'chinnu', 'full', 'weak', 'and', 'swalpa', 'black', 'agidhane']\n",
      "After stop words removal: ['yet', 'ya', 'sapna', 'aunty', 'manege', 'day', 'hogidhe', 'chinnu', 'full', 'weak', 'swalpa', 'black', 'agidhane']\n",
      "After stemming with porters algorithm: ['yet', 'sapna', 'aunti', 'maneg', 'dai', 'hogidh', 'chinnu', 'full', 'weak', 'swalpa', 'black', 'agidhan']\n",
      "Tokenized sentence: ['wat', 'time', 'do', 'u', 'wan', 'meet', 'me', 'later']\n",
      "After stop words removal: ['wat', 'time', 'u', 'wan', 'meet', 'later']\n",
      "After stemming with porters algorithm: ['wat', 'time', 'wan', 'meet', 'later']\n",
      "Tokenized sentence: ['hey', 'i', 'will', 'be', 'really', 'pretty', 'late', 'you', 'want', 'to', 'go', 'for', 'the', 'lesson', 'first', 'i', 'will', 'join', 'you', 'i', 'm', 'only', 'reaching', 'tp', 'mrt']\n",
      "After stop words removal: ['hey', 'really', 'pretty', 'late', 'want', 'go', 'lesson', 'first', 'join', 'reaching', 'tp', 'mrt']\n",
      "reach\n",
      "After stemming with porters algorithm: ['hei', 'realli', 'pretti', 'late', 'want', 'lesson', 'first', 'join', 'reac', 'mrt']\n",
      "Tokenized sentence: ['for', 'ur', 'chance', 'to', 'win', 'a', 'wkly', 'shopping', 'spree', 'txt', 'shop', 'to', 't', 's', 'c', 's', 'www', 'txt', 'shop', 'com', 'custcare', 'x', 'p', 'wk']\n",
      "After stop words removal: ['ur', 'chance', 'win', 'wkly', 'shopping', 'spree', 'txt', 'shop', 'c', 'www', 'txt', 'shop', 'com', 'custcare', 'x', 'p', 'wk']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['chanc', 'win', 'wkly', 'shop', 'spree', 'txt', 'shop', 'www', 'txt', 'shop', 'com', 'custcar']\n",
      "Tokenized sentence: ['ok', 'take', 'care', 'umma', 'to', 'you', 'too']\n",
      "After stop words removal: ['ok', 'take', 'care', 'umma']\n",
      "After stemming with porters algorithm: ['take', 'care', 'umma']\n",
      "Tokenized sentence: ['there', 's', 'someone', 'here', 'that', 'has', 'a', 'year', 'lt', 'gt', 'toyota', 'camry', 'like', 'mr', 'olayiwola', 's', 'own', 'mileage', 'is', 'lt', 'gt', 'k', 'its', 'clean', 'but', 'i', 'need', 'to', 'know', 'how', 'much', 'will', 'it', 'sell', 'for', 'if', 'i', 'can', 'raise', 'the', 'dough', 'for', 'it', 'how', 'soon', 'after', 'landing', 'will', 'it', 'sell', 'holla', 'back']\n",
      "After stop words removal: ['someone', 'year', 'lt', 'gt', 'toyota', 'camry', 'like', 'mr', 'olayiwola', 'mileage', 'lt', 'gt', 'k', 'clean', 'need', 'know', 'much', 'sell', 'raise', 'dough', 'soon', 'landing', 'sell', 'holla', 'back']\n",
      "land\n",
      "After stemming with porters algorithm: ['someon', 'year', 'toyota', 'camri', 'like', 'olayiwola', 'mileag', 'clean', 'need', 'know', 'much', 'sell', 'rais', 'dough', 'soon', 'lan', 'sell', 'holla', 'back']\n",
      "Tokenized sentence: ['and', 'maybe', 'some', 'pressies']\n",
      "After stop words removal: ['maybe', 'pressies']\n",
      "After stemming with porters algorithm: ['mayb', 'pressi']\n",
      "Tokenized sentence: ['hanks', 'lotsly']\n",
      "After stop words removal: ['hanks', 'lotsly']\n",
      "After stemming with porters algorithm: ['hank', 'lotsli']\n",
      "Tokenized sentence: ['dude', 'what', 's', 'up', 'how', 'teresa', 'hope', 'you', 'have', 'been', 'okay', 'when', 'i', 'didnt', 'hear', 'from', 'these', 'people', 'i', 'called', 'them', 'and', 'they', 'had', 'received', 'the', 'package', 'since', 'dec', 'lt', 'gt', 'just', 'thot', 'you', 'ld', 'like', 'to', 'know', 'do', 'have', 'a', 'fantastic', 'year', 'and', 'all', 'the', 'best', 'with', 'your', 'reading', 'plus', 'if', 'you', 'can', 'really', 'really', 'bam', 'first', 'aid', 'for', 'usmle', 'then', 'your', 'work', 'is', 'done']\n",
      "After stop words removal: ['dude', 'teresa', 'hope', 'okay', 'didnt', 'hear', 'people', 'called', 'received', 'package', 'since', 'dec', 'lt', 'gt', 'thot', 'ld', 'like', 'know', 'fantastic', 'year', 'best', 'reading', 'plus', 'really', 'really', 'bam', 'first', 'aid', 'usmle', 'work', 'done']\n",
      "read\n",
      "After stemming with porters algorithm: ['dude', 'teresa', 'hope', 'okai', 'didnt', 'hear', 'peopl', 'call', 'receiv', 'packag', 'sinc', 'dec', 'thot', 'like', 'know', 'fantast', 'year', 'best', 'read', 'plu', 'realli', 'realli', 'bam', 'first', 'aid', 'usml', 'work', 'done']\n",
      "Tokenized sentence: ['go', 'home', 'liao', 'ask', 'dad', 'to', 'pick', 'me', 'up', 'at']\n",
      "After stop words removal: ['go', 'home', 'liao', 'ask', 'dad', 'pick']\n",
      "After stemming with porters algorithm: ['home', 'liao', 'ask', 'dad', 'pick']\n",
      "Tokenized sentence: ['my', 'sister', 'in', 'law', 'hope', 'you', 'are', 'having', 'a', 'great', 'month', 'just', 'saying', 'hey', 'abiola']\n",
      "After stop words removal: ['sister', 'law', 'hope', 'great', 'month', 'saying', 'hey', 'abiola']\n",
      "say\n",
      "After stemming with porters algorithm: ['sister', 'law', 'hope', 'great', 'month', 'sai', 'hei', 'abiola']\n",
      "Tokenized sentence: ['audrie', 'lousy', 'autocorrect']\n",
      "After stop words removal: ['audrie', 'lousy', 'autocorrect']\n",
      "After stemming with porters algorithm: ['audri', 'lousi', 'autocorrect']\n",
      "Tokenized sentence: ['boo', 'babe', 'u', 'enjoyin', 'yourjob', 'u', 'seemed', 'b', 'gettin', 'on', 'well', 'hunny', 'hope', 'ure', 'ok', 'take', 'care', 'i', 'llspeak', 'u', 'soonlots', 'of', 'loveme', 'xxxx']\n",
      "After stop words removal: ['boo', 'babe', 'u', 'enjoyin', 'yourjob', 'u', 'seemed', 'b', 'gettin', 'well', 'hunny', 'hope', 'ure', 'ok', 'take', 'care', 'llspeak', 'u', 'soonlots', 'loveme', 'xxxx']\n",
      "After stemming with porters algorithm: ['boo', 'babe', 'enjoyin', 'yourjob', 'seem', 'gettin', 'well', 'hunni', 'hope', 'ur', 'take', 'care', 'llspeak', 'soonlot', 'lovem', 'xxxx']\n",
      "Tokenized sentence: ['anyway', 'holla', 'at', 'me', 'whenever', 'you', 're', 'around', 'because', 'i', 'need', 'an', 'excuse', 'to', 'go', 'creep', 'on', 'people', 'in', 'sarasota']\n",
      "After stop words removal: ['anyway', 'holla', 'whenever', 'around', 'need', 'excuse', 'go', 'creep', 'people', 'sarasota']\n",
      "After stemming with porters algorithm: ['anywai', 'holla', 'whenev', 'around', 'need', 'excus', 'creep', 'peopl', 'sarasota']\n",
      "Tokenized sentence: ['aiyar', 'sorry', 'lor', 'forgot', 'tell', 'u']\n",
      "After stop words removal: ['aiyar', 'sorry', 'lor', 'forgot', 'tell', 'u']\n",
      "After stemming with porters algorithm: ['aiyar', 'sorri', 'lor', 'forgot', 'tell']\n",
      "Tokenized sentence: ['from', 'next', 'month', 'get', 'upto', 'more', 'calls', 'ur', 'standard', 'network', 'charge', 'activate', 'call', 'c', 'wire', 'net', 'st', 'terms', 'pobox', 'm', 'uz', 'cost', 'min', 'mobcudb', 'more']\n",
      "After stop words removal: ['next', 'month', 'get', 'upto', 'calls', 'ur', 'standard', 'network', 'charge', 'activate', 'call', 'c', 'wire', 'net', 'st', 'terms', 'pobox', 'uz', 'cost', 'min', 'mobcudb']\n",
      "After stemming with porters algorithm: ['next', 'month', 'get', 'upto', 'call', 'standard', 'network', 'charg', 'activ', 'call', 'wire', 'net', 'term', 'pobox', 'cost', 'min', 'mobcudb']\n",
      "Tokenized sentence: ['oh', 'for', 'fuck', 's', 'sake', 'she', 's', 'in', 'like', 'tallahassee']\n",
      "After stop words removal: ['oh', 'fuck', 'sake', 'like', 'tallahassee']\n",
      "After stemming with porters algorithm: ['fuck', 'sake', 'like', 'tallahasse']\n",
      "Tokenized sentence: ['eh', 'den', 'sat', 'u', 'book', 'e', 'kb', 'liao', 'huh']\n",
      "After stop words removal: ['eh', 'den', 'sat', 'u', 'book', 'e', 'kb', 'liao', 'huh']\n",
      "After stemming with porters algorithm: ['den', 'sat', 'book', 'liao', 'huh']\n",
      "Tokenized sentence: ['g', 'wants', 'to', 'know', 'where', 'the', 'fuck', 'you', 'are']\n",
      "After stop words removal: ['g', 'wants', 'know', 'fuck']\n",
      "After stemming with porters algorithm: ['want', 'know', 'fuck']\n",
      "Tokenized sentence: ['i', 'sent', 'my', 'scores', 'to', 'sophas', 'and', 'i', 'had', 'to', 'do', 'secondary', 'application', 'for', 'a', 'few', 'schools', 'i', 'think', 'if', 'you', 'are', 'thinking', 'of', 'applying', 'do', 'a', 'research', 'on', 'cost', 'also', 'contact', 'joke', 'ogunrinde', 'her', 'school', 'is', 'one', 'me', 'the', 'less', 'expensive', 'ones']\n",
      "After stop words removal: ['sent', 'scores', 'sophas', 'secondary', 'application', 'schools', 'think', 'thinking', 'applying', 'research', 'cost', 'also', 'contact', 'joke', 'ogunrinde', 'school', 'one', 'less', 'expensive', 'ones']\n",
      "think\n",
      "apply\n",
      "After stemming with porters algorithm: ['sent', 'score', 'sopha', 'secondari', 'applic', 'school', 'think', 'thin', 'appl', 'research', 'cost', 'also', 'contact', 'joke', 'ogunrind', 'school', 'on', 'less', 'expens', 'on']\n",
      "Tokenized sentence: ['wow', 'i', 'never', 'realized', 'that', 'you', 'were', 'so', 'embarassed', 'by', 'your', 'accomodations', 'i', 'thought', 'you', 'liked', 'it', 'since', 'i', 'was', 'doing', 'the', 'best', 'i', 'could', 'and', 'you', 'always', 'seemed', 'so', 'happy', 'about', 'the', 'cave', 'i', 'm', 'sorry', 'i', 'didn', 't', 'and', 'don', 't', 'have', 'more', 'to', 'give', 'i', 'm', 'sorry', 'i', 'offered', 'i', 'm', 'sorry', 'your', 'room', 'was', 'so', 'embarassing']\n",
      "After stop words removal: ['wow', 'never', 'realized', 'embarassed', 'accomodations', 'thought', 'liked', 'since', 'best', 'could', 'always', 'seemed', 'happy', 'cave', 'sorry', 'give', 'sorry', 'offered', 'sorry', 'room', 'embarassing']\n",
      "realize\n",
      "embarass\n",
      "After stemming with porters algorithm: ['wow', 'never', 'realiz', 'embarass', 'accomod', 'thought', 'like', 'sinc', 'best', 'could', 'alwai', 'seem', 'happi', 'cave', 'sorri', 'give', 'sorri', 'offer', 'sorri', 'room', 'embarass']\n",
      "Tokenized sentence: ['yunny', 'i', 'm', 'goin', 'to', 'be', 'late']\n",
      "After stop words removal: ['yunny', 'goin', 'late']\n",
      "After stemming with porters algorithm: ['yunni', 'goin', 'late']\n",
      "Tokenized sentence: ['hi', 'mate', 'its', 'rv', 'did', 'u', 'hav', 'a', 'nice', 'hol', 'just', 'a', 'message', 'say', 'hello', 'coz', 'haven', 't', 'sent', 'u', 'in', 'ages', 'started', 'driving', 'so', 'stay', 'off', 'roads', 'rvx']\n",
      "After stop words removal: ['hi', 'mate', 'rv', 'u', 'hav', 'nice', 'hol', 'message', 'say', 'hello', 'coz', 'sent', 'u', 'ages', 'started', 'driving', 'stay', 'roads', 'rvx']\n",
      "driv\n",
      "After stemming with porters algorithm: ['mate', 'hav', 'nice', 'hol', 'messag', 'sai', 'hello', 'coz', 'sent', 'ag', 'star', 'drive', 'stai', 'road', 'rvx']\n",
      "Tokenized sentence: ['i', 'don', 't', 'know', 'u', 'and', 'u', 'don', 't', 'know', 'me', 'send', 'chat', 'to', 'now', 'and', 'let', 's', 'find', 'each', 'other', 'only', 'p', 'msg', 'rcvd', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'years', 'or', 'over']\n",
      "After stop words removal: ['know', 'u', 'u', 'know', 'send', 'chat', 'let', 'find', 'p', 'msg', 'rcvd', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'years']\n",
      "After stemming with porters algorithm: ['know', 'know', 'send', 'chat', 'let', 'find', 'msg', 'rcvd', 'suit', 'land', 'row', 'ldn', 'year']\n",
      "Tokenized sentence: ['same', 'wana', 'plan', 'a', 'trip', 'sometme', 'then']\n",
      "After stop words removal: ['wana', 'plan', 'trip', 'sometme']\n",
      "After stemming with porters algorithm: ['wana', 'plan', 'trip', 'sometm']\n",
      "Tokenized sentence: ['wat', 'makes', 'some', 'people', 'dearer', 'is', 'not', 'just', 'de', 'happiness', 'dat', 'u', 'feel', 'when', 'u', 'meet', 'them', 'but', 'de', 'pain', 'u', 'feel', 'when', 'u', 'miss', 'dem']\n",
      "After stop words removal: ['wat', 'makes', 'people', 'dearer', 'de', 'happiness', 'dat', 'u', 'feel', 'u', 'meet', 'de', 'pain', 'u', 'feel', 'u', 'miss', 'dem']\n",
      "After stemming with porters algorithm: ['wat', 'make', 'peopl', 'dearer', 'happi', 'dat', 'feel', 'meet', 'pain', 'feel', 'miss', 'dem']\n",
      "Tokenized sentence: ['u', 'were', 'outbid', 'by', 'simonwatson', 'on', 'the', 'shinco', 'dvd', 'plyr', 'bid', 'again', 'visit', 'sms', 'ac', 'smsrewards', 'end', 'bid', 'notifications', 'reply', 'end', 'out']\n",
      "After stop words removal: ['u', 'outbid', 'simonwatson', 'shinco', 'dvd', 'plyr', 'bid', 'visit', 'sms', 'ac', 'smsrewards', 'end', 'bid', 'notifications', 'reply', 'end']\n",
      "After stemming with porters algorithm: ['outbid', 'simonwatson', 'shinco', 'dvd', 'plyr', 'bid', 'visit', 'sm', 'smsreward', 'end', 'bid', 'notif', 'repli', 'end']\n",
      "Tokenized sentence: ['k', 'i', 'm', 'leaving', 'soon', 'be', 'there', 'a', 'little', 'after']\n",
      "After stop words removal: ['k', 'leaving', 'soon', 'little']\n",
      "leav\n",
      "After stemming with porters algorithm: ['leav', 'soon', 'littl']\n",
      "Tokenized sentence: ['depends', 'on', 'quality', 'if', 'you', 'want', 'the', 'type', 'i', 'sent', 'boye', 'faded', 'glory', 'then', 'about', 'if', 'you', 'want', 'ralphs', 'maybe']\n",
      "After stop words removal: ['depends', 'quality', 'want', 'type', 'sent', 'boye', 'faded', 'glory', 'want', 'ralphs', 'maybe']\n",
      "After stemming with porters algorithm: ['depend', 'qualiti', 'want', 'type', 'sent', 'boy', 'fade', 'glori', 'want', 'ralph', 'mayb']\n",
      "Tokenized sentence: ['sorry', 'in', 'meeting', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'meeting', 'call', 'later']\n",
      "meet\n",
      "After stemming with porters algorithm: ['sorri', 'meet', 'call', 'later']\n",
      "Tokenized sentence: ['i', 'got', 'another', 'job', 'the', 'one', 'at', 'the', 'hospital', 'doing', 'data', 'analysis', 'or', 'something', 'starts', 'on', 'monday', 'not', 'sure', 'when', 'my', 'thesis', 'will', 'got', 'finished']\n",
      "After stop words removal: ['got', 'another', 'job', 'one', 'hospital', 'data', 'analysis', 'something', 'starts', 'monday', 'sure', 'thesis', 'got', 'finished']\n",
      "someth\n",
      "After stemming with porters algorithm: ['got', 'anoth', 'job', 'on', 'hospit', 'data', 'analysi', 'somet', 'start', 'mondai', 'sure', 'thesi', 'got', 'finis']\n",
      "Tokenized sentence: ['if', 'i', 'not', 'meeting', 'all', 'rite', 'then', 'i', 'll', 'go', 'home', 'lor', 'if', 'dun', 'feel', 'like', 'comin', 'it', 's', 'ok']\n",
      "After stop words removal: ['meeting', 'rite', 'go', 'home', 'lor', 'dun', 'feel', 'like', 'comin', 'ok']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'rite', 'home', 'lor', 'dun', 'feel', 'like', 'comin']\n",
      "Tokenized sentence: ['ur', 'going', 'bahamas', 'callfreefone', 'and', 'speak', 'to', 'a', 'live', 'operator', 'to', 'claim', 'either', 'bahamas', 'cruise', 'of', 'cash', 'only', 'to', 'opt', 'out', 'txt', 'x', 'to']\n",
      "After stop words removal: ['ur', 'going', 'bahamas', 'callfreefone', 'speak', 'live', 'operator', 'claim', 'either', 'bahamas', 'cruise', 'cash', 'opt', 'txt', 'x']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'bahama', 'callfreefon', 'speak', 'live', 'oper', 'claim', 'either', 'bahama', 'cruis', 'cash', 'opt', 'txt']\n",
      "Tokenized sentence: ['have', 'good', 'weekend']\n",
      "After stop words removal: ['good', 'weekend']\n",
      "After stemming with porters algorithm: ['good', 'weekend']\n",
      "Tokenized sentence: ['its', 'a', 'site', 'to', 'simulate', 'the', 'test', 'it', 'just', 'gives', 'you', 'very', 'tough', 'questions', 'to', 'test', 'your', 'readiness']\n",
      "After stop words removal: ['site', 'simulate', 'test', 'gives', 'tough', 'questions', 'test', 'readiness']\n",
      "After stemming with porters algorithm: ['site', 'simul', 'test', 'give', 'tough', 'quest', 'test', 'readi']\n",
      "Tokenized sentence: ['and', 'by', 'when', 'you', 're', 'done', 'i', 'mean', 'now']\n",
      "After stop words removal: ['done', 'mean']\n",
      "After stemming with porters algorithm: ['done', 'mean']\n",
      "Tokenized sentence: ['last', 'chance', 'claim', 'ur', 'worth', 'of', 'discount', 'vouchers', 'today', 'text', 'shop', 'to', 'now', 'savamob', 'offers', 'mobile', 't', 'cs', 'savamob', 'pobox', 'm', 'uz', 'sub']\n",
      "After stop words removal: ['last', 'chance', 'claim', 'ur', 'worth', 'discount', 'vouchers', 'today', 'text', 'shop', 'savamob', 'offers', 'mobile', 'cs', 'savamob', 'pobox', 'uz', 'sub']\n",
      "After stemming with porters algorithm: ['last', 'chanc', 'claim', 'worth', 'discount', 'voucher', 'todai', 'text', 'shop', 'savamob', 'offer', 'mobil', 'savamob', 'pobox', 'sub']\n",
      "Tokenized sentence: ['and', 'whenever', 'you', 'and', 'i', 'see', 'we', 'can', 'still', 'hook', 'up', 'too']\n",
      "After stop words removal: ['whenever', 'see', 'still', 'hook']\n",
      "After stemming with porters algorithm: ['whenev', 'see', 'still', 'hook']\n",
      "Tokenized sentence: ['carry', 'on', 'not', 'disturbing', 'both', 'of', 'you']\n",
      "After stop words removal: ['carry', 'disturbing']\n",
      "disturb\n",
      "After stemming with porters algorithm: ['carri', 'distur']\n",
      "Tokenized sentence: ['hope', 'you', 'are', 'feeling', 'great', 'pls', 'fill', 'me', 'in', 'abiola']\n",
      "After stop words removal: ['hope', 'feeling', 'great', 'pls', 'fill', 'abiola']\n",
      "feel\n",
      "After stemming with porters algorithm: ['hope', 'feel', 'great', 'pl', 'fill', 'abiola']\n",
      "Tokenized sentence: ['meet', 'you', 'in', 'corporation', 'st', 'outside', 'gap', 'you', 'can', 'see', 'how', 'my', 'mind', 'is', 'working']\n",
      "After stop words removal: ['meet', 'corporation', 'st', 'outside', 'gap', 'see', 'mind', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['meet', 'corpor', 'outsid', 'gap', 'see', 'mind', 'wor']\n",
      "Tokenized sentence: ['hello', 'baby', 'did', 'you', 'get', 'back', 'to', 'your', 'mom', 's', 'are', 'you', 'setting', 'up', 'the', 'computer', 'now', 'filling', 'your', 'belly', 'how', 'goes', 'it', 'loverboy', 'i', 'miss', 'you', 'already', 'sighs']\n",
      "After stop words removal: ['hello', 'baby', 'get', 'back', 'mom', 'setting', 'computer', 'filling', 'belly', 'goes', 'loverboy', 'miss', 'already', 'sighs']\n",
      "sett\n",
      "fill\n",
      "After stemming with porters algorithm: ['hello', 'babi', 'get', 'back', 'mom', 'set', 'comput', 'fill', 'belli', 'goe', 'loverboi', 'miss', 'alreadi', 'sigh']\n",
      "Tokenized sentence: ['aslamalaikkum', 'insha', 'allah', 'tohar', 'beeen', 'muht', 'albi', 'mufti', 'mahfuuz', 'meaning', 'same', 'here']\n",
      "After stop words removal: ['aslamalaikkum', 'insha', 'allah', 'tohar', 'beeen', 'muht', 'albi', 'mufti', 'mahfuuz', 'meaning']\n",
      "mean\n",
      "After stemming with porters algorithm: ['aslamalaikkum', 'insha', 'allah', 'tohar', 'beeen', 'muht', 'albi', 'mufti', 'mahfuuz', 'mean']\n",
      "Tokenized sentence: ['fine', 'i', 'miss', 'you', 'very', 'much']\n",
      "After stop words removal: ['fine', 'miss', 'much']\n",
      "After stemming with porters algorithm: ['fine', 'miss', 'much']\n",
      "Tokenized sentence: ['you', 'will', 'go', 'to', 'walmart', 'i', 'll', 'stay']\n",
      "After stop words removal: ['go', 'walmart', 'stay']\n",
      "After stemming with porters algorithm: ['walmart', 'stai']\n",
      "Tokenized sentence: ['happy', 'new', 'year', 'my', 'no', 'man']\n",
      "After stop words removal: ['happy', 'new', 'year', 'man']\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year', 'man']\n",
      "Tokenized sentence: ['what', 'u', 'wearing']\n",
      "After stop words removal: ['u', 'wearing']\n",
      "wear\n",
      "After stemming with porters algorithm: ['wear']\n",
      "Tokenized sentence: ['u', 'so', 'lousy', 'run', 'already', 'come', 'back', 'then', 'half', 'dead', 'hee']\n",
      "After stop words removal: ['u', 'lousy', 'run', 'already', 'come', 'back', 'half', 'dead', 'hee']\n",
      "After stemming with porters algorithm: ['lousi', 'run', 'alreadi', 'come', 'back', 'half', 'dead', 'hee']\n",
      "Tokenized sentence: ['ow', 'u', 'dey', 'i', 'paid', 'thousad', 'i', 'told', 'u', 'would', 'call']\n",
      "After stop words removal: ['ow', 'u', 'dey', 'paid', 'thousad', 'told', 'u', 'would', 'call']\n",
      "After stemming with porters algorithm: ['dei', 'paid', 'thousad', 'told', 'would', 'call']\n",
      "Tokenized sentence: ['pls', 'help', 'me', 'tell', 'sura', 'that', 'i', 'm', 'expecting', 'a', 'battery', 'from', 'hont', 'and', 'that', 'if', 'should', 'pls', 'send', 'me', 'a', 'message', 'about', 'how', 'to', 'download', 'movies', 'thanks']\n",
      "After stop words removal: ['pls', 'help', 'tell', 'sura', 'expecting', 'battery', 'hont', 'pls', 'send', 'message', 'download', 'movies', 'thanks']\n",
      "expect\n",
      "After stemming with porters algorithm: ['pl', 'help', 'tell', 'sura', 'expec', 'batteri', 'hont', 'pl', 'send', 'messag', 'download', 'movi', 'thank']\n",
      "Tokenized sentence: ['how', 'are', 'u', 'i', 'have', 'missed', 'u', 'i', 'havent', 'been', 'up', 'much', 'a', 'bit', 'bored', 'with', 'the', 'holiday', 'want', 'go', 'bak', 'college', 'sad', 'isnt', 'it', 'xx']\n",
      "After stop words removal: ['u', 'missed', 'u', 'havent', 'much', 'bit', 'bored', 'holiday', 'want', 'go', 'bak', 'college', 'sad', 'isnt', 'xx']\n",
      "After stemming with porters algorithm: ['miss', 'havent', 'much', 'bit', 'bore', 'holidai', 'want', 'bak', 'colleg', 'sad', 'isnt']\n",
      "Tokenized sentence: ['then', 'why', 'you', 'came', 'to', 'hostel']\n",
      "After stop words removal: ['came', 'hostel']\n",
      "After stemming with porters algorithm: ['came', 'hostel']\n",
      "Tokenized sentence: ['no', 'few', 'hours', 'before', 'went', 'to', 'hair', 'cut']\n",
      "After stop words removal: ['hours', 'went', 'hair', 'cut']\n",
      "After stemming with porters algorithm: ['hour', 'went', 'hair', 'cut']\n",
      "Tokenized sentence: ['i', 'was', 'wondering', 'if', 'it', 'would', 'be', 'okay', 'for', 'you', 'to', 'call', 'uncle', 'john', 'and', 'let', 'him', 'know', 'that', 'things', 'are', 'not', 'the', 'same', 'in', 'nigeria', 'as', 'they', 'r', 'here', 'that', 'lt', 'gt', 'dollars', 'is', 'years', 'sent', 'and', 'that', 'you', 'know', 'its', 'a', 'strain', 'but', 'i', 'plan', 'to', 'pay', 'back', 'every', 'dime', 'he', 'gives', 'every', 'dime', 'so', 'for', 'me', 'to', 'expect', 'anything', 'from', 'you', 'is', 'not', 'practical', 'something', 'like', 'that']\n",
      "After stop words removal: ['wondering', 'would', 'okay', 'call', 'uncle', 'john', 'let', 'know', 'things', 'nigeria', 'r', 'lt', 'gt', 'dollars', 'years', 'sent', 'know', 'strain', 'plan', 'pay', 'back', 'every', 'dime', 'gives', 'every', 'dime', 'expect', 'anything', 'practical', 'something', 'like']\n",
      "wonder\n",
      "anyth\n",
      "someth\n",
      "After stemming with porters algorithm: ['wonder', 'would', 'okai', 'call', 'uncl', 'john', 'let', 'know', 'thing', 'nigeria', 'dollar', 'year', 'sent', 'know', 'strain', 'plan', 'pai', 'back', 'everi', 'dime', 'give', 'everi', 'dime', 'expect', 'anyt', 'practic', 'somet', 'like']\n",
      "Tokenized sentence: ['or', 'ill', 'be', 'a', 'little', 'closer', 'like', 'at', 'the', 'bus', 'stop', 'on', 'the', 'same', 'street']\n",
      "After stop words removal: ['ill', 'little', 'closer', 'like', 'bus', 'stop', 'street']\n",
      "After stemming with porters algorithm: ['ill', 'littl', 'closer', 'like', 'bu', 'stop', 'street']\n",
      "Tokenized sentence: ['yes', 'obviously', 'but', 'you', 'are', 'the', 'eggs', 'pert', 'and', 'the', 'potato', 'head', 'speak', 'soon']\n",
      "After stop words removal: ['yes', 'obviously', 'eggs', 'pert', 'potato', 'head', 'speak', 'soon']\n",
      "After stemming with porters algorithm: ['ye', 'obvious', 'egg', 'pert', 'potato', 'head', 'speak', 'soon']\n",
      "Tokenized sentence: ['do', 'u', 'konw', 'waht', 'is', 'rael', 'friendship', 'im', 'gving', 'yuo', 'an', 'exmpel', 'jsut', 'ese', 'tihs', 'msg', 'evrey', 'splleing', 'of', 'tihs', 'msg', 'is', 'wrnog', 'bt', 'sitll', 'yuo', 'can', 'raed', 'it', 'wihtuot', 'ayn', 'mitsake', 'goodnight', 'amp', 'have', 'a', 'nice', 'sleep', 'sweet', 'dreams']\n",
      "After stop words removal: ['u', 'konw', 'waht', 'rael', 'friendship', 'im', 'gving', 'yuo', 'exmpel', 'jsut', 'ese', 'tihs', 'msg', 'evrey', 'splleing', 'tihs', 'msg', 'wrnog', 'bt', 'sitll', 'yuo', 'raed', 'wihtuot', 'ayn', 'mitsake', 'goodnight', 'amp', 'nice', 'sleep', 'sweet', 'dreams']\n",
      "splle\n",
      "After stemming with porters algorithm: ['konw', 'waht', 'rael', 'friendship', 'gving', 'yuo', 'exmpel', 'jsut', 'es', 'tih', 'msg', 'evrei', 'splle', 'tih', 'msg', 'wrnog', 'sitll', 'yuo', 'ra', 'wihtuot', 'ayn', 'mitsak', 'goodnight', 'amp', 'nice', 'sleep', 'sweet', 'dream']\n",
      "Tokenized sentence: ['ok', 'then', 'i', 'll', 'let', 'him', 'noe', 'later', 'n', 'ask', 'him', 'call', 'u', 'tmr']\n",
      "After stop words removal: ['ok', 'let', 'noe', 'later', 'n', 'ask', 'call', 'u', 'tmr']\n",
      "After stemming with porters algorithm: ['let', 'noe', 'later', 'ask', 'call', 'tmr']\n",
      "Tokenized sentence: ['my', 'ex', 'wife', 'was', 'not', 'able', 'to', 'have', 'kids', 'do', 'you', 'want', 'kids', 'one', 'day']\n",
      "After stop words removal: ['ex', 'wife', 'able', 'kids', 'want', 'kids', 'one', 'day']\n",
      "After stemming with porters algorithm: ['wife', 'abl', 'kid', 'want', 'kid', 'on', 'dai']\n",
      "Tokenized sentence: ['oh', 'god', 'i', 'm', 'gonna', 'google', 'nearby', 'cliffs', 'now']\n",
      "After stop words removal: ['oh', 'god', 'gonna', 'google', 'nearby', 'cliffs']\n",
      "After stemming with porters algorithm: ['god', 'gonna', 'googl', 'nearbi', 'cliff']\n",
      "Tokenized sentence: ['i', 'am', 'late', 'i', 'will', 'be', 'there', 'at']\n",
      "After stop words removal: ['late']\n",
      "After stemming with porters algorithm: ['late']\n",
      "Tokenized sentence: ['the', 'length', 'is', 'e', 'same', 'but', 'e', 'top', 'shorter', 'n', 'i', 'got', 'a', 'fringe', 'now', 'i', 'thk', 'i', 'm', 'not', 'going', 'liao', 'too', 'lazy', 'dun', 'wan', 'distract', 'u', 'also']\n",
      "After stop words removal: ['length', 'e', 'e', 'top', 'shorter', 'n', 'got', 'fringe', 'thk', 'going', 'liao', 'lazy', 'dun', 'wan', 'distract', 'u', 'also']\n",
      "go\n",
      "After stemming with porters algorithm: ['length', 'top', 'shorter', 'got', 'fring', 'thk', 'go', 'liao', 'lazi', 'dun', 'wan', 'distract', 'also']\n",
      "Tokenized sentence: ['hahaha', 'use', 'your', 'brain', 'dear']\n",
      "After stop words removal: ['hahaha', 'use', 'brain', 'dear']\n",
      "After stemming with porters algorithm: ['hahaha', 'us', 'brain', 'dear']\n",
      "Tokenized sentence: ['you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'from', 'landline', 'delivery', 'within', 'days', 't', 'cs', 'box', 'm', 'bp', 'yr', 'warranty', 'ppm', 'p', 'p']\n",
      "After stop words removal: ['awarded', 'sipix', 'digital', 'camera', 'call', 'landline', 'delivery', 'within', 'days', 'cs', 'box', 'bp', 'yr', 'warranty', 'ppm', 'p', 'p']\n",
      "After stemming with porters algorithm: ['awar', 'sipix', 'digit', 'camera', 'call', 'landlin', 'deliveri', 'within', 'dai', 'box', 'warranti', 'ppm']\n",
      "Tokenized sentence: ['good', 'afternoon', 'sunshine', 'how', 'dawns', 'that', 'day', 'are', 'we', 'refreshed', 'and', 'happy', 'to', 'be', 'alive', 'do', 'we', 'breathe', 'in', 'the', 'air', 'and', 'smile', 'i', 'think', 'of', 'you', 'my', 'love', 'as', 'always']\n",
      "After stop words removal: ['good', 'afternoon', 'sunshine', 'dawns', 'day', 'refreshed', 'happy', 'alive', 'breathe', 'air', 'smile', 'think', 'love', 'always']\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'sunshin', 'dawn', 'dai', 'refres', 'happi', 'aliv', 'breath', 'air', 'smile', 'think', 'love', 'alwai']\n",
      "Tokenized sentence: ['fighting', 'with', 'the', 'world', 'is', 'easy', 'u', 'either', 'win', 'or', 'lose', 'bt', 'fightng', 'with', 'some', 'who', 'is', 'close', 'to', 'u', 'is', 'dificult', 'if', 'u', 'lose', 'u', 'lose', 'if', 'u', 'win', 'u', 'still', 'lose']\n",
      "After stop words removal: ['fighting', 'world', 'easy', 'u', 'either', 'win', 'lose', 'bt', 'fightng', 'close', 'u', 'dificult', 'u', 'lose', 'u', 'lose', 'u', 'win', 'u', 'still', 'lose']\n",
      "fight\n",
      "After stemming with porters algorithm: ['figh', 'world', 'easi', 'either', 'win', 'lose', 'fightng', 'close', 'dificult', 'lose', 'lose', 'win', 'still', 'lose']\n",
      "Tokenized sentence: ['cant', 'believe', 'i', 'said', 'so', 'many', 'things', 'to', 'you', 'this', 'morning', 'when', 'all', 'i', 'really', 'wanted', 'to', 'say', 'was', 'good', 'morning', 'i', 'love', 'you', 'have', 'a', 'beautiful', 'morning', 'see', 'you', 'in', 'the', 'library', 'later']\n",
      "After stop words removal: ['cant', 'believe', 'said', 'many', 'things', 'morning', 'really', 'wanted', 'say', 'good', 'morning', 'love', 'beautiful', 'morning', 'see', 'library', 'later']\n",
      "morn\n",
      "morn\n",
      "morn\n",
      "After stemming with porters algorithm: ['cant', 'believ', 'said', 'mani', 'thing', 'mor', 'realli', 'wan', 'sai', 'good', 'mor', 'love', 'beauti', 'mor', 'see', 'librari', 'later']\n",
      "Tokenized sentence: ['was', 'playng', 'doors', 'game', 'and', 'gt', 'racing', 'on', 'phone', 'lol']\n",
      "After stop words removal: ['playng', 'doors', 'game', 'gt', 'racing', 'phone', 'lol']\n",
      "rac\n",
      "After stemming with porters algorithm: ['playng', 'door', 'game', 'race', 'phone', 'lol']\n",
      "Tokenized sentence: ['i', 'think', 'i', 'am', 'disturbing', 'her', 'da']\n",
      "After stop words removal: ['think', 'disturbing', 'da']\n",
      "disturb\n",
      "After stemming with porters algorithm: ['think', 'distur']\n",
      "Tokenized sentence: ['printer', 'is', 'cool', 'i', 'mean', 'groovy', 'wine', 'is', 'groovying']\n",
      "After stop words removal: ['printer', 'cool', 'mean', 'groovy', 'wine', 'groovying']\n",
      "groovy\n",
      "After stemming with porters algorithm: ['printer', 'cool', 'mean', 'groovi', 'wine', 'groov']\n",
      "Tokenized sentence: ['petey', 'boy', 'whereare', 'you', 'me', 'and', 'all', 'your', 'friendsare', 'in', 'thekingshead', 'come', 'down', 'if', 'you', 'canlove', 'nic']\n",
      "After stop words removal: ['petey', 'boy', 'whereare', 'friendsare', 'thekingshead', 'come', 'canlove', 'nic']\n",
      "After stemming with porters algorithm: ['petei', 'boi', 'wherear', 'friendsar', 'thekingshead', 'come', 'canlov', 'nic']\n",
      "Tokenized sentence: ['k', 'and', 'you', 're', 'sure', 'i', 'don', 't', 'have', 'to', 'have', 'consent', 'forms', 'to', 'do', 'it', 'v']\n",
      "After stop words removal: ['k', 'sure', 'consent', 'forms', 'v']\n",
      "After stemming with porters algorithm: ['sure', 'consent', 'form']\n",
      "Tokenized sentence: ['hey', 'congrats', 'u', 'id', 'luv', 'but', 'ive', 'had', 'go', 'home']\n",
      "After stop words removal: ['hey', 'congrats', 'u', 'id', 'luv', 'ive', 'go', 'home']\n",
      "After stemming with porters algorithm: ['hei', 'congrat', 'luv', 'iv', 'home']\n",
      "Tokenized sentence: ['new', 'tones', 'this', 'week', 'include', 'mcfly', 'all', 'ab', 'sara', 'jorge', 'shock', 'will', 'smith', 'switch', 'to', 'order', 'follow', 'instructions', 'on', 'next', 'message']\n",
      "After stop words removal: ['new', 'tones', 'week', 'include', 'mcfly', 'ab', 'sara', 'jorge', 'shock', 'smith', 'switch', 'order', 'follow', 'instructions', 'next', 'message']\n",
      "After stemming with porters algorithm: ['new', 'tone', 'week', 'includ', 'mcfly', 'sara', 'jorg', 'shock', 'smith', 'switch', 'order', 'follow', 'instruct', 'next', 'messag']\n",
      "Tokenized sentence: ['cheers', 'for', 'the', 'message', 'zogtorius', 'i', 've', 'been', 'staring', 'at', 'my', 'phone', 'for', 'an', 'age', 'deciding', 'whether', 'to', 'text', 'or', 'not']\n",
      "After stop words removal: ['cheers', 'message', 'zogtorius', 'staring', 'phone', 'age', 'deciding', 'whether', 'text']\n",
      "star\n",
      "decid\n",
      "After stemming with porters algorithm: ['cheer', 'messag', 'zogtoriu', 'stare', 'phone', 'ag', 'decid', 'whether', 'text']\n",
      "Tokenized sentence: ['the', 'current', 'leading', 'bid', 'is', 'to', 'pause', 'this', 'auction', 'send', 'out', 'customer', 'care']\n",
      "After stop words removal: ['current', 'leading', 'bid', 'pause', 'auction', 'send', 'customer', 'care']\n",
      "lead\n",
      "After stemming with porters algorithm: ['current', 'lead', 'bid', 'paus', 'auct', 'send', 'custom', 'care']\n",
      "Tokenized sentence: ['we', 'll', 'join', 'the', 'lt', 'gt', 'bus']\n",
      "After stop words removal: ['join', 'lt', 'gt', 'bus']\n",
      "After stemming with porters algorithm: ['join', 'bu']\n",
      "Tokenized sentence: ['lol', 'alright', 'i', 'was', 'thinkin', 'that', 'too', 'haha']\n",
      "After stop words removal: ['lol', 'alright', 'thinkin', 'haha']\n",
      "After stemming with porters algorithm: ['lol', 'alright', 'thinkin', 'haha']\n",
      "Tokenized sentence: ['no', 'dice', 'art', 'class', 'thru', 'thanks', 'though', 'any', 'idea', 'what', 'time', 'i', 'should', 'come', 'tomorrow']\n",
      "After stop words removal: ['dice', 'art', 'class', 'thru', 'thanks', 'though', 'idea', 'time', 'come', 'tomorrow']\n",
      "After stemming with porters algorithm: ['dice', 'art', 'class', 'thru', 'thank', 'though', 'idea', 'time', 'come', 'tomorrow']\n",
      "Tokenized sentence: ['have', 'you', 'bookedthe', 'hut', 'and', 'also', 'your', 'time', 'off', 'how', 'are', 'you', 'by', 'the', 'way']\n",
      "After stop words removal: ['bookedthe', 'hut', 'also', 'time', 'way']\n",
      "After stemming with porters algorithm: ['bookedth', 'hut', 'also', 'time', 'wai']\n",
      "Tokenized sentence: ['cool', 'text', 'me', 'when', 'you', 're', 'parked']\n",
      "After stop words removal: ['cool', 'text', 'parked']\n",
      "After stemming with porters algorithm: ['cool', 'text', 'par']\n",
      "Tokenized sentence: ['how', 'much', 'r', 'willing', 'to', 'pay']\n",
      "After stop words removal: ['much', 'r', 'willing', 'pay']\n",
      "will\n",
      "After stemming with porters algorithm: ['much', 'will', 'pai']\n",
      "Tokenized sentence: ['hi', 'baby', 'im', 'cruisin', 'with', 'my', 'girl', 'friend', 'what', 'r', 'u', 'up', 'give', 'me', 'a', 'call', 'in', 'and', 'hour', 'at', 'home', 'if', 'thats', 'alright', 'or', 'fone', 'me', 'on', 'this', 'fone', 'now', 'love', 'jenny', 'xxx']\n",
      "After stop words removal: ['hi', 'baby', 'im', 'cruisin', 'girl', 'friend', 'r', 'u', 'give', 'call', 'hour', 'home', 'thats', 'alright', 'fone', 'fone', 'love', 'jenny', 'xxx']\n",
      "After stemming with porters algorithm: ['babi', 'cruisin', 'girl', 'friend', 'give', 'call', 'hour', 'home', 'that', 'alright', 'fone', 'fone', 'love', 'jenni', 'xxx']\n",
      "Tokenized sentence: ['no', 'on', 'the', 'way', 'home', 'so', 'if', 'not', 'for', 'the', 'long', 'dry', 'spell', 'the', 'season', 'would', 'have', 'been', 'over']\n",
      "After stop words removal: ['way', 'home', 'long', 'dry', 'spell', 'season', 'would']\n",
      "After stemming with porters algorithm: ['wai', 'home', 'long', 'dry', 'spell', 'season', 'would']\n",
      "Tokenized sentence: ['nobody', 'can', 'decide', 'where', 'to', 'eat', 'and', 'dad', 'wants', 'chinese']\n",
      "After stop words removal: ['nobody', 'decide', 'eat', 'dad', 'wants', 'chinese']\n",
      "After stemming with porters algorithm: ['nobodi', 'decid', 'eat', 'dad', 'want', 'chines']\n",
      "Tokenized sentence: ['who', 'u', 'talking', 'about']\n",
      "After stop words removal: ['u', 'talking']\n",
      "talk\n",
      "After stemming with porters algorithm: ['tal']\n",
      "Tokenized sentence: ['u', 'are', 'subscribed', 'to', 'the', 'best', 'mobile', 'content', 'service', 'in', 'the', 'uk', 'for', 'per', 'ten', 'days', 'until', 'you', 'send', 'stop', 'to', 'helpline']\n",
      "After stop words removal: ['u', 'subscribed', 'best', 'mobile', 'content', 'service', 'uk', 'per', 'ten', 'days', 'send', 'stop', 'helpline']\n",
      "After stemming with porters algorithm: ['subscrib', 'best', 'mobil', 'content', 'servic', 'per', 'ten', 'dai', 'send', 'stop', 'helplin']\n",
      "Tokenized sentence: ['oh', 'unintentionally', 'not', 'bad', 'timing', 'great', 'fingers', 'the', 'trains', 'play', 'along', 'will', 'give', 'fifteen', 'min', 'warning']\n",
      "After stop words removal: ['oh', 'unintentionally', 'bad', 'timing', 'great', 'fingers', 'trains', 'play', 'along', 'give', 'fifteen', 'min', 'warning']\n",
      "tim\n",
      "warn\n",
      "After stemming with porters algorithm: ['unintent', 'bad', 'time', 'great', 'finger', 'train', 'plai', 'along', 'give', 'fifteen', 'min', 'war']\n",
      "Tokenized sentence: ['u', 'sleeping', 'now', 'or', 'you', 'going', 'to', 'take', 'haha', 'i', 'got', 'spys', 'wat', 'me', 'online', 'checking', 'n', 'replying', 'mails', 'lor']\n",
      "After stop words removal: ['u', 'sleeping', 'going', 'take', 'haha', 'got', 'spys', 'wat', 'online', 'checking', 'n', 'replying', 'mails', 'lor']\n",
      "sleep\n",
      "go\n",
      "check\n",
      "reply\n",
      "After stemming with porters algorithm: ['sleep', 'go', 'take', 'haha', 'got', 'spy', 'wat', 'onlin', 'chec', 'repl', 'mail', 'lor']\n",
      "Tokenized sentence: ['carlos', 'is', 'taking', 'his', 'sweet', 'time', 'as', 'usual', 'so', 'let', 'me', 'know', 'when', 'you', 'and', 'patty', 'are', 'done', 'want', 'to', 'smoke', 'and', 'i', 'll', 'tell', 'him', 'to', 'haul', 'ass']\n",
      "After stop words removal: ['carlos', 'taking', 'sweet', 'time', 'usual', 'let', 'know', 'patty', 'done', 'want', 'smoke', 'tell', 'haul', 'ass']\n",
      "tak\n",
      "After stemming with porters algorithm: ['carlo', 'take', 'sweet', 'time', 'usual', 'let', 'know', 'patti', 'done', 'want', 'smoke', 'tell', 'haul', 'ass']\n",
      "Tokenized sentence: ['no', 'problem', 'with', 'the', 'renewal', 'i', 'll', 'do', 'it', 'right', 'away', 'but', 'i', 'dont', 'know', 'his', 'details']\n",
      "After stop words removal: ['problem', 'renewal', 'right', 'away', 'dont', 'know', 'details']\n",
      "After stemming with porters algorithm: ['problem', 'renew', 'right', 'awai', 'dont', 'know', 'detail']\n",
      "Tokenized sentence: ['dear', 'how', 'is', 'chechi', 'did', 'you', 'talk', 'to', 'her']\n",
      "After stop words removal: ['dear', 'chechi', 'talk']\n",
      "After stemming with porters algorithm: ['dear', 'chechi', 'talk']\n",
      "Tokenized sentence: ['how', 'do', 'you', 'guys', 'go', 'to', 'see', 'movies', 'on', 'your', 'side']\n",
      "After stop words removal: ['guys', 'go', 'see', 'movies', 'side']\n",
      "After stemming with porters algorithm: ['gui', 'see', 'movi', 'side']\n",
      "Tokenized sentence: ['hey', 'u', 'still', 'at', 'the', 'gym']\n",
      "After stop words removal: ['hey', 'u', 'still', 'gym']\n",
      "After stemming with porters algorithm: ['hei', 'still', 'gym']\n",
      "Tokenized sentence: ['where', 'can', 'download', 'clear', 'movies', 'dvd', 'copies']\n",
      "After stop words removal: ['download', 'clear', 'movies', 'dvd', 'copies']\n",
      "After stemming with porters algorithm: ['download', 'clear', 'movi', 'dvd', 'copi']\n",
      "Tokenized sentence: ['have', 'got', 'few', 'things', 'to', 'do', 'may', 'be', 'in', 'pub', 'later']\n",
      "After stop words removal: ['got', 'things', 'may', 'pub', 'later']\n",
      "After stemming with porters algorithm: ['got', 'thing', 'mai', 'pub', 'later']\n",
      "Tokenized sentence: ['free', 'message', 'jamster', 'get', 'the', 'crazy', 'frog', 'sound', 'now', 'for', 'poly', 'text', 'mad', 'for', 'real', 'text', 'mad', 'to', 'crazy', 'sounds', 'for', 'just', 'gbp', 'week', 'only', 't', 'c', 's', 'apply']\n",
      "After stop words removal: ['free', 'message', 'jamster', 'get', 'crazy', 'frog', 'sound', 'poly', 'text', 'mad', 'real', 'text', 'mad', 'crazy', 'sounds', 'gbp', 'week', 'c', 'apply']\n",
      "After stemming with porters algorithm: ['free', 'messag', 'jamster', 'get', 'crazi', 'frog', 'sound', 'poli', 'text', 'mad', 'real', 'text', 'mad', 'crazi', 'sound', 'gbp', 'week', 'appli']\n",
      "Tokenized sentence: ['same', 'to', 'u']\n",
      "After stop words removal: ['u']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['customer', 'place', 'i', 'wil', 'cal', 'u', 'sir']\n",
      "After stop words removal: ['customer', 'place', 'wil', 'cal', 'u', 'sir']\n",
      "After stemming with porters algorithm: ['custom', 'place', 'wil', 'cal', 'sir']\n",
      "Tokenized sentence: ['hmmm', 'how', 'many', 'players', 'selected']\n",
      "After stop words removal: ['hmmm', 'many', 'players', 'selected']\n",
      "After stemming with porters algorithm: ['hmmm', 'mani', 'player', 'selec']\n",
      "Tokenized sentence: ['dont', 'let', 'studying', 'stress', 'you', 'out', 'l', 'r']\n",
      "After stop words removal: ['dont', 'let', 'studying', 'stress', 'l', 'r']\n",
      "study\n",
      "After stemming with porters algorithm: ['dont', 'let', 'stud', 'stress']\n",
      "Tokenized sentence: ['nope', 'i', 'just', 'forgot', 'will', 'show', 'next', 'week']\n",
      "After stop words removal: ['nope', 'forgot', 'show', 'next', 'week']\n",
      "After stemming with porters algorithm: ['nope', 'forgot', 'show', 'next', 'week']\n",
      "Tokenized sentence: ['you', 'want', 'to', 'go']\n",
      "After stop words removal: ['want', 'go']\n",
      "After stemming with porters algorithm: ['want']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'very', 'very', 'very', 'very', 'bad', 'girl', 'or', 'lady']\n",
      "After stop words removal: ['bad', 'girl', 'lady']\n",
      "After stemming with porters algorithm: ['bad', 'girl', 'ladi']\n",
      "Tokenized sentence: ['todays', 'vodafone', 'numbers', 'ending', 'with', 'are', 'selected', 'to', 'a', 'receive', 'a', 'award', 'if', 'your', 'number', 'matches', 'call', 'to', 'receive', 'your', 'award']\n",
      "After stop words removal: ['todays', 'vodafone', 'numbers', 'ending', 'selected', 'receive', 'award', 'number', 'matches', 'call', 'receive', 'award']\n",
      "end\n",
      "After stemming with porters algorithm: ['todai', 'vodafon', 'number', 'en', 'selec', 'receiv', 'award', 'number', 'match', 'call', 'receiv', 'award']\n",
      "Tokenized sentence: ['pls', 'go', 'there', 'today', 'lt', 'gt', 'i', 'dont', 'want', 'any', 'excuses']\n",
      "After stop words removal: ['pls', 'go', 'today', 'lt', 'gt', 'dont', 'want', 'excuses']\n",
      "After stemming with porters algorithm: ['pl', 'todai', 'dont', 'want', 'excus']\n",
      "Tokenized sentence: ['did', 'you', 'get', 'any', 'gift', 'this', 'year', 'i', 'didnt', 'get', 'anything', 'so', 'bad']\n",
      "After stop words removal: ['get', 'gift', 'year', 'didnt', 'get', 'anything', 'bad']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['get', 'gift', 'year', 'didnt', 'get', 'anyt', 'bad']\n",
      "Tokenized sentence: ['so', 'li', 'hai', 'me', 'bored', 'now', 'da', 'lecturer', 'repeating', 'last', 'weeks', 'stuff', 'waste', 'time']\n",
      "After stop words removal: ['li', 'hai', 'bored', 'da', 'lecturer', 'repeating', 'last', 'weeks', 'stuff', 'waste', 'time']\n",
      "repeat\n",
      "repeate\n",
      "After stemming with porters algorithm: ['hai', 'bore', 'lectur', 'repeat', 'last', 'week', 'stuff', 'wast', 'time']\n",
      "Tokenized sentence: ['yup', 'i', 'm', 'elaborating', 'on', 'the', 'safety', 'aspects', 'and', 'some', 'other', 'issues']\n",
      "After stop words removal: ['yup', 'elaborating', 'safety', 'aspects', 'issues']\n",
      "elaborat\n",
      "elaborate\n",
      "After stemming with porters algorithm: ['yup', 'elabor', 'safeti', 'aspect', 'issu']\n",
      "Tokenized sentence: ['do', 'you', 'want', 'anytime', 'any', 'network', 'mins', 'text', 'and', 'a', 'new', 'video', 'phone', 'for', 'only', 'five', 'pounds', 'per', 'week', 'call', 'now', 'or', 'reply', 'for', 'delivery', 'tomorrow']\n",
      "After stop words removal: ['want', 'anytime', 'network', 'mins', 'text', 'new', 'video', 'phone', 'five', 'pounds', 'per', 'week', 'call', 'reply', 'delivery', 'tomorrow']\n",
      "After stemming with porters algorithm: ['want', 'anytim', 'network', 'min', 'text', 'new', 'video', 'phone', 'five', 'pound', 'per', 'week', 'call', 'repli', 'deliveri', 'tomorrow']\n",
      "Tokenized sentence: ['bring', 'it', 'if', 'you', 'got', 'it']\n",
      "After stop words removal: ['bring', 'got']\n",
      "After stemming with porters algorithm: ['bring', 'got']\n",
      "Tokenized sentence: ['you', 'have', 'got', 'tallent', 'but', 'you', 'are', 'wasting']\n",
      "After stop words removal: ['got', 'tallent', 'wasting']\n",
      "wast\n",
      "After stemming with porters algorithm: ['got', 'tallent', 'was']\n",
      "Tokenized sentence: ['geeee', 'i', 'miss', 'you', 'already', 'you', 'know', 'your', 'all', 'i', 'can', 'think', 'about', 'fuck', 'i', 'can', 't', 'wait', 'till', 'next', 'year', 'when', 'we', 'will', 'be', 'together', 'loving', 'kiss']\n",
      "After stop words removal: ['geeee', 'miss', 'already', 'know', 'think', 'fuck', 'wait', 'till', 'next', 'year', 'together', 'loving', 'kiss']\n",
      "lov\n",
      "After stemming with porters algorithm: ['geeee', 'miss', 'alreadi', 'know', 'think', 'fuck', 'wait', 'till', 'next', 'year', 'togeth', 'love', 'kiss']\n",
      "Tokenized sentence: ['i', 'will', 'spoil', 'you', 'in', 'bed', 'as', 'well']\n",
      "After stop words removal: ['spoil', 'bed', 'well']\n",
      "After stemming with porters algorithm: ['spoil', 'bed', 'well']\n",
      "Tokenized sentence: ['actually', 'i', 'deleted', 'my', 'old', 'website', 'now', 'i', 'm', 'blogging', 'at', 'magicalsongs', 'blogspot', 'com']\n",
      "After stop words removal: ['actually', 'deleted', 'old', 'website', 'blogging', 'magicalsongs', 'blogspot', 'com']\n",
      "blogg\n",
      "After stemming with porters algorithm: ['actual', 'delet', 'old', 'websit', 'blog', 'magicalsong', 'blogspot', 'com']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['we', 'have', 'sent', 'jd', 'for', 'customer', 'service', 'cum', 'accounts', 'executive', 'to', 'ur', 'mail', 'id', 'for', 'details', 'contact', 'us']\n",
      "After stop words removal: ['sent', 'jd', 'customer', 'service', 'cum', 'accounts', 'executive', 'ur', 'mail', 'id', 'details', 'contact', 'us']\n",
      "After stemming with porters algorithm: ['sent', 'custom', 'servic', 'cum', 'account', 'execut', 'mail', 'detail', 'contact']\n",
      "Tokenized sentence: ['free', 'entry', 'to', 'the', 'gr', 'prizes', 'wkly', 'comp', 'a', 'chance', 'to', 'win', 'the', 'latest', 'nokia', 'psp', 'or', 'cash', 'every', 'wk', 'txt', 'great', 'to', 'http', 'www', 'gr', 'prizes', 'com']\n",
      "After stop words removal: ['free', 'entry', 'gr', 'prizes', 'wkly', 'comp', 'chance', 'win', 'latest', 'nokia', 'psp', 'cash', 'every', 'wk', 'txt', 'great', 'http', 'www', 'gr', 'prizes', 'com']\n",
      "After stemming with porters algorithm: ['free', 'entri', 'priz', 'wkly', 'comp', 'chanc', 'win', 'latest', 'nokia', 'psp', 'cash', 'everi', 'txt', 'great', 'http', 'www', 'priz', 'com']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call']\n",
      "Tokenized sentence: ['prakesh', 'is', 'there', 'know']\n",
      "After stop words removal: ['prakesh', 'know']\n",
      "After stemming with porters algorithm: ['prakesh', 'know']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'contact', 'you', 're', 'your', 'reply', 'to', 'our', 'offer', 'of', 'a', 'video', 'phone', 'anytime', 'any', 'network', 'mins', 'half', 'price', 'line', 'rental', 'camcorder', 'reply', 'or', 'call']\n",
      "After stop words removal: ['tried', 'contact', 'reply', 'offer', 'video', 'phone', 'anytime', 'network', 'mins', 'half', 'price', 'line', 'rental', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['tri', 'contact', 'repli', 'offer', 'video', 'phone', 'anytim', 'network', 'min', 'half', 'price', 'line', 'rental', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['yup', 'i', 'thk', 'they', 'r', 'e', 'teacher', 'said', 'that', 'will', 'make', 'my', 'face', 'look', 'longer', 'darren', 'ask', 'me', 'not', 'cut', 'too', 'short']\n",
      "After stop words removal: ['yup', 'thk', 'r', 'e', 'teacher', 'said', 'make', 'face', 'look', 'longer', 'darren', 'ask', 'cut', 'short']\n",
      "After stemming with porters algorithm: ['yup', 'thk', 'teacher', 'said', 'make', 'face', 'look', 'longer', 'darren', 'ask', 'cut', 'short']\n",
      "Tokenized sentence: ['i', 'called', 'but', 'no', 'one', 'pick', 'up', 'e', 'phone', 'i', 'ask', 'both', 'of', 'them', 'already', 'they', 'said', 'ok']\n",
      "After stop words removal: ['called', 'one', 'pick', 'e', 'phone', 'ask', 'already', 'said', 'ok']\n",
      "After stemming with porters algorithm: ['call', 'on', 'pick', 'phone', 'ask', 'alreadi', 'said']\n",
      "Tokenized sentence: ['i', 'll', 'text', 'carlos', 'and', 'let', 'you', 'know', 'hang', 'on']\n",
      "After stop words removal: ['text', 'carlos', 'let', 'know', 'hang']\n",
      "After stemming with porters algorithm: ['text', 'carlo', 'let', 'know', 'hang']\n",
      "Tokenized sentence: ['my', 'love', 'i', 'hope', 'your', 'not', 'doing', 'anything', 'drastic', 'don', 't', 'you', 'dare', 'sell', 'your', 'pc', 'or', 'your', 'phone']\n",
      "After stop words removal: ['love', 'hope', 'anything', 'drastic', 'dare', 'sell', 'pc', 'phone']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['love', 'hope', 'anyt', 'drastic', 'dare', 'sell', 'phone']\n",
      "Tokenized sentence: ['that', 's', 'cool', 'he', 'll', 'be', 'here', 'all', 'night', 'lemme', 'know', 'when', 'you', 're', 'around']\n",
      "After stop words removal: ['cool', 'night', 'lemme', 'know', 'around']\n",
      "After stemming with porters algorithm: ['cool', 'night', 'lemm', 'know', 'around']\n",
      "Tokenized sentence: ['what', 'makes', 'you', 'most', 'happy']\n",
      "After stop words removal: ['makes', 'happy']\n",
      "After stemming with porters algorithm: ['make', 'happi']\n",
      "Tokenized sentence: ['she', 's', 'find', 'i', 'sent', 'you', 'an', 'offline', 'message', 'to', 'know', 'how', 'anjola', 's', 'now']\n",
      "After stop words removal: ['find', 'sent', 'offline', 'message', 'know', 'anjola']\n",
      "After stemming with porters algorithm: ['find', 'sent', 'offlin', 'messag', 'know', 'anjola']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['take', 'us', 'out', 'shopping', 'and', 'mark', 'will', 'distract', 'isaiah', 'd']\n",
      "After stop words removal: ['take', 'us', 'shopping', 'mark', 'distract', 'isaiah']\n",
      "shopp\n",
      "After stemming with porters algorithm: ['take', 'shop', 'mark', 'distract', 'isaiah']\n",
      "Tokenized sentence: ['dear', 'u', 've', 'been', 'invited', 'to', 'xchat', 'this', 'is', 'our', 'final', 'attempt', 'to', 'contact', 'u', 'txt', 'chat', 'to', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stop words removal: ['dear', 'u', 'invited', 'xchat', 'final', 'attempt', 'contact', 'u', 'txt', 'chat', 'p', 'msgrcvdhg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'yrs']\n",
      "After stemming with porters algorithm: ['dear', 'invit', 'xchat', 'final', 'attempt', 'contact', 'txt', 'chat', 'msgrcvdhg', 'suit', 'land', 'row', 'ldn', 'yr']\n",
      "Tokenized sentence: ['for', 'the', 'first', 'time', 'in', 'the', 'history', 'need', 'comfort', 'and', 'luxury', 'are', 'sold', 'at', 'same', 'price', 'in', 'india', 'onion', 'rs', 'lt', 'gt', 'petrol', 'rs', 'lt', 'gt', 'beer', 'rs', 'lt', 'gt', 'shesil', 'lt', 'gt']\n",
      "After stop words removal: ['first', 'time', 'history', 'need', 'comfort', 'luxury', 'sold', 'price', 'india', 'onion', 'rs', 'lt', 'gt', 'petrol', 'rs', 'lt', 'gt', 'beer', 'rs', 'lt', 'gt', 'shesil', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['first', 'time', 'histori', 'need', 'comfort', 'luxuri', 'sold', 'price', 'india', 'onion', 'petrol', 'beer', 'shesil']\n",
      "Tokenized sentence: ['k', 'wait', 'chikku', 'il', 'send', 'aftr', 'lt', 'gt', 'mins']\n",
      "After stop words removal: ['k', 'wait', 'chikku', 'il', 'send', 'aftr', 'lt', 'gt', 'mins']\n",
      "After stemming with porters algorithm: ['wait', 'chikku', 'send', 'aftr', 'min']\n",
      "Tokenized sentence: ['just', 'sleeping', 'and', 'surfing']\n",
      "After stop words removal: ['sleeping', 'surfing']\n",
      "sleep\n",
      "surf\n",
      "After stemming with porters algorithm: ['sleep', 'sur']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['can', 'come', 'my', 'room', 'but', 'cannot', 'come', 'my', 'house', 'cos', 'my', 'house', 'still', 'messy', 'haha']\n",
      "After stop words removal: ['come', 'room', 'cannot', 'come', 'house', 'cos', 'house', 'still', 'messy', 'haha']\n",
      "After stemming with porters algorithm: ['come', 'room', 'cannot', 'come', 'hous', 'co', 'hous', 'still', 'messi', 'haha']\n",
      "Tokenized sentence: ['urgent', 'last', 'weekend', 's', 'draw', 'shows', 'that', 'you', 'have', 'won', 'cash', 'or', 'a', 'spanish', 'holiday', 'call', 'now', 'to', 'claim', 't', 'c', 'rstm', 'sw', 'ss', 'ppm']\n",
      "After stop words removal: ['urgent', 'last', 'weekend', 'draw', 'shows', 'cash', 'spanish', 'holiday', 'call', 'claim', 'c', 'rstm', 'sw', 'ss', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'last', 'weekend', 'draw', 'show', 'cash', 'spanish', 'holidai', 'call', 'claim', 'rstm', 'ppm']\n",
      "Tokenized sentence: ['urgent', 'call', 'from', 'landline', 'your', 'complimentary', 'tenerife', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'box', 'hp', 'yf', 'ppm']\n",
      "After stop words removal: ['urgent', 'call', 'landline', 'complimentary', 'tenerife', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'box', 'hp', 'yf', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'call', 'landlin', 'complimentari', 'tenerif', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['happened', 'here', 'while', 'you', 'were', 'adventuring']\n",
      "After stop words removal: ['happened', 'adventuring']\n",
      "adventur\n",
      "After stemming with porters algorithm: ['happen', 'adventur']\n",
      "Tokenized sentence: ['then', 'just', 'eat', 'a', 'shit', 'and', 'wait', 'for', 'ur', 'monkey', 'face', 'bitch', 'u', 'asshole']\n",
      "After stop words removal: ['eat', 'shit', 'wait', 'ur', 'monkey', 'face', 'bitch', 'u', 'asshole']\n",
      "After stemming with porters algorithm: ['eat', 'shit', 'wait', 'monkei', 'face', 'bitch', 'asshol']\n",
      "Tokenized sentence: ['i', 'am', 'in', 'a', 'marriage', 'function']\n",
      "After stop words removal: ['marriage', 'function']\n",
      "After stemming with porters algorithm: ['marriag', 'funct']\n",
      "Tokenized sentence: ['k', 'i', 'will', 'sent', 'it', 'again']\n",
      "After stop words removal: ['k', 'sent']\n",
      "After stemming with porters algorithm: ['sent']\n",
      "Tokenized sentence: ['u']\n",
      "After stop words removal: ['u']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['private', 'your', 'account', 'statement', 'for', 'shows', 'un', 'redeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stop words removal: ['private', 'account', 'statement', 'shows', 'un', 'redeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
      "After stemming with porters algorithm: ['privat', 'account', 'statem', 'show', 'redeem', 'point', 'call', 'identifi', 'code', 'expir']\n",
      "Tokenized sentence: ['those', 'ducking', 'chinchillas']\n",
      "After stop words removal: ['ducking', 'chinchillas']\n",
      "duck\n",
      "After stemming with porters algorithm: ['duc', 'chinchilla']\n",
      "Tokenized sentence: ['yeah', 'probably', 'but', 'not', 'sure', 'ilol', 'let', 'u', 'know', 'but', 'personally', 'i', 'wuldnt', 'bother', 'then', 'again', 'if', 'ur', 'goin', 'to', 'then', 'i', 'mite', 'as', 'well']\n",
      "After stop words removal: ['yeah', 'probably', 'sure', 'ilol', 'let', 'u', 'know', 'personally', 'wuldnt', 'bother', 'ur', 'goin', 'mite', 'well']\n",
      "After stemming with porters algorithm: ['yeah', 'probab', 'sure', 'ilol', 'let', 'know', 'person', 'wuldnt', 'bother', 'goin', 'mite', 'well']\n",
      "Tokenized sentence: ['hmm', 'dunno', 'leh', 'mayb', 'a', 'bag', 'goigng', 'out', 'dat', 'is', 'not', 'too', 'small', 'or', 'jus', 'anything', 'except', 'perfume', 'smth', 'dat', 'i', 'can', 'keep']\n",
      "After stop words removal: ['hmm', 'dunno', 'leh', 'mayb', 'bag', 'goigng', 'dat', 'small', 'jus', 'anything', 'except', 'perfume', 'smth', 'dat', 'keep']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['hmm', 'dunno', 'leh', 'mayb', 'bag', 'goigng', 'dat', 'small', 'ju', 'anyt', 'except', 'perfum', 'smth', 'dat', 'keep']\n",
      "Tokenized sentence: ['good', 'morning', 'plz', 'call', 'me', 'sir']\n",
      "After stop words removal: ['good', 'morning', 'plz', 'call', 'sir']\n",
      "morn\n",
      "After stemming with porters algorithm: ['good', 'mor', 'plz', 'call', 'sir']\n",
      "Tokenized sentence: ['good', 'day', 'to', 'you', 'too', 'pray', 'for', 'me', 'remove', 'the', 'teeth', 'as', 'its', 'painful', 'maintaining', 'other', 'stuff']\n",
      "After stop words removal: ['good', 'day', 'pray', 'remove', 'teeth', 'painful', 'maintaining', 'stuff']\n",
      "maintain\n",
      "After stemming with porters algorithm: ['good', 'dai', 'prai', 'remov', 'teeth', 'pain', 'maintain', 'stuff']\n",
      "Tokenized sentence: ['yeah', 'but', 'which', 'is', 'worse', 'for', 'i']\n",
      "After stop words removal: ['yeah', 'worse']\n",
      "After stemming with porters algorithm: ['yeah', 'wors']\n",
      "Tokenized sentence: ['heart', 'is', 'empty', 'without', 'love', 'mind', 'is', 'empty', 'without', 'wisdom', 'eyes', 'r', 'empty', 'without', 'dreams', 'amp', 'life', 'is', 'empty', 'without', 'frnds', 'so', 'alwys', 'be', 'in', 'touch', 'good', 'night', 'amp', 'sweet', 'dreams']\n",
      "After stop words removal: ['heart', 'empty', 'without', 'love', 'mind', 'empty', 'without', 'wisdom', 'eyes', 'r', 'empty', 'without', 'dreams', 'amp', 'life', 'empty', 'without', 'frnds', 'alwys', 'touch', 'good', 'night', 'amp', 'sweet', 'dreams']\n",
      "After stemming with porters algorithm: ['heart', 'empti', 'without', 'love', 'mind', 'empti', 'without', 'wisdom', 'ey', 'empti', 'without', 'dream', 'amp', 'life', 'empti', 'without', 'frnd', 'alwi', 'touch', 'good', 'night', 'amp', 'sweet', 'dream']\n",
      "Tokenized sentence: ['like', 'a', 'personal', 'sized', 'or', 'what']\n",
      "After stop words removal: ['like', 'personal', 'sized']\n",
      "size\n",
      "After stemming with porters algorithm: ['like', 'person', 'siz']\n",
      "Tokenized sentence: ['must', 'come', 'later', 'i', 'normally', 'bathe', 'him', 'in', 'da', 'afternoon', 'mah']\n",
      "After stop words removal: ['must', 'come', 'later', 'normally', 'bathe', 'da', 'afternoon', 'mah']\n",
      "After stemming with porters algorithm: ['must', 'come', 'later', 'normal', 'bath', 'afternoon', 'mah']\n",
      "Tokenized sentence: ['ooh', 'got', 'i', 'm', 'gonna', 'start', 'belly', 'dancing', 'in', 'moseley', 'weds', 'if', 'u', 'want', 'join', 'me', 'they', 'have', 'a', 'cafe', 'too']\n",
      "After stop words removal: ['ooh', 'got', 'gonna', 'start', 'belly', 'dancing', 'moseley', 'weds', 'u', 'want', 'join', 'cafe']\n",
      "danc\n",
      "After stemming with porters algorithm: ['ooh', 'got', 'gonna', 'start', 'belli', 'dan', 'moselei', 'wed', 'want', 'join', 'cafe']\n",
      "Tokenized sentence: ['love', 'isn', 't', 'a', 'decision', 'it', 's', 'a', 'feeling', 'if', 'we', 'could', 'decide', 'who', 'to', 'love', 'then', 'life', 'would', 'be', 'much', 'simpler', 'but', 'then', 'less', 'magical']\n",
      "After stop words removal: ['love', 'decision', 'feeling', 'could', 'decide', 'love', 'life', 'would', 'much', 'simpler', 'less', 'magical']\n",
      "feel\n",
      "After stemming with porters algorithm: ['love', 'decis', 'feel', 'could', 'decid', 'love', 'life', 'would', 'much', 'simpler', 'less', 'magic']\n",
      "Tokenized sentence: ['i', 'am', 'hot', 'n', 'horny', 'and', 'willing', 'i', 'live', 'local', 'to', 'you', 'text', 'a', 'reply', 'to', 'hear', 'strt', 'back', 'from', 'me', 'p', 'per', 'msg', 'netcollex', 'ltdhelpdesk', 'reply', 'stop', 'to', 'end']\n",
      "After stop words removal: ['hot', 'n', 'horny', 'willing', 'live', 'local', 'text', 'reply', 'hear', 'strt', 'back', 'p', 'per', 'msg', 'netcollex', 'ltdhelpdesk', 'reply', 'stop', 'end']\n",
      "will\n",
      "After stemming with porters algorithm: ['hot', 'horni', 'will', 'live', 'local', 'text', 'repli', 'hear', 'strt', 'back', 'per', 'msg', 'netcollex', 'ltdhelpdesk', 'repli', 'stop', 'end']\n",
      "Tokenized sentence: ['dude', 'got', 'a', 'haircut', 'now', 'its', 'breezy', 'up', 'there']\n",
      "After stop words removal: ['dude', 'got', 'haircut', 'breezy']\n",
      "After stemming with porters algorithm: ['dude', 'got', 'haircut', 'breezi']\n",
      "Tokenized sentence: ['eatin', 'my', 'lunch']\n",
      "After stop words removal: ['eatin', 'lunch']\n",
      "After stemming with porters algorithm: ['eatin', 'lunch']\n",
      "Tokenized sentence: ['bull', 'your', 'plan', 'was', 'to', 'go', 'floating', 'off', 'to', 'ikea', 'with', 'me', 'without', 'a', 'care', 'in', 'the', 'world', 'so', 'i', 'have', 'to', 'live', 'with', 'your', 'mess', 'another', 'day']\n",
      "After stop words removal: ['bull', 'plan', 'go', 'floating', 'ikea', 'without', 'care', 'world', 'live', 'mess', 'another', 'day']\n",
      "float\n",
      "floate\n",
      "After stemming with porters algorithm: ['bull', 'plan', 'float', 'ikea', 'without', 'care', 'world', 'live', 'mess', 'anoth', 'dai']\n",
      "Tokenized sentence: ['goodmorning', 'my', 'grandfather', 'expired', 'so', 'am', 'on', 'leave', 'today']\n",
      "After stop words removal: ['goodmorning', 'grandfather', 'expired', 'leave', 'today']\n",
      "goodmorn\n",
      "After stemming with porters algorithm: ['goodmor', 'grandfath', 'expir', 'leav', 'todai']\n",
      "Tokenized sentence: ['hello', 'my', 'love', 'how', 'goes', 'that', 'day', 'i', 'wish', 'your', 'well', 'and', 'fine', 'babe', 'and', 'hope', 'that', 'you', 'find', 'some', 'job', 'prospects', 'i', 'miss', 'you', 'boytoy', 'a', 'teasing', 'kiss']\n",
      "After stop words removal: ['hello', 'love', 'goes', 'day', 'wish', 'well', 'fine', 'babe', 'hope', 'find', 'job', 'prospects', 'miss', 'boytoy', 'teasing', 'kiss']\n",
      "teas\n",
      "After stemming with porters algorithm: ['hello', 'love', 'goe', 'dai', 'wish', 'well', 'fine', 'babe', 'hope', 'find', 'job', 'prospect', 'miss', 'boytoi', 'teas', 'kiss']\n",
      "Tokenized sentence: ['how', 'do', 'friends', 'help', 'us', 'in', 'problems', 'they', 'give', 'the', 'most', 'stupid', 'suggestion', 'that', 'lands', 'us', 'into', 'another', 'problem', 'and', 'helps', 'us', 'forgt', 'the', 'previous', 'problem']\n",
      "After stop words removal: ['friends', 'help', 'us', 'problems', 'give', 'stupid', 'suggestion', 'lands', 'us', 'another', 'problem', 'helps', 'us', 'forgt', 'previous', 'problem']\n",
      "After stemming with porters algorithm: ['friend', 'help', 'problem', 'give', 'stupid', 'suggest', 'land', 'anoth', 'problem', 'help', 'forgt', 'previou', 'problem']\n",
      "Tokenized sentence: ['hey', 'looks', 'like', 'i', 'was', 'wrong', 'and', 'one', 'of', 'the', 'kappa', 'guys', 'numbers', 'is', 'still', 'on', 'my', 'phone', 'if', 'you', 'want', 'i', 'can', 'text', 'him', 'and', 'see', 'if', 'he', 's', 'around']\n",
      "After stop words removal: ['hey', 'looks', 'like', 'wrong', 'one', 'kappa', 'guys', 'numbers', 'still', 'phone', 'want', 'text', 'see', 'around']\n",
      "After stemming with porters algorithm: ['hei', 'look', 'like', 'wrong', 'on', 'kappa', 'gui', 'number', 'still', 'phone', 'want', 'text', 'see', 'around']\n",
      "Tokenized sentence: ['that', 'means', 'get', 'the', 'door']\n",
      "After stop words removal: ['means', 'get', 'door']\n",
      "After stemming with porters algorithm: ['mean', 'get', 'door']\n",
      "Tokenized sentence: ['take', 'care', 'and', 'sleep', 'well', 'you', 'need', 'to', 'learn', 'to', 'change', 'in', 'life', 'you', 'only', 'need', 'to', 'get', 'convinced', 'on', 'that', 'i', 'will', 'wait', 'but', 'no', 'more', 'conversations', 'between', 'us', 'get', 'convinced', 'by', 'that', 'time', 'your', 'family', 'is', 'over', 'for', 'you', 'in', 'many', 'senses', 'respect', 'them', 'but', 'not', 'overemphasise', 'or', 'u', 'have', 'no', 'role', 'in', 'my', 'life']\n",
      "After stop words removal: ['take', 'care', 'sleep', 'well', 'need', 'learn', 'change', 'life', 'need', 'get', 'convinced', 'wait', 'conversations', 'us', 'get', 'convinced', 'time', 'family', 'many', 'senses', 'respect', 'overemphasise', 'u', 'role', 'life']\n",
      "After stemming with porters algorithm: ['take', 'care', 'sleep', 'well', 'need', 'learn', 'chang', 'life', 'need', 'get', 'convin', 'wait', 'convers', 'get', 'convin', 'time', 'famili', 'mani', 'sens', 'respect', 'overemphasis', 'role', 'life']\n",
      "Tokenized sentence: ['here', 'got', 'ur', 'favorite', 'oyster', 'n', 'got', 'my', 'favorite', 'sashimi', 'ok', 'lar', 'i', 'dun', 'say', 'already', 'wait', 'ur', 'stomach', 'start', 'rumbling']\n",
      "After stop words removal: ['got', 'ur', 'favorite', 'oyster', 'n', 'got', 'favorite', 'sashimi', 'ok', 'lar', 'dun', 'say', 'already', 'wait', 'ur', 'stomach', 'start', 'rumbling']\n",
      "rumbl\n",
      "rumble\n",
      "After stemming with porters algorithm: ['got', 'favorit', 'oyster', 'got', 'favorit', 'sashimi', 'lar', 'dun', 'sai', 'alreadi', 'wait', 'stomach', 'start', 'rumbl']\n",
      "Tokenized sentence: ['sorry', 'de', 'i', 'went', 'to', 'shop']\n",
      "After stop words removal: ['sorry', 'de', 'went', 'shop']\n",
      "After stemming with porters algorithm: ['sorri', 'went', 'shop']\n",
      "Tokenized sentence: ['bot', 'notes', 'oredi', 'cos', 'i', 'juz', 'rem', 'i', 'got']\n",
      "After stop words removal: ['bot', 'notes', 'oredi', 'cos', 'juz', 'rem', 'got']\n",
      "After stemming with porters algorithm: ['bot', 'note', 'oredi', 'co', 'juz', 'rem', 'got']\n",
      "Tokenized sentence: ['hi', 'mobile', 'no', 'lt', 'gt', 'has', 'added', 'you', 'in', 'their', 'contact', 'list', 'on', 'www', 'fullonsms', 'com', 'it', 's', 'a', 'great', 'place', 'to', 'send', 'free', 'sms', 'to', 'people', 'for', 'more', 'visit', 'fullonsms', 'com']\n",
      "After stop words removal: ['hi', 'mobile', 'lt', 'gt', 'added', 'contact', 'list', 'www', 'fullonsms', 'com', 'great', 'place', 'send', 'free', 'sms', 'people', 'visit', 'fullonsms', 'com']\n",
      "After stemming with porters algorithm: ['mobil', 'ad', 'contact', 'list', 'www', 'fullonsm', 'com', 'great', 'place', 'send', 'free', 'sm', 'peopl', 'visit', 'fullonsm', 'com']\n",
      "Tokenized sentence: ['double', 'mins', 'and', 'txts', 'months', 'free', 'bluetooth', 'on', 'orange', 'available', 'on', 'sony', 'nokia', 'motorola', 'phones', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'n', 'dx']\n",
      "After stop words removal: ['double', 'mins', 'txts', 'months', 'free', 'bluetooth', 'orange', 'available', 'sony', 'nokia', 'motorola', 'phones', 'call', 'mobileupd', 'call', 'optout', 'n', 'dx']\n",
      "After stemming with porters algorithm: ['doubl', 'min', 'txt', 'month', 'free', 'bluetooth', 'orang', 'avail', 'soni', 'nokia', 'motorola', 'phone', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['urgent', 'please', 'call', 'from', 'landline', 'your', 'abta', 'complimentary', 'tenerife', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'box', 'cw', 'wx', 'ppm']\n",
      "After stop words removal: ['urgent', 'please', 'call', 'landline', 'abta', 'complimentary', 'tenerife', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'box', 'cw', 'wx', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'pleas', 'call', 'landlin', 'abta', 'complimentari', 'tenerif', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['i', 'dnt', 'wnt', 'to', 'tlk', 'wid', 'u']\n",
      "After stop words removal: ['dnt', 'wnt', 'tlk', 'wid', 'u']\n",
      "After stemming with porters algorithm: ['dnt', 'wnt', 'tlk', 'wid']\n",
      "Tokenized sentence: ['babe', 'i', 'need', 'your', 'advice']\n",
      "After stop words removal: ['babe', 'need', 'advice']\n",
      "After stemming with porters algorithm: ['babe', 'need', 'advic']\n",
      "Tokenized sentence: ['splashmobile', 'choose', 'from', 's', 'of', 'gr', 'tones', 'each', 'wk', 'this', 'is', 'a', 'subscrition', 'service', 'with', 'weekly', 'tones', 'costing', 'p', 'u', 'have', 'one', 'credit', 'kick', 'back', 'and', 'enjoy']\n",
      "After stop words removal: ['splashmobile', 'choose', 'gr', 'tones', 'wk', 'subscrition', 'service', 'weekly', 'tones', 'costing', 'p', 'u', 'one', 'credit', 'kick', 'back', 'enjoy']\n",
      "cost\n",
      "After stemming with porters algorithm: ['splashmobil', 'choos', 'tone', 'subscrit', 'servic', 'weekli', 'tone', 'cos', 'on', 'credit', 'kick', 'back', 'enjoi']\n",
      "Tokenized sentence: ['s', 'from', 'the', 'training', 'manual', 'it', 'show', 'there', 'is', 'no', 'tech', 'process', 'its', 'all', 'about', 'password', 'reset', 'and', 'troubleshooting']\n",
      "After stop words removal: ['training', 'manual', 'show', 'tech', 'process', 'password', 'reset', 'troubleshooting']\n",
      "train\n",
      "troubleshoot\n",
      "After stemming with porters algorithm: ['train', 'manual', 'show', 'tech', 'process', 'password', 'reset', 'troubleshoot']\n",
      "Tokenized sentence: ['are', 'you', 'this', 'much', 'buzy']\n",
      "After stop words removal: ['much', 'buzy']\n",
      "After stemming with porters algorithm: ['much', 'buzi']\n",
      "Tokenized sentence: ['realy', 'sorry', 'i', 'don', 't', 'recognise', 'this', 'number', 'and', 'am', 'now', 'confused', 'who', 'r', 'u', 'please']\n",
      "After stop words removal: ['realy', 'sorry', 'recognise', 'number', 'confused', 'r', 'u', 'please']\n",
      "After stemming with porters algorithm: ['reali', 'sorri', 'recognis', 'number', 'confus', 'pleas']\n",
      "Tokenized sentence: ['cds', 'u', 'congratulations', 'ur', 'awarded', 'of', 'cd', 'gift', 'vouchers', 'or', 'gift', 'guaranteed', 'freeentry', 'wkly', 'draw', 'xt', 'music', 'to', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stop words removal: ['cds', 'u', 'congratulations', 'ur', 'awarded', 'cd', 'gift', 'vouchers', 'gift', 'guaranteed', 'freeentry', 'wkly', 'draw', 'xt', 'music', 'tncs', 'www', 'ldew', 'com', 'win', 'ppmx', 'age']\n",
      "After stemming with porters algorithm: ['cd', 'congratul', 'awar', 'gift', 'voucher', 'gift', 'guaranteed', 'freeentri', 'wkly', 'draw', 'music', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'ag']\n",
      "Tokenized sentence: ['jamster', 'to', 'get', 'your', 'free', 'wallpaper', 'text', 'heart', 'to', 'now', 't', 'c', 'apply', 'only', 'need', 'help', 'call']\n",
      "After stop words removal: ['jamster', 'get', 'free', 'wallpaper', 'text', 'heart', 'c', 'apply', 'need', 'help', 'call']\n",
      "After stemming with porters algorithm: ['jamster', 'get', 'free', 'wallpap', 'text', 'heart', 'appli', 'need', 'help', 'call']\n",
      "Tokenized sentence: ['hi', 'can', 'i', 'please', 'get', 'a', 'lt', 'gt', 'dollar', 'loan', 'from', 'you', 'i', 'll', 'pay', 'you', 'back', 'by', 'mid', 'february', 'pls']\n",
      "After stop words removal: ['hi', 'please', 'get', 'lt', 'gt', 'dollar', 'loan', 'pay', 'back', 'mid', 'february', 'pls']\n",
      "After stemming with porters algorithm: ['pleas', 'get', 'dollar', 'loan', 'pai', 'back', 'mid', 'februari', 'pl']\n",
      "Tokenized sentence: ['have', 'you', 'heard', 'from', 'this', 'week']\n",
      "After stop words removal: ['heard', 'week']\n",
      "After stemming with porters algorithm: ['heard', 'week']\n",
      "Tokenized sentence: ['i', 'called', 'and', 'said', 'all', 'to', 'him', 'then', 'he', 'have', 'to', 'choose', 'this', 'future']\n",
      "After stop words removal: ['called', 'said', 'choose', 'future']\n",
      "After stemming with porters algorithm: ['call', 'said', 'choos', 'futur']\n",
      "Tokenized sentence: ['do', 'you', 'know', 'what', 'mallika', 'sherawat', 'did', 'yesterday', 'find', 'out', 'now', 'lt', 'url', 'gt']\n",
      "After stop words removal: ['know', 'mallika', 'sherawat', 'yesterday', 'find', 'lt', 'url', 'gt']\n",
      "After stemming with porters algorithm: ['know', 'mallika', 'sherawat', 'yesterdai', 'find', 'url']\n",
      "Tokenized sentence: ['been', 'up', 'to', 'ne', 'thing', 'interesting', 'did', 'you', 'have', 'a', 'good', 'birthday', 'when', 'are', 'u', 'wrking', 'nxt', 'i', 'started', 'uni', 'today']\n",
      "After stop words removal: ['ne', 'thing', 'interesting', 'good', 'birthday', 'u', 'wrking', 'nxt', 'started', 'uni', 'today']\n",
      "interest\n",
      "After stemming with porters algorithm: ['thing', 'interes', 'good', 'birthdai', 'wrking', 'nxt', 'star', 'uni', 'todai']\n",
      "Tokenized sentence: ['hey', 'loverboy', 'i', 'love', 'you', 'i', 'had', 'to', 'tell', 'i', 'look', 'at', 'your', 'picture', 'and', 'ache', 'to', 'feel', 'you', 'between', 'my', 'legs', 'fuck', 'i', 'want', 'you', 'i', 'need', 'you', 'i', 'crave', 'you']\n",
      "After stop words removal: ['hey', 'loverboy', 'love', 'tell', 'look', 'picture', 'ache', 'feel', 'legs', 'fuck', 'want', 'need', 'crave']\n",
      "After stemming with porters algorithm: ['hei', 'loverboi', 'love', 'tell', 'look', 'pictur', 'ach', 'feel', 'leg', 'fuck', 'want', 'need', 'crave']\n",
      "Tokenized sentence: ['omg', 'joanna', 'is', 'freaking', 'me', 'out', 'she', 's', 'looked', 'thru', 'all', 'my', 'friends', 'to', 'find', 'photos', 'of', 'me', 'and', 'then', 'she', 's', 'asking', 'about', 'stuff', 'on', 'my', 'myspace', 'which', 'i', 'haven', 't', 'even', 'logged', 'on', 'in', 'like', 'a', 'year']\n",
      "After stop words removal: ['omg', 'joanna', 'freaking', 'looked', 'thru', 'friends', 'find', 'photos', 'asking', 'stuff', 'myspace', 'even', 'logged', 'like', 'year']\n",
      "freak\n",
      "ask\n",
      "After stemming with porters algorithm: ['omg', 'joanna', 'freak', 'look', 'thru', 'friend', 'find', 'photo', 'as', 'stuff', 'myspace', 'even', 'log', 'like', 'year']\n",
      "Tokenized sentence: ['happy', 'new', 'year', 'hope', 'you', 'are', 'having', 'a', 'good', 'semester']\n",
      "After stop words removal: ['happy', 'new', 'year', 'hope', 'good', 'semester']\n",
      "After stemming with porters algorithm: ['happi', 'new', 'year', 'hope', 'good', 'semest']\n",
      "Tokenized sentence: ['i', 'll', 'see', 'but', 'prolly', 'yeah']\n",
      "After stop words removal: ['see', 'prolly', 'yeah']\n",
      "After stemming with porters algorithm: ['see', 'prolli', 'yeah']\n",
      "Tokenized sentence: ['i', 'don', 't', 'know', 'u', 'and', 'u', 'don', 't', 'know', 'me', 'send', 'chat', 'to', 'now', 'and', 'let', 's', 'find', 'each', 'other', 'only', 'p', 'msg', 'rcvd', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'years', 'or', 'over']\n",
      "After stop words removal: ['know', 'u', 'u', 'know', 'send', 'chat', 'let', 'find', 'p', 'msg', 'rcvd', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl', 'ldn', 'years']\n",
      "After stemming with porters algorithm: ['know', 'know', 'send', 'chat', 'let', 'find', 'msg', 'rcvd', 'suit', 'land', 'row', 'ldn', 'year']\n",
      "Tokenized sentence: ['haha', 'can', 'but', 'i', 'm', 'having', 'dinner', 'with', 'my', 'cousin']\n",
      "After stop words removal: ['haha', 'dinner', 'cousin']\n",
      "After stemming with porters algorithm: ['haha', 'dinner', 'cousin']\n",
      "Tokenized sentence: ['weightloss', 'no', 'more', 'girl', 'friends', 'make', 'loads', 'of', 'money', 'on', 'ebay', 'or', 'something', 'and', 'give', 'thanks', 'to', 'god']\n",
      "After stop words removal: ['weightloss', 'girl', 'friends', 'make', 'loads', 'money', 'ebay', 'something', 'give', 'thanks', 'god']\n",
      "someth\n",
      "After stemming with porters algorithm: ['weightloss', 'girl', 'friend', 'make', 'load', 'monei', 'ebai', 'somet', 'give', 'thank', 'god']\n",
      "Tokenized sentence: ['am', 'going', 'to', 'take', 'bath', 'ill', 'place', 'the', 'key', 'in', 'window']\n",
      "After stop words removal: ['going', 'take', 'bath', 'ill', 'place', 'key', 'window']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'take', 'bath', 'ill', 'place', 'kei', 'window']\n",
      "Tokenized sentence: ['mths', 'half', 'price', 'orange', 'line', 'rental', 'latest', 'camera', 'phones', 'free', 'had', 'your', 'phone', 'mths', 'call', 'mobilesdirect', 'free', 'on', 'to', 'update', 'now', 'or', 'stoptxt']\n",
      "After stop words removal: ['mths', 'half', 'price', 'orange', 'line', 'rental', 'latest', 'camera', 'phones', 'free', 'phone', 'mths', 'call', 'mobilesdirect', 'free', 'update', 'stoptxt']\n",
      "After stemming with porters algorithm: ['mth', 'half', 'price', 'orang', 'line', 'rental', 'latest', 'camera', 'phone', 'free', 'phone', 'mth', 'call', 'mobilesdirect', 'free', 'updat', 'stoptxt']\n",
      "Tokenized sentence: ['i', 'think', 'you', 'should', 'go', 'the', 'honesty', 'road', 'call', 'the', 'bank', 'tomorrow', 'its', 'the', 'tough', 'decisions', 'that', 'make', 'us', 'great', 'people']\n",
      "After stop words removal: ['think', 'go', 'honesty', 'road', 'call', 'bank', 'tomorrow', 'tough', 'decisions', 'make', 'us', 'great', 'people']\n",
      "After stemming with porters algorithm: ['think', 'honesti', 'road', 'call', 'bank', 'tomorrow', 'tough', 'decis', 'make', 'great', 'peopl']\n",
      "Tokenized sentence: ['they', 'will', 'pick', 'up', 'and', 'drop', 'in', 'car', 'so', 'no', 'problem']\n",
      "After stop words removal: ['pick', 'drop', 'car', 'problem']\n",
      "After stemming with porters algorithm: ['pick', 'drop', 'car', 'problem']\n",
      "Tokenized sentence: ['will', 'have', 'two', 'more', 'cartons', 'off', 'u', 'and', 'is', 'very', 'pleased', 'with', 'shelves']\n",
      "After stop words removal: ['two', 'cartons', 'u', 'pleased', 'shelves']\n",
      "After stemming with porters algorithm: ['two', 'carton', 'pleas', 'shelv']\n",
      "Tokenized sentence: ['you', 'have', 'an', 'important', 'customer', 'service', 'announcement', 'from', 'premier']\n",
      "After stop words removal: ['important', 'customer', 'service', 'announcement', 'premier']\n",
      "After stemming with porters algorithm: ['import', 'custom', 'servic', 'announc', 'premier']\n",
      "Tokenized sentence: ['hey', 'what', 'how', 'about', 'your', 'project', 'started', 'aha', 'da']\n",
      "After stop words removal: ['hey', 'project', 'started', 'aha', 'da']\n",
      "After stemming with porters algorithm: ['hei', 'project', 'star', 'aha']\n",
      "Tokenized sentence: ['shop', 'till', 'u', 'drop', 'is', 'it', 'you', 'either', 'k', 'k', 'cash', 'or', 'travel', 'voucher', 'call', 'now', 'ntt', 'po', 'box', 'cr', 'bt', 'fixedline', 'cost', 'ppm', 'mobile', 'vary']\n",
      "After stop words removal: ['shop', 'till', 'u', 'drop', 'either', 'k', 'k', 'cash', 'travel', 'voucher', 'call', 'ntt', 'po', 'box', 'cr', 'bt', 'fixedline', 'cost', 'ppm', 'mobile', 'vary']\n",
      "After stemming with porters algorithm: ['shop', 'till', 'drop', 'either', 'cash', 'travel', 'voucher', 'call', 'ntt', 'box', 'fixedlin', 'cost', 'ppm', 'mobil', 'vari']\n",
      "Tokenized sentence: ['where', 's', 'my', 'boytoy', 'i', 'miss', 'you', 'what', 'happened']\n",
      "After stop words removal: ['boytoy', 'miss', 'happened']\n",
      "After stemming with porters algorithm: ['boytoi', 'miss', 'happen']\n",
      "Tokenized sentence: ['not', 'directly', 'behind', 'abt', 'rows', 'behind']\n",
      "After stop words removal: ['directly', 'behind', 'abt', 'rows', 'behind']\n",
      "After stemming with porters algorithm: ['directli', 'behind', 'abt', 'row', 'behind']\n",
      "Tokenized sentence: ['wot', 'about', 'on', 'wed', 'nite', 'i', 'am', 'then', 'but', 'only', 'til']\n",
      "After stop words removal: ['wot', 'wed', 'nite', 'til']\n",
      "After stemming with porters algorithm: ['wot', 'wed', 'nite', 'til']\n",
      "Tokenized sentence: ['im', 'late', 'tellmiss', 'im', 'on', 'my', 'way']\n",
      "After stop words removal: ['im', 'late', 'tellmiss', 'im', 'way']\n",
      "After stemming with porters algorithm: ['late', 'tellmiss', 'wai']\n",
      "Tokenized sentence: ['nt', 'only', 'for', 'driving', 'even', 'for', 'many', 'reasons', 'she', 'is', 'called', 'bbd', 'thts', 'it', 'chikku', 'then', 'hw', 'abt', 'dvg', 'cold', 'heard', 'tht', 'vinobanagar', 'violence', 'hw', 'is', 'the', 'condition', 'and', 'hw', 'ru', 'any', 'problem']\n",
      "After stop words removal: ['nt', 'driving', 'even', 'many', 'reasons', 'called', 'bbd', 'thts', 'chikku', 'hw', 'abt', 'dvg', 'cold', 'heard', 'tht', 'vinobanagar', 'violence', 'hw', 'condition', 'hw', 'ru', 'problem']\n",
      "driv\n",
      "After stemming with porters algorithm: ['drive', 'even', 'mani', 'reason', 'call', 'bbd', 'tht', 'chikku', 'abt', 'dvg', 'cold', 'heard', 'tht', 'vinobanagar', 'violenc', 'condit', 'problem']\n",
      "Tokenized sentence: ['went', 'to', 'ganesh', 'dress', 'shop']\n",
      "After stop words removal: ['went', 'ganesh', 'dress', 'shop']\n",
      "After stemming with porters algorithm: ['went', 'ganesh', 'dress', 'shop']\n",
      "Tokenized sentence: ['so', 'what', 'about', 'you', 'what', 'do', 'you', 'remember']\n",
      "After stop words removal: ['remember']\n",
      "After stemming with porters algorithm: ['rememb']\n",
      "Tokenized sentence: ['no', 'da', 'vijay', 'going', 'to', 'talk', 'in', 'jaya', 'tv']\n",
      "After stop words removal: ['da', 'vijay', 'going', 'talk', 'jaya', 'tv']\n",
      "go\n",
      "After stemming with porters algorithm: ['vijai', 'go', 'talk', 'jaya']\n",
      "Tokenized sentence: ['beautiful', 'truth', 'against', 'gravity', 'read', 'carefully', 'our', 'heart', 'feels', 'light', 'when', 'someone', 'is', 'in', 'it', 'but', 'it', 'feels', 'very', 'heavy', 'when', 'someone', 'leaves', 'it', 'good', 'night']\n",
      "After stop words removal: ['beautiful', 'truth', 'gravity', 'read', 'carefully', 'heart', 'feels', 'light', 'someone', 'feels', 'heavy', 'someone', 'leaves', 'good', 'night']\n",
      "After stemming with porters algorithm: ['beauti', 'truth', 'graviti', 'read', 'carefulli', 'heart', 'feel', 'light', 'someon', 'feel', 'heavi', 'someon', 'leav', 'good', 'night']\n",
      "Tokenized sentence: ['thanks', 'for', 'this', 'hope', 'you', 'had', 'a', 'good', 'day', 'today']\n",
      "After stop words removal: ['thanks', 'hope', 'good', 'day', 'today']\n",
      "After stemming with porters algorithm: ['thank', 'hope', 'good', 'dai', 'todai']\n",
      "Tokenized sentence: ['urgent', 'call', 'from', 'landline', 'your', 'complimentary', 'ibiza', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 't', 'cs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
      "After stop words removal: ['urgent', 'call', 'landline', 'complimentary', 'ibiza', 'holiday', 'cash', 'await', 'collection', 'sae', 'cs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'call', 'landlin', 'complimentari', 'ibiza', 'holidai', 'cash', 'await', 'collect', 'sae', 'box', 'ppm']\n",
      "Tokenized sentence: ['shuhui', 'say', 'change', 'suntec', 'steamboat', 'u', 'noe', 'where', 'where', 'r', 'u', 'now']\n",
      "After stop words removal: ['shuhui', 'say', 'change', 'suntec', 'steamboat', 'u', 'noe', 'r', 'u']\n",
      "After stemming with porters algorithm: ['shuhui', 'sai', 'chang', 'suntec', 'steamboat', 'noe']\n",
      "Tokenized sentence: ['todays', 'voda', 'numbers', 'ending', 'with', 'are', 'selected', 'to', 'receive', 'a', 'reward', 'if', 'you', 'have', 'a', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'apply']\n",
      "After stop words removal: ['todays', 'voda', 'numbers', 'ending', 'selected', 'receive', 'reward', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'apply']\n",
      "end\n",
      "quot\n",
      "After stemming with porters algorithm: ['todai', 'voda', 'number', 'en', 'selec', 'receiv', 'reward', 'match', 'pleas', 'call', 'quot', 'claim', 'code', 'standard', 'rate', 'appli']\n",
      "Tokenized sentence: ['eat', 'jap', 'done', 'oso', 'aft', 'ur', 'lect', 'wat', 'got', 'lect', 'at', 'rite']\n",
      "After stop words removal: ['eat', 'jap', 'done', 'oso', 'aft', 'ur', 'lect', 'wat', 'got', 'lect', 'rite']\n",
      "After stemming with porters algorithm: ['eat', 'jap', 'done', 'oso', 'aft', 'lect', 'wat', 'got', 'lect', 'rite']\n",
      "Tokenized sentence: ['excellent', 'are', 'you', 'ready', 'to', 'moan', 'and', 'scream', 'in', 'ecstasy']\n",
      "After stop words removal: ['excellent', 'ready', 'moan', 'scream', 'ecstasy']\n",
      "After stemming with porters algorithm: ['excel', 'readi', 'moan', 'scream', 'ecstasi']\n",
      "Tokenized sentence: ['thank', 'you', 'winner', 'notified', 'by', 'sms', 'good', 'luck', 'no', 'future', 'marketing', 'reply', 'stop', 'to', 'customer', 'services']\n",
      "After stop words removal: ['thank', 'winner', 'notified', 'sms', 'good', 'luck', 'future', 'marketing', 'reply', 'stop', 'customer', 'services']\n",
      "market\n",
      "After stemming with porters algorithm: ['thank', 'winner', 'notifi', 'sm', 'good', 'luck', 'futur', 'market', 'repli', 'stop', 'custom', 'servic']\n",
      "Tokenized sentence: ['wah', 'okie', 'okie', 'muz', 'make', 'use', 'of', 'e', 'unlimited', 'haha']\n",
      "After stop words removal: ['wah', 'okie', 'okie', 'muz', 'make', 'use', 'e', 'unlimited', 'haha']\n",
      "After stemming with porters algorithm: ['wah', 'oki', 'oki', 'muz', 'make', 'us', 'unlimit', 'haha']\n",
      "Tokenized sentence: ['also', 'maaaan', 'are', 'you', 'missing', 'out']\n",
      "After stop words removal: ['also', 'maaaan', 'missing']\n",
      "miss\n",
      "After stemming with porters algorithm: ['also', 'maaaan', 'miss']\n",
      "Tokenized sentence: ['when', 'login', 'dat', 'time', 'dad', 'fetching', 'home', 'now']\n",
      "After stop words removal: ['login', 'dat', 'time', 'dad', 'fetching', 'home']\n",
      "fetch\n",
      "After stemming with porters algorithm: ['login', 'dat', 'time', 'dad', 'fetc', 'home']\n",
      "Tokenized sentence: ['what', 'will', 'we', 'do', 'in', 'the', 'shower', 'baby']\n",
      "After stop words removal: ['shower', 'baby']\n",
      "After stemming with porters algorithm: ['shower', 'babi']\n",
      "Tokenized sentence: ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'freephone', 'between', 'am', 'pm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
      "After stop words removal: ['please', 'call', 'customer', 'service', 'representative', 'freephone', 'pm', 'guaranteed', 'cash', 'prize']\n",
      "After stemming with porters algorithm: ['pleas', 'call', 'custom', 'servic', 'repres', 'freephon', 'guaranteed', 'cash', 'priz']\n",
      "Tokenized sentence: ['tessy', 'pls', 'do', 'me', 'a', 'favor', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimya', 'pls', 'dnt', 'forget', 'it', 'today', 'is', 'her', 'birthday', 'shijas']\n",
      "After stop words removal: ['tessy', 'pls', 'favor', 'pls', 'convey', 'birthday', 'wishes', 'nimya', 'pls', 'dnt', 'forget', 'today', 'birthday', 'shijas']\n",
      "After stemming with porters algorithm: ['tessi', 'pl', 'favor', 'pl', 'convei', 'birthdai', 'wish', 'nimya', 'pl', 'dnt', 'forget', 'todai', 'birthdai', 'shija']\n",
      "Tokenized sentence: ['ok', 'chinese', 'food', 'on', 'its', 'way', 'when', 'i', 'get', 'fat', 'you', 're', 'paying', 'for', 'my', 'lipo']\n",
      "After stop words removal: ['ok', 'chinese', 'food', 'way', 'get', 'fat', 'paying', 'lipo']\n",
      "pay\n",
      "After stemming with porters algorithm: ['chines', 'food', 'wai', 'get', 'fat', 'pai', 'lipo']\n",
      "Tokenized sentence: ['i', 'guess', 'it', 'is', 'useless', 'calling', 'u', 'something', 'important']\n",
      "After stop words removal: ['guess', 'useless', 'calling', 'u', 'something', 'important']\n",
      "call\n",
      "someth\n",
      "After stemming with porters algorithm: ['guess', 'useless', 'call', 'somet', 'import']\n",
      "Tokenized sentence: ['my', 'tuition', 'is', 'at', 'hm', 'we', 'go', 'for', 'the', 'to', 'one', 'do', 'you', 'mind']\n",
      "After stop words removal: ['tuition', 'hm', 'go', 'one', 'mind']\n",
      "After stemming with porters algorithm: ['tuit', 'on', 'mind']\n",
      "Tokenized sentence: ['are', 'you', 'free', 'now', 'can', 'i', 'call', 'now']\n",
      "After stop words removal: ['free', 'call']\n",
      "After stemming with porters algorithm: ['free', 'call']\n",
      "Tokenized sentence: ['sorry', 'mail']\n",
      "After stop words removal: ['sorry', 'mail']\n",
      "After stemming with porters algorithm: ['sorri', 'mail']\n",
      "Tokenized sentence: ['alrite']\n",
      "After stop words removal: ['alrite']\n",
      "After stemming with porters algorithm: ['alrit']\n",
      "Tokenized sentence: ['of', 'course', 'don', 't', 'tease', 'me', 'you', 'know', 'i', 'simply', 'must', 'see', 'grins', 'do', 'keep', 'me', 'posted', 'my', 'prey', 'loving', 'smile', 'devouring', 'kiss']\n",
      "After stop words removal: ['course', 'tease', 'know', 'simply', 'must', 'see', 'grins', 'keep', 'posted', 'prey', 'loving', 'smile', 'devouring', 'kiss']\n",
      "lov\n",
      "devour\n",
      "After stemming with porters algorithm: ['cours', 'teas', 'know', 'simpli', 'must', 'see', 'grin', 'keep', 'pos', 'prei', 'love', 'smile', 'devour', 'kiss']\n",
      "Tokenized sentence: ['my', 'sister', 'going', 'to', 'earn', 'more', 'than', 'me', 'da']\n",
      "After stop words removal: ['sister', 'going', 'earn', 'da']\n",
      "go\n",
      "After stemming with porters algorithm: ['sister', 'go', 'earn']\n",
      "Tokenized sentence: ['wrong', 'phone', 'this', 'phone', 'i', 'answer', 'this', 'one', 'but', 'assume', 'the', 'other', 'is', 'people', 'i', 'don', 't', 'well']\n",
      "After stop words removal: ['wrong', 'phone', 'phone', 'answer', 'one', 'assume', 'people', 'well']\n",
      "After stemming with porters algorithm: ['wrong', 'phone', 'phone', 'answer', 'on', 'assum', 'peopl', 'well']\n",
      "Tokenized sentence: ['have', 'you', 'ever', 'had', 'one', 'foot', 'before']\n",
      "After stop words removal: ['ever', 'one', 'foot']\n",
      "After stemming with porters algorithm: ['ever', 'on', 'foot']\n",
      "Tokenized sentence: ['ah', 'poop', 'looks', 'like', 'ill', 'prob', 'have', 'to', 'send', 'in', 'my', 'laptop', 'to', 'get', 'fixed', 'cuz', 'it', 'has', 'a', 'gpu', 'problem']\n",
      "After stop words removal: ['ah', 'poop', 'looks', 'like', 'ill', 'prob', 'send', 'laptop', 'get', 'fixed', 'cuz', 'gpu', 'problem']\n",
      "After stemming with porters algorithm: ['poop', 'look', 'like', 'ill', 'prob', 'send', 'laptop', 'get', 'fix', 'cuz', 'gpu', 'problem']\n",
      "Tokenized sentence: ['rofl', 'betta', 'invest', 'in', 'some', 'anti', 'aging', 'products']\n",
      "After stop words removal: ['rofl', 'betta', 'invest', 'anti', 'aging', 'products']\n",
      "ag\n",
      "After stemming with porters algorithm: ['rofl', 'betta', 'invest', 'anti', 'ag', 'product']\n",
      "Tokenized sentence: ['sorry', 'about', 'earlier', 'putting', 'out', 'fires', 'are', 'you', 'around', 'to', 'talk', 'after', 'or', 'do', 'you', 'actually', 'have', 'a', 'life', 'lol']\n",
      "After stop words removal: ['sorry', 'earlier', 'putting', 'fires', 'around', 'talk', 'actually', 'life', 'lol']\n",
      "putt\n",
      "After stemming with porters algorithm: ['sorri', 'earlier', 'put', 'fire', 'around', 'talk', 'actual', 'life', 'lol']\n",
      "Tokenized sentence: ['also', 'that', 'chat', 'was', 'awesome', 'but', 'don', 't', 'make', 'it', 'regular', 'unless', 'you', 'can', 'see', 'her', 'in', 'person']\n",
      "After stop words removal: ['also', 'chat', 'awesome', 'make', 'regular', 'unless', 'see', 'person']\n",
      "After stemming with porters algorithm: ['also', 'chat', 'awesom', 'make', 'regular', 'unless', 'see', 'person']\n",
      "Tokenized sentence: ['dorothy', 'kiefer', 'com', 'bank', 'of', 'granite', 'issues', 'strong', 'buy', 'explosive', 'pick', 'for', 'our', 'members', 'up', 'over', 'nasdaq', 'symbol', 'cdgt', 'that', 'is', 'a', 'per']\n",
      "After stop words removal: ['dorothy', 'kiefer', 'com', 'bank', 'granite', 'issues', 'strong', 'buy', 'explosive', 'pick', 'members', 'nasdaq', 'symbol', 'cdgt', 'per']\n",
      "After stemming with porters algorithm: ['dorothi', 'kiefer', 'com', 'bank', 'granit', 'issu', 'strong', 'bui', 'explos', 'pick', 'member', 'nasdaq', 'symbol', 'cdgt', 'per']\n",
      "Tokenized sentence: ['how', 'much', 'i', 'gave', 'to', 'you', 'morning']\n",
      "After stop words removal: ['much', 'gave', 'morning']\n",
      "morn\n",
      "After stemming with porters algorithm: ['much', 'gave', 'mor']\n",
      "Tokenized sentence: ['u', 'don', 't', 'know', 'how', 'stubborn', 'i', 'am', 'i', 'didn', 't', 'even', 'want', 'to', 'go', 'to', 'the', 'hospital', 'i', 'kept', 'telling', 'mark', 'i', 'm', 'not', 'a', 'weak', 'sucker', 'hospitals', 'are', 'for', 'weak', 'suckers']\n",
      "After stop words removal: ['u', 'know', 'stubborn', 'even', 'want', 'go', 'hospital', 'kept', 'telling', 'mark', 'weak', 'sucker', 'hospitals', 'weak', 'suckers']\n",
      "tell\n",
      "After stemming with porters algorithm: ['know', 'stubborn', 'even', 'want', 'hospit', 'kept', 'tell', 'mark', 'weak', 'sucker', 'hospit', 'weak', 'sucker']\n",
      "Tokenized sentence: ['my', 'superior', 'telling', 'that', 'friday', 'is', 'leave', 'for', 'all', 'other', 'department', 'except', 'ours', 'so', 'it', 'will', 'be', 'leave', 'for', 'you', 'any', 'way', 'call', 'waheed', 'fathima', 'hr', 'and', 'conform', 'it']\n",
      "After stop words removal: ['superior', 'telling', 'friday', 'leave', 'department', 'except', 'leave', 'way', 'call', 'waheed', 'fathima', 'hr', 'conform']\n",
      "tell\n",
      "After stemming with porters algorithm: ['superior', 'tell', 'fridai', 'leav', 'depart', 'except', 'leav', 'wai', 'call', 'wahe', 'fathima', 'conform']\n",
      "Tokenized sentence: ['that', 's', 'not', 'v', 'romantic']\n",
      "After stop words removal: ['v', 'romantic']\n",
      "After stemming with porters algorithm: ['romant']\n",
      "Tokenized sentence: ['camera', 'you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stop words removal: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
      "After stemming with porters algorithm: ['camera', 'awar', 'sipix', 'digit', 'camera', 'call', 'fromm', 'landlin', 'deliveri', 'within', 'dai']\n",
      "Tokenized sentence: ['aiyo', 'cos', 'i', 'sms', 'then', 'neva', 'reply', 'so', 'i', 'wait', 'to', 'reply', 'lar', 'i', 'tot', 'havent', 'finish', 'ur', 'lab', 'wat']\n",
      "After stop words removal: ['aiyo', 'cos', 'sms', 'neva', 'reply', 'wait', 'reply', 'lar', 'tot', 'havent', 'finish', 'ur', 'lab', 'wat']\n",
      "After stemming with porters algorithm: ['aiyo', 'co', 'sm', 'neva', 'repli', 'wait', 'repli', 'lar', 'tot', 'havent', 'finish', 'lab', 'wat']\n",
      "Tokenized sentence: ['i', 've', 'got', 'some', 'salt', 'you', 'can', 'rub', 'it', 'in', 'my', 'open', 'wounds', 'if', 'you', 'like']\n",
      "After stop words removal: ['got', 'salt', 'rub', 'open', 'wounds', 'like']\n",
      "After stemming with porters algorithm: ['got', 'salt', 'rub', 'open', 'wound', 'like']\n",
      "Tokenized sentence: ['same', 'as', 'u', 'dun', 'wan', 'y', 'u', 'dun', 'like', 'me', 'already', 'ah', 'wat', 'u', 'doing', 'now', 'still', 'eating']\n",
      "After stop words removal: ['u', 'dun', 'wan', 'u', 'dun', 'like', 'already', 'ah', 'wat', 'u', 'still', 'eating']\n",
      "eat\n",
      "eate\n",
      "After stemming with porters algorithm: ['dun', 'wan', 'dun', 'like', 'alreadi', 'wat', 'still', 'eat']\n",
      "Tokenized sentence: ['wah', 'lucky', 'man', 'then', 'can', 'save', 'money', 'hee']\n",
      "After stop words removal: ['wah', 'lucky', 'man', 'save', 'money', 'hee']\n",
      "After stemming with porters algorithm: ['wah', 'lucki', 'man', 'save', 'monei', 'hee']\n",
      "Tokenized sentence: ['sir', 'i', 'need', 'axis', 'bank', 'account', 'no', 'and', 'bank', 'address']\n",
      "After stop words removal: ['sir', 'need', 'axis', 'bank', 'account', 'bank', 'address']\n",
      "After stemming with porters algorithm: ['sir', 'need', 'axi', 'bank', 'account', 'bank', 'address']\n",
      "Tokenized sentence: ['haha', 'awesome', 'be', 'there', 'in', 'a', 'minute']\n",
      "After stop words removal: ['haha', 'awesome', 'minute']\n",
      "After stemming with porters algorithm: ['haha', 'awesom', 'minut']\n",
      "Tokenized sentence: ['eh', 'u', 'send', 'wrongly', 'lar']\n",
      "After stop words removal: ['eh', 'u', 'send', 'wrongly', 'lar']\n",
      "After stemming with porters algorithm: ['send', 'wrongli', 'lar']\n",
      "Tokenized sentence: ['got', 'c', 'i', 'lazy', 'to', 'type', 'i', 'forgot', 'in', 'lect', 'i', 'saw', 'a', 'pouch', 'but', 'like', 'not', 'v', 'nice']\n",
      "After stop words removal: ['got', 'c', 'lazy', 'type', 'forgot', 'lect', 'saw', 'pouch', 'like', 'v', 'nice']\n",
      "After stemming with porters algorithm: ['got', 'lazi', 'type', 'forgot', 'lect', 'saw', 'pouch', 'like', 'nice']\n",
      "Tokenized sentence: ['you', 'are', 'a', 'great', 'role', 'model', 'you', 'are', 'giving', 'so', 'much', 'and', 'i', 'really', 'wish', 'each', 'day', 'for', 'a', 'miracle', 'but', 'god', 'as', 'a', 'reason', 'for', 'everything', 'and', 'i', 'must', 'say', 'i', 'wish', 'i', 'knew', 'why', 'but', 'i', 'dont', 'i', 've', 'looked', 'up', 'to', 'you', 'since', 'i', 'was', 'young', 'and', 'i', 'still', 'do', 'have', 'a', 'great', 'day']\n",
      "After stop words removal: ['great', 'role', 'model', 'giving', 'much', 'really', 'wish', 'day', 'miracle', 'god', 'reason', 'everything', 'must', 'say', 'wish', 'knew', 'dont', 'looked', 'since', 'young', 'still', 'great', 'day']\n",
      "giv\n",
      "everyth\n",
      "After stemming with porters algorithm: ['great', 'role', 'model', 'give', 'much', 'realli', 'wish', 'dai', 'mirac', 'god', 'reason', 'everyt', 'must', 'sai', 'wish', 'knew', 'dont', 'look', 'sinc', 'young', 'still', 'great', 'dai']\n",
      "Tokenized sentence: ['i', 'asked', 'you', 'to', 'call', 'him', 'now', 'ok']\n",
      "After stop words removal: ['asked', 'call', 'ok']\n",
      "After stemming with porters algorithm: ['as', 'call']\n",
      "Tokenized sentence: ['wan', 'win', 'a', 'meet', 'greet', 'with', 'westlife', 'u', 'or', 'a', 'm', 'they', 'are', 'currently', 'on', 'what', 'tour', 'unbreakable', 'untamed', 'unkempt', 'text', 'or', 'to', 'cost', 'p', 'std', 'text']\n",
      "After stop words removal: ['wan', 'win', 'meet', 'greet', 'westlife', 'u', 'currently', 'tour', 'unbreakable', 'untamed', 'unkempt', 'text', 'cost', 'p', 'std', 'text']\n",
      "After stemming with porters algorithm: ['wan', 'win', 'meet', 'greet', 'westlif', 'current', 'tour', 'unbreak', 'untam', 'unkempt', 'text', 'cost', 'std', 'text']\n",
      "Tokenized sentence: ['gokila', 'is', 'talking', 'with', 'you', 'aha']\n",
      "After stop words removal: ['gokila', 'talking', 'aha']\n",
      "talk\n",
      "After stemming with porters algorithm: ['gokila', 'tal', 'aha']\n",
      "Tokenized sentence: ['no', 'dear', 'i', 'do', 'have', 'free', 'messages', 'without', 'any', 'recharge', 'hi', 'hi', 'hi']\n",
      "After stop words removal: ['dear', 'free', 'messages', 'without', 'recharge', 'hi', 'hi', 'hi']\n",
      "After stemming with porters algorithm: ['dear', 'free', 'messag', 'without', 'recharg']\n",
      "Tokenized sentence: ['and', 'picking', 'them', 'up', 'from', 'various', 'points', 'going', 'yeovil', 'and', 'they', 'will', 'do', 'the', 'motor', 'project', 'hours', 'and', 'then', 'u', 'take', 'them', 'home', 'max', 'very', 'easy']\n",
      "After stop words removal: ['picking', 'various', 'points', 'going', 'yeovil', 'motor', 'project', 'hours', 'u', 'take', 'home', 'max', 'easy']\n",
      "pick\n",
      "go\n",
      "After stemming with porters algorithm: ['pic', 'variou', 'point', 'go', 'yeovil', 'motor', 'project', 'hour', 'take', 'home', 'max', 'easi']\n",
      "Tokenized sentence: ['lol', 'now', 'i', 'm', 'after', 'that', 'hot', 'air', 'balloon']\n",
      "After stop words removal: ['lol', 'hot', 'air', 'balloon']\n",
      "After stemming with porters algorithm: ['lol', 'hot', 'air', 'balloon']\n",
      "Tokenized sentence: ['when', 'should', 'i', 'come', 'over']\n",
      "After stop words removal: ['come']\n",
      "After stemming with porters algorithm: ['come']\n",
      "Tokenized sentence: ['understand', 'his', 'loss', 'is', 'my', 'gain', 'so', 'do', 'you', 'work', 'school']\n",
      "After stop words removal: ['understand', 'loss', 'gain', 'work', 'school']\n",
      "After stemming with porters algorithm: ['understand', 'loss', 'gain', 'work', 'school']\n",
      "Tokenized sentence: ['howz', 'pain', 'hope', 'u', 'r', 'fine']\n",
      "After stop words removal: ['howz', 'pain', 'hope', 'u', 'r', 'fine']\n",
      "After stemming with porters algorithm: ['howz', 'pain', 'hope', 'fine']\n",
      "Tokenized sentence: ['when', 'is', 'school', 'starting', 'where', 'will', 'you', 'stay', 'what', 's', 'the', 'weather', 'like', 'and', 'the', 'food', 'do', 'you', 'have', 'a', 'social', 'support', 'system', 'like', 'friends', 'in', 'the', 'school', 'all', 'these', 'things', 'are', 'important']\n",
      "After stop words removal: ['school', 'starting', 'stay', 'weather', 'like', 'food', 'social', 'support', 'system', 'like', 'friends', 'school', 'things', 'important']\n",
      "start\n",
      "After stemming with porters algorithm: ['school', 'star', 'stai', 'weather', 'like', 'food', 'social', 'support', 'system', 'like', 'friend', 'school', 'thing', 'import']\n",
      "Tokenized sentence: ['simply', 'sitting', 'and', 'watching', 'match', 'in', 'office']\n",
      "After stop words removal: ['simply', 'sitting', 'watching', 'match', 'office']\n",
      "sitt\n",
      "watch\n",
      "After stemming with porters algorithm: ['simpli', 'sit', 'watc', 'match', 'offic']\n",
      "Tokenized sentence: ['k', 'k', 'how', 'is', 'your', 'sister', 'kids']\n",
      "After stop words removal: ['k', 'k', 'sister', 'kids']\n",
      "After stemming with porters algorithm: ['sister', 'kid']\n",
      "Tokenized sentence: ['yeah', 'i', 'don', 't', 'see', 'why', 'not']\n",
      "After stop words removal: ['yeah', 'see']\n",
      "After stemming with porters algorithm: ['yeah', 'see']\n",
      "Tokenized sentence: ['oh', 'the', 'grand', 'is', 'having', 'a', 'bit', 'of', 'a', 'party', 'but', 'it', 'doesn', 't', 'mention', 'any', 'cover', 'charge', 'so', 'it', 's', 'probably', 'first', 'come', 'first', 'served']\n",
      "After stop words removal: ['oh', 'grand', 'bit', 'party', 'mention', 'cover', 'charge', 'probably', 'first', 'come', 'first', 'served']\n",
      "After stemming with porters algorithm: ['grand', 'bit', 'parti', 'ment', 'cover', 'charg', 'probab', 'first', 'come', 'first', 'ser']\n",
      "Tokenized sentence: ['still', 'attending', 'da', 'talks']\n",
      "After stop words removal: ['still', 'attending', 'da', 'talks']\n",
      "attend\n",
      "After stemming with porters algorithm: ['still', 'atten', 'talk']\n",
      "Tokenized sentence: ['got', 'it', 'mail', 'panren', 'paru']\n",
      "After stop words removal: ['got', 'mail', 'panren', 'paru']\n",
      "After stemming with porters algorithm: ['got', 'mail', 'panren', 'paru']\n",
      "Tokenized sentence: ['hey', 'j', 'r', 'u', 'feeling', 'any', 'better']\n",
      "After stop words removal: ['hey', 'j', 'r', 'u', 'feeling', 'better']\n",
      "feel\n",
      "After stemming with porters algorithm: ['hei', 'feel', 'better']\n",
      "Tokenized sentence: ['good', 'morning', 'im', 'suffering', 'from', 'fever', 'and', 'dysentry', 'will', 'not', 'be', 'able', 'to', 'come', 'to', 'office', 'today']\n",
      "After stop words removal: ['good', 'morning', 'im', 'suffering', 'fever', 'dysentry', 'able', 'come', 'office', 'today']\n",
      "morn\n",
      "suffer\n",
      "After stemming with porters algorithm: ['good', 'mor', 'suffer', 'fever', 'dysentri', 'abl', 'come', 'offic', 'todai']\n",
      "Tokenized sentence: ['watching', 'tv', 'lor', 'nice', 'one', 'then', 'i', 'like', 'lor']\n",
      "After stop words removal: ['watching', 'tv', 'lor', 'nice', 'one', 'like', 'lor']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'lor', 'nice', 'on', 'like', 'lor']\n",
      "Tokenized sentence: ['was', 'a', 'nice', 'day', 'and', 'impressively', 'i', 'was', 'sensible', 'went', 'home', 'early', 'and', 'now', 'feel', 'fine', 'or', 'am', 'i', 'just', 'boring', 'when', 's', 'yours', 'i', 'can', 't', 'remember']\n",
      "After stop words removal: ['nice', 'day', 'impressively', 'sensible', 'went', 'home', 'early', 'feel', 'fine', 'boring', 'remember']\n",
      "bor\n",
      "After stemming with porters algorithm: ['nice', 'dai', 'impress', 'sensib', 'went', 'home', 'earli', 'feel', 'fine', 'bore', 'rememb']\n",
      "Tokenized sentence: ['important', 'information', 'orange', 'user', 'xxxxxx', 'today', 'is', 'ur', 'lucky', 'day', 'find', 'out', 'why', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'there', 's', 'a', 'fantastic', 'prizeawaiting', 'you']\n",
      "After stop words removal: ['important', 'information', 'orange', 'user', 'xxxxxx', 'today', 'ur', 'lucky', 'day', 'find', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'fantastic', 'prizeawaiting']\n",
      "prizeawait\n",
      "After stemming with porters algorithm: ['import', 'inform', 'orang', 'user', 'xxxxxx', 'todai', 'lucki', 'dai', 'find', 'log', 'onto', 'http', 'www', 'urawinn', 'com', 'fantast', 'prizeawait']\n",
      "Tokenized sentence: ['dude', 'while', 'were', 'makin', 'those', 'weirdy', 'brownies', 'my', 'sister', 'made', 'awesome', 'cookies', 'i', 'took', 'pics']\n",
      "After stop words removal: ['dude', 'makin', 'weirdy', 'brownies', 'sister', 'made', 'awesome', 'cookies', 'took', 'pics']\n",
      "After stemming with porters algorithm: ['dude', 'makin', 'weirdi', 'browni', 'sister', 'made', 'awesom', 'cooki', 'took', 'pic']\n",
      "Tokenized sentence: ['if', 'you', 're', 'thinking', 'of', 'lifting', 'me', 'one', 'then', 'no']\n",
      "After stop words removal: ['thinking', 'lifting', 'one']\n",
      "think\n",
      "lift\n",
      "After stemming with porters algorithm: ['thin', 'lif', 'on']\n",
      "Tokenized sentence: ['lol', 'no', 'just', 'trying', 'to', 'make', 'your', 'day', 'a', 'little', 'more', 'interesting']\n",
      "After stop words removal: ['lol', 'trying', 'make', 'day', 'little', 'interesting']\n",
      "interest\n",
      "After stemming with porters algorithm: ['lol', 'trying', 'make', 'dai', 'littl', 'interes']\n",
      "Tokenized sentence: ['jordan', 'got', 'voted', 'out', 'last', 'nite']\n",
      "After stop words removal: ['jordan', 'got', 'voted', 'last', 'nite']\n",
      "After stemming with porters algorithm: ['jordan', 'got', 'vote', 'last', 'nite']\n",
      "Tokenized sentence: ['fine', 'do', 'you', 'remember', 'me']\n",
      "After stop words removal: ['fine', 'remember']\n",
      "After stemming with porters algorithm: ['fine', 'rememb']\n",
      "Tokenized sentence: ['this', 'phone', 'has', 'the', 'weirdest', 'auto', 'correct']\n",
      "After stop words removal: ['phone', 'weirdest', 'auto', 'correct']\n",
      "After stemming with porters algorithm: ['phone', 'weirdest', 'auto', 'correct']\n",
      "Tokenized sentence: ['erutupalam', 'thandiyachu']\n",
      "After stop words removal: ['erutupalam', 'thandiyachu']\n",
      "After stemming with porters algorithm: ['erutupalam', 'thandiyachu']\n",
      "Tokenized sentence: ['win', 'we', 'have', 'a', 'winner', 'mr', 't', 'foley', 'won', 'an', 'ipod', 'more', 'exciting', 'prizes', 'soon', 'so', 'keep', 'an', 'eye', 'on', 'ur', 'mobile', 'or', 'visit', 'www', 'win', 'co', 'uk']\n",
      "After stop words removal: ['win', 'winner', 'mr', 'foley', 'ipod', 'exciting', 'prizes', 'soon', 'keep', 'eye', 'ur', 'mobile', 'visit', 'www', 'win', 'co', 'uk']\n",
      "excit\n",
      "After stemming with porters algorithm: ['win', 'winner', 'folei', 'ipod', 'excit', 'priz', 'soon', 'keep', 'ey', 'mobil', 'visit', 'www', 'win']\n",
      "Tokenized sentence: ['for', 'the', 'most', 'sparkling', 'shopping', 'breaks', 'from', 'per', 'person', 'call', 'or', 'visit', 'www', 'shortbreaks', 'org', 'uk']\n",
      "After stop words removal: ['sparkling', 'shopping', 'breaks', 'per', 'person', 'call', 'visit', 'www', 'shortbreaks', 'org', 'uk']\n",
      "sparkl\n",
      "shopp\n",
      "After stemming with porters algorithm: ['sparkl', 'shop', 'break', 'per', 'person', 'call', 'visit', 'www', 'shortbreak', 'org']\n",
      "Tokenized sentence: ['r', 'u', 'over', 'scratching', 'it']\n",
      "After stop words removal: ['r', 'u', 'scratching']\n",
      "scratch\n",
      "After stemming with porters algorithm: ['scratc']\n",
      "Tokenized sentence: ['have', 'you', 'had', 'a', 'good', 'day', 'mine', 'was', 'really', 'busy', 'are', 'you', 'up', 'to', 'much', 'tomorrow', 'night']\n",
      "After stop words removal: ['good', 'day', 'mine', 'really', 'busy', 'much', 'tomorrow', 'night']\n",
      "After stemming with porters algorithm: ['good', 'dai', 'mine', 'realli', 'busi', 'much', 'tomorrow', 'night']\n",
      "Tokenized sentence: ['okie', 'thanx']\n",
      "After stop words removal: ['okie', 'thanx']\n",
      "After stemming with porters algorithm: ['oki', 'thanx']\n",
      "Tokenized sentence: ['ok', 'thanx', 'gd', 'nite', 'too']\n",
      "After stop words removal: ['ok', 'thanx', 'gd', 'nite']\n",
      "After stemming with porters algorithm: ['thanx', 'nite']\n",
      "Tokenized sentence: ['yar', 'lor', 'actually', 'we', 'quite', 'fast', 'cos', 'da', 'ge', 'slow', 'wat', 'haha']\n",
      "After stop words removal: ['yar', 'lor', 'actually', 'quite', 'fast', 'cos', 'da', 'ge', 'slow', 'wat', 'haha']\n",
      "After stemming with porters algorithm: ['yar', 'lor', 'actual', 'quit', 'fast', 'co', 'slow', 'wat', 'haha']\n",
      "Tokenized sentence: ['never', 'blame', 'a', 'day', 'in', 'ur', 'life', 'good', 'days', 'give', 'u', 'happiness', 'bad', 'days', 'give', 'u', 'experience', 'both', 'are', 'essential', 'in', 'life', 'all', 'are', 'gods', 'blessings', 'good', 'morning']\n",
      "After stop words removal: ['never', 'blame', 'day', 'ur', 'life', 'good', 'days', 'give', 'u', 'happiness', 'bad', 'days', 'give', 'u', 'experience', 'essential', 'life', 'gods', 'blessings', 'good', 'morning']\n",
      "bless\n",
      "morn\n",
      "After stemming with porters algorithm: ['never', 'blame', 'dai', 'life', 'good', 'dai', 'give', 'happi', 'bad', 'dai', 'give', 'experi', 'essenti', 'life', 'god', 'bless', 'good', 'mor']\n",
      "Tokenized sentence: ['nothing', 'i', 'meant', 'that', 'once', 'the', 'money', 'enters', 'your', 'account', 'here', 'the', 'bank', 'will', 'remove', 'its', 'flat', 'rate', 'someone', 'transfered', 'lt', 'gt', 'to', 'my', 'account', 'and', 'lt', 'gt', 'dollars', 'got', 'removed', 'so', 'the', 'banks', 'differ', 'and', 'charges', 'also', 'differ', 'be', 'sure', 'you', 'trust', 'the', 'ja', 'person', 'you', 'are', 'sending', 'account', 'details', 'to', 'cos']\n",
      "After stop words removal: ['nothing', 'meant', 'money', 'enters', 'account', 'bank', 'remove', 'flat', 'rate', 'someone', 'transfered', 'lt', 'gt', 'account', 'lt', 'gt', 'dollars', 'got', 'removed', 'banks', 'differ', 'charges', 'also', 'differ', 'sure', 'trust', 'ja', 'person', 'sending', 'account', 'details', 'cos']\n",
      "noth\n",
      "send\n",
      "After stemming with porters algorithm: ['not', 'meant', 'monei', 'enter', 'account', 'bank', 'remov', 'flat', 'rate', 'someon', 'transfer', 'account', 'dollar', 'got', 'remov', 'bank', 'differ', 'charg', 'also', 'differ', 'sure', 'trust', 'person', 'sen', 'account', 'detail', 'co']\n",
      "Tokenized sentence: ['married', 'local', 'women', 'looking', 'for', 'discreet', 'action', 'now', 'real', 'matches', 'instantly', 'to', 'your', 'phone', 'text', 'match', 'to', 'msg', 'cost', 'p', 'stop', 'txt', 'stop', 'bcmsfwc', 'n', 'xx']\n",
      "After stop words removal: ['married', 'local', 'women', 'looking', 'discreet', 'action', 'real', 'matches', 'instantly', 'phone', 'text', 'match', 'msg', 'cost', 'p', 'stop', 'txt', 'stop', 'bcmsfwc', 'n', 'xx']\n",
      "look\n",
      "After stemming with porters algorithm: ['marri', 'local', 'women', 'look', 'discreet', 'act', 'real', 'match', 'instantli', 'phone', 'text', 'match', 'msg', 'cost', 'stop', 'txt', 'stop', 'bcmsfwc']\n",
      "Tokenized sentence: ['yeah', 'my', 'usual', 'guy', 's', 'out', 'of', 'town', 'but', 'there', 're', 'definitely', 'people', 'around', 'i', 'know']\n",
      "After stop words removal: ['yeah', 'usual', 'guy', 'town', 'definitely', 'people', 'around', 'know']\n",
      "After stemming with porters algorithm: ['yeah', 'usual', 'gui', 'town', 'definit', 'peopl', 'around', 'know']\n",
      "Tokenized sentence: ['cheers', 'for', 'callin', 'babe', 'sozi', 'culdnt', 'talkbut', 'i', 'wannatell', 'u', 'details', 'later', 'wenwecan', 'chat', 'properly', 'x']\n",
      "After stop words removal: ['cheers', 'callin', 'babe', 'sozi', 'culdnt', 'talkbut', 'wannatell', 'u', 'details', 'later', 'wenwecan', 'chat', 'properly', 'x']\n",
      "After stemming with porters algorithm: ['cheer', 'callin', 'babe', 'sozi', 'culdnt', 'talkbut', 'wannatel', 'detail', 'later', 'wenwecan', 'chat', 'properli']\n",
      "Tokenized sentence: ['you', 'have', 'registered', 'sinco', 'as', 'payee', 'log', 'in', 'at', 'icicibank', 'com', 'and', 'enter', 'urn', 'lt', 'gt', 'to', 'confirm', 'beware', 'of', 'frauds', 'do', 'not', 'share', 'or', 'disclose', 'urn', 'to', 'anyone']\n",
      "After stop words removal: ['registered', 'sinco', 'payee', 'log', 'icicibank', 'com', 'enter', 'urn', 'lt', 'gt', 'confirm', 'beware', 'frauds', 'share', 'disclose', 'urn', 'anyone']\n",
      "After stemming with porters algorithm: ['regist', 'sinco', 'paye', 'log', 'icicibank', 'com', 'enter', 'urn', 'confirm', 'bewar', 'fraud', 'share', 'disclos', 'urn', 'anyon']\n",
      "Tokenized sentence: ['he', 'is', 'there', 'you', 'call', 'and', 'meet', 'him']\n",
      "After stop words removal: ['call', 'meet']\n",
      "After stemming with porters algorithm: ['call', 'meet']\n",
      "Tokenized sentence: ['i', 'wish', 'that', 'i', 'was', 'with', 'you', 'holding', 'you', 'tightly', 'making', 'you', 'see', 'how', 'important', 'you', 'are', 'how', 'much', 'you', 'mean', 'to', 'me', 'how', 'much', 'i', 'need', 'you', 'in', 'my', 'life']\n",
      "After stop words removal: ['wish', 'holding', 'tightly', 'making', 'see', 'important', 'much', 'mean', 'much', 'need', 'life']\n",
      "hold\n",
      "mak\n",
      "After stemming with porters algorithm: ['wish', 'hol', 'tightli', 'make', 'see', 'import', 'much', 'mean', 'much', 'need', 'life']\n",
      "Tokenized sentence: ['where', 'are', 'you', 'what', 'do', 'you', 'do', 'how', 'can', 'you', 'stand', 'to', 'be', 'away', 'from', 'me', 'doesn', 't', 'your', 'heart', 'ache', 'without', 'me', 'don', 't', 'you', 'wonder', 'of', 'me', 'don', 't', 'you', 'crave', 'me']\n",
      "After stop words removal: ['stand', 'away', 'heart', 'ache', 'without', 'wonder', 'crave']\n",
      "After stemming with porters algorithm: ['stand', 'awai', 'heart', 'ach', 'without', 'wonder', 'crave']\n",
      "Tokenized sentence: ['win', 'a', 'year', 'supply', 'of', 'cds', 'a', 'store', 'of', 'ur', 'choice', 'worth', 'enter', 'our', 'weekly', 'draw', 'txt', 'music', 'to', 'ts', 'cs', 'www', 'ldew', 'com', 'subs', 'win', 'ppmx']\n",
      "After stop words removal: ['win', 'year', 'supply', 'cds', 'store', 'ur', 'choice', 'worth', 'enter', 'weekly', 'draw', 'txt', 'music', 'ts', 'cs', 'www', 'ldew', 'com', 'subs', 'win', 'ppmx']\n",
      "After stemming with porters algorithm: ['win', 'year', 'suppli', 'cd', 'store', 'choic', 'worth', 'enter', 'weekli', 'draw', 'txt', 'music', 'www', 'ldew', 'com', 'sub', 'win', 'ppmx']\n",
      "Tokenized sentence: ['welcome', 'to', 'select', 'an', 'o', 'service', 'with', 'added', 'benefits', 'you', 'can', 'now', 'call', 'our', 'specially', 'trained', 'advisors', 'free', 'from', 'your', 'mobile', 'by', 'dialling']\n",
      "After stop words removal: ['welcome', 'select', 'service', 'added', 'benefits', 'call', 'specially', 'trained', 'advisors', 'free', 'mobile', 'dialling']\n",
      "diall\n",
      "After stemming with porters algorithm: ['welcom', 'select', 'servic', 'ad', 'benefit', 'call', 'special', 'train', 'advisor', 'free', 'mobil', 'diall']\n",
      "Tokenized sentence: ['how', 'did', 'you', 'find', 'out', 'in', 'a', 'way', 'that', 'didn', 't', 'include', 'all', 'of', 'these', 'details']\n",
      "After stop words removal: ['find', 'way', 'include', 'details']\n",
      "After stemming with porters algorithm: ['find', 'wai', 'includ', 'detail']\n",
      "Tokenized sentence: ['yes', 'ammae', 'life', 'takes', 'lot', 'of', 'turns', 'you', 'can', 'only', 'sit', 'and', 'try', 'to', 'hold', 'the', 'steering']\n",
      "After stop words removal: ['yes', 'ammae', 'life', 'takes', 'lot', 'turns', 'sit', 'try', 'hold', 'steering']\n",
      "steer\n",
      "After stemming with porters algorithm: ['ye', 'amma', 'life', 'take', 'lot', 'turn', 'sit', 'try', 'hold', 'steer']\n",
      "Tokenized sentence: ['do', 'u', 'think', 'that', 'any', 'girl', 'will', 'propose', 'u', 'today', 'by', 'seing', 'ur', 'bloody', 'funky', 'shit', 'fucking', 'face', 'asssssholeeee']\n",
      "After stop words removal: ['u', 'think', 'girl', 'propose', 'u', 'today', 'seing', 'ur', 'bloody', 'funky', 'shit', 'fucking', 'face', 'asssssholeeee']\n",
      "se\n",
      "fuck\n",
      "After stemming with porters algorithm: ['think', 'girl', 'propos', 'todai', 'se', 'bloodi', 'funki', 'shit', 'fuc', 'face', 'asssssholeee']\n",
      "Tokenized sentence: ['s', 's', 'nervous', 'lt', 'gt']\n",
      "After stop words removal: ['nervous', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['nervou']\n",
      "Tokenized sentence: ['st', 'andre', 'virgil', 's', 'cream']\n",
      "After stop words removal: ['st', 'andre', 'virgil', 'cream']\n",
      "After stemming with porters algorithm: ['andr', 'virgil', 'cream']\n",
      "Tokenized sentence: ['not', 'course', 'only', 'maths', 'one', 'day', 'one', 'chapter', 'with', 'in', 'one', 'month', 'we', 'can', 'finish']\n",
      "After stop words removal: ['course', 'maths', 'one', 'day', 'one', 'chapter', 'one', 'month', 'finish']\n",
      "After stemming with porters algorithm: ['cours', 'math', 'on', 'dai', 'on', 'chapter', 'on', 'month', 'finish']\n",
      "Tokenized sentence: ['when', 'are', 'you', 'going', 'to', 'ride', 'your', 'bike']\n",
      "After stop words removal: ['going', 'ride', 'bike']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'ride', 'bike']\n",
      "Tokenized sentence: ['great', 'how', 'is', 'the', 'office', 'today']\n",
      "After stop words removal: ['great', 'office', 'today']\n",
      "After stemming with porters algorithm: ['great', 'offic', 'todai']\n",
      "Tokenized sentence: ['jos', 'ask', 'if', 'u', 'wana', 'meet', 'up']\n",
      "After stop words removal: ['jos', 'ask', 'u', 'wana', 'meet']\n",
      "After stemming with porters algorithm: ['jo', 'ask', 'wana', 'meet']\n",
      "Tokenized sentence: ['it', 'could', 'work', 'we', 'll', 'reach', 'a', 'consensus', 'at', 'the', 'next', 'meeting']\n",
      "After stop words removal: ['could', 'work', 'reach', 'consensus', 'next', 'meeting']\n",
      "meet\n",
      "After stemming with porters algorithm: ['could', 'work', 'reach', 'consensu', 'next', 'meet']\n",
      "Tokenized sentence: ['i', 'am', 'late', 'so', 'call', 'you', 'tomorrow', 'morning', 'take', 'care', 'sweet', 'dreams', 'u', 'and', 'me', 'ummifying', 'bye']\n",
      "After stop words removal: ['late', 'call', 'tomorrow', 'morning', 'take', 'care', 'sweet', 'dreams', 'u', 'ummifying', 'bye']\n",
      "morn\n",
      "ummify\n",
      "After stemming with porters algorithm: ['late', 'call', 'tomorrow', 'mor', 'take', 'care', 'sweet', 'dream', 'ummif', 'bye']\n",
      "Tokenized sentence: ['watching', 'cartoon', 'listening', 'music', 'amp', 'at', 'eve', 'had', 'to', 'go', 'temple', 'amp', 'church', 'what', 'about', 'u']\n",
      "After stop words removal: ['watching', 'cartoon', 'listening', 'music', 'amp', 'eve', 'go', 'temple', 'amp', 'church', 'u']\n",
      "watch\n",
      "listen\n",
      "After stemming with porters algorithm: ['watc', 'cartoon', 'listen', 'music', 'amp', 'ev', 'templ', 'amp', 'church']\n",
      "Tokenized sentence: ['anyway', 'i', 'm', 'going', 'shopping', 'on', 'my', 'own', 'now', 'cos', 'my', 'sis', 'not', 'done', 'yet', 'dun', 'disturb', 'u', 'liao']\n",
      "After stop words removal: ['anyway', 'going', 'shopping', 'cos', 'sis', 'done', 'yet', 'dun', 'disturb', 'u', 'liao']\n",
      "go\n",
      "shopp\n",
      "After stemming with porters algorithm: ['anywai', 'go', 'shop', 'co', 'si', 'done', 'yet', 'dun', 'disturb', 'liao']\n",
      "Tokenized sentence: ['had', 'your', 'mobile', 'mths', 'update', 'to', 'latest', 'orange', 'camera', 'video', 'phones', 'for', 'free', 'save', 's', 'with', 'free', 'texts', 'weekend', 'calls', 'text', 'yes', 'for', 'a', 'callback', 'orno', 'to', 'opt', 'out']\n",
      "After stop words removal: ['mobile', 'mths', 'update', 'latest', 'orange', 'camera', 'video', 'phones', 'free', 'save', 'free', 'texts', 'weekend', 'calls', 'text', 'yes', 'callback', 'orno', 'opt']\n",
      "After stemming with porters algorithm: ['mobil', 'mth', 'updat', 'latest', 'orang', 'camera', 'video', 'phone', 'free', 'save', 'free', 'text', 'weekend', 'call', 'text', 'ye', 'callback', 'orno', 'opt']\n",
      "Tokenized sentence: ['what', 'happen', 'dear', 'why', 'you', 'silent', 'i', 'am', 'tensed']\n",
      "After stop words removal: ['happen', 'dear', 'silent', 'tensed']\n",
      "After stemming with porters algorithm: ['happen', 'dear', 'silent', 'tens']\n",
      "Tokenized sentence: ['nt', 'joking', 'seriously', 'i', 'told']\n",
      "After stop words removal: ['nt', 'joking', 'seriously', 'told']\n",
      "jok\n",
      "After stemming with porters algorithm: ['joke', 'serious', 'told']\n",
      "Tokenized sentence: ['u', 'wake', 'up', 'already', 'wat', 'u', 'doing', 'u', 'picking', 'us', 'up', 'later', 'rite', 'i', 'm', 'taking', 'sq', 'reaching', 'ard', 'smth', 'like', 'dat', 'u', 'can', 'check', 'e', 'arrival', 'time', 'c', 'ya', 'soon']\n",
      "After stop words removal: ['u', 'wake', 'already', 'wat', 'u', 'u', 'picking', 'us', 'later', 'rite', 'taking', 'sq', 'reaching', 'ard', 'smth', 'like', 'dat', 'u', 'check', 'e', 'arrival', 'time', 'c', 'ya', 'soon']\n",
      "pick\n",
      "tak\n",
      "reach\n",
      "After stemming with porters algorithm: ['wake', 'alreadi', 'wat', 'pic', 'later', 'rite', 'take', 'reac', 'ard', 'smth', 'like', 'dat', 'check', 'arriv', 'time', 'soon']\n",
      "Tokenized sentence: ['dude', 'im', 'no', 'longer', 'a', 'pisces', 'im', 'an', 'aquarius', 'now']\n",
      "After stop words removal: ['dude', 'im', 'longer', 'pisces', 'im', 'aquarius']\n",
      "After stemming with porters algorithm: ['dude', 'longer', 'pisc', 'aquariu']\n",
      "Tokenized sentence: ['hi', 'mom', 'we', 'might', 'be', 'back', 'later', 'than', 'lt', 'gt']\n",
      "After stop words removal: ['hi', 'mom', 'might', 'back', 'later', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['mom', 'might', 'back', 'later']\n",
      "Tokenized sentence: ['you', 'will', 'recieve', 'your', 'tone', 'within', 'the', 'next', 'hrs', 'for', 'terms', 'and', 'conditions', 'please', 'see', 'channel', 'u', 'teletext', 'pg']\n",
      "After stop words removal: ['recieve', 'tone', 'within', 'next', 'hrs', 'terms', 'conditions', 'please', 'see', 'channel', 'u', 'teletext', 'pg']\n",
      "After stemming with porters algorithm: ['reciev', 'tone', 'within', 'next', 'hr', 'term', 'condit', 'pleas', 'see', 'channel', 'teletext']\n",
      "Tokenized sentence: ['lmao', 'you', 'know', 'me', 'so', 'well']\n",
      "After stop words removal: ['lmao', 'know', 'well']\n",
      "After stemming with porters algorithm: ['lmao', 'know', 'well']\n",
      "Tokenized sentence: ['your', 'next', 'amazing', 'xxx', 'picsfree', 'video', 'will', 'be', 'sent', 'to', 'you', 'enjoy', 'if', 'one', 'vid', 'is', 'not', 'enough', 'for', 'day', 'text', 'back', 'the', 'keyword', 'picsfree', 'to', 'get', 'the', 'next', 'video']\n",
      "After stop words removal: ['next', 'amazing', 'xxx', 'picsfree', 'video', 'sent', 'enjoy', 'one', 'vid', 'enough', 'day', 'text', 'back', 'keyword', 'picsfree', 'get', 'next', 'video']\n",
      "amaz\n",
      "After stemming with porters algorithm: ['next', 'amaz', 'xxx', 'picsfre', 'video', 'sent', 'enjoi', 'on', 'vid', 'enough', 'dai', 'text', 'back', 'keyword', 'picsfre', 'get', 'next', 'video']\n",
      "Tokenized sentence: ['hi', 'dis', 'is', 'yijue', 'i', 'would', 'be', 'happy', 'to', 'work', 'wif', 'all', 'for', 'gek']\n",
      "After stop words removal: ['hi', 'dis', 'yijue', 'would', 'happy', 'work', 'wif', 'gek']\n",
      "After stemming with porters algorithm: ['di', 'yiju', 'would', 'happi', 'work', 'wif', 'gek']\n",
      "Tokenized sentence: ['ree', 'entry', 'in', 'a', 'weekly', 'comp', 'for', 'a', 'chance', 'to', 'win', 'an', 'ipod', 'txt', 'pod', 'to', 'to', 'get', 'entry', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'for', 'details']\n",
      "After stop words removal: ['ree', 'entry', 'weekly', 'comp', 'chance', 'win', 'ipod', 'txt', 'pod', 'get', 'entry', 'std', 'txt', 'rate', 'c', 'apply', 'details']\n",
      "After stemming with porters algorithm: ['ree', 'entri', 'weekli', 'comp', 'chanc', 'win', 'ipod', 'txt', 'pod', 'get', 'entri', 'std', 'txt', 'rate', 'appli', 'detail']\n",
      "Tokenized sentence: ['you', 'could', 'have', 'seen', 'me', 'i', 'did', 't', 'recognise', 'you', 'face']\n",
      "After stop words removal: ['could', 'seen', 'recognise', 'face']\n",
      "After stemming with porters algorithm: ['could', 'seen', 'recognis', 'face']\n",
      "Tokenized sentence: ['unfortunately', 'i', 've', 'just', 'found', 'out', 'that', 'we', 'have', 'to', 'pick', 'my', 'sister', 'up', 'from', 'the', 'airport', 'that', 'evening', 'so', 'don', 't', 'think', 'i', 'll', 'be', 'going', 'out', 'at', 'all', 'we', 'should', 'try', 'to', 'go', 'out', 'one', 'of', 'th']\n",
      "After stop words removal: ['unfortunately', 'found', 'pick', 'sister', 'airport', 'evening', 'think', 'going', 'try', 'go', 'one', 'th']\n",
      "even\n",
      "go\n",
      "After stemming with porters algorithm: ['unfortun', 'found', 'pick', 'sister', 'airport', 'even', 'think', 'go', 'try', 'on']\n",
      "Tokenized sentence: ['it', 'just', 'seems', 'like', 'weird', 'timing', 'that', 'the', 'night', 'that', 'all', 'you', 'and', 'g', 'want', 'is', 'for', 'me', 'to', 'come', 'smoke', 'is', 'the', 'same', 'day', 'as', 'when', 'a', 'shitstorm', 'is', 'attributed', 'to', 'me', 'always', 'coming', 'over', 'and', 'making', 'everyone', 'smoke']\n",
      "After stop words removal: ['seems', 'like', 'weird', 'timing', 'night', 'g', 'want', 'come', 'smoke', 'day', 'shitstorm', 'attributed', 'always', 'coming', 'making', 'everyone', 'smoke']\n",
      "tim\n",
      "com\n",
      "mak\n",
      "After stemming with porters algorithm: ['seem', 'like', 'weird', 'time', 'night', 'want', 'come', 'smoke', 'dai', 'shitstorm', 'attribut', 'alwai', 'come', 'make', 'everyon', 'smoke']\n",
      "Tokenized sentence: ['im', 'realy', 'soz', 'imat', 'my', 'mums', 'nite', 'what', 'about', 'moro']\n",
      "After stop words removal: ['im', 'realy', 'soz', 'imat', 'mums', 'nite', 'moro']\n",
      "After stemming with porters algorithm: ['reali', 'soz', 'imat', 'mum', 'nite', 'moro']\n",
      "Tokenized sentence: ['tell', 'where', 'you', 'reached']\n",
      "After stop words removal: ['tell', 'reached']\n",
      "After stemming with porters algorithm: ['tell', 'reac']\n",
      "Tokenized sentence: ['doc', 'prescribed', 'me', 'morphine', 'cause', 'the', 'other', 'pain', 'meds', 'aren', 't', 'enough', 'waiting', 'for', 'my', 'mom', 'to', 'bring', 'it', 'that', 'med', 'should', 'kick', 'in', 'fast', 'so', 'i', 'm', 'gonna', 'try', 'to', 'be', 'on', 'later']\n",
      "After stop words removal: ['doc', 'prescribed', 'morphine', 'cause', 'pain', 'meds', 'enough', 'waiting', 'mom', 'bring', 'med', 'kick', 'fast', 'gonna', 'try', 'later']\n",
      "wait\n",
      "After stemming with porters algorithm: ['doc', 'prescrib', 'morphin', 'caus', 'pain', 'med', 'enough', 'wait', 'mom', 'bring', 'med', 'kick', 'fast', 'gonna', 'try', 'later']\n",
      "Tokenized sentence: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'contact', 'u', 'u', 'have', 'won', 'the', 'pound', 'prize', 'claim', 'is', 'easy', 'call', 'now', 'only', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'minute', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['time', 'tri', 'contact', 'pound', 'priz', 'claim', 'easi', 'call', 'per', 'minut', 'nat', 'rate']\n",
      "Tokenized sentence: ['xmas', 'iscoming', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'r', 'weekly', 'draw', 'txt', 'music', 'to', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'age', 'subscription']\n",
      "After stop words removal: ['xmas', 'iscoming', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'r', 'weekly', 'draw', 'txt', 'music', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'age', 'subscription']\n",
      "iscom\n",
      "After stemming with porters algorithm: ['xma', 'iscom', 'awar', 'either', 'gift', 'voucher', 'free', 'entri', 'weekli', 'draw', 'txt', 'music', 'tnc', 'www', 'ldew', 'com', 'win', 'ppmx', 'ag', 'subscript']\n",
      "Tokenized sentence: ['so', 'now', 'my', 'dad', 'is', 'gonna', 'call', 'after', 'he', 'gets', 'out', 'of', 'work', 'and', 'ask', 'all', 'these', 'crazy', 'questions']\n",
      "After stop words removal: ['dad', 'gonna', 'call', 'gets', 'work', 'ask', 'crazy', 'questions']\n",
      "After stemming with porters algorithm: ['dad', 'gonna', 'call', 'get', 'work', 'ask', 'crazi', 'quest']\n",
      "Tokenized sentence: ['hey', 'i', 'want', 'you', 'i', 'crave', 'you', 'i', 'miss', 'you', 'i', 'need', 'you', 'i', 'love', 'you', 'ahmad', 'saeed', 'al', 'hallaq']\n",
      "After stop words removal: ['hey', 'want', 'crave', 'miss', 'need', 'love', 'ahmad', 'saeed', 'al', 'hallaq']\n",
      "After stemming with porters algorithm: ['hei', 'want', 'crave', 'miss', 'need', 'love', 'ahmad', 'saeed', 'hallaq']\n",
      "Tokenized sentence: ['was', 'actually', 'sleeping', 'and', 'still', 'might', 'when', 'u', 'call', 'back', 'so', 'a', 'text', 'is', 'gr', 'you', 'rock', 'sis', 'will', 'send', 'u', 'a', 'text', 'wen', 'i', 'wake']\n",
      "After stop words removal: ['actually', 'sleeping', 'still', 'might', 'u', 'call', 'back', 'text', 'gr', 'rock', 'sis', 'send', 'u', 'text', 'wen', 'wake']\n",
      "sleep\n",
      "After stemming with porters algorithm: ['actual', 'sleep', 'still', 'might', 'call', 'back', 'text', 'rock', 'si', 'send', 'text', 'wen', 'wake']\n",
      "Tokenized sentence: ['what', 'r', 'u', 'cooking', 'me', 'for', 'dinner']\n",
      "After stop words removal: ['r', 'u', 'cooking', 'dinner']\n",
      "cook\n",
      "After stemming with porters algorithm: ['cook', 'dinner']\n",
      "Tokenized sentence: ['yo', 'you', 'at', 'jp', 'and', 'hungry', 'like', 'a', 'mofo']\n",
      "After stop words removal: ['yo', 'jp', 'hungry', 'like', 'mofo']\n",
      "After stemming with porters algorithm: ['hungri', 'like', 'mofo']\n",
      "Tokenized sentence: ['upgrdcentre', 'orange', 'customer', 'you', 'may', 'now', 'claim', 'your', 'free', 'camera', 'phone', 'upgrade', 'for', 'your', 'loyalty', 'call', 'now', 'on', 'offer', 'ends', 'th', 'july', 't', 'c', 's', 'apply', 'opt', 'out', 'available']\n",
      "After stop words removal: ['upgrdcentre', 'orange', 'customer', 'may', 'claim', 'free', 'camera', 'phone', 'upgrade', 'loyalty', 'call', 'offer', 'ends', 'th', 'july', 'c', 'apply', 'opt', 'available']\n",
      "After stemming with porters algorithm: ['upgrdcentr', 'orang', 'custom', 'mai', 'claim', 'free', 'camera', 'phone', 'upgrad', 'loyalti', 'call', 'offer', 'end', 'juli', 'appli', 'opt', 'avail']\n",
      "Tokenized sentence: ['now', 'am', 'free', 'call', 'me', 'pa']\n",
      "After stop words removal: ['free', 'call', 'pa']\n",
      "After stemming with porters algorithm: ['free', 'call']\n",
      "Tokenized sentence: ['old', 'orchard', 'near', 'univ', 'how', 'about', 'you']\n",
      "After stop words removal: ['old', 'orchard', 'near', 'univ']\n",
      "After stemming with porters algorithm: ['old', 'orchard', 'near', 'univ']\n",
      "Tokenized sentence: ['hmv', 'bonus', 'special', 'pounds', 'of', 'genuine', 'hmv', 'vouchers', 'to', 'be', 'won', 'just', 'answer', 'easy', 'questions', 'play', 'now', 'send', 'hmv', 'to', 'more', 'info', 'www', 'percent', 'real', 'com']\n",
      "After stop words removal: ['hmv', 'bonus', 'special', 'pounds', 'genuine', 'hmv', 'vouchers', 'answer', 'easy', 'questions', 'play', 'send', 'hmv', 'info', 'www', 'percent', 'real', 'com']\n",
      "After stemming with porters algorithm: ['hmv', 'bonu', 'special', 'pound', 'genuin', 'hmv', 'voucher', 'answer', 'easi', 'quest', 'plai', 'send', 'hmv', 'info', 'www', 'percent', 'real', 'com']\n",
      "Tokenized sentence: ['okie', 'thanx']\n",
      "After stop words removal: ['okie', 'thanx']\n",
      "After stemming with porters algorithm: ['oki', 'thanx']\n",
      "Tokenized sentence: ['reverse', 'is', 'cheating', 'that', 'is', 'not', 'mathematics']\n",
      "After stop words removal: ['reverse', 'cheating', 'mathematics']\n",
      "cheat\n",
      "cheate\n",
      "After stemming with porters algorithm: ['revers', 'cheat', 'mathemat']\n",
      "Tokenized sentence: ['watching', 'tv', 'now', 'i', 'got', 'new', 'job']\n",
      "After stop words removal: ['watching', 'tv', 'got', 'new', 'job']\n",
      "watch\n",
      "After stemming with porters algorithm: ['watc', 'got', 'new', 'job']\n",
      "Tokenized sentence: ['u', 'repeat', 'e', 'instructions', 'again', 'wat', 's', 'e', 'road', 'name', 'of', 'ur', 'house']\n",
      "After stop words removal: ['u', 'repeat', 'e', 'instructions', 'wat', 'e', 'road', 'name', 'ur', 'house']\n",
      "After stemming with porters algorithm: ['repeat', 'instruct', 'wat', 'road', 'name', 'hous']\n",
      "Tokenized sentence: ['as', 'a', 'registered', 'subscriber', 'yr', 'draw', 'a', 'gift', 'voucher', 'will', 'b', 'entered', 'on', 'receipt', 'of', 'a', 'correct', 'ans', 'when', 'are', 'the', 'next', 'olympics', 'txt', 'ans', 'to']\n",
      "After stop words removal: ['registered', 'subscriber', 'yr', 'draw', 'gift', 'voucher', 'b', 'entered', 'receipt', 'correct', 'ans', 'next', 'olympics', 'txt', 'ans']\n",
      "After stemming with porters algorithm: ['regist', 'subscrib', 'draw', 'gift', 'voucher', 'enter', 'receipt', 'correct', 'an', 'next', 'olympic', 'txt', 'an']\n",
      "Tokenized sentence: ['ok', 'ur', 'typical', 'reply']\n",
      "After stop words removal: ['ok', 'ur', 'typical', 'reply']\n",
      "After stemming with porters algorithm: ['typical', 'repli']\n",
      "Tokenized sentence: ['no', 'yes', 'please', 'been', 'swimming']\n",
      "After stop words removal: ['yes', 'please', 'swimming']\n",
      "swimm\n",
      "After stemming with porters algorithm: ['ye', 'pleas', 'swim']\n",
      "Tokenized sentence: ['do', 'noe', 'if', 'ben', 'is', 'going']\n",
      "After stop words removal: ['noe', 'ben', 'going']\n",
      "go\n",
      "After stemming with porters algorithm: ['noe', 'ben', 'go']\n",
      "Tokenized sentence: ['thanks', 'fills', 'me', 'with', 'complete', 'calm', 'and', 'reassurance']\n",
      "After stop words removal: ['thanks', 'fills', 'complete', 'calm', 'reassurance']\n",
      "After stemming with porters algorithm: ['thank', 'fill', 'complet', 'calm', 'reassur']\n",
      "Tokenized sentence: ['yes', 'how', 'is', 'a', 'pretty', 'lady', 'like', 'you', 'single']\n",
      "After stop words removal: ['yes', 'pretty', 'lady', 'like', 'single']\n",
      "After stemming with porters algorithm: ['ye', 'pretti', 'ladi', 'like', 'singl']\n",
      "Tokenized sentence: ['thk', 'shld', 'b', 'can', 'ya', 'i', 'wana', 'go', 'lessons', 'haha', 'can', 'go', 'for', 'one', 'whole', 'stretch']\n",
      "After stop words removal: ['thk', 'shld', 'b', 'ya', 'wana', 'go', 'lessons', 'haha', 'go', 'one', 'whole', 'stretch']\n",
      "After stemming with porters algorithm: ['thk', 'shld', 'wana', 'lesson', 'haha', 'on', 'whole', 'stretch']\n",
      "Tokenized sentence: ['sunshine', 'hols', 'to', 'claim', 'ur', 'med', 'holiday', 'send', 'a', 'stamped', 'self', 'address', 'envelope', 'to', 'drinks', 'on', 'us', 'uk', 'po', 'box', 'bray', 'wicklow', 'eire', 'quiz', 'starts', 'saturday', 'unsub', 'stop']\n",
      "After stop words removal: ['sunshine', 'hols', 'claim', 'ur', 'med', 'holiday', 'send', 'stamped', 'self', 'address', 'envelope', 'drinks', 'us', 'uk', 'po', 'box', 'bray', 'wicklow', 'eire', 'quiz', 'starts', 'saturday', 'unsub', 'stop']\n",
      "After stemming with porters algorithm: ['sunshin', 'hol', 'claim', 'med', 'holidai', 'send', 'stam', 'self', 'address', 'envelop', 'drink', 'box', 'brai', 'wicklow', 'eir', 'quiz', 'start', 'saturdai', 'unsub', 'stop']\n",
      "Tokenized sentence: ['i', 'love', 'to', 'wine', 'and', 'dine', 'my', 'lady']\n",
      "After stop words removal: ['love', 'wine', 'dine', 'lady']\n",
      "After stemming with porters algorithm: ['love', 'wine', 'dine', 'ladi']\n",
      "Tokenized sentence: ['hi', 'its', 'me', 'you', 'are', 'probably', 'having', 'too', 'much', 'fun', 'to', 'get', 'this', 'message', 'but', 'i', 'thought', 'id', 'txt', 'u', 'cos', 'im', 'bored', 'and', 'james', 'has', 'been', 'farting', 'at', 'me', 'all', 'night']\n",
      "After stop words removal: ['hi', 'probably', 'much', 'fun', 'get', 'message', 'thought', 'id', 'txt', 'u', 'cos', 'im', 'bored', 'james', 'farting', 'night']\n",
      "fart\n",
      "After stemming with porters algorithm: ['probab', 'much', 'fun', 'get', 'messag', 'thought', 'txt', 'co', 'bore', 'jame', 'far', 'night']\n",
      "Tokenized sentence: ['as', 'in', 'different', 'styles']\n",
      "After stop words removal: ['different', 'styles']\n",
      "After stemming with porters algorithm: ['differ', 'style']\n",
      "Tokenized sentence: ['hello', 'my', 'boytoy', 'geeee', 'i', 'miss', 'you', 'already', 'and', 'i', 'just', 'woke', 'up', 'i', 'wish', 'you', 'were', 'here', 'in', 'bed', 'with', 'me', 'cuddling', 'me', 'i', 'love', 'you']\n",
      "After stop words removal: ['hello', 'boytoy', 'geeee', 'miss', 'already', 'woke', 'wish', 'bed', 'cuddling', 'love']\n",
      "cuddl\n",
      "After stemming with porters algorithm: ['hello', 'boytoi', 'geeee', 'miss', 'alreadi', 'woke', 'wish', 'bed', 'cuddl', 'love']\n",
      "Tokenized sentence: ['near', 'kalainar', 'tv', 'office', 'thenampet']\n",
      "After stop words removal: ['near', 'kalainar', 'tv', 'office', 'thenampet']\n",
      "After stemming with porters algorithm: ['near', 'kalainar', 'offic', 'thenampet']\n",
      "Tokenized sentence: ['so', 'is', 'there', 'anything', 'specific', 'i', 'should', 'be', 'doing', 'with', 'regards', 'to', 'jaklin', 'or', 'what', 'because', 'idk', 'what', 'the', 'fuck']\n",
      "After stop words removal: ['anything', 'specific', 'regards', 'jaklin', 'idk', 'fuck']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'specif', 'regard', 'jaklin', 'idk', 'fuck']\n",
      "Tokenized sentence: ['call', 'germany', 'for', 'only', 'pence', 'per', 'minute', 'call', 'from', 'a', 'fixed', 'line', 'via', 'access', 'number', 'no', 'prepayment', 'direct', 'access', 'www', 'telediscount', 'co', 'uk']\n",
      "After stop words removal: ['call', 'germany', 'pence', 'per', 'minute', 'call', 'fixed', 'line', 'via', 'access', 'number', 'prepayment', 'direct', 'access', 'www', 'telediscount', 'co', 'uk']\n",
      "After stemming with porters algorithm: ['call', 'germani', 'penc', 'per', 'minut', 'call', 'fix', 'line', 'via', 'access', 'number', 'prepay', 'direct', 'access', 'www', 'telediscount']\n",
      "Tokenized sentence: ['no', 'i', 'decided', 'that', 'only', 'people', 'who', 'care', 'about', 'stuff', 'vote', 'and', 'caring', 'about', 'stuff', 'is', 'for', 'losers']\n",
      "After stop words removal: ['decided', 'people', 'care', 'stuff', 'vote', 'caring', 'stuff', 'losers']\n",
      "car\n",
      "After stemming with porters algorithm: ['decid', 'peopl', 'care', 'stuff', 'vote', 'care', 'stuff', 'loser']\n",
      "Tokenized sentence: ['hope', 'you', 'are', 'having', 'a', 'great', 'day']\n",
      "After stop words removal: ['hope', 'great', 'day']\n",
      "After stemming with porters algorithm: ['hope', 'great', 'dai']\n",
      "Tokenized sentence: ['great', 'i', 'm', 'in', 'church', 'now', 'will', 'holla', 'when', 'i', 'get', 'out']\n",
      "After stop words removal: ['great', 'church', 'holla', 'get']\n",
      "After stemming with porters algorithm: ['great', 'church', 'holla', 'get']\n",
      "Tokenized sentence: ['oh', 'i', 'will', 'get', 'paid', 'the', 'most', 'outstanding', 'one', 'is', 'for', 'a', 'commercial', 'i', 'did', 'for', 'hasbro', 'in', 'august', 'they', 'made', 'us', 'jump', 'through', 'so', 'many', 'hoops', 'to', 'get', 'paid', 'still', 'not']\n",
      "After stop words removal: ['oh', 'get', 'paid', 'outstanding', 'one', 'commercial', 'hasbro', 'august', 'made', 'us', 'jump', 'many', 'hoops', 'get', 'paid', 'still']\n",
      "outstand\n",
      "After stemming with porters algorithm: ['get', 'paid', 'outstan', 'on', 'commerci', 'hasbro', 'august', 'made', 'jump', 'mani', 'hoop', 'get', 'paid', 'still']\n",
      "Tokenized sentence: ['i', 'wish', 'things', 'were', 'different', 'i', 'wonder', 'when', 'i', 'will', 'be', 'able', 'to', 'show', 'you', 'how', 'much', 'i', 'value', 'you', 'pls', 'continue', 'the', 'brisk', 'walks', 'no', 'drugs', 'without', 'askin', 'me', 'please', 'and', 'find', 'things', 'to', 'laugh', 'about', 'i', 'love', 'you', 'dearly']\n",
      "After stop words removal: ['wish', 'things', 'different', 'wonder', 'able', 'show', 'much', 'value', 'pls', 'continue', 'brisk', 'walks', 'drugs', 'without', 'askin', 'please', 'find', 'things', 'laugh', 'love', 'dearly']\n",
      "After stemming with porters algorithm: ['wish', 'thing', 'differ', 'wonder', 'abl', 'show', 'much', 'valu', 'pl', 'continu', 'brisk', 'walk', 'drug', 'without', 'askin', 'pleas', 'find', 'thing', 'laugh', 'love', 'dearli']\n",
      "Tokenized sentence: ['there', 'r', 'many', 'model', 'sony', 'ericson', 'also', 'der', 'lt', 'gt', 'it', 'luks', 'good', 'bt', 'i', 'forgot', 'modl', 'no']\n",
      "After stop words removal: ['r', 'many', 'model', 'sony', 'ericson', 'also', 'der', 'lt', 'gt', 'luks', 'good', 'bt', 'forgot', 'modl']\n",
      "After stemming with porters algorithm: ['mani', 'model', 'soni', 'ericson', 'also', 'der', 'luk', 'good', 'forgot', 'modl']\n",
      "Tokenized sentence: ['how', 'are', 'you', 'doing', 'hope', 'you', 've', 'settled', 'in', 'for', 'the', 'new', 'school', 'year', 'just', 'wishin', 'you', 'a', 'gr', 'day']\n",
      "After stop words removal: ['hope', 'settled', 'new', 'school', 'year', 'wishin', 'gr', 'day']\n",
      "After stemming with porters algorithm: ['hope', 'settl', 'new', 'school', 'year', 'wishin', 'dai']\n",
      "Tokenized sentence: ['hey', 'iouri', 'gave', 'me', 'your', 'number', 'i', 'm', 'wylie', 'ryan', 's', 'friend']\n",
      "After stop words removal: ['hey', 'iouri', 'gave', 'number', 'wylie', 'ryan', 'friend']\n",
      "After stemming with porters algorithm: ['hei', 'iouri', 'gave', 'number', 'wylie', 'ryan', 'friend']\n",
      "Tokenized sentence: ['i', 'wnt', 'to', 'buy', 'a', 'bmw', 'car', 'urgently', 'its', 'vry', 'urgent', 'but', 'hv', 'a', 'shortage', 'of', 'lt', 'gt', 'lacs', 'there', 'is', 'no', 'source', 'to', 'arng', 'dis', 'amt', 'lt', 'gt', 'lacs', 'thats', 'my', 'prob']\n",
      "After stop words removal: ['wnt', 'buy', 'bmw', 'car', 'urgently', 'vry', 'urgent', 'hv', 'shortage', 'lt', 'gt', 'lacs', 'source', 'arng', 'dis', 'amt', 'lt', 'gt', 'lacs', 'thats', 'prob']\n",
      "After stemming with porters algorithm: ['wnt', 'bui', 'bmw', 'car', 'urgent', 'vry', 'urgent', 'shortag', 'lac', 'sourc', 'arng', 'di', 'amt', 'lac', 'that', 'prob']\n",
      "Tokenized sentence: ['haha', 'awesome', 'omw', 'back', 'now', 'then']\n",
      "After stop words removal: ['haha', 'awesome', 'omw', 'back']\n",
      "After stemming with porters algorithm: ['haha', 'awesom', 'omw', 'back']\n",
      "Tokenized sentence: ['were', 'trying', 'to', 'find', 'a', 'chinese', 'food', 'place', 'around', 'here']\n",
      "After stop words removal: ['trying', 'find', 'chinese', 'food', 'place', 'around']\n",
      "After stemming with porters algorithm: ['trying', 'find', 'chines', 'food', 'place', 'around']\n",
      "Tokenized sentence: ['am', 'up', 'to', 'my', 'eyes', 'in', 'philosophy']\n",
      "After stop words removal: ['eyes', 'philosophy']\n",
      "After stemming with porters algorithm: ['ey', 'philosophi']\n",
      "Tokenized sentence: ['freemsg', 'fav', 'xmas', 'tones', 'reply', 'real']\n",
      "After stop words removal: ['freemsg', 'fav', 'xmas', 'tones', 'reply', 'real']\n",
      "After stemming with porters algorithm: ['freemsg', 'fav', 'xma', 'tone', 'repli', 'real']\n",
      "Tokenized sentence: ['i', 'am', 'at', 'the', 'gas', 'station', 'go', 'there']\n",
      "After stop words removal: ['gas', 'station', 'go']\n",
      "After stemming with porters algorithm: ['ga', 'stat']\n",
      "Tokenized sentence: ['as', 'a', 'valued', 'customer', 'i', 'am', 'pleased', 'to', 'advise', 'you', 'that', 'following', 'recent', 'review', 'of', 'your', 'mob', 'no', 'you', 'are', 'awarded', 'with', 'a', 'bonus', 'prize', 'call']\n",
      "After stop words removal: ['valued', 'customer', 'pleased', 'advise', 'following', 'recent', 'review', 'mob', 'awarded', 'bonus', 'prize', 'call']\n",
      "follow\n",
      "After stemming with porters algorithm: ['valu', 'custom', 'pleas', 'advis', 'follow', 'recent', 'review', 'mob', 'awar', 'bonu', 'priz', 'call']\n",
      "Tokenized sentence: ['did', 'you', 'catch', 'the', 'bus', 'are', 'you', 'frying', 'an', 'egg', 'did', 'you', 'make', 'a', 'tea', 'are', 'you', 'eating', 'your', 'mom', 's', 'left', 'over', 'dinner', 'do', 'you', 'feel', 'my', 'love']\n",
      "After stop words removal: ['catch', 'bus', 'frying', 'egg', 'make', 'tea', 'eating', 'mom', 'left', 'dinner', 'feel', 'love']\n",
      "eat\n",
      "eate\n",
      "After stemming with porters algorithm: ['catch', 'bu', 'frying', 'egg', 'make', 'tea', 'eat', 'mom', 'left', 'dinner', 'feel', 'love']\n",
      "Tokenized sentence: ['am', 'only', 'searching', 'for', 'good', 'dual', 'sim', 'mobile', 'pa']\n",
      "After stop words removal: ['searching', 'good', 'dual', 'sim', 'mobile', 'pa']\n",
      "search\n",
      "After stemming with porters algorithm: ['searc', 'good', 'dual', 'sim', 'mobil']\n",
      "Tokenized sentence: ['fighting', 'with', 'the', 'world', 'is', 'easy', 'u', 'either', 'win', 'or', 'lose', 'bt', 'fightng', 'with', 'some', 'who', 'is', 'close', 'to', 'u', 'is', 'dificult', 'if', 'u', 'lose', 'u', 'lose', 'if', 'u', 'win', 'u', 'still', 'lose']\n",
      "After stop words removal: ['fighting', 'world', 'easy', 'u', 'either', 'win', 'lose', 'bt', 'fightng', 'close', 'u', 'dificult', 'u', 'lose', 'u', 'lose', 'u', 'win', 'u', 'still', 'lose']\n",
      "fight\n",
      "After stemming with porters algorithm: ['figh', 'world', 'easi', 'either', 'win', 'lose', 'fightng', 'close', 'dificult', 'lose', 'lose', 'win', 'still', 'lose']\n",
      "Tokenized sentence: ['please', 'tell', 'me', 'not', 'all', 'of', 'my', 'car', 'keys', 'are', 'in', 'your', 'purse']\n",
      "After stop words removal: ['please', 'tell', 'car', 'keys', 'purse']\n",
      "After stemming with porters algorithm: ['pleas', 'tell', 'car', 'kei', 'purs']\n",
      "Tokenized sentence: ['which', 'is', 'weird', 'because', 'i', 'know', 'i', 'had', 'it', 'at', 'one', 'point']\n",
      "After stop words removal: ['weird', 'know', 'one', 'point']\n",
      "After stemming with porters algorithm: ['weird', 'know', 'on', 'point']\n",
      "Tokenized sentence: ['house', 'maid', 'is', 'the', 'murderer', 'coz', 'the', 'man', 'was', 'murdered', 'on', 'lt', 'gt', 'th', 'january', 'as', 'public', 'holiday', 'all', 'govt', 'instituitions', 'are', 'closed', 'including', 'post', 'office']\n",
      "After stop words removal: ['house', 'maid', 'murderer', 'coz', 'man', 'murdered', 'lt', 'gt', 'th', 'january', 'public', 'holiday', 'govt', 'instituitions', 'closed', 'including', 'post', 'office']\n",
      "includ\n",
      "After stemming with porters algorithm: ['hous', 'maid', 'murder', 'coz', 'man', 'murder', 'januari', 'public', 'holidai', 'govt', 'instituit', 'close', 'includ', 'post', 'offic']\n",
      "Tokenized sentence: ['we', 'are', 'at', 'grandmas', 'oh', 'dear', 'u', 'still', 'ill', 'i', 'felt', 'shit', 'this', 'morning', 'but', 'i', 'think', 'i', 'am', 'just', 'hungover', 'another', 'night', 'then', 'we', 'leave', 'on', 'sat']\n",
      "After stop words removal: ['grandmas', 'oh', 'dear', 'u', 'still', 'ill', 'felt', 'shit', 'morning', 'think', 'hungover', 'another', 'night', 'leave', 'sat']\n",
      "morn\n",
      "After stemming with porters algorithm: ['grandma', 'dear', 'still', 'ill', 'felt', 'shit', 'mor', 'think', 'hungov', 'anoth', 'night', 'leav', 'sat']\n",
      "Tokenized sentence: ['i', 'take', 'it', 'the', 'post', 'has', 'come', 'then', 'you', 'must', 'have', 's', 'of', 'texts', 'now', 'happy', 'reading', 'my', 'one', 'from', 'wiv', 'hello', 'caroline', 'at', 'the', 'end', 'is', 'my', 'favourite', 'bless', 'him']\n",
      "After stop words removal: ['take', 'post', 'come', 'must', 'texts', 'happy', 'reading', 'one', 'wiv', 'hello', 'caroline', 'end', 'favourite', 'bless']\n",
      "read\n",
      "After stemming with porters algorithm: ['take', 'post', 'come', 'must', 'text', 'happi', 'read', 'on', 'wiv', 'hello', 'carolin', 'end', 'favourit', 'bless']\n",
      "Tokenized sentence: ['ok', 'but', 'they', 'said', 'i', 've', 'got', 'wisdom', 'teeth', 'hidden', 'inside', 'n', 'mayb', 'need', 'remove']\n",
      "After stop words removal: ['ok', 'said', 'got', 'wisdom', 'teeth', 'hidden', 'inside', 'n', 'mayb', 'need', 'remove']\n",
      "After stemming with porters algorithm: ['said', 'got', 'wisdom', 'teeth', 'hidden', 'insid', 'mayb', 'need', 'remov']\n",
      "Tokenized sentence: ['wamma', 'get', 'laid', 'want', 'real', 'doggin', 'locations', 'sent', 'direct', 'to', 'your', 'mobile', 'join', 'the', 'uks', 'largest', 'dogging', 'network', 'txt', 'dogs', 'to', 'now', 'nyt', 'ec', 'a', 'lp', 'msg']\n",
      "After stop words removal: ['wamma', 'get', 'laid', 'want', 'real', 'doggin', 'locations', 'sent', 'direct', 'mobile', 'join', 'uks', 'largest', 'dogging', 'network', 'txt', 'dogs', 'nyt', 'ec', 'lp', 'msg']\n",
      "dogg\n",
      "After stemming with porters algorithm: ['wamma', 'get', 'laid', 'want', 'real', 'doggin', 'locat', 'sent', 'direct', 'mobil', 'join', 'uk', 'largest', 'dog', 'network', 'txt', 'dog', 'nyt', 'msg']\n",
      "Tokenized sentence: ['u', 'calling', 'me', 'right', 'call', 'my', 'hand', 'phone']\n",
      "After stop words removal: ['u', 'calling', 'right', 'call', 'hand', 'phone']\n",
      "call\n",
      "After stemming with porters algorithm: ['call', 'right', 'call', 'hand', 'phone']\n",
      "Tokenized sentence: ['awesome', 'text', 'me', 'when', 'you', 're', 'restocked']\n",
      "After stop words removal: ['awesome', 'text', 'restocked']\n",
      "After stemming with porters algorithm: ['awesom', 'text', 'restoc']\n",
      "Tokenized sentence: ['got', 'what', 'it', 'takes', 'take', 'part', 'in', 'the', 'wrc', 'rally', 'in', 'oz', 'u', 'can', 'with', 'lucozade', 'energy', 'text', 'rally', 'le', 'to', 'p', 'see', 'packs', 'or', 'lucozade', 'co', 'uk', 'wrc', 'itcould', 'be', 'u']\n",
      "After stop words removal: ['got', 'takes', 'take', 'part', 'wrc', 'rally', 'oz', 'u', 'lucozade', 'energy', 'text', 'rally', 'le', 'p', 'see', 'packs', 'lucozade', 'co', 'uk', 'wrc', 'itcould', 'u']\n",
      "After stemming with porters algorithm: ['got', 'take', 'take', 'part', 'wrc', 'ralli', 'lucozad', 'energi', 'text', 'ralli', 'see', 'pack', 'lucozad', 'wrc', 'itcould']\n",
      "Tokenized sentence: ['hasn', 't', 'that', 'been', 'the', 'pattern', 'recently', 'crap', 'weekends']\n",
      "After stop words removal: ['pattern', 'recently', 'crap', 'weekends']\n",
      "After stemming with porters algorithm: ['pattern', 'recent', 'crap', 'weekend']\n",
      "Tokenized sentence: ['he', 'remains', 'a', 'bro', 'amongst', 'bros']\n",
      "After stop words removal: ['remains', 'bro', 'amongst', 'bros']\n",
      "After stemming with porters algorithm: ['remain', 'bro', 'amongst', 'bro']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'xxx', 'won', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'reach', 'you', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stop words removal: ['urgent', 'mobile', 'xxx', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'reach', 'call', 'asap', 'box', 'n', 'qp', 'ppm']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'xxx', 'bonu', 'caller', 'priz', 'attempt', 'reach', 'call', 'asap', 'box', 'ppm']\n",
      "Tokenized sentence: ['well', 'there', 's', 'a', 'pattern', 'emerging', 'of', 'my', 'friends', 'telling', 'me', 'to', 'drive', 'up', 'and', 'come', 'smoke', 'with', 'them', 'and', 'then', 'telling', 'me', 'that', 'i', 'm', 'a', 'weed', 'fiend', 'make', 'them', 'smoke', 'too', 'much', 'impede', 'their', 'doing', 'other', 'things', 'so', 'you', 'see', 'how', 'i', 'm', 'hesitant']\n",
      "After stop words removal: ['well', 'pattern', 'emerging', 'friends', 'telling', 'drive', 'come', 'smoke', 'telling', 'weed', 'fiend', 'make', 'smoke', 'much', 'impede', 'things', 'see', 'hesitant']\n",
      "emerg\n",
      "tell\n",
      "tell\n",
      "After stemming with porters algorithm: ['well', 'pattern', 'emer', 'friend', 'tell', 'drive', 'come', 'smoke', 'tell', 'weed', 'fiend', 'make', 'smoke', 'much', 'imped', 'thing', 'see', 'hesit']\n",
      "Tokenized sentence: ['i', 'can', 't', 'believe', 'how', 'attached', 'i', 'am', 'to', 'seeing', 'you', 'every', 'day', 'i', 'know', 'you', 'will', 'do', 'the', 'best', 'you', 'can', 'to', 'get', 'to', 'me', 'babe', 'i', 'will', 'go', 'to', 'teach', 'my', 'class', 'at', 'your', 'midnight']\n",
      "After stop words removal: ['believe', 'attached', 'seeing', 'every', 'day', 'know', 'best', 'get', 'babe', 'go', 'teach', 'class', 'midnight']\n",
      "see\n",
      "After stemming with porters algorithm: ['believ', 'attac', 'see', 'everi', 'dai', 'know', 'best', 'get', 'babe', 'teach', 'class', 'midnight']\n",
      "Tokenized sentence: ['do', 'thing', 'change', 'that', 'sentence', 'into', 'because', 'i', 'want', 'concentrate', 'in', 'my', 'educational', 'career', 'im', 'leaving', 'here']\n",
      "After stop words removal: ['thing', 'change', 'sentence', 'want', 'concentrate', 'educational', 'career', 'im', 'leaving']\n",
      "leav\n",
      "After stemming with porters algorithm: ['thing', 'chang', 'sentenc', 'want', 'concentr', 'educ', 'career', 'leav']\n",
      "Tokenized sentence: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'u', 'find', 'out', 'who', 'they', 'r', 'reveal', 'who', 'thinks', 'ur', 'so', 'special', 'call', 'on', 'stopsms']\n",
      "After stop words removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'u', 'find', 'r', 'reveal', 'thinks', 'ur', 'special', 'call', 'stopsms']\n",
      "look\n",
      "After stemming with porters algorithm: ['secret', 'admir', 'look', 'make', 'contact', 'find', 'reveal', 'think', 'special', 'call', 'stopsm']\n",
      "Tokenized sentence: ['its', 'a', 'great', 'day', 'do', 'have', 'yourself', 'a', 'beautiful', 'one']\n",
      "After stop words removal: ['great', 'day', 'beautiful', 'one']\n",
      "After stemming with porters algorithm: ['great', 'dai', 'beauti', 'on']\n",
      "Tokenized sentence: ['new', 'textbuddy', 'chat', 'horny', 'guys', 'in', 'ur', 'area', 'just', 'p', 'free', 'receive', 'search', 'postcode', 'or', 'at', 'gaytextbuddy', 'com', 'txt', 'one', 'name', 'to']\n",
      "After stop words removal: ['new', 'textbuddy', 'chat', 'horny', 'guys', 'ur', 'area', 'p', 'free', 'receive', 'search', 'postcode', 'gaytextbuddy', 'com', 'txt', 'one', 'name']\n",
      "After stemming with porters algorithm: ['new', 'textbuddi', 'chat', 'horni', 'gui', 'area', 'free', 'receiv', 'search', 'postcod', 'gaytextbuddi', 'com', 'txt', 'on', 'name']\n",
      "Tokenized sentence: ['let', 'me', 'know', 'when', 'you', 've', 'got', 'the', 'money', 'so', 'carlos', 'can', 'make', 'the', 'call']\n",
      "After stop words removal: ['let', 'know', 'got', 'money', 'carlos', 'make', 'call']\n",
      "After stemming with porters algorithm: ['let', 'know', 'got', 'monei', 'carlo', 'make', 'call']\n",
      "Tokenized sentence: ['allo', 'we', 'have', 'braved', 'the', 'buses', 'and', 'taken', 'on', 'the', 'trains', 'and', 'triumphed', 'i', 'mean', 'we', 're', 'in', 'b', 'ham', 'have', 'a', 'jolly', 'good', 'rest', 'of', 'week']\n",
      "After stop words removal: ['allo', 'braved', 'buses', 'taken', 'trains', 'triumphed', 'mean', 'b', 'ham', 'jolly', 'good', 'rest', 'week']\n",
      "After stemming with porters algorithm: ['allo', 'brave', 'buse', 'taken', 'train', 'triump', 'mean', 'ham', 'jolli', 'good', 'rest', 'week']\n",
      "Tokenized sentence: ['did', 'u', 'see', 'what', 'i', 'posted', 'on', 'your', 'facebook']\n",
      "After stop words removal: ['u', 'see', 'posted', 'facebook']\n",
      "After stemming with porters algorithm: ['see', 'pos', 'facebook']\n",
      "Tokenized sentence: ['whatsup', 'there', 'dont', 'u', 'want', 'to', 'sleep']\n",
      "After stop words removal: ['whatsup', 'dont', 'u', 'want', 'sleep']\n",
      "After stemming with porters algorithm: ['whatsup', 'dont', 'want', 'sleep']\n",
      "Tokenized sentence: ['urgent', 'your', 'mobile', 'no', 'was', 'awarded', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'our', 'nd', 'attempt', 'to', 'contact', 'you', 'call', 'box', 'qu', 'bt', 'national', 'rate']\n",
      "After stop words removal: ['urgent', 'mobile', 'awarded', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'contact', 'call', 'box', 'qu', 'bt', 'national', 'rate']\n",
      "After stemming with porters algorithm: ['urgent', 'mobil', 'awar', 'bonu', 'caller', 'priz', 'attempt', 'contact', 'call', 'box', 'nat', 'rate']\n",
      "Tokenized sentence: ['i', 'had', 'it', 'already', 'sabarish', 'asked', 'me', 'to', 'go']\n",
      "After stop words removal: ['already', 'sabarish', 'asked', 'go']\n",
      "After stemming with porters algorithm: ['alreadi', 'sabarish', 'as']\n",
      "Tokenized sentence: ['your', 'bill', 'at', 'is', 'so', 'thats', 'not', 'bad']\n",
      "After stop words removal: ['bill', 'thats', 'bad']\n",
      "After stemming with porters algorithm: ['bill', 'that', 'bad']\n",
      "Tokenized sentence: ['wishing', 'you', 'and', 'your', 'family', 'merry', 'x', 'mas', 'and', 'happy', 'new', 'year', 'in', 'advance']\n",
      "After stop words removal: ['wishing', 'family', 'merry', 'x', 'mas', 'happy', 'new', 'year', 'advance']\n",
      "wish\n",
      "After stemming with porters algorithm: ['wis', 'famili', 'merri', 'ma', 'happi', 'new', 'year', 'advanc']\n",
      "Tokenized sentence: ['just', 'making', 'dinner', 'you']\n",
      "After stop words removal: ['making', 'dinner']\n",
      "mak\n",
      "After stemming with porters algorithm: ['make', 'dinner']\n",
      "Tokenized sentence: ['buzz', 'hey', 'my', 'love', 'i', 'think', 'of', 'you', 'and', 'hope', 'your', 'day', 'goes', 'well', 'did', 'you', 'sleep', 'in', 'i', 'miss', 'you', 'babe', 'i', 'long', 'for', 'the', 'moment', 'we', 'are', 'together', 'again', 'loving', 'smile']\n",
      "After stop words removal: ['buzz', 'hey', 'love', 'think', 'hope', 'day', 'goes', 'well', 'sleep', 'miss', 'babe', 'long', 'moment', 'together', 'loving', 'smile']\n",
      "lov\n",
      "After stemming with porters algorithm: ['buzz', 'hei', 'love', 'think', 'hope', 'dai', 'goe', 'well', 'sleep', 'miss', 'babe', 'long', 'moment', 'togeth', 'love', 'smile']\n",
      "Tokenized sentence: ['ugh', 'its', 'been', 'a', 'long', 'day', 'i', 'm', 'exhausted', 'just', 'want', 'to', 'cuddle', 'up', 'and', 'take', 'a', 'nap']\n",
      "After stop words removal: ['ugh', 'long', 'day', 'exhausted', 'want', 'cuddle', 'take', 'nap']\n",
      "After stemming with porters algorithm: ['ugh', 'long', 'dai', 'exhaus', 'want', 'cuddl', 'take', 'nap']\n",
      "Tokenized sentence: ['i', 'really', 'need', 'kiss', 'u', 'i', 'miss', 'u', 'my', 'baby', 'from', 'ur', 'baby', 'eva']\n",
      "After stop words removal: ['really', 'need', 'kiss', 'u', 'miss', 'u', 'baby', 'ur', 'baby', 'eva']\n",
      "After stemming with porters algorithm: ['realli', 'need', 'kiss', 'miss', 'babi', 'babi', 'eva']\n",
      "Tokenized sentence: ['that', 'seems', 'unnecessarily', 'affectionate']\n",
      "After stop words removal: ['seems', 'unnecessarily', 'affectionate']\n",
      "After stemming with porters algorithm: ['seem', 'unnecessarili', 'affect']\n",
      "Tokenized sentence: ['what', 'time', 'should', 'i', 'tell', 'my', 'friend', 'to', 'be', 'around']\n",
      "After stop words removal: ['time', 'tell', 'friend', 'around']\n",
      "After stemming with porters algorithm: ['time', 'tell', 'friend', 'around']\n",
      "Tokenized sentence: ['alrite', 'jod', 'hows', 'the', 'revision', 'goin', 'keris', 'bin', 'doin', 'a', 'smidgin', 'n', 'e', 'way', 'u', 'wanna', 'cum', 'over', 'after', 'college', 'xx']\n",
      "After stop words removal: ['alrite', 'jod', 'hows', 'revision', 'goin', 'keris', 'bin', 'doin', 'smidgin', 'n', 'e', 'way', 'u', 'wanna', 'cum', 'college', 'xx']\n",
      "After stemming with porters algorithm: ['alrit', 'jod', 'how', 'revis', 'goin', 'keri', 'bin', 'doin', 'smidgin', 'wai', 'wanna', 'cum', 'colleg']\n",
      "Tokenized sentence: ['i', 'dnt', 'wnt', 'to', 'tlk', 'wid', 'u']\n",
      "After stop words removal: ['dnt', 'wnt', 'tlk', 'wid', 'u']\n",
      "After stemming with porters algorithm: ['dnt', 'wnt', 'tlk', 'wid']\n",
      "Tokenized sentence: ['nah', 'i', 'm', 'a', 'perpetual', 'dd']\n",
      "After stop words removal: ['nah', 'perpetual', 'dd']\n",
      "After stemming with porters algorithm: ['nah', 'perpetu']\n",
      "Tokenized sentence: ['come', 'to', 'medical', 'college', 'at', 'pm', 'forward', 'it', 'da']\n",
      "After stop words removal: ['come', 'medical', 'college', 'pm', 'forward', 'da']\n",
      "After stemming with porters algorithm: ['come', 'medic', 'colleg', 'forward']\n",
      "Tokenized sentence: ['congrats', 'mobile', 'g', 'videophones', 'r', 'yours', 'call', 'now', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl', 'bx', 'ip', 'we', 'p']\n",
      "After stop words removal: ['congrats', 'mobile', 'g', 'videophones', 'r', 'call', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl', 'bx', 'ip', 'p']\n",
      "After stemming with porters algorithm: ['congrat', 'mobil', 'videophon', 'call', 'videochat', 'wid', 'mate', 'plai', 'java', 'game', 'dload', 'polyph', 'music', 'nolin', 'rentl']\n",
      "Tokenized sentence: ['havent', 'stuck', 'at', 'orchard', 'in', 'my', 'dad', 's', 'car', 'going', 'dinner', 'now', 'u', 'leh', 'so', 'r', 'they', 'free', 'tonight']\n",
      "After stop words removal: ['havent', 'stuck', 'orchard', 'dad', 'car', 'going', 'dinner', 'u', 'leh', 'r', 'free', 'tonight']\n",
      "go\n",
      "After stemming with porters algorithm: ['havent', 'stuck', 'orchard', 'dad', 'car', 'go', 'dinner', 'leh', 'free', 'tonight']\n",
      "Tokenized sentence: ['looks', 'like', 'you', 'found', 'something', 'to', 'do', 'other', 'than', 'smoke', 'great', 'job']\n",
      "After stop words removal: ['looks', 'like', 'found', 'something', 'smoke', 'great', 'job']\n",
      "someth\n",
      "After stemming with porters algorithm: ['look', 'like', 'found', 'somet', 'smoke', 'great', 'job']\n",
      "Tokenized sentence: ['x', 'course', 'it', 'yrs', 'just', 'so', 'her', 'messages', 'on', 'messenger', 'lik', 'you', 'r', 'sending', 'me']\n",
      "After stop words removal: ['x', 'course', 'yrs', 'messages', 'messenger', 'lik', 'r', 'sending']\n",
      "send\n",
      "After stemming with porters algorithm: ['cours', 'yr', 'messag', 'messeng', 'lik', 'sen']\n",
      "Tokenized sentence: ['vikky', 'come', 'around', 'lt', 'time', 'gt']\n",
      "After stop words removal: ['vikky', 'come', 'around', 'lt', 'time', 'gt']\n",
      "After stemming with porters algorithm: ['vikki', 'come', 'around', 'time']\n",
      "Tokenized sentence: ['goal', 'arsenal', 'henry', 'v', 'liverpool', 'henry', 'scores', 'with', 'a', 'simple', 'shot', 'from', 'yards', 'from', 'a', 'pass', 'by', 'bergkamp', 'to', 'give', 'arsenal', 'a', 'goal', 'margin', 'after', 'mins']\n",
      "After stop words removal: ['goal', 'arsenal', 'henry', 'v', 'liverpool', 'henry', 'scores', 'simple', 'shot', 'yards', 'pass', 'bergkamp', 'give', 'arsenal', 'goal', 'margin', 'mins']\n",
      "After stemming with porters algorithm: ['goal', 'arsen', 'henri', 'liverpool', 'henri', 'score', 'simpl', 'shot', 'yard', 'pass', 'bergkamp', 'give', 'arsen', 'goal', 'margin', 'min']\n",
      "Tokenized sentence: ['then', 'come', 'n', 'pick', 'me', 'at', 'ar']\n",
      "After stop words removal: ['come', 'n', 'pick', 'ar']\n",
      "After stemming with porters algorithm: ['come', 'pick']\n",
      "Tokenized sentence: ['if', 'you', 'don', 't', 'your', 'prize', 'will', 'go', 'to', 'another', 'customer', 't', 'c', 'at', 'www', 't', 'c', 'biz', 'p', 'min', 'polo', 'ltd', 'suite', 'london', 'w', 'j', 'hl', 'please', 'call', 'back', 'if', 'busy']\n",
      "After stop words removal: ['prize', 'go', 'another', 'customer', 'c', 'www', 'c', 'biz', 'p', 'min', 'polo', 'ltd', 'suite', 'london', 'w', 'j', 'hl', 'please', 'call', 'back', 'busy']\n",
      "After stemming with porters algorithm: ['priz', 'anoth', 'custom', 'www', 'biz', 'min', 'polo', 'ltd', 'suit', 'london', 'pleas', 'call', 'back', 'busi']\n",
      "Tokenized sentence: ['enjoy', 'the', 'showers', 'of', 'possessiveness', 'poured', 'on', 'u', 'by', 'ur', 'loved', 'ones', 'bcoz', 'in', 'this', 'world', 'of', 'lies', 'it', 'is', 'a', 'golden', 'gift', 'to', 'be', 'loved', 'truly']\n",
      "After stop words removal: ['enjoy', 'showers', 'possessiveness', 'poured', 'u', 'ur', 'loved', 'ones', 'bcoz', 'world', 'lies', 'golden', 'gift', 'loved', 'truly']\n",
      "After stemming with porters algorithm: ['enjoi', 'shower', 'possess', 'pour', 'love', 'on', 'bcoz', 'world', 'li', 'golden', 'gift', 'love', 'truli']\n",
      "Tokenized sentence: ['sorry', 'was', 'in', 'the', 'bathroom', 'sup']\n",
      "After stop words removal: ['sorry', 'bathroom', 'sup']\n",
      "After stemming with porters algorithm: ['sorri', 'bathroom', 'sup']\n",
      "Tokenized sentence: ['thanks', 'for', 'your', 'message', 'i', 'really', 'appreciate', 'your', 'sacrifice', 'i', 'm', 'not', 'sure', 'of', 'the', 'process', 'of', 'direct', 'pay', 'but', 'will', 'find', 'out', 'on', 'my', 'way', 'back', 'from', 'the', 'test', 'tomorrow', 'i', 'm', 'in', 'class', 'now', 'do', 'have', 'a', 'wonderful', 'day']\n",
      "After stop words removal: ['thanks', 'message', 'really', 'appreciate', 'sacrifice', 'sure', 'process', 'direct', 'pay', 'find', 'way', 'back', 'test', 'tomorrow', 'class', 'wonderful', 'day']\n",
      "After stemming with porters algorithm: ['thank', 'messag', 'realli', 'appreci', 'sacrific', 'sure', 'process', 'direct', 'pai', 'find', 'wai', 'back', 'test', 'tomorrow', 'class', 'wonder', 'dai']\n",
      "Tokenized sentence: ['i', 'm', 'in', 'a', 'meeting', 'call', 'me', 'later', 'at']\n",
      "After stop words removal: ['meeting', 'call', 'later']\n",
      "meet\n",
      "After stemming with porters algorithm: ['meet', 'call', 'later']\n",
      "Tokenized sentence: ['oh', 'k', 'i', 'will', 'come', 'tomorrow']\n",
      "After stop words removal: ['oh', 'k', 'come', 'tomorrow']\n",
      "After stemming with porters algorithm: ['come', 'tomorrow']\n",
      "Tokenized sentence: ['ok', 'lor', 'reaching', 'then', 'message', 'me']\n",
      "After stop words removal: ['ok', 'lor', 'reaching', 'message']\n",
      "reach\n",
      "After stemming with porters algorithm: ['lor', 'reac', 'messag']\n",
      "Tokenized sentence: ['let', 'there', 'be', 'snow', 'let', 'there', 'be', 'snow', 'this', 'kind', 'of', 'weather', 'brings', 'ppl', 'together', 'so', 'friendships', 'can', 'grow']\n",
      "After stop words removal: ['let', 'snow', 'let', 'snow', 'kind', 'weather', 'brings', 'ppl', 'together', 'friendships', 'grow']\n",
      "After stemming with porters algorithm: ['let', 'snow', 'let', 'snow', 'kind', 'weather', 'bring', 'ppl', 'togeth', 'friendship', 'grow']\n",
      "Tokenized sentence: ['it', 'is', 'a', 'good', 'thing', 'i', 'm', 'now', 'getting', 'the', 'connection', 'to', 'bw']\n",
      "After stop words removal: ['good', 'thing', 'getting', 'connection', 'bw']\n",
      "gett\n",
      "After stemming with porters algorithm: ['good', 'thing', 'get', 'connect']\n",
      "Tokenized sentence: ['wot', 'u', 'wanna', 'do', 'then', 'missy']\n",
      "After stop words removal: ['wot', 'u', 'wanna', 'missy']\n",
      "After stemming with porters algorithm: ['wot', 'wanna', 'missi']\n",
      "Tokenized sentence: ['oh', 'haha', 'den', 'we', 'shld', 'had', 'went', 'today', 'too', 'gee', 'nvm', 'la', 'kaiez', 'i', 'dun', 'mind', 'goin', 'jazz', 'oso', 'scared', 'hiphop', 'open', 'cant', 'catch', 'up']\n",
      "After stop words removal: ['oh', 'haha', 'den', 'shld', 'went', 'today', 'gee', 'nvm', 'la', 'kaiez', 'dun', 'mind', 'goin', 'jazz', 'oso', 'scared', 'hiphop', 'open', 'cant', 'catch']\n",
      "After stemming with porters algorithm: ['haha', 'den', 'shld', 'went', 'todai', 'gee', 'nvm', 'kaiez', 'dun', 'mind', 'goin', 'jazz', 'oso', 'scare', 'hiphop', 'open', 'cant', 'catch']\n",
      "Tokenized sentence: ['man', 'this', 'bus', 'is', 'so', 'so', 'so', 'slow', 'i', 'think', 'you', 're', 'gonna', 'get', 'there', 'before', 'me']\n",
      "After stop words removal: ['man', 'bus', 'slow', 'think', 'gonna', 'get']\n",
      "After stemming with porters algorithm: ['man', 'bu', 'slow', 'think', 'gonna', 'get']\n",
      "Tokenized sentence: ['ok', 'pa', 'nothing', 'problem']\n",
      "After stop words removal: ['ok', 'pa', 'nothing', 'problem']\n",
      "noth\n",
      "After stemming with porters algorithm: ['not', 'problem']\n",
      "Tokenized sentence: ['not', 'from', 'this', 'campus', 'are', 'you', 'in', 'the', 'library']\n",
      "After stop words removal: ['campus', 'library']\n",
      "After stemming with porters algorithm: ['campu', 'librari']\n",
      "Tokenized sentence: ['he', 'also', 'knows', 'about', 'lunch', 'menu', 'only', 'da', 'i', 'know']\n",
      "After stop words removal: ['also', 'knows', 'lunch', 'menu', 'da', 'know']\n",
      "After stemming with porters algorithm: ['also', 'know', 'lunch', 'menu', 'know']\n",
      "Tokenized sentence: ['ur', 'cash', 'balance', 'is', 'currently', 'pounds', 'to', 'maximize', 'ur', 'cash', 'in', 'now', 'send', 'cash', 'to', 'only', 'p', 'msg', 'cc', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stop words removal: ['ur', 'cash', 'balance', 'currently', 'pounds', 'maximize', 'ur', 'cash', 'send', 'cash', 'p', 'msg', 'cc', 'hg', 'suite', 'lands', 'row', 'w', 'j', 'hl']\n",
      "After stemming with porters algorithm: ['cash', 'balanc', 'current', 'pound', 'maxim', 'cash', 'send', 'cash', 'msg', 'suit', 'land', 'row']\n",
      "Tokenized sentence: ['are', 'you', 'this', 'much', 'buzy']\n",
      "After stop words removal: ['much', 'buzy']\n",
      "After stemming with porters algorithm: ['much', 'buzi']\n",
      "Tokenized sentence: ['urgent', 'important', 'information', 'for', 'o', 'user', 'today', 'is', 'your', 'lucky', 'day', 'find', 'out', 'why', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'there', 'is', 'a', 'fantastic', 'surprise', 'awaiting', 'for', 'you']\n",
      "After stop words removal: ['urgent', 'important', 'information', 'user', 'today', 'lucky', 'day', 'find', 'log', 'onto', 'http', 'www', 'urawinner', 'com', 'fantastic', 'surprise', 'awaiting']\n",
      "await\n",
      "After stemming with porters algorithm: ['urgent', 'import', 'inform', 'user', 'todai', 'lucki', 'dai', 'find', 'log', 'onto', 'http', 'www', 'urawinn', 'com', 'fantast', 'surpris', 'await']\n",
      "Tokenized sentence: ['frnd', 's', 'not', 'juz', 'a', 'word', 'not', 'merely', 'a', 'relationship', 'its', 'a', 'silent', 'promise', 'which', 'says', 'i', 'will', 'be', 'with', 'you', 'wherevr', 'whenevr', 'forevr', 'gudnyt', 'dear']\n",
      "After stop words removal: ['frnd', 'juz', 'word', 'merely', 'relationship', 'silent', 'promise', 'says', 'wherevr', 'whenevr', 'forevr', 'gudnyt', 'dear']\n",
      "After stemming with porters algorithm: ['frnd', 'juz', 'word', 'mere', 'relationship', 'silent', 'promis', 'sai', 'wherevr', 'whenevr', 'forevr', 'gudnyt', 'dear']\n",
      "Tokenized sentence: ['i', 'can', 'probably', 'come', 'by', 'everybody', 's', 'done', 'around', 'lt', 'gt', 'right']\n",
      "After stop words removal: ['probably', 'come', 'everybody', 'done', 'around', 'lt', 'gt', 'right']\n",
      "After stemming with porters algorithm: ['probab', 'come', 'everybodi', 'done', 'around', 'right']\n",
      "Tokenized sentence: ['hi', 'test', 'on', 'lt', 'gt', 'rd']\n",
      "After stop words removal: ['hi', 'test', 'lt', 'gt', 'rd']\n",
      "After stemming with porters algorithm: ['test']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['ceri', 'u', 'rebel', 'sweet', 'dreamz', 'me', 'little', 'buddy', 'c', 'ya', 'moro', 'who', 'needs', 'blokes']\n",
      "After stop words removal: ['ceri', 'u', 'rebel', 'sweet', 'dreamz', 'little', 'buddy', 'c', 'ya', 'moro', 'needs', 'blokes']\n",
      "After stemming with porters algorithm: ['ceri', 'rebel', 'sweet', 'dreamz', 'littl', 'buddi', 'moro', 'need', 'bloke']\n",
      "Tokenized sentence: ['no', 'nokia', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'nok', 'to', 'st', 'tone', 'free', 'so', 'get', 'txtin', 'now', 'and', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stop words removal: ['nokia', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'nok', 'st', 'tone', 'free', 'get', 'txtin', 'tell', 'ur', 'friends', 'p', 'tone', 'reply', 'hl', 'info']\n",
      "After stemming with porters algorithm: ['nokia', 'tone', 'mob', 'everi', 'week', 'txt', 'nok', 'tone', 'free', 'get', 'txtin', 'tell', 'friend', 'tone', 'repli', 'info']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n",
      "Tokenized sentence: ['free', 'unlimited', 'hardcore', 'porn', 'direct', 'your', 'mobile', 'txt', 'porn', 'to', 'get', 'free', 'access', 'for', 'hrs', 'then', 'chrgd', 'p', 'per', 'day', 'txt', 'stop', 'exit', 'this', 'msg', 'is', 'free']\n",
      "After stop words removal: ['free', 'unlimited', 'hardcore', 'porn', 'direct', 'mobile', 'txt', 'porn', 'get', 'free', 'access', 'hrs', 'chrgd', 'p', 'per', 'day', 'txt', 'stop', 'exit', 'msg', 'free']\n",
      "After stemming with porters algorithm: ['free', 'unlimit', 'hardcor', 'porn', 'direct', 'mobil', 'txt', 'porn', 'get', 'free', 'access', 'hr', 'chrgd', 'per', 'dai', 'txt', 'stop', 'exit', 'msg', 'free']\n",
      "Tokenized sentence: ['hi', 'juan', 'im', 'coming', 'home', 'on', 'fri', 'hey', 'of', 'course', 'i', 'expect', 'a', 'welcome', 'party', 'and', 'lots', 'of', 'presents', 'ill', 'phone', 'u', 'when', 'i', 'get', 'back', 'loads', 'of', 'love', 'nicky', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x']\n",
      "After stop words removal: ['hi', 'juan', 'im', 'coming', 'home', 'fri', 'hey', 'course', 'expect', 'welcome', 'party', 'lots', 'presents', 'ill', 'phone', 'u', 'get', 'back', 'loads', 'love', 'nicky', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x']\n",
      "com\n",
      "After stemming with porters algorithm: ['juan', 'come', 'home', 'fri', 'hei', 'cours', 'expect', 'welcom', 'parti', 'lot', 'present', 'ill', 'phone', 'get', 'back', 'load', 'love', 'nicki']\n",
      "Tokenized sentence: ['lol', 'or', 'i', 'could', 'just', 'starve', 'and', 'lose', 'a', 'pound', 'by', 'the', 'end', 'of', 'the', 'day']\n",
      "After stop words removal: ['lol', 'could', 'starve', 'lose', 'pound', 'end', 'day']\n",
      "After stemming with porters algorithm: ['lol', 'could', 'starv', 'lose', 'pound', 'end', 'dai']\n",
      "Tokenized sentence: ['can', 'you', 'plz', 'tell', 'me', 'the', 'ans', 'bslvyl', 'sent', 'via', 'fullonsms', 'com']\n",
      "After stop words removal: ['plz', 'tell', 'ans', 'bslvyl', 'sent', 'via', 'fullonsms', 'com']\n",
      "After stemming with porters algorithm: ['plz', 'tell', 'an', 'bslvyl', 'sent', 'via', 'fullonsm', 'com']\n",
      "Tokenized sentence: ['reply', 'to', 'win', 'weekly', 'where', 'will', 'the', 'fifa', 'world', 'cup', 'be', 'held', 'send', 'stop', 'to', 'to', 'end', 'service']\n",
      "After stop words removal: ['reply', 'win', 'weekly', 'fifa', 'world', 'cup', 'held', 'send', 'stop', 'end', 'service']\n",
      "After stemming with porters algorithm: ['repli', 'win', 'weekli', 'fifa', 'world', 'cup', 'held', 'send', 'stop', 'end', 'servic']\n",
      "Tokenized sentence: ['a', 'famous', 'quote', 'when', 'you', 'develop', 'the', 'ability', 'to', 'listen', 'to', 'anything', 'unconditionally', 'without', 'losing', 'your', 'temper', 'or', 'self', 'confidence', 'it', 'means', 'you', 'are', 'married']\n",
      "After stop words removal: ['famous', 'quote', 'develop', 'ability', 'listen', 'anything', 'unconditionally', 'without', 'losing', 'temper', 'self', 'confidence', 'means', 'married']\n",
      "anyth\n",
      "los\n",
      "After stemming with porters algorithm: ['famou', 'quot', 'develop', 'abil', 'listen', 'anyt', 'uncondit', 'without', 'lose', 'temper', 'self', 'confid', 'mean', 'marri']\n",
      "Tokenized sentence: ['wishing', 'you', 'and', 'your', 'family', 'merry', 'x', 'mas', 'and', 'happy', 'new', 'year', 'in', 'advance']\n",
      "After stop words removal: ['wishing', 'family', 'merry', 'x', 'mas', 'happy', 'new', 'year', 'advance']\n",
      "wish\n",
      "After stemming with porters algorithm: ['wis', 'famili', 'merri', 'ma', 'happi', 'new', 'year', 'advanc']\n",
      "Tokenized sentence: ['i', 'm', 'cool', 'ta', 'luv', 'but', 'v', 'tired', 'cause', 'i', 'have', 'been', 'doin', 'loads', 'of', 'planning', 'all', 'wk', 'we', 'have', 'got', 'our', 'social', 'services', 'inspection', 'at', 'the', 'nursery', 'take', 'care', 'spk', 'sn', 'x']\n",
      "After stop words removal: ['cool', 'ta', 'luv', 'v', 'tired', 'cause', 'doin', 'loads', 'planning', 'wk', 'got', 'social', 'services', 'inspection', 'nursery', 'take', 'care', 'spk', 'sn', 'x']\n",
      "plann\n",
      "After stemming with porters algorithm: ['cool', 'luv', 'tire', 'caus', 'doin', 'load', 'plan', 'got', 'social', 'servic', 'inspect', 'nurseri', 'take', 'care', 'spk']\n",
      "Tokenized sentence: ['loans', 'for', 'any', 'purpose', 'even', 'if', 'you', 'have', 'bad', 'credit', 'tenants', 'welcome', 'call', 'noworriesloans', 'com', 'on']\n",
      "After stop words removal: ['loans', 'purpose', 'even', 'bad', 'credit', 'tenants', 'welcome', 'call', 'noworriesloans', 'com']\n",
      "After stemming with porters algorithm: ['loan', 'purpos', 'even', 'bad', 'credit', 'tenant', 'welcom', 'call', 'noworriesloan', 'com']\n",
      "Tokenized sentence: ['u', 'horrible', 'gal', 'u', 'knew', 'dat', 'i', 'was', 'going', 'out', 'wif', 'him', 'yest', 'n', 'u', 'still', 'come', 'n', 'ask', 'me']\n",
      "After stop words removal: ['u', 'horrible', 'gal', 'u', 'knew', 'dat', 'going', 'wif', 'yest', 'n', 'u', 'still', 'come', 'n', 'ask']\n",
      "go\n",
      "After stemming with porters algorithm: ['horrib', 'gal', 'knew', 'dat', 'go', 'wif', 'yest', 'still', 'come', 'ask']\n",
      "Tokenized sentence: ['every', 'monday', 'nxt', 'week', 'vl', 'be', 'completing']\n",
      "After stop words removal: ['every', 'monday', 'nxt', 'week', 'vl', 'completing']\n",
      "complet\n",
      "After stemming with porters algorithm: ['everi', 'mondai', 'nxt', 'week', 'complet']\n",
      "Tokenized sentence: ['i', 'anything', 'lor']\n",
      "After stop words removal: ['anything', 'lor']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['anyt', 'lor']\n",
      "Tokenized sentence: ['thinking', 'of', 'u', 'x']\n",
      "After stop words removal: ['thinking', 'u', 'x']\n",
      "think\n",
      "After stemming with porters algorithm: ['thin']\n",
      "Tokenized sentence: ['i', 'll', 'text', 'now', 'all', 'creepy', 'like', 'so', 'he', 'won', 't', 'think', 'that', 'we', 'forgot']\n",
      "After stop words removal: ['text', 'creepy', 'like', 'think', 'forgot']\n",
      "After stemming with porters algorithm: ['text', 'creepi', 'like', 'think', 'forgot']\n",
      "Tokenized sentence: ['same', 'here', 'but', 'i', 'consider', 'walls', 'and', 'bunkers', 'and', 'shit', 'important', 'just', 'because', 'i', 'never', 'play', 'on', 'peaceful', 'but', 'i', 'guess', 'your', 'place', 'is', 'high', 'enough', 'that', 'it', 'don', 't', 'matter']\n",
      "After stop words removal: ['consider', 'walls', 'bunkers', 'shit', 'important', 'never', 'play', 'peaceful', 'guess', 'place', 'high', 'enough', 'matter']\n",
      "After stemming with porters algorithm: ['consid', 'wall', 'bunker', 'shit', 'import', 'never', 'plai', 'peac', 'guess', 'place', 'high', 'enough', 'matter']\n",
      "Tokenized sentence: ['come', 'to', 'mahal', 'bus', 'stop', 'lt', 'decimal', 'gt']\n",
      "After stop words removal: ['come', 'mahal', 'bus', 'stop', 'lt', 'decimal', 'gt']\n",
      "After stemming with porters algorithm: ['come', 'mahal', 'bu', 'stop', 'decim']\n",
      "Tokenized sentence: ['but', 'your', 'not', 'here']\n",
      "After stop words removal: []\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['claire', 'here', 'am', 'havin', 'borin', 'time', 'am', 'now', 'alone', 'u', 'wanna', 'cum', 'over', 'nite', 'chat', 'now', 'hope', 'c', 'u', 'luv', 'claire', 'xx', 'calls', 'minmoremobsemspobox', 'po', 'wa']\n",
      "After stop words removal: ['claire', 'havin', 'borin', 'time', 'alone', 'u', 'wanna', 'cum', 'nite', 'chat', 'hope', 'c', 'u', 'luv', 'claire', 'xx', 'calls', 'minmoremobsemspobox', 'po', 'wa']\n",
      "After stemming with porters algorithm: ['clair', 'havin', 'borin', 'time', 'alon', 'wanna', 'cum', 'nite', 'chat', 'hope', 'luv', 'clair', 'call', 'minmoremobsemspobox']\n",
      "Tokenized sentence: ['are', 'you', 'sure', 'you', 'don', 't', 'mean', 'get', 'here']\n",
      "After stop words removal: ['sure', 'mean', 'get']\n",
      "After stemming with porters algorithm: ['sure', 'mean', 'get']\n",
      "Tokenized sentence: ['i', 'm', 'job', 'profile', 'seems', 'like', 'bpo']\n",
      "After stop words removal: ['job', 'profile', 'seems', 'like', 'bpo']\n",
      "After stemming with porters algorithm: ['job', 'profil', 'seem', 'like', 'bpo']\n",
      "Tokenized sentence: ['how', 'much', 'did', 'ur', 'hdd', 'casing', 'cost']\n",
      "After stop words removal: ['much', 'ur', 'hdd', 'casing', 'cost']\n",
      "cas\n",
      "After stemming with porters algorithm: ['much', 'hdd', 'case', 'cost']\n",
      "Tokenized sentence: ['i', 've', 'reached', 'sch', 'already']\n",
      "After stop words removal: ['reached', 'sch', 'already']\n",
      "After stemming with porters algorithm: ['reac', 'sch', 'alreadi']\n",
      "Tokenized sentence: ['aight', 'yo', 'dats', 'straight', 'dogg']\n",
      "After stop words removal: ['aight', 'yo', 'dats', 'straight', 'dogg']\n",
      "After stemming with porters algorithm: ['aight', 'dat', 'straight', 'dogg']\n",
      "Tokenized sentence: ['hey', 'do', 'you', 'want', 'anything', 'to', 'buy']\n",
      "After stop words removal: ['hey', 'want', 'anything', 'buy']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['hei', 'want', 'anyt', 'bui']\n",
      "Tokenized sentence: ['yo', 'sorry', 'was', 'in', 'the', 'shower', 'sup']\n",
      "After stop words removal: ['yo', 'sorry', 'shower', 'sup']\n",
      "After stemming with porters algorithm: ['sorri', 'shower', 'sup']\n",
      "Tokenized sentence: ['i', 'll', 'get', 'there', 'at', 'unless', 'you', 'guys', 'want', 'me', 'to', 'come', 'some', 'time', 'sooner']\n",
      "After stop words removal: ['get', 'unless', 'guys', 'want', 'come', 'time', 'sooner']\n",
      "After stemming with porters algorithm: ['get', 'unless', 'gui', 'want', 'come', 'time', 'sooner']\n",
      "Tokenized sentence: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
      "After stop words removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
      "After stemming with porters algorithm: ['cant', 'pick', 'phone', 'right', 'pl', 'send', 'messag']\n",
      "Tokenized sentence: ['convey', 'my', 'regards', 'to', 'him']\n",
      "After stop words removal: ['convey', 'regards']\n",
      "After stemming with porters algorithm: ['convei', 'regard']\n",
      "Tokenized sentence: ['now', 'project', 'pa', 'after', 'that', 'only', 'i', 'can', 'come']\n",
      "After stop words removal: ['project', 'pa', 'come']\n",
      "After stemming with porters algorithm: ['project', 'come']\n",
      "Tokenized sentence: ['k', 'i', 'might', 'come', 'by', 'tonight', 'then', 'if', 'my', 'class', 'lets', 'out', 'early']\n",
      "After stop words removal: ['k', 'might', 'come', 'tonight', 'class', 'lets', 'early']\n",
      "After stemming with porters algorithm: ['might', 'come', 'tonight', 'class', 'let', 'earli']\n",
      "Tokenized sentence: ['kick', 'off', 'a', 'new', 'season', 'with', 'wks', 'free', 'goals', 'news', 'to', 'ur', 'mobile', 'txt', 'ur', 'club', 'name', 'to', 'eg', 'villa', 'to']\n",
      "After stop words removal: ['kick', 'new', 'season', 'wks', 'free', 'goals', 'news', 'ur', 'mobile', 'txt', 'ur', 'club', 'name', 'eg', 'villa']\n",
      "After stemming with porters algorithm: ['kick', 'new', 'season', 'wk', 'free', 'goal', 'new', 'mobil', 'txt', 'club', 'name', 'villa']\n",
      "Tokenized sentence: ['where', 'are', 'you', 'lover', 'i', 'need', 'you']\n",
      "After stop words removal: ['lover', 'need']\n",
      "After stemming with porters algorithm: ['lover', 'need']\n",
      "Tokenized sentence: ['that', 'depends', 'how', 'would', 'you', 'like', 'to', 'be', 'treated']\n",
      "After stop words removal: ['depends', 'would', 'like', 'treated']\n",
      "treate\n",
      "After stemming with porters algorithm: ['depend', 'would', 'like', 'treat']\n",
      "Tokenized sentence: ['your', 'chance', 'to', 'be', 'on', 'a', 'reality', 'fantasy', 'show', 'call', 'now', 'just', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'is', 'a', 'national', 'rate', 'call']\n",
      "After stop words removal: ['chance', 'reality', 'fantasy', 'show', 'call', 'p', 'per', 'min', 'ntt', 'ltd', 'po', 'box', 'croydon', 'cr', 'wb', 'national', 'rate', 'call']\n",
      "After stemming with porters algorithm: ['chanc', 'realiti', 'fantasi', 'show', 'call', 'per', 'min', 'ntt', 'ltd', 'box', 'croydon', 'nat', 'rate', 'call']\n",
      "Tokenized sentence: ['we', 'tried', 'to', 'contact', 'you', 're', 'your', 'reply', 'to', 'our', 'offer', 'of', 'a', 'video', 'handset', 'anytime', 'any', 'networks', 'mins', 'unlimited', 'text', 'camcorder', 'reply', 'or', 'call', 'now']\n",
      "After stop words removal: ['tried', 'contact', 'reply', 'offer', 'video', 'handset', 'anytime', 'networks', 'mins', 'unlimited', 'text', 'camcorder', 'reply', 'call']\n",
      "After stemming with porters algorithm: ['tri', 'contact', 'repli', 'offer', 'video', 'handset', 'anytim', 'network', 'min', 'unlimit', 'text', 'camcord', 'repli', 'call']\n",
      "Tokenized sentence: ['u', 'r', 'too', 'much', 'close', 'to', 'my', 'heart', 'if', 'u', 'go', 'away', 'i', 'will', 'be', 'shattered', 'plz', 'stay', 'with', 'me']\n",
      "After stop words removal: ['u', 'r', 'much', 'close', 'heart', 'u', 'go', 'away', 'shattered', 'plz', 'stay']\n",
      "After stemming with porters algorithm: ['much', 'close', 'heart', 'awai', 'shatter', 'plz', 'stai']\n",
      "Tokenized sentence: ['hey', 'mr', 'whats', 'the', 'name', 'of', 'that', 'bill', 'brison', 'book', 'the', 'one', 'about', 'language', 'and', 'words']\n",
      "After stop words removal: ['hey', 'mr', 'whats', 'name', 'bill', 'brison', 'book', 'one', 'language', 'words']\n",
      "After stemming with porters algorithm: ['hei', 'what', 'name', 'bill', 'brison', 'book', 'on', 'languag', 'word']\n",
      "Tokenized sentence: ['that', 'is', 'wondarfull', 'song']\n",
      "After stop words removal: ['wondarfull', 'song']\n",
      "After stemming with porters algorithm: ['wondarful', 'song']\n",
      "Tokenized sentence: ['hi', 'baby', 'ive', 'just', 'got', 'back', 'from', 'work', 'and', 'i', 'was', 'wanting', 'to', 'see', 'u', 'allday', 'i', 'hope', 'i', 'didnt', 'piss', 'u', 'off', 'on', 'the', 'phone', 'today', 'if', 'u', 'are', 'up', 'give', 'me', 'a', 'call', 'xxx']\n",
      "After stop words removal: ['hi', 'baby', 'ive', 'got', 'back', 'work', 'wanting', 'see', 'u', 'allday', 'hope', 'didnt', 'piss', 'u', 'phone', 'today', 'u', 'give', 'call', 'xxx']\n",
      "want\n",
      "After stemming with porters algorithm: ['babi', 'iv', 'got', 'back', 'work', 'wan', 'see', 'alldai', 'hope', 'didnt', 'piss', 'phone', 'todai', 'give', 'call', 'xxx']\n",
      "Tokenized sentence: ['urgent', 'you', 'have', 'won', 'a', 'week', 'free', 'membership', 'in', 'our', 'prize', 'jackpot', 'txt', 'the', 'word', 'claim', 'to', 'no', 't', 'c', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw', 'a', 'rw']\n",
      "After stop words removal: ['urgent', 'week', 'free', 'membership', 'prize', 'jackpot', 'txt', 'word', 'claim', 'c', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw', 'rw']\n",
      "After stemming with porters algorithm: ['urgent', 'week', 'free', 'membership', 'priz', 'jackpot', 'txt', 'word', 'claim', 'www', 'dbuk', 'net', 'lccltd', 'pobox', 'ldnw']\n",
      "Tokenized sentence: ['hey', 'cutie', 'how', 'goes', 'it', 'here', 'in', 'wales', 'its', 'kinda', 'ok', 'there', 'is', 'like', 'hills', 'and', 'shit', 'but', 'i', 'still', 'avent', 'killed', 'myself']\n",
      "After stop words removal: ['hey', 'cutie', 'goes', 'wales', 'kinda', 'ok', 'like', 'hills', 'shit', 'still', 'avent', 'killed']\n",
      "After stemming with porters algorithm: ['hei', 'cuti', 'goe', 'wale', 'kinda', 'like', 'hill', 'shit', 'still', 'avent', 'kill']\n",
      "Tokenized sentence: ['ok', 'no', 'wahala', 'just', 'remember', 'that', 'a', 'friend', 'in', 'need']\n",
      "After stop words removal: ['ok', 'wahala', 'remember', 'friend', 'need']\n",
      "After stemming with porters algorithm: ['wahala', 'rememb', 'friend', 'need']\n",
      "Tokenized sentence: ['and', 'how', 'you', 'will', 'do', 'that', 'princess']\n",
      "After stop words removal: ['princess']\n",
      "After stemming with porters algorithm: ['princess']\n",
      "Tokenized sentence: ['i', 'don', 't', 'know', 'jack', 'shit', 'about', 'anything', 'or', 'i', 'd', 'say', 'ask', 'something', 'helpful', 'but', 'if', 'you', 'want', 'you', 'can', 'pretend', 'that', 'i', 'did', 'and', 'just', 'text', 'me', 'whatever', 'in', 'response', 'to', 'the', 'hypotheticalhuagauahahuagahyuhagga']\n",
      "After stop words removal: ['know', 'jack', 'shit', 'anything', 'say', 'ask', 'something', 'helpful', 'want', 'pretend', 'text', 'whatever', 'response', 'hypotheticalhuagauahahuagahyuhagga']\n",
      "anyth\n",
      "someth\n",
      "After stemming with porters algorithm: ['know', 'jack', 'shit', 'anyt', 'sai', 'ask', 'somet', 'help', 'want', 'pretend', 'text', 'whatev', 'respons', 'hypotheticalhuagauahahuagahyuhagga']\n",
      "Tokenized sentence: ['oic', 'cos', 'me', 'n', 'my', 'sis', 'got', 'no', 'lunch', 'today', 'my', 'dad', 'went', 'out', 'so', 'dunno', 'whether', 'eat', 'in', 'sch', 'or', 'wat']\n",
      "After stop words removal: ['oic', 'cos', 'n', 'sis', 'got', 'lunch', 'today', 'dad', 'went', 'dunno', 'whether', 'eat', 'sch', 'wat']\n",
      "After stemming with porters algorithm: ['oic', 'co', 'si', 'got', 'lunch', 'todai', 'dad', 'went', 'dunno', 'whether', 'eat', 'sch', 'wat']\n",
      "Tokenized sentence: ['yes', 'we', 'are', 'chatting', 'too']\n",
      "After stop words removal: ['yes', 'chatting']\n",
      "chatt\n",
      "After stemming with porters algorithm: ['ye', 'chat']\n",
      "Tokenized sentence: ['as', 'i', 'entered', 'my', 'cabin', 'my', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'i', 'felt', 'special', 'she', 'askd', 'me', 'lunch', 'after', 'lunch', 'she', 'invited', 'me', 'to', 'her', 'apartment', 'we', 'went', 'there']\n",
      "After stop words removal: ['entered', 'cabin', 'pa', 'said', 'happy', 'b', 'day', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invited', 'apartment', 'went']\n",
      "After stemming with porters algorithm: ['enter', 'cabin', 'said', 'happi', 'dai', 'boss', 'felt', 'special', 'askd', 'lunch', 'lunch', 'invit', 'apart', 'went']\n",
      "Tokenized sentence: ['i', 'm', 'done', 'oredi']\n",
      "After stop words removal: ['done', 'oredi']\n",
      "After stemming with porters algorithm: ['done', 'oredi']\n",
      "Tokenized sentence: ['am', 'new', 'club', 'dont', 'fink', 'we', 'met', 'yet', 'will', 'b', 'gr', 'c', 'u', 'please', 'leave', 'msg', 'day', 'wiv', 'ur', 'area', 'reply', 'promised', 'carlie', 'x', 'calls', 'minmobsmore', 'lkpobox', 'hp', 'fl']\n",
      "After stop words removal: ['new', 'club', 'dont', 'fink', 'met', 'yet', 'b', 'gr', 'c', 'u', 'please', 'leave', 'msg', 'day', 'wiv', 'ur', 'area', 'reply', 'promised', 'carlie', 'x', 'calls', 'minmobsmore', 'lkpobox', 'hp', 'fl']\n",
      "After stemming with porters algorithm: ['new', 'club', 'dont', 'fink', 'met', 'yet', 'pleas', 'leav', 'msg', 'dai', 'wiv', 'area', 'repli', 'promis', 'carli', 'call', 'minmobsmor', 'lkpobox']\n",
      "Tokenized sentence: ['he', 'says', 'hi', 'and', 'to', 'get', 'your', 'ass', 'back', 'to', 'south', 'tampa', 'preferably', 'at', 'a', 'kegger']\n",
      "After stop words removal: ['says', 'hi', 'get', 'ass', 'back', 'south', 'tampa', 'preferably', 'kegger']\n",
      "After stemming with porters algorithm: ['sai', 'get', 'ass', 'back', 'south', 'tampa', 'prefer', 'kegger']\n",
      "Tokenized sentence: ['i', 'am', 'back', 'bit', 'long', 'cos', 'of', 'accident', 'on', 'a', 'had', 'to', 'divert', 'via', 'wadebridge', 'i', 'had', 'a', 'brilliant', 'weekend', 'thanks', 'speak', 'soon', 'lots', 'of', 'love']\n",
      "After stop words removal: ['back', 'bit', 'long', 'cos', 'accident', 'divert', 'via', 'wadebridge', 'brilliant', 'weekend', 'thanks', 'speak', 'soon', 'lots', 'love']\n",
      "After stemming with porters algorithm: ['back', 'bit', 'long', 'co', 'accid', 'divert', 'via', 'wadebridg', 'brilliant', 'weekend', 'thank', 'speak', 'soon', 'lot', 'love']\n",
      "Tokenized sentence: ['now', 'i', 'm', 'going', 'out', 'dinner', 'soon']\n",
      "After stop words removal: ['going', 'dinner', 'soon']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'dinner', 'soon']\n",
      "Tokenized sentence: ['no', 'she', 'didnt', 'i', 'will', 'search', 'online', 'and', 'let', 'you', 'know']\n",
      "After stop words removal: ['didnt', 'search', 'online', 'let', 'know']\n",
      "After stemming with porters algorithm: ['didnt', 'search', 'onlin', 'let', 'know']\n",
      "Tokenized sentence: ['your', 'b', 'u', 'voucher', 'w', 'c', 'is', 'marsms', 'log', 'onto', 'www', 'b', 'utele', 'com', 'for', 'discount', 'credit', 'to', 'opt', 'out', 'reply', 'stop', 'customer', 'care', 'call']\n",
      "After stop words removal: ['b', 'u', 'voucher', 'w', 'c', 'marsms', 'log', 'onto', 'www', 'b', 'utele', 'com', 'discount', 'credit', 'opt', 'reply', 'stop', 'customer', 'care', 'call']\n",
      "After stemming with porters algorithm: ['voucher', 'marsm', 'log', 'onto', 'www', 'utel', 'com', 'discount', 'credit', 'opt', 'repli', 'stop', 'custom', 'care', 'call']\n",
      "Tokenized sentence: ['no', 'plm', 'i', 'will', 'come', 'da', 'on', 'the', 'way']\n",
      "After stop words removal: ['plm', 'come', 'da', 'way']\n",
      "After stemming with porters algorithm: ['plm', 'come', 'wai']\n",
      "Tokenized sentence: ['the', 'affidavit', 'says', 'lt', 'gt', 'e', 'twiggs', 'st', 'division', 'g', 'courtroom', 'lt', 'gt', 'lt', 'time', 'gt', 'am', 'i', 'll', 'double', 'check', 'and', 'text', 'you', 'again', 'tomorrow']\n",
      "After stop words removal: ['affidavit', 'says', 'lt', 'gt', 'e', 'twiggs', 'st', 'division', 'g', 'courtroom', 'lt', 'gt', 'lt', 'time', 'gt', 'double', 'check', 'text', 'tomorrow']\n",
      "After stemming with porters algorithm: ['affidavit', 'sai', 'twigg', 'divis', 'courtroom', 'time', 'doubl', 'check', 'text', 'tomorrow']\n",
      "Tokenized sentence: ['no', 'its', 'true', 'k', 'do', 'u', 'knw', 'dis', 'no', 'lt', 'gt']\n",
      "After stop words removal: ['true', 'k', 'u', 'knw', 'dis', 'lt', 'gt']\n",
      "After stemming with porters algorithm: ['true', 'knw', 'di']\n",
      "Tokenized sentence: ['prabha', 'i', 'm', 'soryda', 'realy', 'frm', 'heart', 'i', 'm', 'sory']\n",
      "After stop words removal: ['prabha', 'soryda', 'realy', 'frm', 'heart', 'sory']\n",
      "After stemming with porters algorithm: ['prabha', 'soryda', 'reali', 'frm', 'heart', 'sori']\n",
      "Tokenized sentence: ['tmrw', 'im', 'finishing', 'doors']\n",
      "After stop words removal: ['tmrw', 'im', 'finishing', 'doors']\n",
      "finish\n",
      "After stemming with porters algorithm: ['tmrw', 'finis', 'door']\n",
      "Tokenized sentence: ['gumby', 's', 'has', 'a', 'special', 'where', 'a', 'lt', 'gt', 'cheese', 'pizza', 'is', 'so', 'i', 'know', 'what', 'we', 're', 'doin', 'tonight']\n",
      "After stop words removal: ['gumby', 'special', 'lt', 'gt', 'cheese', 'pizza', 'know', 'doin', 'tonight']\n",
      "After stemming with porters algorithm: ['gumbi', 'special', 'chees', 'pizza', 'know', 'doin', 'tonight']\n",
      "Tokenized sentence: ['good', 'afternoon', 'sexy', 'buns', 'how', 'goes', 'the', 'job', 'search', 'i', 'wake', 'and', 'you', 'are', 'my', 'first', 'thought', 'as', 'always', 'my', 'love', 'i', 'wish', 'your', 'fine', 'and', 'happy', 'and', 'know', 'i', 'adore', 'you']\n",
      "After stop words removal: ['good', 'afternoon', 'sexy', 'buns', 'goes', 'job', 'search', 'wake', 'first', 'thought', 'always', 'love', 'wish', 'fine', 'happy', 'know', 'adore']\n",
      "After stemming with porters algorithm: ['good', 'afternoon', 'sexi', 'bun', 'goe', 'job', 'search', 'wake', 'first', 'thought', 'alwai', 'love', 'wish', 'fine', 'happi', 'know', 'ador']\n",
      "Tokenized sentence: ['yes', 'mum', 'lookin', 'strong']\n",
      "After stop words removal: ['yes', 'mum', 'lookin', 'strong']\n",
      "After stemming with porters algorithm: ['ye', 'mum', 'lookin', 'strong']\n",
      "Tokenized sentence: ['then', 'u', 'go', 'back', 'urself', 'lor']\n",
      "After stop words removal: ['u', 'go', 'back', 'urself', 'lor']\n",
      "After stemming with porters algorithm: ['back', 'urself', 'lor']\n",
      "Tokenized sentence: ['oh', 'as', 'usual', 'vijay', 'film', 'or', 'its', 'different']\n",
      "After stop words removal: ['oh', 'usual', 'vijay', 'film', 'different']\n",
      "After stemming with porters algorithm: ['usual', 'vijai', 'film', 'differ']\n",
      "Tokenized sentence: ['oops', 'got', 'that', 'bit']\n",
      "After stop words removal: ['oops', 'got', 'bit']\n",
      "After stemming with porters algorithm: ['oop', 'got', 'bit']\n",
      "Tokenized sentence: ['eastenders', 'tv', 'quiz', 'what', 'flower', 'does', 'dot', 'compare', 'herself', 'to', 'd', 'violet', 'e', 'tulip', 'f', 'lily', 'txt', 'd', 'e', 'or', 'f', 'to', 'now', 'chance', 'win', 'cash', 'wkent', 'p']\n",
      "After stop words removal: ['eastenders', 'tv', 'quiz', 'flower', 'dot', 'compare', 'violet', 'e', 'tulip', 'f', 'lily', 'txt', 'e', 'f', 'chance', 'win', 'cash', 'wkent', 'p']\n",
      "After stemming with porters algorithm: ['eastend', 'quiz', 'flower', 'dot', 'compar', 'violet', 'tulip', 'lili', 'txt', 'chanc', 'win', 'cash', 'wkent']\n",
      "Tokenized sentence: ['i', 'd', 'say', 'that', 's', 'a', 'good', 'sign', 'but', 'well', 'you', 'know', 'my', 'track', 'record', 'at', 'reading', 'women']\n",
      "After stop words removal: ['say', 'good', 'sign', 'well', 'know', 'track', 'record', 'reading', 'women']\n",
      "read\n",
      "After stemming with porters algorithm: ['sai', 'good', 'sign', 'well', 'know', 'track', 'record', 'read', 'women']\n",
      "Tokenized sentence: ['gal', 'n', 'boy', 'walking', 'in', 'd', 'park', 'gal', 'can', 'i', 'hold', 'ur', 'hand', 'boy', 'y', 'do', 'u', 'think', 'i', 'would', 'run', 'away', 'gal', 'no', 'jst', 'wana', 'c', 'how', 'it', 'feels', 'walking', 'in', 'heaven', 'with', 'an', 'prince', 'gn']\n",
      "After stop words removal: ['gal', 'n', 'boy', 'walking', 'park', 'gal', 'hold', 'ur', 'hand', 'boy', 'u', 'think', 'would', 'run', 'away', 'gal', 'jst', 'wana', 'c', 'feels', 'walking', 'heaven', 'prince', 'gn']\n",
      "walk\n",
      "walk\n",
      "After stemming with porters algorithm: ['gal', 'boi', 'wal', 'park', 'gal', 'hold', 'hand', 'boi', 'think', 'would', 'run', 'awai', 'gal', 'jst', 'wana', 'feel', 'wal', 'heaven', 'princ']\n",
      "Tokenized sentence: ['ok', 'thanx']\n",
      "After stop words removal: ['ok', 'thanx']\n",
      "After stemming with porters algorithm: ['thanx']\n",
      "Tokenized sentence: ['i', 'am', 'not', 'sure', 'about', 'night', 'menu', 'i', 'know', 'only', 'about', 'noon', 'menu']\n",
      "After stop words removal: ['sure', 'night', 'menu', 'know', 'noon', 'menu']\n",
      "After stemming with porters algorithm: ['sure', 'night', 'menu', 'know', 'noon', 'menu']\n",
      "Tokenized sentence: ['free', 'nokia', 'or', 'motorola', 'with', 'upto', 'mths', 'price', 'linerental', 'free', 'x', 'net', 'mins', 'txt', 'mth', 'free', 'b', 'tooth', 'call', 'mobileupd', 'on', 'or', 'call', 'optout', 'd', 'wv']\n",
      "After stop words removal: ['free', 'nokia', 'motorola', 'upto', 'mths', 'price', 'linerental', 'free', 'x', 'net', 'mins', 'txt', 'mth', 'free', 'b', 'tooth', 'call', 'mobileupd', 'call', 'optout', 'wv']\n",
      "After stemming with porters algorithm: ['free', 'nokia', 'motorola', 'upto', 'mth', 'price', 'liner', 'free', 'net', 'min', 'txt', 'mth', 'free', 'tooth', 'call', 'mobileupd', 'call', 'optout']\n",
      "Tokenized sentence: ['pls', 'what', 's', 'the', 'full', 'name', 'of', 'joke', 's', 'school', 'cos', 'fees', 'in', 'university', 'of', 'florida', 'seem', 'to', 'actually', 'be', 'lt', 'gt', 'k', 'pls', 'holla', 'back']\n",
      "After stop words removal: ['pls', 'full', 'name', 'joke', 'school', 'cos', 'fees', 'university', 'florida', 'seem', 'actually', 'lt', 'gt', 'k', 'pls', 'holla', 'back']\n",
      "After stemming with porters algorithm: ['pl', 'full', 'name', 'joke', 'school', 'co', 'fee', 'univers', 'florida', 'seem', 'actual', 'pl', 'holla', 'back']\n",
      "Tokenized sentence: ['i', 'donno', 'its', 'in', 'your', 'genes', 'or', 'something']\n",
      "After stop words removal: ['donno', 'genes', 'something']\n",
      "someth\n",
      "After stemming with porters algorithm: ['donno', 'gene', 'somet']\n",
      "Tokenized sentence: ['hello', 'thanx', 'for', 'taking', 'that', 'call', 'i', 'got', 'a', 'job', 'starts', 'on', 'monday']\n",
      "After stop words removal: ['hello', 'thanx', 'taking', 'call', 'got', 'job', 'starts', 'monday']\n",
      "tak\n",
      "After stemming with porters algorithm: ['hello', 'thanx', 'take', 'call', 'got', 'job', 'start', 'mondai']\n",
      "Tokenized sentence: ['ok']\n",
      "After stop words removal: ['ok']\n",
      "After stemming with porters algorithm: []\n",
      "Tokenized sentence: ['but', 'your', 'brother', 'transfered', 'only', 'lt', 'gt', 'lt', 'gt', 'pa']\n",
      "After stop words removal: ['brother', 'transfered', 'lt', 'gt', 'lt', 'gt', 'pa']\n",
      "After stemming with porters algorithm: ['brother', 'transfer']\n",
      "Tokenized sentence: ['was', 'gr', 'to', 'see', 'that', 'message', 'so', 'when', 'r', 'u', 'leaving', 'congrats', 'dear', 'what', 'school', 'and', 'wat', 'r', 'ur', 'plans']\n",
      "After stop words removal: ['gr', 'see', 'message', 'r', 'u', 'leaving', 'congrats', 'dear', 'school', 'wat', 'r', 'ur', 'plans']\n",
      "leav\n",
      "After stemming with porters algorithm: ['see', 'messag', 'leav', 'congrat', 'dear', 'school', 'wat', 'plan']\n",
      "Tokenized sentence: ['k', 'k', 'when', 'will', 'you', 'give', 'treat']\n",
      "After stop words removal: ['k', 'k', 'give', 'treat']\n",
      "After stemming with porters algorithm: ['give', 'treat']\n",
      "Tokenized sentence: ['was', 'the', 'actual', 'exam', 'harder', 'than', 'nbme']\n",
      "After stop words removal: ['actual', 'exam', 'harder', 'nbme']\n",
      "After stemming with porters algorithm: ['actual', 'exam', 'harder', 'nbme']\n",
      "Tokenized sentence: ['sure', 'i', 'am', 'driving', 'but', 'will', 'reach', 'my', 'destination', 'soon']\n",
      "After stop words removal: ['sure', 'driving', 'reach', 'destination', 'soon']\n",
      "driv\n",
      "After stemming with porters algorithm: ['sure', 'drive', 'reach', 'destin', 'soon']\n",
      "Tokenized sentence: ['are', 'there', 'ta', 'jobs', 'available', 'let', 'me', 'know', 'please', 'cos', 'i', 'really', 'need', 'to', 'start', 'working']\n",
      "After stop words removal: ['ta', 'jobs', 'available', 'let', 'know', 'please', 'cos', 'really', 'need', 'start', 'working']\n",
      "work\n",
      "After stemming with porters algorithm: ['job', 'avail', 'let', 'know', 'pleas', 'co', 'realli', 'need', 'start', 'wor']\n",
      "Tokenized sentence: ['its', 'ok', 'my', 'arm', 'is', 'feeling', 'weak', 'cuz', 'i', 'got', 'a', 'shot', 'so', 'we', 'can', 'go', 'another', 'time']\n",
      "After stop words removal: ['ok', 'arm', 'feeling', 'weak', 'cuz', 'got', 'shot', 'go', 'another', 'time']\n",
      "feel\n",
      "After stemming with porters algorithm: ['arm', 'feel', 'weak', 'cuz', 'got', 'shot', 'anoth', 'time']\n",
      "Tokenized sentence: ['just', 'wondering', 'the', 'others', 'just', 'took', 'off']\n",
      "After stop words removal: ['wondering', 'others', 'took']\n",
      "wonder\n",
      "After stemming with porters algorithm: ['wonder', 'other', 'took']\n",
      "Tokenized sentence: ['hi', 'my', 'darlin', 'im', 'on', 'my', 'way', 'to', 'london', 'and', 'we', 'have', 'just', 'been', 'smashed', 'into', 'by', 'another', 'driver', 'and', 'have', 'a', 'big', 'dent', 'im', 'really', 'missing', 'u', 'what', 'have', 'u', 'been', 'up', 'to', 'xxx']\n",
      "After stop words removal: ['hi', 'darlin', 'im', 'way', 'london', 'smashed', 'another', 'driver', 'big', 'dent', 'im', 'really', 'missing', 'u', 'u', 'xxx']\n",
      "miss\n",
      "After stemming with porters algorithm: ['darlin', 'wai', 'london', 'smas', 'anoth', 'driver', 'big', 'dent', 'realli', 'miss', 'xxx']\n",
      "Tokenized sentence: ['i', 'have', 'printed', 'it', 'oh', 'so', 'lt', 'gt', 'come', 'upstairs']\n",
      "After stop words removal: ['printed', 'oh', 'lt', 'gt', 'come', 'upstairs']\n",
      "After stemming with porters algorithm: ['prin', 'come', 'upstair']\n",
      "Tokenized sentence: ['and', 'pls', 'pls', 'drink', 'plenty', 'plenty', 'water']\n",
      "After stop words removal: ['pls', 'pls', 'drink', 'plenty', 'plenty', 'water']\n",
      "After stemming with porters algorithm: ['pl', 'pl', 'drink', 'plenti', 'plenti', 'water']\n",
      "Tokenized sentence: ['host', 'based', 'idps', 'for', 'linux', 'systems']\n",
      "After stop words removal: ['host', 'based', 'idps', 'linux', 'systems']\n",
      "After stemming with porters algorithm: ['host', 'base', 'idp', 'linux', 'system']\n",
      "Tokenized sentence: ['hey', 'i', 'm', 'bored', 'so', 'i', 'm', 'thinking', 'of', 'u', 'so', 'wat', 'r', 'u', 'doing']\n",
      "After stop words removal: ['hey', 'bored', 'thinking', 'u', 'wat', 'r', 'u']\n",
      "think\n",
      "After stemming with porters algorithm: ['hei', 'bore', 'thin', 'wat']\n",
      "Tokenized sentence: ['yes', 'watching', 'footie', 'but', 'worried', 'we', 're', 'going', 'to', 'blow', 'it', 'phil', 'neville']\n",
      "After stop words removal: ['yes', 'watching', 'footie', 'worried', 'going', 'blow', 'phil', 'neville']\n",
      "watch\n",
      "go\n",
      "After stemming with porters algorithm: ['ye', 'watc', 'footi', 'worri', 'go', 'blow', 'phil', 'nevil']\n",
      "Tokenized sentence: ['ok', 'ok', 'ok', 'then', 'whats', 'ur', 'todays', 'plan']\n",
      "After stop words removal: ['ok', 'ok', 'ok', 'whats', 'ur', 'todays', 'plan']\n",
      "After stemming with porters algorithm: ['what', 'todai', 'plan']\n",
      "Tokenized sentence: ['lol', 'ok', 'ill', 'try', 'to', 'send', 'be', 'warned', 'sprint', 'is', 'dead', 'slow', 'you', 'll', 'prolly', 'get', 'it', 'tomorrow']\n",
      "After stop words removal: ['lol', 'ok', 'ill', 'try', 'send', 'warned', 'sprint', 'dead', 'slow', 'prolly', 'get', 'tomorrow']\n",
      "After stemming with porters algorithm: ['lol', 'ill', 'try', 'send', 'war', 'sprint', 'dead', 'slow', 'prolli', 'get', 'tomorrow']\n",
      "Tokenized sentence: ['oh', 'wow', 'thats', 'gay', 'will', 'firmware', 'update', 'help']\n",
      "After stop words removal: ['oh', 'wow', 'thats', 'gay', 'firmware', 'update', 'help']\n",
      "After stemming with porters algorithm: ['wow', 'that', 'gai', 'firmwar', 'updat', 'help']\n",
      "Tokenized sentence: ['i', 'think', 'i', 'm', 'waiting', 'for', 'the', 'same', 'bus', 'inform', 'me', 'when', 'you', 'get', 'there', 'if', 'you', 'ever', 'get', 'there']\n",
      "After stop words removal: ['think', 'waiting', 'bus', 'inform', 'get', 'ever', 'get']\n",
      "wait\n",
      "After stemming with porters algorithm: ['think', 'wait', 'bu', 'inform', 'get', 'ever', 'get']\n",
      "Tokenized sentence: ['burger', 'king', 'wanna', 'play', 'footy', 'at', 'a', 'top', 'stadium', 'get', 'burger', 'king', 'before', 'st', 'sept', 'and', 'go', 'large', 'or', 'super', 'with', 'coca', 'cola', 'and', 'walk', 'out', 'a', 'winner']\n",
      "After stop words removal: ['burger', 'king', 'wanna', 'play', 'footy', 'top', 'stadium', 'get', 'burger', 'king', 'st', 'sept', 'go', 'large', 'super', 'coca', 'cola', 'walk', 'winner']\n",
      "After stemming with porters algorithm: ['burger', 'king', 'wanna', 'plai', 'footi', 'top', 'stadium', 'get', 'burger', 'king', 'sept', 'larg', 'super', 'coca', 'cola', 'walk', 'winner']\n",
      "Tokenized sentence: ['if', 'india', 'win', 'or', 'level', 'series', 'means', 'this', 'is', 'record']\n",
      "After stop words removal: ['india', 'win', 'level', 'series', 'means', 'record']\n",
      "After stemming with porters algorithm: ['india', 'win', 'level', 'seri', 'mean', 'record']\n",
      "Tokenized sentence: ['freemsg', 'hey', 'there', 'darling', 'it', 's', 'been', 'week', 's', 'now', 'and', 'no', 'word', 'back', 'i', 'd', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'to', 'send', 'to', 'rcv']\n",
      "After stop words removal: ['freemsg', 'hey', 'darling', 'week', 'word', 'back', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'rcv']\n",
      "darl\n",
      "After stemming with porters algorithm: ['freemsg', 'hei', 'darl', 'week', 'word', 'back', 'like', 'fun', 'still', 'xxx', 'std', 'chg', 'send', 'rcv']\n",
      "Tokenized sentence: ['babe', 'i', 'm', 'answering', 'you', 'can', 't', 'you', 'see', 'me', 'maybe', 'you', 'd', 'better', 'reboot', 'ym', 'i', 'got', 'the', 'photo', 'it', 's', 'great']\n",
      "After stop words removal: ['babe', 'answering', 'see', 'maybe', 'better', 'reboot', 'ym', 'got', 'photo', 'great']\n",
      "answer\n",
      "After stemming with porters algorithm: ['babe', 'answer', 'see', 'mayb', 'better', 'reboot', 'got', 'photo', 'great']\n",
      "Tokenized sentence: ['under', 'the', 'sea', 'there', 'lays', 'a', 'rock', 'in', 'the', 'rock', 'there', 'is', 'an', 'envelope', 'in', 'the', 'envelope', 'there', 'is', 'a', 'paper', 'on', 'the', 'paper', 'there', 'are', 'words']\n",
      "After stop words removal: ['sea', 'lays', 'rock', 'rock', 'envelope', 'envelope', 'paper', 'paper', 'words']\n",
      "After stemming with porters algorithm: ['sea', 'lai', 'rock', 'rock', 'envelop', 'envelop', 'paper', 'paper', 'word']\n",
      "Tokenized sentence: ['fuck', 'babe', 'what', 'happened', 'to', 'you', 'how', 'come', 'you', 'never', 'came', 'back']\n",
      "After stop words removal: ['fuck', 'babe', 'happened', 'come', 'never', 'came', 'back']\n",
      "After stemming with porters algorithm: ['fuck', 'babe', 'happen', 'come', 'never', 'came', 'back']\n",
      "Tokenized sentence: ['thanx']\n",
      "After stop words removal: ['thanx']\n",
      "After stemming with porters algorithm: ['thanx']\n",
      "Tokenized sentence: ['well', 'im', 'computerless', 'time', 'to', 'make', 'some', 'oreo', 'truffles']\n",
      "After stop words removal: ['well', 'im', 'computerless', 'time', 'make', 'oreo', 'truffles']\n",
      "After stemming with porters algorithm: ['well', 'computerless', 'time', 'make', 'oreo', 'truffl']\n",
      "Tokenized sentence: ['today', 'i', 'm', 'not', 'workin', 'but', 'not', 'free', 'oso', 'gee', 'thgt', 'u', 'workin', 'at', 'ur', 'fren', 's', 'shop']\n",
      "After stop words removal: ['today', 'workin', 'free', 'oso', 'gee', 'thgt', 'u', 'workin', 'ur', 'fren', 'shop']\n",
      "After stemming with porters algorithm: ['todai', 'workin', 'free', 'oso', 'gee', 'thgt', 'workin', 'fren', 'shop']\n",
      "Tokenized sentence: ['sir', 'waiting', 'for', 'your', 'mail']\n",
      "After stop words removal: ['sir', 'waiting', 'mail']\n",
      "wait\n",
      "After stemming with porters algorithm: ['sir', 'wait', 'mail']\n",
      "Tokenized sentence: ['in', 'the', 'end', 'she', 'might', 'still', 'vomit', 'but', 'its', 'okay', 'not', 'everything', 'will', 'come', 'out']\n",
      "After stop words removal: ['end', 'might', 'still', 'vomit', 'okay', 'everything', 'come']\n",
      "everyth\n",
      "After stemming with porters algorithm: ['end', 'might', 'still', 'vomit', 'okai', 'everyt', 'come']\n",
      "Tokenized sentence: ['you', 'available', 'now', 'i', 'm', 'like', 'right', 'around', 'hillsborough', 'amp', 'lt', 'gt', 'th']\n",
      "After stop words removal: ['available', 'like', 'right', 'around', 'hillsborough', 'amp', 'lt', 'gt', 'th']\n",
      "After stemming with porters algorithm: ['avail', 'like', 'right', 'around', 'hillsborough', 'amp']\n",
      "Tokenized sentence: ['can', 'you', 'just', 'come', 'in', 'for', 'a', 'sec', 'there', 's', 'somebody', 'here', 'i', 'want', 'you', 'to', 'see']\n",
      "After stop words removal: ['come', 'sec', 'somebody', 'want', 'see']\n",
      "After stemming with porters algorithm: ['come', 'sec', 'somebodi', 'want', 'see']\n",
      "Tokenized sentence: ['you', 'have', 'won', 'cash', 'or', 'a', 'prize', 'to', 'claim', 'call', 't', 'c', 'rstm', 'sw', 'ss', 'ppm']\n",
      "After stop words removal: ['cash', 'prize', 'claim', 'call', 'c', 'rstm', 'sw', 'ss', 'ppm']\n",
      "After stemming with porters algorithm: ['cash', 'priz', 'claim', 'call', 'rstm', 'ppm']\n",
      "Tokenized sentence: ['ooooooh', 'i', 'forgot', 'to', 'tell', 'u', 'i', 'can', 'get', 'on', 'yoville', 'on', 'my', 'phone']\n",
      "After stop words removal: ['ooooooh', 'forgot', 'tell', 'u', 'get', 'yoville', 'phone']\n",
      "After stemming with porters algorithm: ['ooooooh', 'forgot', 'tell', 'get', 'yovil', 'phone']\n",
      "Tokenized sentence: ['lmao', 'nice']\n",
      "After stop words removal: ['lmao', 'nice']\n",
      "After stemming with porters algorithm: ['lmao', 'nice']\n",
      "Tokenized sentence: ['but', 'i', 'll', 'b', 'going', 'sch', 'on', 'mon', 'my', 'sis', 'need', 'take', 'smth']\n",
      "After stop words removal: ['b', 'going', 'sch', 'mon', 'sis', 'need', 'take', 'smth']\n",
      "go\n",
      "After stemming with porters algorithm: ['go', 'sch', 'mon', 'si', 'need', 'take', 'smth']\n",
      "Tokenized sentence: ['do', 'you', 'hide', 'anythiing', 'or', 'keeping', 'distance', 'from', 'me']\n",
      "After stop words removal: ['hide', 'anythiing', 'keeping', 'distance']\n",
      "anythi\n",
      "keep\n",
      "After stemming with porters algorithm: ['hide', 'anythi', 'keep', 'distanc']\n",
      "Tokenized sentence: ['talk', 'to', 'g', 'and', 'x', 'about', 'that']\n",
      "After stop words removal: ['talk', 'g', 'x']\n",
      "After stemming with porters algorithm: ['talk']\n",
      "Tokenized sentence: ['k', 'do', 'it', 'at', 'evening', 'da', 'urgent']\n",
      "After stop words removal: ['k', 'evening', 'da', 'urgent']\n",
      "even\n",
      "After stemming with porters algorithm: ['even', 'urgent']\n",
      "Tokenized sentence: ['okie', 'but', 'i', 'scared', 'u', 'say', 'i', 'fat', 'then', 'u', 'dun', 'wan', 'me', 'already']\n",
      "After stop words removal: ['okie', 'scared', 'u', 'say', 'fat', 'u', 'dun', 'wan', 'already']\n",
      "After stemming with porters algorithm: ['oki', 'scare', 'sai', 'fat', 'dun', 'wan', 'alreadi']\n",
      "Tokenized sentence: ['urgent', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'contact', 'u', 'u', 'have', 'won', 'call', 'b', 't', 'csbcm', 'wc', 'n', 'xx', 'callcost', 'ppmmobilesvary', 'max']\n",
      "After stop words removal: ['urgent', 'nd', 'attempt', 'contact', 'u', 'u', 'call', 'b', 'csbcm', 'wc', 'n', 'xx', 'callcost', 'ppmmobilesvary', 'max']\n",
      "After stemming with porters algorithm: ['urgent', 'attempt', 'contact', 'call', 'csbcm', 'callcost', 'ppmmobilesvari', 'max']\n",
      "Tokenized sentence: ['edison', 'has', 'rightly', 'said', 'a', 'fool', 'can', 'ask', 'more', 'questions', 'than', 'a', 'wise', 'man', 'can', 'answer', 'now', 'you', 'know', 'why', 'all', 'of', 'us', 'are', 'speechless', 'during', 'viva', 'gm']\n",
      "After stop words removal: ['edison', 'rightly', 'said', 'fool', 'ask', 'questions', 'wise', 'man', 'answer', 'know', 'us', 'speechless', 'viva', 'gm']\n",
      "After stemming with porters algorithm: ['edison', 'rightli', 'said', 'fool', 'ask', 'quest', 'wise', 'man', 'answer', 'know', 'speechless', 'viva']\n",
      "Tokenized sentence: ['yes', 'i', 'am', 'a', 'one', 'woman', 'man', 'please', 'tell', 'me', 'your', 'likes', 'and', 'dislikes', 'in', 'bed']\n",
      "After stop words removal: ['yes', 'one', 'woman', 'man', 'please', 'tell', 'likes', 'dislikes', 'bed']\n",
      "After stemming with porters algorithm: ['ye', 'on', 'woman', 'man', 'pleas', 'tell', 'like', 'dislik', 'bed']\n",
      "Tokenized sentence: ['yeah', 'go', 'on', 'then', 'bored', 'and', 'depressed', 'sittin', 'waitin', 'for', 'phone', 'to', 'ring', 'hope', 'the', 'wind', 'drops', 'though', 'scary']\n",
      "After stop words removal: ['yeah', 'go', 'bored', 'depressed', 'sittin', 'waitin', 'phone', 'ring', 'hope', 'wind', 'drops', 'though', 'scary']\n",
      "After stemming with porters algorithm: ['yeah', 'bore', 'depress', 'sittin', 'waitin', 'phone', 'ring', 'hope', 'wind', 'drop', 'though', 'scari']\n",
      "Tokenized sentence: ['k', 'i', 'll', 'be', 'sure', 'to', 'get', 'up', 'before', 'noon', 'and', 'see', 'what', 's', 'what']\n",
      "After stop words removal: ['k', 'sure', 'get', 'noon', 'see']\n",
      "After stemming with porters algorithm: ['sure', 'get', 'noon', 'see']\n",
      "Tokenized sentence: ['i', 'have', 'sleeping', 'bags', 'blanket', 'and', 'paper', 'and', 'phone', 'details', 'anything', 'else']\n",
      "After stop words removal: ['sleeping', 'bags', 'blanket', 'paper', 'phone', 'details', 'anything', 'else']\n",
      "sleep\n",
      "anyth\n",
      "After stemming with porters algorithm: ['sleep', 'bag', 'blanket', 'paper', 'phone', 'detail', 'anyt', 'els']\n",
      "Tokenized sentence: ['block', 'breaker', 'now', 'comes', 'in', 'deluxe', 'format', 'with', 'new', 'features', 'and', 'great', 'graphics', 'from', 't', 'mobile', 'buy', 'for', 'just', 'by', 'replying', 'get', 'bbdeluxe', 'and', 'take', 'the', 'challenge']\n",
      "After stop words removal: ['block', 'breaker', 'comes', 'deluxe', 'format', 'new', 'features', 'great', 'graphics', 'mobile', 'buy', 'replying', 'get', 'bbdeluxe', 'take', 'challenge']\n",
      "reply\n",
      "After stemming with porters algorithm: ['block', 'breaker', 'come', 'delux', 'format', 'new', 'featur', 'great', 'graphic', 'mobil', 'bui', 'repl', 'get', 'bbdelux', 'take', 'challeng']\n",
      "Tokenized sentence: ['with', 'my', 'sis', 'lor', 'we', 'juz', 'watched', 'italian', 'job']\n",
      "After stop words removal: ['sis', 'lor', 'juz', 'watched', 'italian', 'job']\n",
      "After stemming with porters algorithm: ['si', 'lor', 'juz', 'watc', 'italian', 'job']\n",
      "Tokenized sentence: ['neva', 'mind', 'it', 's', 'ok']\n",
      "After stop words removal: ['neva', 'mind', 'ok']\n",
      "After stemming with porters algorithm: ['neva', 'mind']\n",
      "Tokenized sentence: ['well', 'done', 'england', 'get', 'the', 'official', 'poly', 'ringtone', 'or', 'colour', 'flag', 'on', 'yer', 'mobile', 'text', 'tone', 'or', 'flag', 'to', 'now', 'opt', 'out', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stop words removal: ['well', 'done', 'england', 'get', 'official', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'text', 'tone', 'flag', 'opt', 'txt', 'eng', 'stop', 'box', 'w', 'wx']\n",
      "After stemming with porters algorithm: ['well', 'done', 'england', 'get', 'offici', 'poli', 'rington', 'colour', 'flag', 'yer', 'mobil', 'text', 'tone', 'flag', 'opt', 'txt', 'eng', 'stop', 'box']\n",
      "Tokenized sentence: ['ok', 'i', 'm', 'coming', 'home', 'now']\n",
      "After stop words removal: ['ok', 'coming', 'home']\n",
      "com\n",
      "After stemming with porters algorithm: ['come', 'home']\n",
      "Tokenized sentence: ['hello', 'darling', 'how', 'are', 'you', 'today', 'i', 'would', 'love', 'to', 'have', 'a', 'chat', 'why', 'dont', 'you', 'tell', 'me', 'what', 'you', 'look', 'like', 'and', 'what', 'you', 'are', 'in', 'to', 'sexy']\n",
      "After stop words removal: ['hello', 'darling', 'today', 'would', 'love', 'chat', 'dont', 'tell', 'look', 'like', 'sexy']\n",
      "darl\n",
      "After stemming with porters algorithm: ['hello', 'darl', 'todai', 'would', 'love', 'chat', 'dont', 'tell', 'look', 'like', 'sexi']\n",
      "Tokenized sentence: ['i', 'dont', 'want', 'to', 'hear', 'anything']\n",
      "After stop words removal: ['dont', 'want', 'hear', 'anything']\n",
      "anyth\n",
      "After stemming with porters algorithm: ['dont', 'want', 'hear', 'anyt']\n",
      "Tokenized sentence: ['k', 'k', 'this', 'month', 'kotees', 'birthday', 'know']\n",
      "After stop words removal: ['k', 'k', 'month', 'kotees', 'birthday', 'know']\n",
      "After stemming with porters algorithm: ['month', 'kote', 'birthdai', 'know']\n",
      "Tokenized sentence: ['hey', 'i', 've', 'booked', 'the', 'lessons', 'on', 'sun', 'liao']\n",
      "After stop words removal: ['hey', 'booked', 'lessons', 'sun', 'liao']\n",
      "After stemming with porters algorithm: ['hei', 'book', 'lesson', 'sun', 'liao']\n",
      "Tokenized sentence: ['just', 'sleeping', 'and', 'surfing']\n",
      "After stop words removal: ['sleeping', 'surfing']\n",
      "sleep\n",
      "surf\n",
      "After stemming with porters algorithm: ['sleep', 'sur']\n",
      "Tokenized sentence: ['photoshop', 'makes', 'my', 'computer', 'shut', 'down']\n",
      "After stop words removal: ['photoshop', 'makes', 'computer', 'shut']\n",
      "After stemming with porters algorithm: ['photoshop', 'make', 'comput', 'shut']\n",
      "Tokenized sentence: ['sorry', 'da', 'thangam', 'very', 'very', 'sorry', 'i', 'am', 'held', 'up', 'with', 'prasad']\n",
      "After stop words removal: ['sorry', 'da', 'thangam', 'sorry', 'held', 'prasad']\n",
      "After stemming with porters algorithm: ['sorri', 'thangam', 'sorri', 'held', 'prasad']\n",
      "Tokenized sentence: ['what', 's', 'up', 'bruv', 'hope', 'you', 'had', 'a', 'great', 'break', 'do', 'have', 'a', 'rewarding', 'semester']\n",
      "After stop words removal: ['bruv', 'hope', 'great', 'break', 'rewarding', 'semester']\n",
      "reward\n",
      "After stemming with porters algorithm: ['bruv', 'hope', 'great', 'break', 'rewar', 'semest']\n",
      "Tokenized sentence: ['i', 'll', 'always', 'be', 'there', 'even', 'if', 'its', 'just', 'in', 'spirit', 'i', 'll', 'get', 'a', 'bb', 'soon', 'just', 'trying', 'to', 'be', 'sure', 'i', 'need', 'it']\n",
      "After stop words removal: ['always', 'even', 'spirit', 'get', 'bb', 'soon', 'trying', 'sure', 'need']\n",
      "After stemming with porters algorithm: ['alwai', 'even', 'spirit', 'get', 'soon', 'trying', 'sure', 'need']\n",
      "Tokenized sentence: ['they', 'released', 'vday', 'shirts', 'and', 'when', 'u', 'put', 'it', 'on', 'it', 'makes', 'your', 'bottom', 'half', 'naked', 'instead', 'of', 'those', 'white', 'underwear']\n",
      "After stop words removal: ['released', 'vday', 'shirts', 'u', 'put', 'makes', 'bottom', 'half', 'naked', 'instead', 'white', 'underwear']\n",
      "After stemming with porters algorithm: ['releas', 'vdai', 'shirt', 'put', 'make', 'bottom', 'half', 'nake', 'instead', 'white', 'underwear']\n",
      "Tokenized sentence: ['mro', 'i', 'am', 'not', 'coming', 'to', 'gym', 'machan', 'goodnight']\n",
      "After stop words removal: ['mro', 'coming', 'gym', 'machan', 'goodnight']\n",
      "com\n",
      "After stemming with porters algorithm: ['mro', 'come', 'gym', 'machan', 'goodnight']\n",
      "Tokenized sentence: ['i', 'can', 't', 'i', 'don', 't', 'have', 'her', 'number']\n",
      "After stop words removal: ['number']\n",
      "After stemming with porters algorithm: ['number']\n",
      "Tokenized sentence: ['spoons', 'it', 'is', 'then', 'okay']\n",
      "After stop words removal: ['spoons', 'okay']\n",
      "After stemming with porters algorithm: ['spoon', 'okai']\n",
      "Tokenized sentence: ['i', 'wud', 'never', 'mind', 'if', 'u', 'dont', 'miss', 'me', 'or', 'if', 'u', 'dont', 'need', 'me', 'but', 'u', 'wil', 'really', 'hurt', 'me', 'wen', 'u', 'need', 'me', 'amp', 'u', 'dont', 'tell', 'me', 'take', 'care']\n",
      "After stop words removal: ['wud', 'never', 'mind', 'u', 'dont', 'miss', 'u', 'dont', 'need', 'u', 'wil', 'really', 'hurt', 'wen', 'u', 'need', 'amp', 'u', 'dont', 'tell', 'take', 'care']\n",
      "After stemming with porters algorithm: ['wud', 'never', 'mind', 'dont', 'miss', 'dont', 'need', 'wil', 'realli', 'hurt', 'wen', 'need', 'amp', 'dont', 'tell', 'take', 'care']\n",
      "Tokenized sentence: ['nights', 'we', 'nt', 'staying', 'at', 'port', 'step', 'liao', 'too', 'ex']\n",
      "After stop words removal: ['nights', 'nt', 'staying', 'port', 'step', 'liao', 'ex']\n",
      "stay\n",
      "After stemming with porters algorithm: ['night', 'stai', 'port', 'step', 'liao']\n",
      "Tokenized sentence: ['thanks', 'chikku', 'gud', 'nyt']\n",
      "After stop words removal: ['thanks', 'chikku', 'gud', 'nyt']\n",
      "After stemming with porters algorithm: ['thank', 'chikku', 'gud', 'nyt']\n",
      "Tokenized sentence: ['aww', 'you', 'must', 'be', 'nearly', 'dead', 'well', 'jez', 'iscoming', 'over', 'todo', 'some', 'workand', 'that', 'whilltake', 'forever']\n",
      "After stop words removal: ['aww', 'must', 'nearly', 'dead', 'well', 'jez', 'iscoming', 'todo', 'workand', 'whilltake', 'forever']\n",
      "iscom\n",
      "After stemming with porters algorithm: ['aww', 'must', 'nearli', 'dead', 'well', 'jez', 'iscom', 'todo', 'workand', 'whilltak', 'forev']\n",
      "Tokenized sentence: ['water', 'logging', 'in', 'desert', 'geoenvironmental', 'implications']\n",
      "After stop words removal: ['water', 'logging', 'desert', 'geoenvironmental', 'implications']\n",
      "logg\n",
      "After stemming with porters algorithm: ['water', 'log', 'desert', 'geoenviron', 'implic']\n",
      "Tokenized sentence: ['today', 'is', 'accept', 'day', 'u', 'accept', 'me', 'as', 'brother', 'sister', 'lover', 'dear', 'best', 'clos', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'no', 'rply', 'means', 'enemy']\n",
      "After stop words removal: ['today', 'accept', 'day', 'u', 'accept', 'brother', 'sister', 'lover', 'dear', 'best', 'clos', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'rply', 'means', 'enemy']\n",
      "After stemming with porters algorithm: ['todai', 'accept', 'dai', 'accept', 'brother', 'sister', 'lover', 'dear', 'best', 'clo', 'lvblefrnd', 'jstfrnd', 'cutefrnd', 'lifpartnr', 'belovd', 'swtheart', 'bstfrnd', 'rply', 'mean', 'enemi']\n",
      "Tokenized sentence: ['im', 'in', 'inperialmusic', 'listening', 'the', 'weirdest', 'track', 'ever', 'by', 'leafcutter', 'john', 'sounds', 'like', 'insects', 'being', 'molested', 'someone', 'plumbing', 'remixed', 'by', 'evil', 'men', 'on', 'acid']\n",
      "After stop words removal: ['im', 'inperialmusic', 'listening', 'weirdest', 'track', 'ever', 'leafcutter', 'john', 'sounds', 'like', 'insects', 'molested', 'someone', 'plumbing', 'remixed', 'evil', 'men', 'acid']\n",
      "listen\n",
      "plumb\n",
      "After stemming with porters algorithm: ['inperialmus', 'listen', 'weirdest', 'track', 'ever', 'leafcutt', 'john', 'sound', 'like', 'insect', 'moles', 'someon', 'plum', 'remix', 'evil', 'men', 'acid']\n",
      "Tokenized sentence: ['sorry', 'i', 'now', 'then', 'c', 'ur', 'msg', 'yar', 'lor', 'so', 'poor', 'thing', 'but', 'only', 'one', 'night', 'tmr', 'u', 'll', 'have', 'a', 'brand', 'new', 'room', 'sleep', 'in']\n",
      "After stop words removal: ['sorry', 'c', 'ur', 'msg', 'yar', 'lor', 'poor', 'thing', 'one', 'night', 'tmr', 'u', 'brand', 'new', 'room', 'sleep']\n",
      "After stemming with porters algorithm: ['sorri', 'msg', 'yar', 'lor', 'poor', 'thing', 'on', 'night', 'tmr', 'brand', 'new', 'room', 'sleep']\n",
      "Tokenized sentence: ['refused', 'a', 'loan', 'secured', 'or', 'unsecured', 'can', 't', 'get', 'credit', 'call', 'free', 'now', 'or', 'text', 'back', 'help', 'we', 'will']\n",
      "After stop words removal: ['refused', 'loan', 'secured', 'unsecured', 'get', 'credit', 'call', 'free', 'text', 'back', 'help']\n",
      "After stemming with porters algorithm: ['refus', 'loan', 'secur', 'unsecur', 'get', 'credit', 'call', 'free', 'text', 'back', 'help']\n",
      "Tokenized sentence: ['i', 'can', 't', 'wait', 'for', 'cornwall', 'hope', 'tonight', 'isn', 't', 'too', 'bad', 'as', 'well', 'but', 'it', 's', 'rock', 'night', 'shite', 'anyway', 'i', 'm', 'going', 'for', 'a', 'kip', 'now', 'have', 'a', 'good', 'night', 'speak', 'to', 'you', 'soon']\n",
      "After stop words removal: ['wait', 'cornwall', 'hope', 'tonight', 'bad', 'well', 'rock', 'night', 'shite', 'anyway', 'going', 'kip', 'good', 'night', 'speak', 'soon']\n",
      "go\n",
      "After stemming with porters algorithm: ['wait', 'cornwal', 'hope', 'tonight', 'bad', 'well', 'rock', 'night', 'shite', 'anywai', 'go', 'kip', 'good', 'night', 'speak', 'soon']\n",
      "Tokenized sentence: ['ya', 'told', 'she', 'was', 'asking', 'wats', 'matter']\n",
      "After stop words removal: ['ya', 'told', 'asking', 'wats', 'matter']\n",
      "ask\n",
      "After stemming with porters algorithm: ['told', 'as', 'wat', 'matter']\n",
      "Tokenized sentence: ['valentines', 'day', 'special', 'win', 'over', 'in', 'our', 'quiz', 'and', 'take', 'your', 'partner', 'on', 'the', 'trip', 'of', 'a', 'lifetime', 'send', 'go', 'to', 'now', 'p', 'msg', 'rcvd', 'custcare']\n",
      "After stop words removal: ['valentines', 'day', 'special', 'win', 'quiz', 'take', 'partner', 'trip', 'lifetime', 'send', 'go', 'p', 'msg', 'rcvd', 'custcare']\n",
      "After stemming with porters algorithm: ['valentin', 'dai', 'special', 'win', 'quiz', 'take', 'partner', 'trip', 'lifetim', 'send', 'msg', 'rcvd', 'custcar']\n",
      "Tokenized sentence: ['a', 'xmas', 'reward', 'is', 'waiting', 'for', 'you', 'our', 'computer', 'has', 'randomly', 'picked', 'you', 'from', 'our', 'loyal', 'mobile', 'customers', 'to', 'receive', 'a', 'reward', 'just', 'call']\n",
      "After stop words removal: ['xmas', 'reward', 'waiting', 'computer', 'randomly', 'picked', 'loyal', 'mobile', 'customers', 'receive', 'reward', 'call']\n",
      "wait\n",
      "After stemming with porters algorithm: ['xma', 'reward', 'wait', 'comput', 'randomli', 'pic', 'loyal', 'mobil', 'custom', 'receiv', 'reward', 'call']\n",
      "Tokenized sentence: ['your', 'weekly', 'cool', 'mob', 'tones', 'are', 'ready', 'to', 'download', 'this', 'weeks', 'new', 'tones', 'include', 'crazy', 'frog', 'axel', 'f', 'akon', 'lonely', 'black', 'eyed', 'dont', 'p', 'more', 'info', 'in', 'n']\n",
      "After stop words removal: ['weekly', 'cool', 'mob', 'tones', 'ready', 'download', 'weeks', 'new', 'tones', 'include', 'crazy', 'frog', 'axel', 'f', 'akon', 'lonely', 'black', 'eyed', 'dont', 'p', 'info', 'n']\n",
      "After stemming with porters algorithm: ['weekli', 'cool', 'mob', 'tone', 'readi', 'download', 'week', 'new', 'tone', 'includ', 'crazi', 'frog', 'axel', 'akon', 'lone', 'black', 'ei', 'dont', 'info']\n",
      "Tokenized sentence: ['cmon', 'babe', 'make', 'me', 'horny', 'turn', 'me', 'on', 'txt', 'me', 'your', 'fantasy', 'now', 'babe', 'im', 'hot', 'sticky', 'and', 'need', 'you', 'now', 'all', 'replies', 'cost', 'cancel', 'send', 'stop']\n",
      "After stop words removal: ['cmon', 'babe', 'make', 'horny', 'turn', 'txt', 'fantasy', 'babe', 'im', 'hot', 'sticky', 'need', 'replies', 'cost', 'cancel', 'send', 'stop']\n",
      "After stemming with porters algorithm: ['cmon', 'babe', 'make', 'horni', 'turn', 'txt', 'fantasi', 'babe', 'hot', 'sticki', 'need', 'repli', 'cost', 'cancel', 'send', 'stop']\n",
      "Tokenized sentence: ['sorry', 'i', 'll', 'call', 'later']\n",
      "After stop words removal: ['sorry', 'call', 'later']\n",
      "After stemming with porters algorithm: ['sorri', 'call', 'later']\n"
     ]
    }
   ],
   "source": [
    "df[\"transformed_tweets\"] = df[\"tweets\"].apply(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweets</th>\n",
       "      <th>transformed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank u!</td>\n",
       "      <td>[thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Going for dinner.msg you after.</td>\n",
       "      <td>[go, dinner, msg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi i won't b ard 4 christmas. But do enjoy n m...</td>\n",
       "      <td>[ard, christma, enjoi, merri, ma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1. Tension face 2. Smiling face 3. Waste face ...</td>\n",
       "      <td>[tension, face, smile, face, wast, face, innoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1 I don't have her number and 2 its gonna be a...</td>\n",
       "      <td>[number, gonna, massiv, pain, ass, rather, get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                             tweets  \\\n",
       "0      0                                           Thank u!   \n",
       "1      0                    Going for dinner.msg you after.   \n",
       "2      0  Hi i won't b ard 4 christmas. But do enjoy n m...   \n",
       "3      0  1. Tension face 2. Smiling face 3. Waste face ...   \n",
       "4      0  1 I don't have her number and 2 its gonna be a...   \n",
       "\n",
       "                                  transformed_tweets  \n",
       "0                                            [thank]  \n",
       "1                                  [go, dinner, msg]  \n",
       "2                  [ard, christma, enjoi, merri, ma]  \n",
       "3  [tension, face, smile, face, wast, face, innoc...  \n",
       "4  [number, gonna, massiv, pain, ass, rather, get...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (5402, 2000) (5402,)\n",
      "Validation Set Shape: (2316, 2000) (2316,)\n",
      "Test Set Shape: (1115, 2000) (1115,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model, Vectorizer, Training History, Test Results, and Confusion Matrix Saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
    "\n",
    "# Ensure results directory exists\n",
    "os.makedirs(\"model_results\", exist_ok=True)\n",
    "\n",
    "# Prepare data\n",
    "X = df[\"transformed_tweets\"].apply(lambda words: \" \".join(words))  # Convert list to space-separated string\n",
    "y = df[\"class\"]\n",
    "\n",
    "# Convert text to TF-IDF features (better for SVM)\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X_tfidf = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Split data into training (80%) and test (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_tfidf, np.array(y), test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE to balance classes in training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_val_sm, y_train_val_sm = smote.fit_resample(X_train_val, y_train_val)\n",
    "\n",
    "# Split training data into training (70%) and validation (30%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val_sm, y_train_val_sm, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Check the shapes of X and y\n",
    "print(\"Training Set Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation Set Shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test Set Shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Create and train SVM model\n",
    "svm_model = SVC(kernel=\"linear\", probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Compute log loss (since we used probability=True)\n",
    "train_loss = log_loss(y_train, svm_model.predict_proba(X_train))\n",
    "val_loss = log_loss(y_val, svm_model.predict_proba(X_val))\n",
    "test_loss = log_loss(y_test, svm_model.predict_proba(X_test))\n",
    "\n",
    "# Store training history\n",
    "history_df = pd.DataFrame({\n",
    "    \"epoch\": range(1, 2),  # Since SVM is not iterative, we store one entry\n",
    "    \"train_loss\": [train_loss],\n",
    "    \"train_accuracy\": [train_accuracy],\n",
    "    \"val_loss\": [val_loss],\n",
    "    \"val_accuracy\": [val_accuracy]\n",
    "})\n",
    "\n",
    "history_df.to_csv(\"model_results/training_history.csv\", index=False)\n",
    "\n",
    "# Save test results\n",
    "test_results_df = pd.DataFrame({\"test_loss\": [test_loss], \"test_accuracy\": [test_accuracy]})\n",
    "test_results_df.to_csv(\"model_results/test_results.csv\", index=False)\n",
    "\n",
    "# Save the SVM model using pickle\n",
    "with open(\"model_results/svm_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "\n",
    "# Save the vectorizer using pickle\n",
    "with open(\"model_results/vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"model_results/confusion_matrix.png\")  # Save the confusion matrix as an image\n",
    "plt.show()\n",
    "\n",
    "print(\"SVM Model, Vectorizer, Training History, Test Results, and Confusion Matrix Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and label encoder saving\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the stored model and vectorizer\n",
    "with open(\"model_results/svm_model.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "with open(\"model_results/vectorizer.pkl\", \"rb\") as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "\n",
    "def get_predicted_class(sentence):\n",
    "    \"\"\"\n",
    "    Predicts the class of a given sentence using the trained SVM model.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence to classify.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted class (0 or 1).\n",
    "    \"\"\"\n",
    "    # Transform the input sentence using the stored vectorizer\n",
    "    transformed_input = loaded_vectorizer.transform([sentence]).toarray()\n",
    "    # Predict class\n",
    "    prediction = loaded_model.predict(transformed_input)\n",
    "    print(f\"Predicted Class: {prediction[0]}\")\n",
    "    return prediction[0]\n",
    "\n",
    "# Example usage:\n",
    "# get_predicted_class(\"Your text message here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enou..\" \n",
    "k = get_predicted_class(sentence) \n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = pd.read_csv(\"data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class                                                         1\n",
       "tweets        U can WIN �100 of Music Gift Vouchers every we...\n",
       "Unnamed: 2                                                  NaN\n",
       "Unnamed: 3                                                  NaN\n",
       "Unnamed: 4                                                  NaN\n",
       "Name: 41, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp[tp[\"class\"] == 0].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
